[07:04:03.479] TRAIN: iteration 1 : loss : 0.543003, loss_ce: 0.526484, loss_dice: 0.559521
[07:04:03.698] TRAIN: iteration 2 : loss : 0.521477, loss_ce: 0.488264, loss_dice: 0.554690
[07:04:04.952] TRAIN: iteration 3 : loss : 0.483123, loss_ce: 0.426343, loss_dice: 0.539902
[07:04:05.157] TRAIN: iteration 4 : loss : 0.441039, loss_ce: 0.353781, loss_dice: 0.528297
[07:04:05.363] TRAIN: iteration 5 : loss : 0.402073, loss_ce: 0.283673, loss_dice: 0.520472
[07:04:05.568] TRAIN: iteration 6 : loss : 0.366052, loss_ce: 0.222097, loss_dice: 0.510007
[07:04:05.776] TRAIN: iteration 7 : loss : 0.339707, loss_ce: 0.171666, loss_dice: 0.507748
[07:04:05.981] TRAIN: iteration 8 : loss : 0.317435, loss_ce: 0.137199, loss_dice: 0.497671
[07:04:14.453] TRAIN: iteration 9 : loss : 0.300080, loss_ce: 0.107173, loss_dice: 0.492988
[07:04:15.974] TRAIN: iteration 10 : loss : 0.288971, loss_ce: 0.083537, loss_dice: 0.494405
[07:04:18.492] TRAIN: iteration 11 : loss : 0.280517, loss_ce: 0.065390, loss_dice: 0.495644
[07:04:18.700] TRAIN: iteration 12 : loss : 0.273574, loss_ce: 0.056534, loss_dice: 0.490615
[07:04:18.906] TRAIN: iteration 13 : loss : 0.268110, loss_ce: 0.043165, loss_dice: 0.493056
[07:04:19.114] TRAIN: iteration 14 : loss : 0.267158, loss_ce: 0.034689, loss_dice: 0.499628
[07:04:19.320] TRAIN: iteration 15 : loss : 0.264178, loss_ce: 0.027942, loss_dice: 0.500413
[07:04:19.527] TRAIN: iteration 16 : loss : 0.260363, loss_ce: 0.024708, loss_dice: 0.496019
[07:04:28.774] TRAIN: iteration 17 : loss : 0.257333, loss_ce: 0.024185, loss_dice: 0.490481
[07:04:32.660] TRAIN: iteration 18 : loss : 0.258992, loss_ce: 0.027194, loss_dice: 0.490790
[07:04:33.711] TRAIN: iteration 19 : loss : 0.270697, loss_ce: 0.052092, loss_dice: 0.489303
[07:04:33.919] TRAIN: iteration 20 : loss : 0.272720, loss_ce: 0.056956, loss_dice: 0.488484
[07:04:34.191] TRAIN: iteration 21 : loss : 0.255827, loss_ce: 0.011567, loss_dice: 0.500087
[07:04:34.396] TRAIN: iteration 22 : loss : 0.252599, loss_ce: 0.011856, loss_dice: 0.493341
[07:04:34.601] TRAIN: iteration 23 : loss : 0.253936, loss_ce: 0.011888, loss_dice: 0.495984
[07:04:34.806] TRAIN: iteration 24 : loss : 0.254416, loss_ce: 0.008781, loss_dice: 0.500052
[07:04:42.382] TRAIN: iteration 25 : loss : 0.256252, loss_ce: 0.018656, loss_dice: 0.493848
[07:04:45.883] TRAIN: iteration 26 : loss : 0.251148, loss_ce: 0.010716, loss_dice: 0.491579
[07:04:47.498] TRAIN: iteration 27 : loss : 0.268422, loss_ce: 0.041415, loss_dice: 0.495430
[07:04:47.704] TRAIN: iteration 28 : loss : 0.255113, loss_ce: 0.015994, loss_dice: 0.494233
[07:04:47.909] TRAIN: iteration 29 : loss : 0.271513, loss_ce: 0.046710, loss_dice: 0.496315
[07:04:48.115] TRAIN: iteration 30 : loss : 0.252537, loss_ce: 0.010948, loss_dice: 0.494126
[07:04:48.321] TRAIN: iteration 31 : loss : 0.251400, loss_ce: 0.009528, loss_dice: 0.493272
[07:04:48.527] TRAIN: iteration 32 : loss : 0.247859, loss_ce: 0.005853, loss_dice: 0.489864
[07:04:55.483] TRAIN: iteration 33 : loss : 0.253252, loss_ce: 0.015311, loss_dice: 0.491192
[07:04:58.534] TRAIN: iteration 34 : loss : 0.257735, loss_ce: 0.020425, loss_dice: 0.495045
[07:05:01.085] TRAIN: iteration 35 : loss : 0.253795, loss_ce: 0.010266, loss_dice: 0.497324
[07:05:01.293] TRAIN: iteration 36 : loss : 0.248444, loss_ce: 0.005663, loss_dice: 0.491224
[07:05:01.498] TRAIN: iteration 37 : loss : 0.258687, loss_ce: 0.023150, loss_dice: 0.494224
[07:05:01.704] TRAIN: iteration 38 : loss : 0.258520, loss_ce: 0.024009, loss_dice: 0.493032
[07:05:01.909] TRAIN: iteration 39 : loss : 0.254131, loss_ce: 0.013662, loss_dice: 0.494599
[07:05:02.116] TRAIN: iteration 40 : loss : 0.252336, loss_ce: 0.009425, loss_dice: 0.495248
[07:05:08.096] TRAIN: iteration 41 : loss : 0.255688, loss_ce: 0.020441, loss_dice: 0.490936
[07:05:11.087] TRAIN: iteration 42 : loss : 0.252625, loss_ce: 0.005223, loss_dice: 0.500026
[07:05:14.013] TRAIN: iteration 43 : loss : 0.258034, loss_ce: 0.020432, loss_dice: 0.495637
[07:05:14.219] TRAIN: iteration 44 : loss : 0.251333, loss_ce: 0.017723, loss_dice: 0.484944
[07:05:14.425] TRAIN: iteration 45 : loss : 0.252057, loss_ce: 0.010515, loss_dice: 0.493599
[07:05:14.631] TRAIN: iteration 46 : loss : 0.260623, loss_ce: 0.025743, loss_dice: 0.495503
[07:05:14.836] TRAIN: iteration 47 : loss : 0.254037, loss_ce: 0.021267, loss_dice: 0.486807
[07:05:15.042] TRAIN: iteration 48 : loss : 0.260577, loss_ce: 0.029625, loss_dice: 0.491530
[07:05:20.214] TRAIN: iteration 49 : loss : 0.250908, loss_ce: 0.010928, loss_dice: 0.490888
[07:05:22.884] TRAIN: iteration 50 : loss : 0.256297, loss_ce: 0.027180, loss_dice: 0.485413
[07:05:26.186] TRAIN: iteration 51 : loss : 0.254595, loss_ce: 0.034570, loss_dice: 0.474621
[07:05:26.397] TRAIN: iteration 52 : loss : 0.247396, loss_ce: 0.013797, loss_dice: 0.480995
[07:05:26.602] TRAIN: iteration 53 : loss : 0.242287, loss_ce: 0.019145, loss_dice: 0.465430
[07:05:26.807] TRAIN: iteration 54 : loss : 0.257461, loss_ce: 0.014595, loss_dice: 0.500327
[07:05:27.013] TRAIN: iteration 55 : loss : 0.256002, loss_ce: 0.014055, loss_dice: 0.497949
[07:05:27.218] TRAIN: iteration 56 : loss : 0.256142, loss_ce: 0.013601, loss_dice: 0.498682
[07:05:33.468] TRAIN: iteration 57 : loss : 0.250061, loss_ce: 0.016251, loss_dice: 0.483871
[07:05:34.659] TRAIN: iteration 58 : loss : 0.250972, loss_ce: 0.011578, loss_dice: 0.490366
[07:05:40.489] TRAIN: iteration 59 : loss : 0.255982, loss_ce: 0.016988, loss_dice: 0.494976
[07:05:40.701] TRAIN: iteration 60 : loss : 0.247504, loss_ce: 0.011283, loss_dice: 0.483725
[07:05:40.940] TRAIN: iteration 61 : loss : 0.253881, loss_ce: 0.011668, loss_dice: 0.496093
[07:05:41.146] TRAIN: iteration 62 : loss : 0.264192, loss_ce: 0.033225, loss_dice: 0.495158
[07:05:41.354] TRAIN: iteration 63 : loss : 0.253954, loss_ce: 0.014171, loss_dice: 0.493737
[07:05:41.560] TRAIN: iteration 64 : loss : 0.250441, loss_ce: 0.013865, loss_dice: 0.487017
[07:05:45.589] TRAIN: iteration 65 : loss : 0.247261, loss_ce: 0.017514, loss_dice: 0.477009
[07:05:47.643] TRAIN: iteration 66 : loss : 0.246046, loss_ce: 0.017061, loss_dice: 0.475030
[07:05:52.459] TRAIN: iteration 67 : loss : 0.249997, loss_ce: 0.009273, loss_dice: 0.490722
[07:05:52.664] TRAIN: iteration 68 : loss : 0.260922, loss_ce: 0.032064, loss_dice: 0.489781
[07:05:52.870] TRAIN: iteration 69 : loss : 0.254588, loss_ce: 0.009048, loss_dice: 0.500128
[07:05:53.075] TRAIN: iteration 70 : loss : 0.250976, loss_ce: 0.010889, loss_dice: 0.491064
[07:05:53.282] TRAIN: iteration 71 : loss : 0.231772, loss_ce: 0.010836, loss_dice: 0.452709
[07:05:53.489] TRAIN: iteration 72 : loss : 0.245373, loss_ce: 0.018650, loss_dice: 0.472096
[07:05:58.816] TRAIN: iteration 73 : loss : 0.248659, loss_ce: 0.012107, loss_dice: 0.485210
[07:06:01.626] TRAIN: iteration 74 : loss : 0.252560, loss_ce: 0.008344, loss_dice: 0.496776
[07:06:05.537] TRAIN: iteration 75 : loss : 0.240226, loss_ce: 0.009385, loss_dice: 0.471067
[07:06:05.742] TRAIN: iteration 76 : loss : 0.253624, loss_ce: 0.008099, loss_dice: 0.499148
[07:06:05.948] TRAIN: iteration 77 : loss : 0.256181, loss_ce: 0.015628, loss_dice: 0.496734
[07:06:06.155] TRAIN: iteration 78 : loss : 0.258440, loss_ce: 0.022981, loss_dice: 0.493899
[07:06:06.361] TRAIN: iteration 79 : loss : 0.241878, loss_ce: 0.010516, loss_dice: 0.473239
[07:06:06.566] TRAIN: iteration 80 : loss : 0.254088, loss_ce: 0.008822, loss_dice: 0.499354
[07:06:10.760] TRAIN: iteration 81 : loss : 0.255862, loss_ce: 0.022045, loss_dice: 0.489678
[07:06:13.811] TRAIN: iteration 82 : loss : 0.240411, loss_ce: 0.014452, loss_dice: 0.466371
[07:06:19.786] TRAIN: iteration 83 : loss : 0.252702, loss_ce: 0.018154, loss_dice: 0.487250
[07:06:19.992] TRAIN: iteration 84 : loss : 0.249919, loss_ce: 0.031512, loss_dice: 0.468326
[07:06:20.198] TRAIN: iteration 85 : loss : 0.248356, loss_ce: 0.016851, loss_dice: 0.479861
[07:06:20.405] TRAIN: iteration 86 : loss : 0.246322, loss_ce: 0.023170, loss_dice: 0.469473
[07:06:20.610] TRAIN: iteration 87 : loss : 0.240661, loss_ce: 0.018694, loss_dice: 0.462628
[07:06:20.816] TRAIN: iteration 88 : loss : 0.244856, loss_ce: 0.021200, loss_dice: 0.468512
[07:06:23.144] TRAIN: iteration 89 : loss : 0.258977, loss_ce: 0.021803, loss_dice: 0.496151
[07:06:25.875] TRAIN: iteration 90 : loss : 0.238992, loss_ce: 0.025472, loss_dice: 0.452513
[07:06:32.541] TRAIN: iteration 91 : loss : 0.256098, loss_ce: 0.026990, loss_dice: 0.485205
[07:06:32.752] TRAIN: iteration 92 : loss : 0.245246, loss_ce: 0.017384, loss_dice: 0.473107
[07:06:32.959] TRAIN: iteration 93 : loss : 0.237336, loss_ce: 0.013335, loss_dice: 0.461337
[07:06:33.166] TRAIN: iteration 94 : loss : 0.243182, loss_ce: 0.015937, loss_dice: 0.470427
[07:06:33.371] TRAIN: iteration 95 : loss : 0.244874, loss_ce: 0.013062, loss_dice: 0.476687
[07:06:33.577] TRAIN: iteration 96 : loss : 0.248848, loss_ce: 0.009212, loss_dice: 0.488484
[07:06:35.935] TRAIN: iteration 97 : loss : 0.250676, loss_ce: 0.013375, loss_dice: 0.487978
[07:06:37.960] TRAIN: iteration 98 : loss : 0.241298, loss_ce: 0.010349, loss_dice: 0.472247
[07:06:45.720] TRAIN: iteration 99 : loss : 0.263002, loss_ce: 0.047196, loss_dice: 0.478807
[07:06:45.926] TRAIN: iteration 100 : loss : 0.256414, loss_ce: 0.012595, loss_dice: 0.500233
[07:06:46.167] TRAIN: iteration 101 : loss : 0.257195, loss_ce: 0.014090, loss_dice: 0.500300
[07:06:46.372] TRAIN: iteration 102 : loss : 0.257324, loss_ce: 0.014341, loss_dice: 0.500306
[07:06:46.580] TRAIN: iteration 103 : loss : 0.257273, loss_ce: 0.014248, loss_dice: 0.500297
[07:06:46.786] TRAIN: iteration 104 : loss : 0.251081, loss_ce: 0.014116, loss_dice: 0.488045
[07:06:48.236] TRAIN: iteration 105 : loss : 0.237775, loss_ce: 0.011105, loss_dice: 0.464445
[07:06:50.696] TRAIN: iteration 106 : loss : 0.254991, loss_ce: 0.009841, loss_dice: 0.500141
[07:06:58.756] TRAIN: iteration 107 : loss : 0.254395, loss_ce: 0.008678, loss_dice: 0.500111
[07:06:58.963] TRAIN: iteration 108 : loss : 0.241189, loss_ce: 0.016019, loss_dice: 0.466359
[07:06:59.169] TRAIN: iteration 109 : loss : 0.233983, loss_ce: 0.012204, loss_dice: 0.455761
[07:06:59.375] TRAIN: iteration 110 : loss : 0.226054, loss_ce: 0.037690, loss_dice: 0.414418
[07:06:59.581] TRAIN: iteration 111 : loss : 0.257243, loss_ce: 0.023884, loss_dice: 0.490603
[07:06:59.787] TRAIN: iteration 112 : loss : 0.236004, loss_ce: 0.027730, loss_dice: 0.444279
[07:07:00.586] TRAIN: iteration 113 : loss : 0.219532, loss_ce: 0.031427, loss_dice: 0.407637
[07:07:03.402] TRAIN: iteration 114 : loss : 0.257141, loss_ce: 0.015859, loss_dice: 0.498424
[07:07:11.338] TRAIN: iteration 115 : loss : 0.233364, loss_ce: 0.013123, loss_dice: 0.453605
[07:07:11.547] TRAIN: iteration 116 : loss : 0.246740, loss_ce: 0.016459, loss_dice: 0.477021
[07:07:11.753] TRAIN: iteration 117 : loss : 0.245788, loss_ce: 0.026146, loss_dice: 0.465430
[07:07:11.959] TRAIN: iteration 118 : loss : 0.250605, loss_ce: 0.015963, loss_dice: 0.485247
[07:07:12.174] TRAIN: iteration 119 : loss : 0.255498, loss_ce: 0.010782, loss_dice: 0.500214
[07:07:12.379] TRAIN: iteration 120 : loss : 0.253818, loss_ce: 0.010616, loss_dice: 0.497021
[07:07:13.514] TRAIN: iteration 121 : loss : 0.250086, loss_ce: 0.013856, loss_dice: 0.486316
[07:07:15.034] TRAIN: iteration 122 : loss : 0.257909, loss_ce: 0.023666, loss_dice: 0.492152
[07:07:23.527] TRAIN: iteration 123 : loss : 0.248336, loss_ce: 0.012773, loss_dice: 0.483898
[07:07:23.733] TRAIN: iteration 124 : loss : 0.253198, loss_ce: 0.007230, loss_dice: 0.499166
[07:07:23.939] TRAIN: iteration 125 : loss : 0.253248, loss_ce: 0.006450, loss_dice: 0.500045
[07:07:24.145] TRAIN: iteration 126 : loss : 0.249895, loss_ce: 0.012144, loss_dice: 0.487646
[07:07:24.351] TRAIN: iteration 127 : loss : 0.245757, loss_ce: 0.010045, loss_dice: 0.481469
[07:07:24.556] TRAIN: iteration 128 : loss : 0.243348, loss_ce: 0.008972, loss_dice: 0.477724
[07:07:27.708] TRAIN: iteration 129 : loss : 0.246645, loss_ce: 0.013539, loss_dice: 0.479751
[07:07:28.076] TRAIN: iteration 130 : loss : 0.244302, loss_ce: 0.016406, loss_dice: 0.472199
[07:07:35.739] TRAIN: iteration 131 : loss : 0.262018, loss_ce: 0.051651, loss_dice: 0.472384
[07:07:35.944] TRAIN: iteration 132 : loss : 0.259120, loss_ce: 0.041936, loss_dice: 0.476303
[07:07:36.150] TRAIN: iteration 133 : loss : 0.240949, loss_ce: 0.025104, loss_dice: 0.456794
[07:07:36.355] TRAIN: iteration 134 : loss : 0.251648, loss_ce: 0.016784, loss_dice: 0.486513
[07:07:36.562] TRAIN: iteration 135 : loss : 0.241112, loss_ce: 0.017990, loss_dice: 0.464235
[07:07:36.768] TRAIN: iteration 136 : loss : 0.250617, loss_ce: 0.016227, loss_dice: 0.485008
[07:07:39.806] TRAIN: iteration 137 : loss : 0.229826, loss_ce: 0.018870, loss_dice: 0.440782
[07:07:40.013] TRAIN: iteration 138 : loss : 0.237254, loss_ce: 0.018961, loss_dice: 0.455548
[07:07:48.346] TRAIN: iteration 139 : loss : 0.267397, loss_ce: 0.044841, loss_dice: 0.489953
[07:07:48.558] TRAIN: iteration 140 : loss : 0.256156, loss_ce: 0.016567, loss_dice: 0.495745
[07:07:48.800] TRAIN: iteration 141 : loss : 0.251146, loss_ce: 0.017822, loss_dice: 0.484470
[07:07:49.006] TRAIN: iteration 142 : loss : 0.232501, loss_ce: 0.016110, loss_dice: 0.448891
[07:07:49.213] TRAIN: iteration 143 : loss : 0.235340, loss_ce: 0.017096, loss_dice: 0.453584
[07:07:49.419] TRAIN: iteration 144 : loss : 0.248574, loss_ce: 0.014141, loss_dice: 0.483008
[07:07:52.842] TRAIN: iteration 145 : loss : 0.253940, loss_ce: 0.022528, loss_dice: 0.485352
[07:07:53.047] TRAIN: iteration 146 : loss : 0.224820, loss_ce: 0.014230, loss_dice: 0.435410
[07:08:00.938] TRAIN: iteration 147 : loss : 0.238170, loss_ce: 0.012634, loss_dice: 0.463706
[07:08:01.146] TRAIN: iteration 148 : loss : 0.251418, loss_ce: 0.012111, loss_dice: 0.490725
[07:08:01.353] TRAIN: iteration 149 : loss : 0.229826, loss_ce: 0.024019, loss_dice: 0.435633
[07:08:01.559] TRAIN: iteration 150 : loss : 0.237096, loss_ce: 0.012779, loss_dice: 0.461414
[07:08:01.764] TRAIN: iteration 151 : loss : 0.256270, loss_ce: 0.014158, loss_dice: 0.498381
[07:08:01.971] TRAIN: iteration 152 : loss : 0.242455, loss_ce: 0.016237, loss_dice: 0.468673
[07:08:05.225] TRAIN: iteration 153 : loss : 0.251935, loss_ce: 0.013401, loss_dice: 0.490470
[07:08:05.590] TRAIN: iteration 154 : loss : 0.246665, loss_ce: 0.010119, loss_dice: 0.483212
[07:08:13.871] TRAIN: iteration 155 : loss : 0.243408, loss_ce: 0.011810, loss_dice: 0.475007
[07:08:14.080] TRAIN: iteration 156 : loss : 0.252923, loss_ce: 0.005784, loss_dice: 0.500062
[07:08:14.290] TRAIN: iteration 157 : loss : 0.240643, loss_ce: 0.015595, loss_dice: 0.465690
[07:08:14.496] TRAIN: iteration 158 : loss : 0.270056, loss_ce: 0.041354, loss_dice: 0.498759
[07:08:14.702] TRAIN: iteration 159 : loss : 0.271419, loss_ce: 0.052042, loss_dice: 0.490795
[07:08:14.908] TRAIN: iteration 160 : loss : 0.242563, loss_ce: 0.010059, loss_dice: 0.475067
[07:08:18.342] TRAIN: iteration 161 : loss : 0.228275, loss_ce: 0.009112, loss_dice: 0.447438
[07:08:18.568] TRAIN: iteration 162 : loss : 0.251876, loss_ce: 0.007207, loss_dice: 0.496545
[07:08:26.992] TRAIN: iteration 163 : loss : 0.212612, loss_ce: 0.016788, loss_dice: 0.408436
[07:08:27.203] TRAIN: iteration 164 : loss : 0.222701, loss_ce: 0.013586, loss_dice: 0.431816
[07:08:27.409] TRAIN: iteration 165 : loss : 0.242943, loss_ce: 0.017640, loss_dice: 0.468246
[07:08:27.615] TRAIN: iteration 166 : loss : 0.249989, loss_ce: 0.020190, loss_dice: 0.479789
[07:08:27.820] TRAIN: iteration 167 : loss : 0.257472, loss_ce: 0.017526, loss_dice: 0.497417
[07:08:28.026] TRAIN: iteration 168 : loss : 0.252260, loss_ce: 0.017133, loss_dice: 0.487386
[07:08:31.020] TRAIN: iteration 169 : loss : 0.246426, loss_ce: 0.019438, loss_dice: 0.473414
[07:08:31.753] TRAIN: iteration 170 : loss : 0.238665, loss_ce: 0.012732, loss_dice: 0.464598
[07:08:40.553] TRAIN: iteration 171 : loss : 0.222333, loss_ce: 0.013659, loss_dice: 0.431008
[07:08:40.761] TRAIN: iteration 172 : loss : 0.243267, loss_ce: 0.007966, loss_dice: 0.478568
[07:08:40.967] TRAIN: iteration 173 : loss : 0.246851, loss_ce: 0.008819, loss_dice: 0.484882
[07:08:41.173] TRAIN: iteration 174 : loss : 0.231187, loss_ce: 0.007690, loss_dice: 0.454683
[07:08:41.388] TRAIN: iteration 175 : loss : 0.241988, loss_ce: 0.011221, loss_dice: 0.472756
[07:08:41.594] TRAIN: iteration 176 : loss : 0.235641, loss_ce: 0.006735, loss_dice: 0.464546
[07:08:43.166] TRAIN: iteration 177 : loss : 0.226546, loss_ce: 0.010672, loss_dice: 0.442420
[07:08:44.276] TRAIN: iteration 178 : loss : 0.231882, loss_ce: 0.016975, loss_dice: 0.446790
[07:08:53.561] TRAIN: iteration 179 : loss : 0.237519, loss_ce: 0.014056, loss_dice: 0.460983
[07:08:53.766] TRAIN: iteration 180 : loss : 0.220709, loss_ce: 0.020442, loss_dice: 0.420977
[07:08:54.005] TRAIN: iteration 181 : loss : 0.216617, loss_ce: 0.025324, loss_dice: 0.407909
[07:08:54.211] TRAIN: iteration 182 : loss : 0.256028, loss_ce: 0.023434, loss_dice: 0.488622
[07:08:54.417] TRAIN: iteration 183 : loss : 0.243167, loss_ce: 0.028093, loss_dice: 0.458242
[07:08:54.625] TRAIN: iteration 184 : loss : 0.259909, loss_ce: 0.027812, loss_dice: 0.492007
[07:08:54.833] TRAIN: iteration 185 : loss : 0.247576, loss_ce: 0.026668, loss_dice: 0.468484
[07:08:56.424] TRAIN: iteration 186 : loss : 0.246461, loss_ce: 0.023875, loss_dice: 0.469046
[07:09:06.036] TRAIN: iteration 187 : loss : 0.223032, loss_ce: 0.031059, loss_dice: 0.415005
[07:09:06.242] TRAIN: iteration 188 : loss : 0.250881, loss_ce: 0.016010, loss_dice: 0.485752
[07:09:06.450] TRAIN: iteration 189 : loss : 0.223919, loss_ce: 0.017793, loss_dice: 0.430046
[07:09:06.656] TRAIN: iteration 190 : loss : 0.245448, loss_ce: 0.011486, loss_dice: 0.479410
[07:09:06.865] TRAIN: iteration 191 : loss : 0.230597, loss_ce: 0.010972, loss_dice: 0.450221
[07:09:07.072] TRAIN: iteration 192 : loss : 0.245001, loss_ce: 0.007474, loss_dice: 0.482527
[07:09:07.283] TRAIN: iteration 193 : loss : 0.235190, loss_ce: 0.007768, loss_dice: 0.462612
[07:09:08.303] TRAIN: iteration 194 : loss : 0.241708, loss_ce: 0.006042, loss_dice: 0.477374
[07:09:19.136] TRAIN: iteration 195 : loss : 0.235985, loss_ce: 0.013917, loss_dice: 0.458052
[07:09:19.348] TRAIN: iteration 196 : loss : 0.233652, loss_ce: 0.018057, loss_dice: 0.449248
[07:09:19.555] TRAIN: iteration 197 : loss : 0.223925, loss_ce: 0.010886, loss_dice: 0.436964
[07:09:19.760] TRAIN: iteration 198 : loss : 0.247644, loss_ce: 0.023500, loss_dice: 0.471788
[07:09:19.967] TRAIN: iteration 199 : loss : 0.229166, loss_ce: 0.015455, loss_dice: 0.442877
[07:09:20.174] TRAIN: iteration 200 : loss : 0.196757, loss_ce: 0.020043, loss_dice: 0.373471
[07:09:20.412] TRAIN: iteration 201 : loss : 0.227407, loss_ce: 0.018642, loss_dice: 0.436172
[07:09:20.662] TRAIN: iteration 202 : loss : 0.198033, loss_ce: 0.022847, loss_dice: 0.373219
[07:09:31.629] TRAIN: iteration 203 : loss : 0.213924, loss_ce: 0.016272, loss_dice: 0.411576
[07:09:31.845] TRAIN: iteration 204 : loss : 0.243553, loss_ce: 0.009516, loss_dice: 0.477591
[07:09:32.052] TRAIN: iteration 205 : loss : 0.206406, loss_ce: 0.006681, loss_dice: 0.406132
[07:09:32.258] TRAIN: iteration 206 : loss : 0.252041, loss_ce: 0.004033, loss_dice: 0.500049
[07:09:32.463] TRAIN: iteration 207 : loss : 0.251465, loss_ce: 0.002912, loss_dice: 0.500017
[07:09:32.669] TRAIN: iteration 208 : loss : 0.248541, loss_ce: 0.004125, loss_dice: 0.492957
[07:09:32.874] TRAIN: iteration 209 : loss : 0.243835, loss_ce: 0.002583, loss_dice: 0.485088
[07:09:34.109] TRAIN: iteration 210 : loss : 0.249515, loss_ce: 0.001885, loss_dice: 0.497145
[07:09:44.163] TRAIN: iteration 211 : loss : 0.248195, loss_ce: 0.005303, loss_dice: 0.491086
[07:09:44.376] TRAIN: iteration 212 : loss : 0.251256, loss_ce: 0.009998, loss_dice: 0.492514
[07:09:44.583] TRAIN: iteration 213 : loss : 0.254372, loss_ce: 0.015683, loss_dice: 0.493061
[07:09:44.789] TRAIN: iteration 214 : loss : 0.249414, loss_ce: 0.008331, loss_dice: 0.490498
[07:09:44.995] TRAIN: iteration 215 : loss : 0.248120, loss_ce: 0.005516, loss_dice: 0.490725
[07:09:45.204] TRAIN: iteration 216 : loss : 0.244813, loss_ce: 0.002899, loss_dice: 0.486728
[07:09:45.410] TRAIN: iteration 217 : loss : 0.253698, loss_ce: 0.018375, loss_dice: 0.489022
[07:09:47.077] TRAIN: iteration 218 : loss : 0.252754, loss_ce: 0.009851, loss_dice: 0.495657
[07:09:56.764] TRAIN: iteration 219 : loss : 0.242031, loss_ce: 0.005047, loss_dice: 0.479014
[07:09:56.980] TRAIN: iteration 220 : loss : 0.243363, loss_ce: 0.010243, loss_dice: 0.476483
[07:09:57.216] TRAIN: iteration 221 : loss : 0.237584, loss_ce: 0.006643, loss_dice: 0.468526
[07:09:57.422] TRAIN: iteration 222 : loss : 0.230931, loss_ce: 0.008091, loss_dice: 0.453770
[07:09:57.631] TRAIN: iteration 223 : loss : 0.235304, loss_ce: 0.011917, loss_dice: 0.458691
[07:09:57.837] TRAIN: iteration 224 : loss : 0.224553, loss_ce: 0.014749, loss_dice: 0.434358
[07:09:58.045] TRAIN: iteration 225 : loss : 0.244266, loss_ce: 0.015818, loss_dice: 0.472714
[07:09:59.269] TRAIN: iteration 226 : loss : 0.231954, loss_ce: 0.019877, loss_dice: 0.444030
[07:10:09.129] TRAIN: iteration 227 : loss : 0.247720, loss_ce: 0.018485, loss_dice: 0.476955
[07:10:09.335] TRAIN: iteration 228 : loss : 0.252432, loss_ce: 0.018644, loss_dice: 0.486219
[07:10:09.540] TRAIN: iteration 229 : loss : 0.206214, loss_ce: 0.019666, loss_dice: 0.392762
[07:10:09.746] TRAIN: iteration 230 : loss : 0.246431, loss_ce: 0.018861, loss_dice: 0.474002
[07:10:09.956] TRAIN: iteration 231 : loss : 0.248146, loss_ce: 0.016729, loss_dice: 0.479563
[07:10:10.162] TRAIN: iteration 232 : loss : 0.240419, loss_ce: 0.013719, loss_dice: 0.467120
[07:10:10.370] TRAIN: iteration 233 : loss : 0.218403, loss_ce: 0.013894, loss_dice: 0.422912
[07:10:10.774] TRAIN: iteration 234 : loss : 0.209902, loss_ce: 0.012981, loss_dice: 0.406822
[07:10:22.405] TRAIN: iteration 235 : loss : 0.242419, loss_ce: 0.008368, loss_dice: 0.476470
[07:10:22.612] TRAIN: iteration 236 : loss : 0.228295, loss_ce: 0.015146, loss_dice: 0.441444
[07:10:22.819] TRAIN: iteration 237 : loss : 0.199047, loss_ce: 0.008526, loss_dice: 0.389567
[07:10:23.026] TRAIN: iteration 238 : loss : 0.207445, loss_ce: 0.015986, loss_dice: 0.398904
[07:10:23.232] TRAIN: iteration 239 : loss : 0.252512, loss_ce: 0.019227, loss_dice: 0.485796
[07:10:23.439] TRAIN: iteration 240 : loss : 0.233133, loss_ce: 0.013498, loss_dice: 0.452768
[07:10:23.676] TRAIN: iteration 241 : loss : 0.209957, loss_ce: 0.019451, loss_dice: 0.400463
[07:10:23.881] TRAIN: iteration 242 : loss : 0.244751, loss_ce: 0.017150, loss_dice: 0.472351
[07:10:35.904] TRAIN: iteration 243 : loss : 0.248809, loss_ce: 0.023406, loss_dice: 0.474212
[07:10:36.111] TRAIN: iteration 244 : loss : 0.245154, loss_ce: 0.015594, loss_dice: 0.474713
[07:10:36.316] TRAIN: iteration 245 : loss : 0.254323, loss_ce: 0.012615, loss_dice: 0.496031
[07:10:36.521] TRAIN: iteration 246 : loss : 0.216246, loss_ce: 0.011978, loss_dice: 0.420514
[07:10:36.728] TRAIN: iteration 247 : loss : 0.235404, loss_ce: 0.009604, loss_dice: 0.461203
[07:10:36.934] TRAIN: iteration 248 : loss : 0.236068, loss_ce: 0.009178, loss_dice: 0.462958
[07:10:37.141] TRAIN: iteration 249 : loss : 0.242553, loss_ce: 0.042659, loss_dice: 0.442448
[07:10:37.347] TRAIN: iteration 250 : loss : 0.253776, loss_ce: 0.007397, loss_dice: 0.500155
[07:10:48.351] TRAIN: iteration 251 : loss : 0.204473, loss_ce: 0.011302, loss_dice: 0.397643
[07:10:48.557] TRAIN: iteration 252 : loss : 0.229696, loss_ce: 0.011938, loss_dice: 0.447454
[07:10:48.763] TRAIN: iteration 253 : loss : 0.239794, loss_ce: 0.021269, loss_dice: 0.458318
[07:10:48.970] TRAIN: iteration 254 : loss : 0.221911, loss_ce: 0.014594, loss_dice: 0.429229
[07:10:49.177] TRAIN: iteration 255 : loss : 0.231343, loss_ce: 0.022317, loss_dice: 0.440370
[07:10:49.385] TRAIN: iteration 256 : loss : 0.184365, loss_ce: 0.015087, loss_dice: 0.353643
[07:10:49.591] TRAIN: iteration 257 : loss : 0.253037, loss_ce: 0.012082, loss_dice: 0.493993
[07:10:49.799] TRAIN: iteration 258 : loss : 0.192723, loss_ce: 0.014698, loss_dice: 0.370747
[07:11:00.898] TRAIN: iteration 259 : loss : 0.230806, loss_ce: 0.018324, loss_dice: 0.443289
[07:11:01.104] TRAIN: iteration 260 : loss : 0.255528, loss_ce: 0.010691, loss_dice: 0.500365
[07:11:01.345] TRAIN: iteration 261 : loss : 0.255153, loss_ce: 0.009978, loss_dice: 0.500329
[07:11:01.562] TRAIN: iteration 262 : loss : 0.254864, loss_ce: 0.009434, loss_dice: 0.500294
[07:11:01.773] TRAIN: iteration 263 : loss : 0.218599, loss_ce: 0.010887, loss_dice: 0.426311
[07:11:01.978] TRAIN: iteration 264 : loss : 0.212639, loss_ce: 0.013939, loss_dice: 0.411340
[07:11:02.184] TRAIN: iteration 265 : loss : 0.240314, loss_ce: 0.012834, loss_dice: 0.467795
[07:11:02.393] TRAIN: iteration 266 : loss : 0.254701, loss_ce: 0.009295, loss_dice: 0.500108
[07:11:13.442] TRAIN: iteration 267 : loss : 0.251745, loss_ce: 0.011365, loss_dice: 0.492125
[07:11:13.652] TRAIN: iteration 268 : loss : 0.200241, loss_ce: 0.010061, loss_dice: 0.390420
[07:11:13.858] TRAIN: iteration 269 : loss : 0.254072, loss_ce: 0.007929, loss_dice: 0.500215
[07:11:14.064] TRAIN: iteration 270 : loss : 0.223883, loss_ce: 0.013121, loss_dice: 0.434645
[07:11:14.270] TRAIN: iteration 271 : loss : 0.214284, loss_ce: 0.017713, loss_dice: 0.410856
[07:11:14.477] TRAIN: iteration 272 : loss : 0.254916, loss_ce: 0.009471, loss_dice: 0.500360
[07:11:14.683] TRAIN: iteration 273 : loss : 0.192500, loss_ce: 0.019784, loss_dice: 0.365215
[07:11:14.890] TRAIN: iteration 274 : loss : 0.255618, loss_ce: 0.012448, loss_dice: 0.498787
[07:11:26.126] TRAIN: iteration 275 : loss : 0.255162, loss_ce: 0.012805, loss_dice: 0.497520
[07:11:26.332] TRAIN: iteration 276 : loss : 0.251201, loss_ce: 0.012596, loss_dice: 0.489807
[07:11:26.537] TRAIN: iteration 277 : loss : 0.168246, loss_ce: 0.016448, loss_dice: 0.320044
[07:11:26.743] TRAIN: iteration 278 : loss : 0.240379, loss_ce: 0.018529, loss_dice: 0.462229
[07:11:26.949] TRAIN: iteration 279 : loss : 0.210949, loss_ce: 0.013557, loss_dice: 0.408341
[07:11:27.159] TRAIN: iteration 280 : loss : 0.246372, loss_ce: 0.011298, loss_dice: 0.481446
[07:11:28.181] TRAIN: iteration 281 : loss : 0.229146, loss_ce: 0.019535, loss_dice: 0.438756
[07:11:28.390] TRAIN: iteration 282 : loss : 0.249093, loss_ce: 0.011233, loss_dice: 0.486952
[07:11:39.664] TRAIN: iteration 283 : loss : 0.242478, loss_ce: 0.013554, loss_dice: 0.471402
[07:11:39.871] TRAIN: iteration 284 : loss : 0.254479, loss_ce: 0.008641, loss_dice: 0.500316
[07:11:40.079] TRAIN: iteration 285 : loss : 0.248388, loss_ce: 0.009231, loss_dice: 0.487545
[07:11:40.284] TRAIN: iteration 286 : loss : 0.242131, loss_ce: 0.006690, loss_dice: 0.477573
[07:11:40.491] TRAIN: iteration 287 : loss : 0.252526, loss_ce: 0.004977, loss_dice: 0.500075
[07:11:40.699] TRAIN: iteration 288 : loss : 0.252149, loss_ce: 0.004252, loss_dice: 0.500047
[07:11:40.909] TRAIN: iteration 289 : loss : 0.240974, loss_ce: 0.020841, loss_dice: 0.461107
[07:11:41.115] TRAIN: iteration 290 : loss : 0.239740, loss_ce: 0.007048, loss_dice: 0.472431
[07:11:52.452] TRAIN: iteration 291 : loss : 0.241869, loss_ce: 0.006647, loss_dice: 0.477091
[07:11:52.663] TRAIN: iteration 292 : loss : 0.252070, loss_ce: 0.004101, loss_dice: 0.500038
[07:11:52.869] TRAIN: iteration 293 : loss : 0.210398, loss_ce: 0.009361, loss_dice: 0.411435
[07:11:53.075] TRAIN: iteration 294 : loss : 0.214065, loss_ce: 0.009187, loss_dice: 0.418943
[07:11:53.281] TRAIN: iteration 295 : loss : 0.226942, loss_ce: 0.035752, loss_dice: 0.418132
[07:11:53.488] TRAIN: iteration 296 : loss : 0.242546, loss_ce: 0.015989, loss_dice: 0.469102
[07:11:53.694] TRAIN: iteration 297 : loss : 0.261139, loss_ce: 0.021003, loss_dice: 0.501275
[07:11:53.904] TRAIN: iteration 298 : loss : 0.244247, loss_ce: 0.024348, loss_dice: 0.464146
[07:12:05.006] TRAIN: iteration 299 : loss : 0.244066, loss_ce: 0.028627, loss_dice: 0.459506
[07:12:05.211] TRAIN: iteration 300 : loss : 0.254782, loss_ce: 0.029128, loss_dice: 0.480436
[07:12:05.212] NaN or Inf found in input tensor.
[07:12:05.426] TRAIN: iteration 301 : loss : 0.202495, loss_ce: 0.031724, loss_dice: 0.373267
[07:12:05.633] TRAIN: iteration 302 : loss : 0.246949, loss_ce: 0.029728, loss_dice: 0.464170
[07:12:05.841] TRAIN: iteration 303 : loss : 0.247289, loss_ce: 0.026771, loss_dice: 0.467808
[07:12:06.048] TRAIN: iteration 304 : loss : 0.220031, loss_ce: 0.025936, loss_dice: 0.414125
[07:12:06.257] TRAIN: iteration 305 : loss : 0.247890, loss_ce: 0.055928, loss_dice: 0.439852
[07:12:06.464] TRAIN: iteration 306 : loss : 0.252361, loss_ce: 0.020581, loss_dice: 0.484141
[07:12:18.446] TRAIN: iteration 307 : loss : 0.258626, loss_ce: 0.016389, loss_dice: 0.500863
[07:12:18.652] TRAIN: iteration 308 : loss : 0.234468, loss_ce: 0.013859, loss_dice: 0.455077
[07:12:18.858] TRAIN: iteration 309 : loss : 0.199797, loss_ce: 0.020510, loss_dice: 0.379083
[07:12:19.067] TRAIN: iteration 310 : loss : 0.229090, loss_ce: 0.015035, loss_dice: 0.443145
[07:12:19.275] TRAIN: iteration 311 : loss : 0.250077, loss_ce: 0.009773, loss_dice: 0.490381
[07:12:19.481] TRAIN: iteration 312 : loss : 0.229173, loss_ce: 0.009706, loss_dice: 0.448639
[07:12:19.689] TRAIN: iteration 313 : loss : 0.234745, loss_ce: 0.008590, loss_dice: 0.460900
[07:12:19.898] TRAIN: iteration 314 : loss : 0.238473, loss_ce: 0.007762, loss_dice: 0.469184
[07:12:30.846] TRAIN: iteration 315 : loss : 0.256748, loss_ce: 0.016866, loss_dice: 0.496629
[07:12:31.052] TRAIN: iteration 316 : loss : 0.223325, loss_ce: 0.010535, loss_dice: 0.436115
[07:12:31.259] TRAIN: iteration 317 : loss : 0.221751, loss_ce: 0.005540, loss_dice: 0.437962
[07:12:31.469] TRAIN: iteration 318 : loss : 0.212498, loss_ce: 0.006086, loss_dice: 0.418910
[07:12:31.675] TRAIN: iteration 319 : loss : 0.252677, loss_ce: 0.006693, loss_dice: 0.498661
[07:12:31.882] TRAIN: iteration 320 : loss : 0.237224, loss_ce: 0.018410, loss_dice: 0.456037
[07:12:32.120] TRAIN: iteration 321 : loss : 0.219265, loss_ce: 0.008605, loss_dice: 0.429925
[07:12:32.326] TRAIN: iteration 322 : loss : 0.239306, loss_ce: 0.008963, loss_dice: 0.469649
[07:12:44.555] TRAIN: iteration 323 : loss : 0.240918, loss_ce: 0.017297, loss_dice: 0.464539
[07:12:44.762] TRAIN: iteration 324 : loss : 0.233933, loss_ce: 0.032643, loss_dice: 0.435224
[07:12:44.975] TRAIN: iteration 325 : loss : 0.228769, loss_ce: 0.016037, loss_dice: 0.441500
[07:12:45.182] TRAIN: iteration 326 : loss : 0.210441, loss_ce: 0.022418, loss_dice: 0.398464
[07:12:45.390] TRAIN: iteration 327 : loss : 0.221533, loss_ce: 0.015628, loss_dice: 0.427438
[07:12:45.597] TRAIN: iteration 328 : loss : 0.210259, loss_ce: 0.016664, loss_dice: 0.403853
[07:12:45.805] TRAIN: iteration 329 : loss : 0.202058, loss_ce: 0.019072, loss_dice: 0.385043
[07:12:46.011] TRAIN: iteration 330 : loss : 0.221271, loss_ce: 0.039426, loss_dice: 0.403115
[07:12:56.514] TRAIN: iteration 331 : loss : 0.206182, loss_ce: 0.025763, loss_dice: 0.386601
[07:12:56.725] TRAIN: iteration 332 : loss : 0.181833, loss_ce: 0.020314, loss_dice: 0.343352
[07:12:56.932] TRAIN: iteration 333 : loss : 0.255825, loss_ce: 0.013499, loss_dice: 0.498151
[07:12:57.140] TRAIN: iteration 334 : loss : 0.239575, loss_ce: 0.014982, loss_dice: 0.464169
[07:12:57.348] TRAIN: iteration 335 : loss : 0.248420, loss_ce: 0.016327, loss_dice: 0.480513
[07:12:57.554] TRAIN: iteration 336 : loss : 0.220422, loss_ce: 0.014761, loss_dice: 0.426084
[07:12:57.761] TRAIN: iteration 337 : loss : 0.233416, loss_ce: 0.021252, loss_dice: 0.445581
[07:12:57.967] TRAIN: iteration 338 : loss : 0.255191, loss_ce: 0.011599, loss_dice: 0.498783
[07:13:08.664] TRAIN: iteration 339 : loss : 0.200424, loss_ce: 0.012242, loss_dice: 0.388605
[07:13:08.871] TRAIN: iteration 340 : loss : 0.181658, loss_ce: 0.014118, loss_dice: 0.349198
[07:13:09.108] TRAIN: iteration 341 : loss : 0.229349, loss_ce: 0.013358, loss_dice: 0.445341
[07:13:09.316] TRAIN: iteration 342 : loss : 0.211310, loss_ce: 0.008558, loss_dice: 0.414062
[07:13:09.523] TRAIN: iteration 343 : loss : 0.182440, loss_ce: 0.009902, loss_dice: 0.354978
[07:13:09.729] TRAIN: iteration 344 : loss : 0.214463, loss_ce: 0.007476, loss_dice: 0.421449
[07:13:09.935] TRAIN: iteration 345 : loss : 0.233088, loss_ce: 0.009115, loss_dice: 0.457062
[07:13:10.142] TRAIN: iteration 346 : loss : 0.184216, loss_ce: 0.011225, loss_dice: 0.357206
[07:13:20.926] TRAIN: iteration 347 : loss : 0.226858, loss_ce: 0.010338, loss_dice: 0.443378
[07:13:21.134] TRAIN: iteration 348 : loss : 0.238142, loss_ce: 0.011125, loss_dice: 0.465159
[07:13:21.342] TRAIN: iteration 349 : loss : 0.230805, loss_ce: 0.016987, loss_dice: 0.444623
[07:13:21.552] TRAIN: iteration 350 : loss : 0.202938, loss_ce: 0.010608, loss_dice: 0.395268
[07:13:21.759] TRAIN: iteration 351 : loss : 0.231632, loss_ce: 0.013192, loss_dice: 0.450071
[07:13:21.967] TRAIN: iteration 352 : loss : 0.255175, loss_ce: 0.009918, loss_dice: 0.500431
[07:13:22.196] TRAIN: iteration 353 : loss : 0.209864, loss_ce: 0.013767, loss_dice: 0.405962
[07:13:22.402] TRAIN: iteration 354 : loss : 0.243641, loss_ce: 0.011191, loss_dice: 0.476090
[07:13:32.417] TRAIN: iteration 355 : loss : 0.255206, loss_ce: 0.010009, loss_dice: 0.500403
[07:13:32.623] TRAIN: iteration 356 : loss : 0.228432, loss_ce: 0.010883, loss_dice: 0.445982
[07:13:32.829] TRAIN: iteration 357 : loss : 0.239569, loss_ce: 0.010263, loss_dice: 0.468875
[07:13:33.036] TRAIN: iteration 358 : loss : 0.192954, loss_ce: 0.016450, loss_dice: 0.369458
[07:13:33.244] TRAIN: iteration 359 : loss : 0.204188, loss_ce: 0.012192, loss_dice: 0.396184
[07:13:33.449] TRAIN: iteration 360 : loss : 0.190602, loss_ce: 0.012733, loss_dice: 0.368471
[07:13:33.450] NaN or Inf found in input tensor.
[07:13:35.663] TRAIN: iteration 361 : loss : 0.207484, loss_ce: 0.021326, loss_dice: 0.393641
[07:13:35.869] TRAIN: iteration 362 : loss : 0.207738, loss_ce: 0.013532, loss_dice: 0.401945
[07:13:44.455] TRAIN: iteration 363 : loss : 0.256865, loss_ce: 0.013662, loss_dice: 0.500068
[07:13:44.661] TRAIN: iteration 364 : loss : 0.239467, loss_ce: 0.014067, loss_dice: 0.464866
[07:13:44.866] TRAIN: iteration 365 : loss : 0.216772, loss_ce: 0.015612, loss_dice: 0.417932
[07:13:45.072] TRAIN: iteration 366 : loss : 0.252994, loss_ce: 0.011359, loss_dice: 0.494629
[07:13:45.285] TRAIN: iteration 367 : loss : 0.255481, loss_ce: 0.011772, loss_dice: 0.499191
[07:13:45.492] TRAIN: iteration 368 : loss : 0.199514, loss_ce: 0.011909, loss_dice: 0.387119
[07:13:49.242] TRAIN: iteration 369 : loss : 0.252842, loss_ce: 0.008235, loss_dice: 0.497450
[07:13:49.449] TRAIN: iteration 370 : loss : 0.216199, loss_ce: 0.007880, loss_dice: 0.424518
[07:13:57.675] TRAIN: iteration 371 : loss : 0.201944, loss_ce: 0.008774, loss_dice: 0.395115
[07:13:57.884] TRAIN: iteration 372 : loss : 0.250525, loss_ce: 0.006747, loss_dice: 0.494303
[07:13:58.091] TRAIN: iteration 373 : loss : 0.225095, loss_ce: 0.008070, loss_dice: 0.442120
[07:13:58.296] TRAIN: iteration 374 : loss : 0.188344, loss_ce: 0.012116, loss_dice: 0.364573
[07:13:58.501] TRAIN: iteration 375 : loss : 0.207071, loss_ce: 0.013012, loss_dice: 0.401130
[07:13:58.708] TRAIN: iteration 376 : loss : 0.254787, loss_ce: 0.009193, loss_dice: 0.500382
[07:14:01.397] TRAIN: iteration 377 : loss : 0.204614, loss_ce: 0.027599, loss_dice: 0.381630
[07:14:01.604] TRAIN: iteration 378 : loss : 0.257010, loss_ce: 0.013331, loss_dice: 0.500689
[07:14:09.024] TRAIN: iteration 379 : loss : 0.244628, loss_ce: 0.018067, loss_dice: 0.471189
[07:14:09.229] TRAIN: iteration 380 : loss : 0.223718, loss_ce: 0.019685, loss_dice: 0.427751
[07:14:09.469] TRAIN: iteration 381 : loss : 0.261099, loss_ce: 0.020893, loss_dice: 0.501305
[07:14:09.676] TRAIN: iteration 382 : loss : 0.210857, loss_ce: 0.024847, loss_dice: 0.396868
[07:14:09.885] TRAIN: iteration 383 : loss : 0.213247, loss_ce: 0.028877, loss_dice: 0.397618
[07:14:10.097] TRAIN: iteration 384 : loss : 0.199541, loss_ce: 0.022032, loss_dice: 0.377050
[07:14:14.449] TRAIN: iteration 385 : loss : 0.257252, loss_ce: 0.020581, loss_dice: 0.493922
[07:14:14.655] TRAIN: iteration 386 : loss : 0.235729, loss_ce: 0.017160, loss_dice: 0.454297
[07:14:21.676] TRAIN: iteration 387 : loss : 0.251645, loss_ce: 0.016201, loss_dice: 0.487089
[07:14:21.884] TRAIN: iteration 388 : loss : 0.246316, loss_ce: 0.015600, loss_dice: 0.477032
[07:14:22.091] TRAIN: iteration 389 : loss : 0.232536, loss_ce: 0.012079, loss_dice: 0.452992
[07:14:22.297] TRAIN: iteration 390 : loss : 0.223574, loss_ce: 0.010264, loss_dice: 0.436885
[07:14:22.503] TRAIN: iteration 391 : loss : 0.190218, loss_ce: 0.010064, loss_dice: 0.370372
[07:14:22.708] TRAIN: iteration 392 : loss : 0.249599, loss_ce: 0.030535, loss_dice: 0.468662
[07:14:27.701] TRAIN: iteration 393 : loss : 0.255524, loss_ce: 0.047683, loss_dice: 0.463365
[07:14:27.907] TRAIN: iteration 394 : loss : 0.194409, loss_ce: 0.007536, loss_dice: 0.381283
[07:14:34.965] TRAIN: iteration 395 : loss : 0.209440, loss_ce: 0.009621, loss_dice: 0.409260
[07:14:35.172] TRAIN: iteration 396 : loss : 0.209013, loss_ce: 0.006205, loss_dice: 0.411821
[07:14:35.376] TRAIN: iteration 397 : loss : 0.208622, loss_ce: 0.005880, loss_dice: 0.411364
[07:14:35.581] TRAIN: iteration 398 : loss : 0.209214, loss_ce: 0.007079, loss_dice: 0.411349
[07:14:35.789] TRAIN: iteration 399 : loss : 0.235962, loss_ce: 0.044986, loss_dice: 0.426937
[07:14:35.994] TRAIN: iteration 400 : loss : 0.208765, loss_ce: 0.007080, loss_dice: 0.410450
[07:14:39.767] TRAIN: iteration 401 : loss : 0.252251, loss_ce: 0.006848, loss_dice: 0.497654
[07:14:39.973] TRAIN: iteration 402 : loss : 0.230980, loss_ce: 0.011771, loss_dice: 0.450188
[07:14:47.663] TRAIN: iteration 403 : loss : 0.203524, loss_ce: 0.012711, loss_dice: 0.394337
[07:14:47.870] TRAIN: iteration 404 : loss : 0.234432, loss_ce: 0.006906, loss_dice: 0.461958
[07:14:48.076] TRAIN: iteration 405 : loss : 0.226775, loss_ce: 0.009572, loss_dice: 0.443979
[07:14:48.281] TRAIN: iteration 406 : loss : 0.183163, loss_ce: 0.010385, loss_dice: 0.355940
[07:14:48.486] TRAIN: iteration 407 : loss : 0.203832, loss_ce: 0.008591, loss_dice: 0.399072
[07:14:48.692] TRAIN: iteration 408 : loss : 0.181788, loss_ce: 0.007635, loss_dice: 0.355941
[07:14:53.728] TRAIN: iteration 409 : loss : 0.244644, loss_ce: 0.012094, loss_dice: 0.477194
[07:14:53.937] TRAIN: iteration 410 : loss : 0.214885, loss_ce: 0.008472, loss_dice: 0.421298
[07:15:00.857] TRAIN: iteration 411 : loss : 0.251108, loss_ce: 0.018222, loss_dice: 0.483994
[07:15:01.064] TRAIN: iteration 412 : loss : 0.205954, loss_ce: 0.004448, loss_dice: 0.407459
[07:15:01.269] TRAIN: iteration 413 : loss : 0.186775, loss_ce: 0.008404, loss_dice: 0.365146
[07:15:01.475] TRAIN: iteration 414 : loss : 0.230066, loss_ce: 0.008791, loss_dice: 0.451341
[07:15:01.682] TRAIN: iteration 415 : loss : 0.249836, loss_ce: 0.005043, loss_dice: 0.494630
[07:15:01.888] TRAIN: iteration 416 : loss : 0.235557, loss_ce: 0.005981, loss_dice: 0.465134
[07:15:06.570] TRAIN: iteration 417 : loss : 0.252771, loss_ce: 0.005369, loss_dice: 0.500173
[07:15:06.776] TRAIN: iteration 418 : loss : 0.158163, loss_ce: 0.007278, loss_dice: 0.309048
[07:15:13.902] TRAIN: iteration 419 : loss : 0.253201, loss_ce: 0.006188, loss_dice: 0.500215
[07:15:14.108] TRAIN: iteration 420 : loss : 0.154231, loss_ce: 0.007443, loss_dice: 0.301018
[07:15:14.347] TRAIN: iteration 421 : loss : 0.202497, loss_ce: 0.005774, loss_dice: 0.399220
[07:15:14.552] TRAIN: iteration 422 : loss : 0.250466, loss_ce: 0.007712, loss_dice: 0.493221
[07:15:14.757] TRAIN: iteration 423 : loss : 0.182272, loss_ce: 0.013093, loss_dice: 0.351451
[07:15:14.964] TRAIN: iteration 424 : loss : 0.211875, loss_ce: 0.012761, loss_dice: 0.410989
[07:15:19.646] TRAIN: iteration 425 : loss : 0.162620, loss_ce: 0.007756, loss_dice: 0.317485
[07:15:19.853] TRAIN: iteration 426 : loss : 0.234637, loss_ce: 0.022291, loss_dice: 0.446982
[07:15:26.808] TRAIN: iteration 427 : loss : 0.254067, loss_ce: 0.007793, loss_dice: 0.500340
[07:15:27.016] TRAIN: iteration 428 : loss : 0.254371, loss_ce: 0.008329, loss_dice: 0.500413
[07:15:27.221] TRAIN: iteration 429 : loss : 0.236337, loss_ce: 0.009652, loss_dice: 0.463021
[07:15:27.428] TRAIN: iteration 430 : loss : 0.255395, loss_ce: 0.010257, loss_dice: 0.500533
[07:15:27.634] TRAIN: iteration 431 : loss : 0.248015, loss_ce: 0.011260, loss_dice: 0.484771
[07:15:27.841] TRAIN: iteration 432 : loss : 0.232979, loss_ce: 0.010803, loss_dice: 0.455155
[07:15:32.089] TRAIN: iteration 433 : loss : 0.231104, loss_ce: 0.016476, loss_dice: 0.445731
[07:15:32.296] TRAIN: iteration 434 : loss : 0.195314, loss_ce: 0.024935, loss_dice: 0.365693
[07:15:39.077] TRAIN: iteration 435 : loss : 0.186685, loss_ce: 0.011761, loss_dice: 0.361609
[07:15:39.282] TRAIN: iteration 436 : loss : 0.199962, loss_ce: 0.011587, loss_dice: 0.388337
[07:15:39.487] TRAIN: iteration 437 : loss : 0.211030, loss_ce: 0.011907, loss_dice: 0.410152
[07:15:39.693] TRAIN: iteration 438 : loss : 0.207151, loss_ce: 0.011074, loss_dice: 0.403228
[07:15:39.898] TRAIN: iteration 439 : loss : 0.202584, loss_ce: 0.015075, loss_dice: 0.390094
[07:15:40.105] TRAIN: iteration 440 : loss : 0.223049, loss_ce: 0.034800, loss_dice: 0.411298
[07:15:45.150] TRAIN: iteration 441 : loss : 0.208590, loss_ce: 0.010304, loss_dice: 0.406876
[07:15:45.357] TRAIN: iteration 442 : loss : 0.208888, loss_ce: 0.007737, loss_dice: 0.410040
[07:15:51.514] TRAIN: iteration 443 : loss : 0.141537, loss_ce: 0.007798, loss_dice: 0.275277
[07:15:51.719] TRAIN: iteration 444 : loss : 0.155732, loss_ce: 0.008866, loss_dice: 0.302597
[07:15:51.925] TRAIN: iteration 445 : loss : 0.235749, loss_ce: 0.010319, loss_dice: 0.461178
[07:15:52.130] TRAIN: iteration 446 : loss : 0.209739, loss_ce: 0.010873, loss_dice: 0.408605
[07:15:52.337] TRAIN: iteration 447 : loss : 0.201408, loss_ce: 0.008927, loss_dice: 0.393889
[07:15:52.543] TRAIN: iteration 448 : loss : 0.176099, loss_ce: 0.008114, loss_dice: 0.344084
[07:15:58.737] TRAIN: iteration 449 : loss : 0.201253, loss_ce: 0.007831, loss_dice: 0.394675
[07:15:58.945] TRAIN: iteration 450 : loss : 0.240503, loss_ce: 0.028809, loss_dice: 0.452196
[07:16:04.097] TRAIN: iteration 451 : loss : 0.216283, loss_ce: 0.026577, loss_dice: 0.405989
[07:16:04.305] TRAIN: iteration 452 : loss : 0.232489, loss_ce: 0.013996, loss_dice: 0.450981
[07:16:04.512] TRAIN: iteration 453 : loss : 0.228722, loss_ce: 0.008639, loss_dice: 0.448806
[07:16:04.717] TRAIN: iteration 454 : loss : 0.186360, loss_ce: 0.010152, loss_dice: 0.362567
[07:16:04.923] TRAIN: iteration 455 : loss : 0.209522, loss_ce: 0.022951, loss_dice: 0.396092
[07:16:05.130] TRAIN: iteration 456 : loss : 0.157095, loss_ce: 0.011313, loss_dice: 0.302877
[07:16:11.624] TRAIN: iteration 457 : loss : 0.190824, loss_ce: 0.010126, loss_dice: 0.371521
[07:16:11.830] TRAIN: iteration 458 : loss : 0.227556, loss_ce: 0.008015, loss_dice: 0.447097
[07:16:16.192] TRAIN: iteration 459 : loss : 0.213411, loss_ce: 0.008742, loss_dice: 0.418080
[07:16:16.406] TRAIN: iteration 460 : loss : 0.233017, loss_ce: 0.006322, loss_dice: 0.459713
[07:16:16.640] TRAIN: iteration 461 : loss : 0.214606, loss_ce: 0.028387, loss_dice: 0.400825
[07:16:16.846] TRAIN: iteration 462 : loss : 0.161346, loss_ce: 0.007089, loss_dice: 0.315603
[07:16:17.054] TRAIN: iteration 463 : loss : 0.209044, loss_ce: 0.005466, loss_dice: 0.412622
[07:16:17.261] TRAIN: iteration 464 : loss : 0.167166, loss_ce: 0.008860, loss_dice: 0.325473
[07:16:24.443] TRAIN: iteration 465 : loss : 0.214046, loss_ce: 0.006514, loss_dice: 0.421578
[07:16:24.657] TRAIN: iteration 466 : loss : 0.246921, loss_ce: 0.008946, loss_dice: 0.484897
[07:16:28.190] TRAIN: iteration 467 : loss : 0.220035, loss_ce: 0.010364, loss_dice: 0.429707
[07:16:28.395] TRAIN: iteration 468 : loss : 0.243973, loss_ce: 0.011130, loss_dice: 0.476815
[07:16:28.606] TRAIN: iteration 469 : loss : 0.233590, loss_ce: 0.015566, loss_dice: 0.451613
[07:16:28.813] TRAIN: iteration 470 : loss : 0.177181, loss_ce: 0.008024, loss_dice: 0.346339
[07:16:29.019] TRAIN: iteration 471 : loss : 0.237427, loss_ce: 0.010847, loss_dice: 0.464007
[07:16:29.225] TRAIN: iteration 472 : loss : 0.217445, loss_ce: 0.018080, loss_dice: 0.416809
[07:16:36.786] TRAIN: iteration 473 : loss : 0.174072, loss_ce: 0.010438, loss_dice: 0.337705
[07:16:36.992] TRAIN: iteration 474 : loss : 0.186542, loss_ce: 0.011509, loss_dice: 0.361575
[07:16:40.350] TRAIN: iteration 475 : loss : 0.206455, loss_ce: 0.009034, loss_dice: 0.403876
[07:16:40.661] TRAIN: iteration 476 : loss : 0.157532, loss_ce: 0.010852, loss_dice: 0.304212
[07:16:40.869] TRAIN: iteration 477 : loss : 0.221238, loss_ce: 0.014994, loss_dice: 0.427481
[07:16:41.081] TRAIN: iteration 478 : loss : 0.218742, loss_ce: 0.009172, loss_dice: 0.428312
[07:16:41.288] TRAIN: iteration 479 : loss : 0.205044, loss_ce: 0.006334, loss_dice: 0.403755
[07:16:41.494] TRAIN: iteration 480 : loss : 0.252733, loss_ce: 0.005301, loss_dice: 0.500165
[07:16:48.544] TRAIN: iteration 481 : loss : 0.242532, loss_ce: 0.004952, loss_dice: 0.480111
[07:16:48.755] TRAIN: iteration 482 : loss : 0.251736, loss_ce: 0.003415, loss_dice: 0.500057
[07:16:52.577] TRAIN: iteration 483 : loss : 0.247549, loss_ce: 0.003473, loss_dice: 0.491625
[07:16:52.789] TRAIN: iteration 484 : loss : 0.225414, loss_ce: 0.011807, loss_dice: 0.439021
[07:16:52.995] TRAIN: iteration 485 : loss : 0.204882, loss_ce: 0.007905, loss_dice: 0.401859
[07:16:53.201] TRAIN: iteration 486 : loss : 0.229894, loss_ce: 0.015217, loss_dice: 0.444571
[07:16:53.411] TRAIN: iteration 487 : loss : 0.205505, loss_ce: 0.011329, loss_dice: 0.399681
[07:16:53.617] TRAIN: iteration 488 : loss : 0.244697, loss_ce: 0.029278, loss_dice: 0.460117
[07:17:00.347] TRAIN: iteration 489 : loss : 0.207224, loss_ce: 0.012385, loss_dice: 0.402063
[07:17:00.556] TRAIN: iteration 490 : loss : 0.147433, loss_ce: 0.011278, loss_dice: 0.283588
[07:17:05.528] TRAIN: iteration 491 : loss : 0.237605, loss_ce: 0.010261, loss_dice: 0.464950
[07:17:05.736] TRAIN: iteration 492 : loss : 0.191470, loss_ce: 0.011281, loss_dice: 0.371659
[07:17:05.942] TRAIN: iteration 493 : loss : 0.214090, loss_ce: 0.019700, loss_dice: 0.408479
[07:17:06.147] TRAIN: iteration 494 : loss : 0.255304, loss_ce: 0.010043, loss_dice: 0.500565
[07:17:06.354] TRAIN: iteration 495 : loss : 0.256056, loss_ce: 0.011451, loss_dice: 0.500660
[07:17:06.563] TRAIN: iteration 496 : loss : 0.155740, loss_ce: 0.012519, loss_dice: 0.298960
[07:17:11.615] TRAIN: iteration 497 : loss : 0.138301, loss_ce: 0.010410, loss_dice: 0.266192
[07:17:11.821] TRAIN: iteration 498 : loss : 0.184310, loss_ce: 0.009922, loss_dice: 0.358698
[07:17:18.776] TRAIN: iteration 499 : loss : 0.172186, loss_ce: 0.006242, loss_dice: 0.338131
[07:17:18.981] TRAIN: iteration 500 : loss : 0.275909, loss_ce: 0.055908, loss_dice: 0.495910
[07:17:19.217] TRAIN: iteration 501 : loss : 0.247888, loss_ce: 0.014069, loss_dice: 0.481707
[07:17:19.422] TRAIN: iteration 502 : loss : 0.249285, loss_ce: 0.008431, loss_dice: 0.490139
[07:17:19.628] TRAIN: iteration 503 : loss : 0.238375, loss_ce: 0.002792, loss_dice: 0.473957
[07:17:19.833] TRAIN: iteration 504 : loss : 0.246666, loss_ce: 0.005123, loss_dice: 0.488208
[07:17:23.719] TRAIN: iteration 505 : loss : 0.239161, loss_ce: 0.007987, loss_dice: 0.470334
[07:17:23.925] TRAIN: iteration 506 : loss : 0.252640, loss_ce: 0.012774, loss_dice: 0.492506
[07:17:31.746] TRAIN: iteration 507 : loss : 0.244288, loss_ce: 0.008441, loss_dice: 0.480135
[07:17:31.952] TRAIN: iteration 508 : loss : 0.252015, loss_ce: 0.004012, loss_dice: 0.500018
[07:17:32.157] TRAIN: iteration 509 : loss : 0.225865, loss_ce: 0.013195, loss_dice: 0.438535
[07:17:32.363] TRAIN: iteration 510 : loss : 0.230002, loss_ce: 0.008455, loss_dice: 0.451549
[07:17:32.568] TRAIN: iteration 511 : loss : 0.236917, loss_ce: 0.007733, loss_dice: 0.466100
[07:17:32.774] TRAIN: iteration 512 : loss : 0.206231, loss_ce: 0.013266, loss_dice: 0.399196
[07:17:36.684] TRAIN: iteration 513 : loss : 0.241776, loss_ce: 0.010462, loss_dice: 0.473090
[07:17:36.889] TRAIN: iteration 514 : loss : 0.222756, loss_ce: 0.013725, loss_dice: 0.431788
[07:17:43.602] TRAIN: iteration 515 : loss : 0.227174, loss_ce: 0.021589, loss_dice: 0.432758
[07:17:43.808] TRAIN: iteration 516 : loss : 0.235902, loss_ce: 0.014432, loss_dice: 0.457372
[07:17:44.013] TRAIN: iteration 517 : loss : 0.257347, loss_ce: 0.014238, loss_dice: 0.500456
[07:17:44.219] TRAIN: iteration 518 : loss : 0.253268, loss_ce: 0.014386, loss_dice: 0.492149
[07:17:44.424] TRAIN: iteration 519 : loss : 0.185009, loss_ce: 0.015530, loss_dice: 0.354488
[07:17:44.630] TRAIN: iteration 520 : loss : 0.180663, loss_ce: 0.025631, loss_dice: 0.335695
[07:17:49.644] TRAIN: iteration 521 : loss : 0.233538, loss_ce: 0.011991, loss_dice: 0.455086
[07:17:49.850] TRAIN: iteration 522 : loss : 0.171983, loss_ce: 0.009974, loss_dice: 0.333993
[07:17:55.339] TRAIN: iteration 523 : loss : 0.205523, loss_ce: 0.006484, loss_dice: 0.404561
[07:17:55.549] TRAIN: iteration 524 : loss : 0.191336, loss_ce: 0.021508, loss_dice: 0.361164
[07:17:55.755] TRAIN: iteration 525 : loss : 0.255927, loss_ce: 0.011413, loss_dice: 0.500442
[07:17:55.965] TRAIN: iteration 526 : loss : 0.206805, loss_ce: 0.016684, loss_dice: 0.396925
[07:17:56.346] TRAIN: iteration 527 : loss : 0.229702, loss_ce: 0.018029, loss_dice: 0.441375
[07:17:56.552] TRAIN: iteration 528 : loss : 0.206648, loss_ce: 0.017306, loss_dice: 0.395991
[07:18:02.022] TRAIN: iteration 529 : loss : 0.183514, loss_ce: 0.016576, loss_dice: 0.350451
[07:18:02.230] TRAIN: iteration 530 : loss : 0.225345, loss_ce: 0.013943, loss_dice: 0.436748
[07:18:06.735] TRAIN: iteration 531 : loss : 0.232881, loss_ce: 0.012430, loss_dice: 0.453333
[07:18:06.940] TRAIN: iteration 532 : loss : 0.190124, loss_ce: 0.012210, loss_dice: 0.368039
[07:18:07.146] TRAIN: iteration 533 : loss : 0.185461, loss_ce: 0.018276, loss_dice: 0.352646
[07:18:07.353] TRAIN: iteration 534 : loss : 0.239501, loss_ce: 0.011475, loss_dice: 0.467528
[07:18:07.559] TRAIN: iteration 535 : loss : 0.220708, loss_ce: 0.043179, loss_dice: 0.398238
[07:18:07.765] TRAIN: iteration 536 : loss : 0.152684, loss_ce: 0.009766, loss_dice: 0.295602
[07:18:13.697] TRAIN: iteration 537 : loss : 0.157384, loss_ce: 0.012682, loss_dice: 0.302085
[07:18:13.902] TRAIN: iteration 538 : loss : 0.169336, loss_ce: 0.010559, loss_dice: 0.328113
[07:18:19.920] TRAIN: iteration 539 : loss : 0.219372, loss_ce: 0.009064, loss_dice: 0.429679
[07:18:20.166] TRAIN: iteration 540 : loss : 0.254211, loss_ce: 0.008092, loss_dice: 0.500331
[07:18:20.384] TRAIN: iteration 541 : loss : 0.211351, loss_ce: 0.010751, loss_dice: 0.411951
[07:18:20.590] TRAIN: iteration 542 : loss : 0.249693, loss_ce: 0.012954, loss_dice: 0.486433
[07:18:20.796] TRAIN: iteration 543 : loss : 0.200740, loss_ce: 0.012320, loss_dice: 0.389161
[07:18:21.002] TRAIN: iteration 544 : loss : 0.253775, loss_ce: 0.007248, loss_dice: 0.500302
[07:18:25.866] TRAIN: iteration 545 : loss : 0.200839, loss_ce: 0.007628, loss_dice: 0.394051
[07:18:26.077] TRAIN: iteration 546 : loss : 0.228509, loss_ce: 0.008494, loss_dice: 0.448524
[07:18:33.161] TRAIN: iteration 547 : loss : 0.238045, loss_ce: 0.021755, loss_dice: 0.454335
[07:18:33.367] TRAIN: iteration 548 : loss : 0.232461, loss_ce: 0.006218, loss_dice: 0.458704
[07:18:33.573] TRAIN: iteration 549 : loss : 0.222156, loss_ce: 0.018263, loss_dice: 0.426049
[07:18:33.779] TRAIN: iteration 550 : loss : 0.255474, loss_ce: 0.016578, loss_dice: 0.494369
[07:18:33.986] TRAIN: iteration 551 : loss : 0.172908, loss_ce: 0.010548, loss_dice: 0.335269
[07:18:34.192] TRAIN: iteration 552 : loss : 0.233725, loss_ce: 0.024294, loss_dice: 0.443156
[07:18:38.459] TRAIN: iteration 553 : loss : 0.260620, loss_ce: 0.054840, loss_dice: 0.466400
[07:18:38.664] TRAIN: iteration 554 : loss : 0.192236, loss_ce: 0.020842, loss_dice: 0.363630
[07:18:45.416] TRAIN: iteration 555 : loss : 0.201481, loss_ce: 0.014508, loss_dice: 0.388453
[07:18:45.623] TRAIN: iteration 556 : loss : 0.210899, loss_ce: 0.011903, loss_dice: 0.409895
[07:18:45.828] TRAIN: iteration 557 : loss : 0.233421, loss_ce: 0.028199, loss_dice: 0.438643
[07:18:46.040] TRAIN: iteration 558 : loss : 0.264660, loss_ce: 0.033107, loss_dice: 0.496212
[07:18:46.252] TRAIN: iteration 559 : loss : 0.166735, loss_ce: 0.016179, loss_dice: 0.317290
[07:18:46.458] TRAIN: iteration 560 : loss : 0.191586, loss_ce: 0.019823, loss_dice: 0.363350
[07:18:49.882] TRAIN: iteration 561 : loss : 0.199226, loss_ce: 0.014359, loss_dice: 0.384094
[07:18:50.615] TRAIN: iteration 562 : loss : 0.165946, loss_ce: 0.015558, loss_dice: 0.316335
[07:18:56.564] TRAIN: iteration 563 : loss : 0.256295, loss_ce: 0.011963, loss_dice: 0.500628
[07:18:56.770] TRAIN: iteration 564 : loss : 0.231745, loss_ce: 0.031732, loss_dice: 0.431759
[07:18:56.977] TRAIN: iteration 565 : loss : 0.179874, loss_ce: 0.012905, loss_dice: 0.346842
[07:18:57.183] TRAIN: iteration 566 : loss : 0.194037, loss_ce: 0.014904, loss_dice: 0.373169
[07:18:57.389] TRAIN: iteration 567 : loss : 0.219193, loss_ce: 0.012637, loss_dice: 0.425748
[07:18:57.595] TRAIN: iteration 568 : loss : 0.220238, loss_ce: 0.036447, loss_dice: 0.404028
[07:19:01.507] TRAIN: iteration 569 : loss : 0.216144, loss_ce: 0.010676, loss_dice: 0.421613
[07:19:02.724] TRAIN: iteration 570 : loss : 0.254171, loss_ce: 0.008100, loss_dice: 0.500241
[07:19:08.377] TRAIN: iteration 571 : loss : 0.182581, loss_ce: 0.011119, loss_dice: 0.354043
[07:19:08.582] TRAIN: iteration 572 : loss : 0.244824, loss_ce: 0.014239, loss_dice: 0.475409
[07:19:08.788] TRAIN: iteration 573 : loss : 0.178436, loss_ce: 0.009321, loss_dice: 0.347552
[07:19:08.994] TRAIN: iteration 574 : loss : 0.192373, loss_ce: 0.015521, loss_dice: 0.369226
[07:19:09.200] TRAIN: iteration 575 : loss : 0.202924, loss_ce: 0.014291, loss_dice: 0.391556
[07:19:09.406] TRAIN: iteration 576 : loss : 0.255498, loss_ce: 0.010500, loss_dice: 0.500496
[07:19:14.808] TRAIN: iteration 577 : loss : 0.255910, loss_ce: 0.011280, loss_dice: 0.500540
[07:19:15.015] TRAIN: iteration 578 : loss : 0.212877, loss_ce: 0.022235, loss_dice: 0.403519
[07:19:20.292] TRAIN: iteration 579 : loss : 0.229431, loss_ce: 0.019333, loss_dice: 0.439530
[07:19:20.499] TRAIN: iteration 580 : loss : 0.201267, loss_ce: 0.018979, loss_dice: 0.383556
[07:19:20.733] TRAIN: iteration 581 : loss : 0.252469, loss_ce: 0.015182, loss_dice: 0.489757
[07:19:20.938] TRAIN: iteration 582 : loss : 0.207152, loss_ce: 0.016005, loss_dice: 0.398299
[07:19:21.150] TRAIN: iteration 583 : loss : 0.222560, loss_ce: 0.014257, loss_dice: 0.430863
[07:19:21.356] TRAIN: iteration 584 : loss : 0.221074, loss_ce: 0.012770, loss_dice: 0.429378
[07:19:26.367] TRAIN: iteration 585 : loss : 0.225131, loss_ce: 0.011379, loss_dice: 0.438884
[07:19:26.572] TRAIN: iteration 586 : loss : 0.231539, loss_ce: 0.013005, loss_dice: 0.450073
[07:19:32.763] TRAIN: iteration 587 : loss : 0.167695, loss_ce: 0.009840, loss_dice: 0.325550
[07:19:32.968] TRAIN: iteration 588 : loss : 0.207912, loss_ce: 0.007416, loss_dice: 0.408407
[07:19:33.174] TRAIN: iteration 589 : loss : 0.225554, loss_ce: 0.012001, loss_dice: 0.439107
[07:19:33.380] TRAIN: iteration 590 : loss : 0.175388, loss_ce: 0.014121, loss_dice: 0.336655
[07:19:33.586] TRAIN: iteration 591 : loss : 0.253250, loss_ce: 0.006286, loss_dice: 0.500214
[07:19:33.792] TRAIN: iteration 592 : loss : 0.203875, loss_ce: 0.012935, loss_dice: 0.394815
[07:19:37.486] TRAIN: iteration 593 : loss : 0.226724, loss_ce: 0.009304, loss_dice: 0.444144
[07:19:38.169] TRAIN: iteration 594 : loss : 0.165897, loss_ce: 0.009445, loss_dice: 0.322349
[07:19:45.241] TRAIN: iteration 595 : loss : 0.253945, loss_ce: 0.007522, loss_dice: 0.500367
[07:19:45.449] TRAIN: iteration 596 : loss : 0.272855, loss_ce: 0.068167, loss_dice: 0.477544
[07:19:45.657] TRAIN: iteration 597 : loss : 0.240187, loss_ce: 0.007332, loss_dice: 0.473042
[07:19:45.863] TRAIN: iteration 598 : loss : 0.144877, loss_ce: 0.008671, loss_dice: 0.281083
[07:19:46.070] TRAIN: iteration 599 : loss : 0.212013, loss_ce: 0.006735, loss_dice: 0.417291
[07:19:46.277] TRAIN: iteration 600 : loss : 0.244811, loss_ce: 0.005944, loss_dice: 0.483678
[07:19:49.107] TRAIN: iteration 601 : loss : 0.158053, loss_ce: 0.007403, loss_dice: 0.308703
[07:19:50.408] TRAIN: iteration 602 : loss : 0.131988, loss_ce: 0.007144, loss_dice: 0.256832
[07:19:57.021] TRAIN: iteration 603 : loss : 0.240849, loss_ce: 0.005354, loss_dice: 0.476345
[07:19:57.227] TRAIN: iteration 604 : loss : 0.189168, loss_ce: 0.015265, loss_dice: 0.363072
[07:19:57.467] TRAIN: iteration 605 : loss : 0.252795, loss_ce: 0.005364, loss_dice: 0.500226
[07:19:57.673] TRAIN: iteration 606 : loss : 0.198806, loss_ce: 0.008149, loss_dice: 0.389463
[07:19:57.879] TRAIN: iteration 607 : loss : 0.177640, loss_ce: 0.009510, loss_dice: 0.345769
[07:19:58.085] TRAIN: iteration 608 : loss : 0.193274, loss_ce: 0.009391, loss_dice: 0.377158
[07:20:02.118] TRAIN: iteration 609 : loss : 0.252466, loss_ce: 0.007740, loss_dice: 0.497192
[07:20:02.990] TRAIN: iteration 610 : loss : 0.213389, loss_ce: 0.011901, loss_dice: 0.414878
[07:20:09.247] TRAIN: iteration 611 : loss : 0.201362, loss_ce: 0.015792, loss_dice: 0.386932
[07:20:09.459] TRAIN: iteration 612 : loss : 0.237380, loss_ce: 0.016823, loss_dice: 0.457937
[07:20:09.664] TRAIN: iteration 613 : loss : 0.254742, loss_ce: 0.008991, loss_dice: 0.500494
[07:20:09.871] TRAIN: iteration 614 : loss : 0.194692, loss_ce: 0.009878, loss_dice: 0.379506
[07:20:10.077] TRAIN: iteration 615 : loss : 0.183111, loss_ce: 0.010344, loss_dice: 0.355878
[07:20:10.282] TRAIN: iteration 616 : loss : 0.212317, loss_ce: 0.011586, loss_dice: 0.413049
[07:20:13.272] TRAIN: iteration 617 : loss : 0.254146, loss_ce: 0.007879, loss_dice: 0.500412
[07:20:15.941] TRAIN: iteration 618 : loss : 0.223216, loss_ce: 0.013943, loss_dice: 0.432488
[07:20:21.415] TRAIN: iteration 619 : loss : 0.194418, loss_ce: 0.018247, loss_dice: 0.370589
[07:20:21.622] TRAIN: iteration 620 : loss : 0.236761, loss_ce: 0.014930, loss_dice: 0.458592
[07:20:21.858] TRAIN: iteration 621 : loss : 0.236912, loss_ce: 0.008958, loss_dice: 0.464867
[07:20:22.063] TRAIN: iteration 622 : loss : 0.254262, loss_ce: 0.008158, loss_dice: 0.500365
[07:20:22.269] TRAIN: iteration 623 : loss : 0.169857, loss_ce: 0.012883, loss_dice: 0.326831
[07:20:22.475] TRAIN: iteration 624 : loss : 0.253385, loss_ce: 0.009412, loss_dice: 0.497357
[07:20:25.947] TRAIN: iteration 625 : loss : 0.254968, loss_ce: 0.009456, loss_dice: 0.500480
[07:20:28.838] TRAIN: iteration 626 : loss : 0.244474, loss_ce: 0.011077, loss_dice: 0.477871
[07:20:32.314] TRAIN: iteration 627 : loss : 0.239412, loss_ce: 0.030345, loss_dice: 0.448479
[07:20:32.519] TRAIN: iteration 628 : loss : 0.190193, loss_ce: 0.012597, loss_dice: 0.367789
[07:20:33.091] TRAIN: iteration 629 : loss : 0.197999, loss_ce: 0.012046, loss_dice: 0.383951
[07:20:33.296] TRAIN: iteration 630 : loss : 0.204311, loss_ce: 0.008975, loss_dice: 0.399648
[07:20:33.503] TRAIN: iteration 631 : loss : 0.206386, loss_ce: 0.019314, loss_dice: 0.393458
[07:20:33.709] TRAIN: iteration 632 : loss : 0.253643, loss_ce: 0.007037, loss_dice: 0.500248
[07:20:37.381] TRAIN: iteration 633 : loss : 0.170889, loss_ce: 0.007222, loss_dice: 0.334555
[07:20:40.293] TRAIN: iteration 634 : loss : 0.177415, loss_ce: 0.010718, loss_dice: 0.344111
[07:20:45.763] TRAIN: iteration 635 : loss : 0.200852, loss_ce: 0.009240, loss_dice: 0.392464
[07:20:45.969] TRAIN: iteration 636 : loss : 0.206272, loss_ce: 0.010848, loss_dice: 0.401696
[07:20:46.175] TRAIN: iteration 637 : loss : 0.204752, loss_ce: 0.008850, loss_dice: 0.400655
[07:20:46.381] TRAIN: iteration 638 : loss : 0.199936, loss_ce: 0.009773, loss_dice: 0.390100
[07:20:46.587] TRAIN: iteration 639 : loss : 0.198739, loss_ce: 0.009955, loss_dice: 0.387522
[07:20:46.792] TRAIN: iteration 640 : loss : 0.152303, loss_ce: 0.008145, loss_dice: 0.296462
[07:20:49.044] TRAIN: iteration 641 : loss : 0.137810, loss_ce: 0.010797, loss_dice: 0.264823
[07:20:51.193] TRAIN: iteration 642 : loss : 0.151289, loss_ce: 0.008706, loss_dice: 0.293872
[07:20:58.000] TRAIN: iteration 643 : loss : 0.247294, loss_ce: 0.021107, loss_dice: 0.473481
[07:20:58.206] TRAIN: iteration 644 : loss : 0.252396, loss_ce: 0.004627, loss_dice: 0.500166
[07:20:58.566] TRAIN: iteration 645 : loss : 0.179156, loss_ce: 0.011704, loss_dice: 0.346608
[07:20:58.772] TRAIN: iteration 646 : loss : 0.206867, loss_ce: 0.005250, loss_dice: 0.408483
[07:20:58.978] TRAIN: iteration 647 : loss : 0.192354, loss_ce: 0.005721, loss_dice: 0.378987
[07:20:59.186] TRAIN: iteration 648 : loss : 0.245308, loss_ce: 0.009282, loss_dice: 0.481335
[07:21:01.409] TRAIN: iteration 649 : loss : 0.252810, loss_ce: 0.005581, loss_dice: 0.500040
[07:21:02.746] TRAIN: iteration 650 : loss : 0.245261, loss_ce: 0.004861, loss_dice: 0.485660
[07:21:09.659] TRAIN: iteration 651 : loss : 0.142826, loss_ce: 0.010296, loss_dice: 0.275356
[07:21:09.863] TRAIN: iteration 652 : loss : 0.199506, loss_ce: 0.024014, loss_dice: 0.374999
[07:21:11.138] TRAIN: iteration 653 : loss : 0.154848, loss_ce: 0.007402, loss_dice: 0.302294
[07:21:11.347] TRAIN: iteration 654 : loss : 0.185762, loss_ce: 0.008723, loss_dice: 0.362801
[07:21:11.553] TRAIN: iteration 655 : loss : 0.253682, loss_ce: 0.006989, loss_dice: 0.500375
[07:21:11.759] TRAIN: iteration 656 : loss : 0.182456, loss_ce: 0.008461, loss_dice: 0.356451
[07:21:13.100] TRAIN: iteration 657 : loss : 0.253116, loss_ce: 0.005920, loss_dice: 0.500311
[07:21:15.252] TRAIN: iteration 658 : loss : 0.252877, loss_ce: 0.005492, loss_dice: 0.500262
[07:21:22.539] TRAIN: iteration 659 : loss : 0.151743, loss_ce: 0.006560, loss_dice: 0.296927
[07:21:22.745] TRAIN: iteration 660 : loss : 0.246270, loss_ce: 0.006106, loss_dice: 0.486434
[07:21:22.983] TRAIN: iteration 661 : loss : 0.252529, loss_ce: 0.006007, loss_dice: 0.499052
[07:21:23.190] TRAIN: iteration 662 : loss : 0.252222, loss_ce: 0.004269, loss_dice: 0.500176
[07:21:23.397] TRAIN: iteration 663 : loss : 0.244840, loss_ce: 0.029348, loss_dice: 0.460332
[07:21:23.603] TRAIN: iteration 664 : loss : 0.171495, loss_ce: 0.014252, loss_dice: 0.328738
[07:21:25.264] TRAIN: iteration 665 : loss : 0.251785, loss_ce: 0.005372, loss_dice: 0.498197
[07:21:27.337] TRAIN: iteration 666 : loss : 0.145785, loss_ce: 0.006854, loss_dice: 0.284716
[07:21:34.043] TRAIN: iteration 667 : loss : 0.202125, loss_ce: 0.009317, loss_dice: 0.394934
[07:21:34.250] TRAIN: iteration 668 : loss : 0.177459, loss_ce: 0.007207, loss_dice: 0.347711
[07:21:35.653] TRAIN: iteration 669 : loss : 0.245731, loss_ce: 0.009368, loss_dice: 0.482094
[07:21:35.859] TRAIN: iteration 670 : loss : 0.214066, loss_ce: 0.011327, loss_dice: 0.416805
[07:21:36.065] TRAIN: iteration 671 : loss : 0.180936, loss_ce: 0.028688, loss_dice: 0.333184
[07:21:36.272] TRAIN: iteration 672 : loss : 0.224310, loss_ce: 0.005157, loss_dice: 0.443464
[07:21:37.771] TRAIN: iteration 673 : loss : 0.242054, loss_ce: 0.011057, loss_dice: 0.473050
[07:21:40.072] TRAIN: iteration 674 : loss : 0.244292, loss_ce: 0.009839, loss_dice: 0.478745
[07:21:45.856] TRAIN: iteration 675 : loss : 0.173199, loss_ce: 0.011734, loss_dice: 0.334664
[07:21:46.067] TRAIN: iteration 676 : loss : 0.241166, loss_ce: 0.005136, loss_dice: 0.477196
[07:21:48.520] TRAIN: iteration 677 : loss : 0.230199, loss_ce: 0.018611, loss_dice: 0.441786
[07:21:48.727] TRAIN: iteration 678 : loss : 0.230263, loss_ce: 0.009818, loss_dice: 0.450708
[07:21:48.933] TRAIN: iteration 679 : loss : 0.253675, loss_ce: 0.009196, loss_dice: 0.498154
[07:21:49.139] TRAIN: iteration 680 : loss : 0.191969, loss_ce: 0.009391, loss_dice: 0.374547
[07:21:49.853] TRAIN: iteration 681 : loss : 0.155236, loss_ce: 0.006107, loss_dice: 0.304365
[07:21:52.474] TRAIN: iteration 682 : loss : 0.252639, loss_ce: 0.005135, loss_dice: 0.500142
[07:21:57.187] TRAIN: iteration 683 : loss : 0.173018, loss_ce: 0.019120, loss_dice: 0.326915
[07:21:58.281] TRAIN: iteration 684 : loss : 0.224827, loss_ce: 0.006926, loss_dice: 0.442728
[07:22:01.291] TRAIN: iteration 685 : loss : 0.228938, loss_ce: 0.007181, loss_dice: 0.450696
[07:22:01.501] TRAIN: iteration 686 : loss : 0.218588, loss_ce: 0.009186, loss_dice: 0.427989
[07:22:01.709] TRAIN: iteration 687 : loss : 0.166155, loss_ce: 0.010158, loss_dice: 0.322152
[07:22:01.915] TRAIN: iteration 688 : loss : 0.243213, loss_ce: 0.010363, loss_dice: 0.476062
[07:22:02.121] TRAIN: iteration 689 : loss : 0.254704, loss_ce: 0.008973, loss_dice: 0.500435
[07:22:05.534] TRAIN: iteration 690 : loss : 0.254489, loss_ce: 0.009843, loss_dice: 0.499136
[07:22:09.435] TRAIN: iteration 691 : loss : 0.200196, loss_ce: 0.015089, loss_dice: 0.385303
[07:22:11.141] TRAIN: iteration 692 : loss : 0.157317, loss_ce: 0.010593, loss_dice: 0.304040
[07:22:13.336] TRAIN: iteration 693 : loss : 0.163275, loss_ce: 0.010257, loss_dice: 0.316293
[07:22:13.549] TRAIN: iteration 694 : loss : 0.140302, loss_ce: 0.014169, loss_dice: 0.266435
[07:22:13.755] TRAIN: iteration 695 : loss : 0.226785, loss_ce: 0.008868, loss_dice: 0.444701
[07:22:13.962] TRAIN: iteration 696 : loss : 0.180030, loss_ce: 0.017463, loss_dice: 0.342597
[07:22:14.777] TRAIN: iteration 697 : loss : 0.154786, loss_ce: 0.008968, loss_dice: 0.300604
[07:22:17.271] TRAIN: iteration 698 : loss : 0.199369, loss_ce: 0.008413, loss_dice: 0.390324
[07:22:21.712] TRAIN: iteration 699 : loss : 0.216120, loss_ce: 0.008902, loss_dice: 0.423337
[07:22:24.058] TRAIN: iteration 700 : loss : 0.242889, loss_ce: 0.007748, loss_dice: 0.478029
[07:22:25.539] TRAIN: iteration 701 : loss : 0.202885, loss_ce: 0.016848, loss_dice: 0.388921
[07:22:25.745] TRAIN: iteration 702 : loss : 0.239510, loss_ce: 0.005952, loss_dice: 0.473067
[07:22:25.951] TRAIN: iteration 703 : loss : 0.245449, loss_ce: 0.013431, loss_dice: 0.477467
[07:22:26.156] TRAIN: iteration 704 : loss : 0.211679, loss_ce: 0.006361, loss_dice: 0.416997
[07:22:27.006] TRAIN: iteration 705 : loss : 0.243735, loss_ce: 0.015038, loss_dice: 0.472431
[07:22:29.166] TRAIN: iteration 706 : loss : 0.209886, loss_ce: 0.006353, loss_dice: 0.413420
[07:22:32.996] TRAIN: iteration 707 : loss : 0.192847, loss_ce: 0.011734, loss_dice: 0.373960
[07:22:35.989] TRAIN: iteration 708 : loss : 0.137616, loss_ce: 0.007200, loss_dice: 0.268032
[07:22:38.553] TRAIN: iteration 709 : loss : 0.235328, loss_ce: 0.034395, loss_dice: 0.436261
[07:22:38.759] TRAIN: iteration 710 : loss : 0.242628, loss_ce: 0.007191, loss_dice: 0.478064
[07:22:38.965] TRAIN: iteration 711 : loss : 0.170947, loss_ce: 0.006859, loss_dice: 0.335034
[07:22:39.170] TRAIN: iteration 712 : loss : 0.184099, loss_ce: 0.012521, loss_dice: 0.355678
[07:22:39.376] TRAIN: iteration 713 : loss : 0.252884, loss_ce: 0.005584, loss_dice: 0.500185
[07:22:40.929] TRAIN: iteration 714 : loss : 0.124631, loss_ce: 0.010230, loss_dice: 0.239033
[07:22:45.067] TRAIN: iteration 715 : loss : 0.236814, loss_ce: 0.006613, loss_dice: 0.467015
[07:22:48.068] TRAIN: iteration 716 : loss : 0.190387, loss_ce: 0.008525, loss_dice: 0.372248
[07:22:51.463] TRAIN: iteration 717 : loss : 0.189842, loss_ce: 0.006525, loss_dice: 0.373160
[07:22:51.669] TRAIN: iteration 718 : loss : 0.238468, loss_ce: 0.006018, loss_dice: 0.470918
[07:22:51.875] TRAIN: iteration 719 : loss : 0.145779, loss_ce: 0.006901, loss_dice: 0.284657
[07:22:52.081] TRAIN: iteration 720 : loss : 0.252219, loss_ce: 0.004309, loss_dice: 0.500128
[07:22:52.333] TRAIN: iteration 721 : loss : 0.238222, loss_ce: 0.034537, loss_dice: 0.441907
[07:22:54.440] TRAIN: iteration 722 : loss : 0.207895, loss_ce: 0.007046, loss_dice: 0.408744
[07:22:57.396] TRAIN: iteration 723 : loss : 0.159154, loss_ce: 0.007743, loss_dice: 0.310564
[07:23:01.289] TRAIN: iteration 724 : loss : 0.207392, loss_ce: 0.005538, loss_dice: 0.409245
[07:23:04.536] TRAIN: iteration 725 : loss : 0.136619, loss_ce: 0.008095, loss_dice: 0.265142
[07:23:04.744] TRAIN: iteration 726 : loss : 0.257058, loss_ce: 0.014833, loss_dice: 0.499282
[07:23:04.950] TRAIN: iteration 727 : loss : 0.178735, loss_ce: 0.008359, loss_dice: 0.349110
[07:23:05.158] TRAIN: iteration 728 : loss : 0.175755, loss_ce: 0.019469, loss_dice: 0.332042
[07:23:05.364] TRAIN: iteration 729 : loss : 0.223446, loss_ce: 0.008221, loss_dice: 0.438670
[07:23:06.895] TRAIN: iteration 730 : loss : 0.177372, loss_ce: 0.011276, loss_dice: 0.343468
[07:23:09.302] TRAIN: iteration 731 : loss : 0.194447, loss_ce: 0.014652, loss_dice: 0.374241
[07:23:13.928] TRAIN: iteration 732 : loss : 0.169376, loss_ce: 0.010255, loss_dice: 0.328496
[07:23:18.577] TRAIN: iteration 733 : loss : 0.220758, loss_ce: 0.027698, loss_dice: 0.413818
[07:23:18.783] TRAIN: iteration 734 : loss : 0.239257, loss_ce: 0.007815, loss_dice: 0.470699
[07:23:18.991] TRAIN: iteration 735 : loss : 0.253201, loss_ce: 0.007453, loss_dice: 0.498949
[07:23:19.198] TRAIN: iteration 736 : loss : 0.194147, loss_ce: 0.015750, loss_dice: 0.372544
[07:23:19.403] TRAIN: iteration 737 : loss : 0.249327, loss_ce: 0.009040, loss_dice: 0.489614
[07:23:19.609] TRAIN: iteration 738 : loss : 0.254552, loss_ce: 0.008625, loss_dice: 0.500479
[07:23:21.503] TRAIN: iteration 739 : loss : 0.248511, loss_ce: 0.012074, loss_dice: 0.484948
[07:23:25.992] TRAIN: iteration 740 : loss : 0.137154, loss_ce: 0.013729, loss_dice: 0.260580
[07:23:25.993] NaN or Inf found in input tensor.
[07:23:30.354] TRAIN: iteration 741 : loss : 0.254199, loss_ce: 0.008008, loss_dice: 0.500391
[07:23:30.565] TRAIN: iteration 742 : loss : 0.253516, loss_ce: 0.009128, loss_dice: 0.497903
[07:23:30.770] TRAIN: iteration 743 : loss : 0.141434, loss_ce: 0.011152, loss_dice: 0.271715
[07:23:30.976] TRAIN: iteration 744 : loss : 0.219622, loss_ce: 0.013991, loss_dice: 0.425253
[07:23:31.182] TRAIN: iteration 745 : loss : 0.212338, loss_ce: 0.017841, loss_dice: 0.406834
[07:23:31.387] TRAIN: iteration 746 : loss : 0.234430, loss_ce: 0.008378, loss_dice: 0.460482
[07:23:35.219] TRAIN: iteration 747 : loss : 0.219621, loss_ce: 0.016231, loss_dice: 0.423010
[07:23:39.430] TRAIN: iteration 748 : loss : 0.241164, loss_ce: 0.016344, loss_dice: 0.465984
[07:23:41.711] TRAIN: iteration 749 : loss : 0.252704, loss_ce: 0.006993, loss_dice: 0.498416
[07:23:41.917] TRAIN: iteration 750 : loss : 0.190191, loss_ce: 0.008306, loss_dice: 0.372076
[07:23:42.125] TRAIN: iteration 751 : loss : 0.177009, loss_ce: 0.007531, loss_dice: 0.346486
[07:23:42.331] TRAIN: iteration 752 : loss : 0.253172, loss_ce: 0.006085, loss_dice: 0.500259
[07:23:42.538] TRAIN: iteration 753 : loss : 0.236621, loss_ce: 0.005612, loss_dice: 0.467630
[07:23:43.876] TRAIN: iteration 754 : loss : 0.252244, loss_ce: 0.004380, loss_dice: 0.500107
[07:23:48.278] TRAIN: iteration 755 : loss : 0.252037, loss_ce: 0.003979, loss_dice: 0.500095
[07:23:51.604] TRAIN: iteration 756 : loss : 0.245379, loss_ce: 0.008173, loss_dice: 0.482586
[07:23:54.760] TRAIN: iteration 757 : loss : 0.204408, loss_ce: 0.007962, loss_dice: 0.400855
[07:23:54.965] TRAIN: iteration 758 : loss : 0.228884, loss_ce: 0.046246, loss_dice: 0.411522
[07:23:55.171] TRAIN: iteration 759 : loss : 0.252055, loss_ce: 0.004022, loss_dice: 0.500087
[07:23:55.377] TRAIN: iteration 760 : loss : 0.209290, loss_ce: 0.005327, loss_dice: 0.413254
[07:23:55.613] TRAIN: iteration 761 : loss : 0.211291, loss_ce: 0.025086, loss_dice: 0.397496
[07:23:55.819] TRAIN: iteration 762 : loss : 0.166981, loss_ce: 0.013907, loss_dice: 0.320054
[07:24:00.550] TRAIN: iteration 763 : loss : 0.193665, loss_ce: 0.008710, loss_dice: 0.378619
[07:24:04.991] TRAIN: iteration 764 : loss : 0.254484, loss_ce: 0.008542, loss_dice: 0.500426
[07:24:06.185] TRAIN: iteration 765 : loss : 0.255466, loss_ce: 0.010525, loss_dice: 0.500407
[07:24:06.391] TRAIN: iteration 766 : loss : 0.248490, loss_ce: 0.013698, loss_dice: 0.483283
[07:24:06.597] TRAIN: iteration 767 : loss : 0.208930, loss_ce: 0.010561, loss_dice: 0.407298
[07:24:06.803] TRAIN: iteration 768 : loss : 0.206985, loss_ce: 0.015243, loss_dice: 0.398726
[07:24:07.009] TRAIN: iteration 769 : loss : 0.239013, loss_ce: 0.018166, loss_dice: 0.459860
[07:24:07.502] TRAIN: iteration 770 : loss : 0.258379, loss_ce: 0.019387, loss_dice: 0.497372
[07:24:13.221] TRAIN: iteration 771 : loss : 0.199274, loss_ce: 0.029115, loss_dice: 0.369434
[07:24:16.895] TRAIN: iteration 772 : loss : 0.219238, loss_ce: 0.012641, loss_dice: 0.425835
[07:24:19.676] TRAIN: iteration 773 : loss : 0.237011, loss_ce: 0.010561, loss_dice: 0.463461
[07:24:19.881] TRAIN: iteration 774 : loss : 0.158720, loss_ce: 0.013330, loss_dice: 0.304110
[07:24:20.086] TRAIN: iteration 775 : loss : 0.212470, loss_ce: 0.010767, loss_dice: 0.414173
[07:24:20.294] TRAIN: iteration 776 : loss : 0.186696, loss_ce: 0.010944, loss_dice: 0.362447
[07:24:20.499] TRAIN: iteration 777 : loss : 0.188865, loss_ce: 0.021079, loss_dice: 0.356652
[07:24:20.704] TRAIN: iteration 778 : loss : 0.234179, loss_ce: 0.010786, loss_dice: 0.457572
[07:24:25.355] TRAIN: iteration 779 : loss : 0.204977, loss_ce: 0.007657, loss_dice: 0.402296
[07:24:29.361] TRAIN: iteration 780 : loss : 0.196327, loss_ce: 0.016030, loss_dice: 0.376625
[07:24:31.752] TRAIN: iteration 781 : loss : 0.136036, loss_ce: 0.007410, loss_dice: 0.264662
[07:24:31.957] TRAIN: iteration 782 : loss : 0.248776, loss_ce: 0.005340, loss_dice: 0.492212
[07:24:32.164] TRAIN: iteration 783 : loss : 0.196122, loss_ce: 0.005165, loss_dice: 0.387078
[07:24:32.369] TRAIN: iteration 784 : loss : 0.219371, loss_ce: 0.010120, loss_dice: 0.428623
[07:24:32.575] TRAIN: iteration 785 : loss : 0.206259, loss_ce: 0.013734, loss_dice: 0.398784
[07:24:33.111] TRAIN: iteration 786 : loss : 0.240836, loss_ce: 0.006069, loss_dice: 0.475604
[07:24:38.025] TRAIN: iteration 787 : loss : 0.252117, loss_ce: 0.004149, loss_dice: 0.500086
[07:24:41.483] TRAIN: iteration 788 : loss : 0.214215, loss_ce: 0.007649, loss_dice: 0.420781
[07:24:44.017] TRAIN: iteration 789 : loss : 0.217868, loss_ce: 0.006559, loss_dice: 0.429176
[07:24:44.223] TRAIN: iteration 790 : loss : 0.212506, loss_ce: 0.017165, loss_dice: 0.407846
[07:24:44.429] TRAIN: iteration 791 : loss : 0.178117, loss_ce: 0.009386, loss_dice: 0.346848
[07:24:44.635] TRAIN: iteration 792 : loss : 0.237116, loss_ce: 0.009705, loss_dice: 0.464526
[07:24:44.841] TRAIN: iteration 793 : loss : 0.209470, loss_ce: 0.009316, loss_dice: 0.409625
[07:24:46.058] TRAIN: iteration 794 : loss : 0.183796, loss_ce: 0.028471, loss_dice: 0.339121
[07:24:50.231] TRAIN: iteration 795 : loss : 0.181775, loss_ce: 0.012589, loss_dice: 0.350961
[07:24:54.676] TRAIN: iteration 796 : loss : 0.240226, loss_ce: 0.010117, loss_dice: 0.470335
[07:24:56.247] TRAIN: iteration 797 : loss : 0.255142, loss_ce: 0.009836, loss_dice: 0.500448
[07:24:56.452] TRAIN: iteration 798 : loss : 0.206735, loss_ce: 0.009881, loss_dice: 0.403588
[07:24:56.656] TRAIN: iteration 799 : loss : 0.139429, loss_ce: 0.014297, loss_dice: 0.264562
[07:24:56.862] TRAIN: iteration 800 : loss : 0.126020, loss_ce: 0.010453, loss_dice: 0.241586
[07:24:57.097] TRAIN: iteration 801 : loss : 0.175685, loss_ce: 0.009337, loss_dice: 0.342034
[07:24:58.472] TRAIN: iteration 802 : loss : 0.233424, loss_ce: 0.008008, loss_dice: 0.458841
[07:25:03.505] TRAIN: iteration 803 : loss : 0.147830, loss_ce: 0.008987, loss_dice: 0.286674
[07:25:07.837] TRAIN: iteration 804 : loss : 0.150187, loss_ce: 0.009499, loss_dice: 0.290875
[07:25:08.053] TRAIN: iteration 805 : loss : 0.190994, loss_ce: 0.013560, loss_dice: 0.368429
[07:25:08.261] TRAIN: iteration 806 : loss : 0.221483, loss_ce: 0.023943, loss_dice: 0.419023
[07:25:08.468] TRAIN: iteration 807 : loss : 0.227460, loss_ce: 0.008844, loss_dice: 0.446075
[07:25:08.673] TRAIN: iteration 808 : loss : 0.234872, loss_ce: 0.006554, loss_dice: 0.463189
[07:25:08.878] TRAIN: iteration 809 : loss : 0.253658, loss_ce: 0.007005, loss_dice: 0.500310
[07:25:11.601] TRAIN: iteration 810 : loss : 0.206597, loss_ce: 0.009784, loss_dice: 0.403410
[07:25:14.983] TRAIN: iteration 811 : loss : 0.170867, loss_ce: 0.007134, loss_dice: 0.334601
[07:25:19.782] TRAIN: iteration 812 : loss : 0.207311, loss_ce: 0.006375, loss_dice: 0.408247
[07:25:19.997] TRAIN: iteration 813 : loss : 0.193521, loss_ce: 0.013531, loss_dice: 0.373511
[07:25:20.219] TRAIN: iteration 814 : loss : 0.203117, loss_ce: 0.008918, loss_dice: 0.397317
[07:25:20.425] TRAIN: iteration 815 : loss : 0.136843, loss_ce: 0.005113, loss_dice: 0.268572
[07:25:20.632] TRAIN: iteration 816 : loss : 0.161973, loss_ce: 0.005551, loss_dice: 0.318395
[07:25:20.838] TRAIN: iteration 817 : loss : 0.158555, loss_ce: 0.007004, loss_dice: 0.310105
[07:25:24.847] TRAIN: iteration 818 : loss : 0.206803, loss_ce: 0.005026, loss_dice: 0.408580
[07:25:27.286] TRAIN: iteration 819 : loss : 0.219536, loss_ce: 0.006501, loss_dice: 0.432571
[07:25:31.482] TRAIN: iteration 820 : loss : 0.238230, loss_ce: 0.005172, loss_dice: 0.471288
[07:25:31.860] TRAIN: iteration 821 : loss : 0.246248, loss_ce: 0.005267, loss_dice: 0.487229
[07:25:32.066] TRAIN: iteration 822 : loss : 0.159807, loss_ce: 0.012572, loss_dice: 0.307042
[07:25:32.272] TRAIN: iteration 823 : loss : 0.192802, loss_ce: 0.016807, loss_dice: 0.368796
[07:25:32.478] TRAIN: iteration 824 : loss : 0.241697, loss_ce: 0.016980, loss_dice: 0.466413
[07:25:32.685] TRAIN: iteration 825 : loss : 0.165640, loss_ce: 0.010401, loss_dice: 0.320880
[07:25:37.735] TRAIN: iteration 826 : loss : 0.180253, loss_ce: 0.010524, loss_dice: 0.349981
[07:25:39.558] TRAIN: iteration 827 : loss : 0.172097, loss_ce: 0.010139, loss_dice: 0.334055
[07:25:42.932] TRAIN: iteration 828 : loss : 0.251362, loss_ce: 0.008904, loss_dice: 0.493820
[07:25:43.915] TRAIN: iteration 829 : loss : 0.181240, loss_ce: 0.009092, loss_dice: 0.353388
[07:25:44.120] TRAIN: iteration 830 : loss : 0.154975, loss_ce: 0.010049, loss_dice: 0.299902
[07:25:44.326] TRAIN: iteration 831 : loss : 0.253770, loss_ce: 0.007171, loss_dice: 0.500370
[07:25:44.537] TRAIN: iteration 832 : loss : 0.210038, loss_ce: 0.010158, loss_dice: 0.409918
[07:25:44.806] TRAIN: iteration 833 : loss : 0.196857, loss_ce: 0.007639, loss_dice: 0.386075
[07:25:49.875] TRAIN: iteration 834 : loss : 0.234751, loss_ce: 0.010618, loss_dice: 0.458884
[07:25:51.223] TRAIN: iteration 835 : loss : 0.247922, loss_ce: 0.006062, loss_dice: 0.489781
[07:25:55.821] TRAIN: iteration 836 : loss : 0.155110, loss_ce: 0.006457, loss_dice: 0.303763
[07:25:56.027] TRAIN: iteration 837 : loss : 0.238078, loss_ce: 0.005845, loss_dice: 0.470311
[07:25:56.234] TRAIN: iteration 838 : loss : 0.252460, loss_ce: 0.004740, loss_dice: 0.500181
[07:25:56.439] TRAIN: iteration 839 : loss : 0.147689, loss_ce: 0.010120, loss_dice: 0.285257
[07:25:56.645] TRAIN: iteration 840 : loss : 0.252532, loss_ce: 0.005575, loss_dice: 0.499489
[07:25:57.141] TRAIN: iteration 841 : loss : 0.183348, loss_ce: 0.013003, loss_dice: 0.353693
[07:26:03.162] TRAIN: iteration 842 : loss : 0.199157, loss_ce: 0.031909, loss_dice: 0.366406
[07:26:03.369] TRAIN: iteration 843 : loss : 0.260405, loss_ce: 0.021083, loss_dice: 0.499728
[07:26:08.104] TRAIN: iteration 844 : loss : 0.252746, loss_ce: 0.005267, loss_dice: 0.500226
[07:26:08.310] TRAIN: iteration 845 : loss : 0.172242, loss_ce: 0.007228, loss_dice: 0.337256
[07:26:12.305] TRAIN: iteration 846 : loss : 0.255747, loss_ce: 0.012608, loss_dice: 0.498885
[07:26:12.513] TRAIN: iteration 847 : loss : 0.236358, loss_ce: 0.011742, loss_dice: 0.460973
[07:26:12.718] TRAIN: iteration 848 : loss : 0.253671, loss_ce: 0.006977, loss_dice: 0.500365
[07:26:12.923] TRAIN: iteration 849 : loss : 0.126811, loss_ce: 0.011159, loss_dice: 0.242464
[07:26:15.988] TRAIN: iteration 850 : loss : 0.220833, loss_ce: 0.046967, loss_dice: 0.394700
[07:26:16.196] TRAIN: iteration 851 : loss : 0.159486, loss_ce: 0.011059, loss_dice: 0.307912
[07:26:19.606] TRAIN: iteration 852 : loss : 0.247910, loss_ce: 0.011745, loss_dice: 0.484076
[07:26:20.655] TRAIN: iteration 853 : loss : 0.219286, loss_ce: 0.009990, loss_dice: 0.428581
[07:26:24.037] TRAIN: iteration 854 : loss : 0.178742, loss_ce: 0.017681, loss_dice: 0.339803
[07:26:24.243] TRAIN: iteration 855 : loss : 0.254905, loss_ce: 0.009348, loss_dice: 0.500462
[07:26:24.448] TRAIN: iteration 856 : loss : 0.159115, loss_ce: 0.010771, loss_dice: 0.307459
[07:26:24.653] TRAIN: iteration 857 : loss : 0.135885, loss_ce: 0.009761, loss_dice: 0.262009
[07:26:28.291] TRAIN: iteration 858 : loss : 0.159067, loss_ce: 0.009151, loss_dice: 0.308982
[07:26:30.471] TRAIN: iteration 859 : loss : 0.204290, loss_ce: 0.008315, loss_dice: 0.400266
[07:26:32.910] TRAIN: iteration 860 : loss : 0.224975, loss_ce: 0.008736, loss_dice: 0.441213
[07:26:33.146] TRAIN: iteration 861 : loss : 0.179877, loss_ce: 0.014749, loss_dice: 0.345004
[07:26:36.639] TRAIN: iteration 862 : loss : 0.232268, loss_ce: 0.005392, loss_dice: 0.459143
[07:26:36.845] TRAIN: iteration 863 : loss : 0.146882, loss_ce: 0.007080, loss_dice: 0.286683
[07:26:37.052] TRAIN: iteration 864 : loss : 0.155560, loss_ce: 0.004618, loss_dice: 0.306502
[07:26:37.258] TRAIN: iteration 865 : loss : 0.199572, loss_ce: 0.012996, loss_dice: 0.386149
[07:26:41.231] TRAIN: iteration 866 : loss : 0.152745, loss_ce: 0.008837, loss_dice: 0.296654
[07:26:43.501] TRAIN: iteration 867 : loss : 0.244607, loss_ce: 0.003379, loss_dice: 0.485835
[07:26:45.044] TRAIN: iteration 868 : loss : 0.251106, loss_ce: 0.003178, loss_dice: 0.499035
[07:26:45.577] TRAIN: iteration 869 : loss : 0.220937, loss_ce: 0.035099, loss_dice: 0.406776
[07:26:48.889] TRAIN: iteration 870 : loss : 0.202576, loss_ce: 0.009766, loss_dice: 0.395386
[07:26:49.096] TRAIN: iteration 871 : loss : 0.152086, loss_ce: 0.008513, loss_dice: 0.295659
[07:26:49.303] TRAIN: iteration 872 : loss : 0.241198, loss_ce: 0.009540, loss_dice: 0.472855
[07:26:49.509] TRAIN: iteration 873 : loss : 0.252761, loss_ce: 0.006880, loss_dice: 0.498643
[07:26:52.887] TRAIN: iteration 874 : loss : 0.173453, loss_ce: 0.020759, loss_dice: 0.326148
[07:26:55.770] TRAIN: iteration 875 : loss : 0.249129, loss_ce: 0.006447, loss_dice: 0.491810
[07:26:57.990] TRAIN: iteration 876 : loss : 0.194066, loss_ce: 0.008798, loss_dice: 0.379334
[07:26:58.200] TRAIN: iteration 877 : loss : 0.242139, loss_ce: 0.008791, loss_dice: 0.475487
[07:27:01.229] TRAIN: iteration 878 : loss : 0.204342, loss_ce: 0.010517, loss_dice: 0.398167
[07:27:01.434] TRAIN: iteration 879 : loss : 0.223305, loss_ce: 0.009334, loss_dice: 0.437276
[07:27:01.640] TRAIN: iteration 880 : loss : 0.252354, loss_ce: 0.008674, loss_dice: 0.496035
[07:27:01.881] TRAIN: iteration 881 : loss : 0.224506, loss_ce: 0.011082, loss_dice: 0.437930
[07:27:04.561] TRAIN: iteration 882 : loss : 0.184782, loss_ce: 0.019188, loss_dice: 0.350376
[07:27:07.926] TRAIN: iteration 883 : loss : 0.221411, loss_ce: 0.010559, loss_dice: 0.432264
[07:27:10.140] TRAIN: iteration 884 : loss : 0.218697, loss_ce: 0.009980, loss_dice: 0.427414
[07:27:10.349] TRAIN: iteration 885 : loss : 0.186169, loss_ce: 0.009619, loss_dice: 0.362719
[07:27:13.715] TRAIN: iteration 886 : loss : 0.218595, loss_ce: 0.009137, loss_dice: 0.428052
[07:27:13.923] TRAIN: iteration 887 : loss : 0.238440, loss_ce: 0.012158, loss_dice: 0.464722
[07:27:14.132] TRAIN: iteration 888 : loss : 0.164377, loss_ce: 0.008812, loss_dice: 0.319943
[07:27:14.338] TRAIN: iteration 889 : loss : 0.125855, loss_ce: 0.008209, loss_dice: 0.243500
[07:27:16.478] TRAIN: iteration 890 : loss : 0.245995, loss_ce: 0.005633, loss_dice: 0.486357
[07:27:19.992] TRAIN: iteration 891 : loss : 0.137391, loss_ce: 0.006557, loss_dice: 0.268225
[07:27:22.496] TRAIN: iteration 892 : loss : 0.143109, loss_ce: 0.006748, loss_dice: 0.279469
[07:27:23.077] TRAIN: iteration 893 : loss : 0.176125, loss_ce: 0.007290, loss_dice: 0.344961
[07:27:26.462] TRAIN: iteration 894 : loss : 0.180005, loss_ce: 0.010172, loss_dice: 0.349839
[07:27:26.675] TRAIN: iteration 895 : loss : 0.205327, loss_ce: 0.009344, loss_dice: 0.401310
[07:27:26.882] TRAIN: iteration 896 : loss : 0.252766, loss_ce: 0.005280, loss_dice: 0.500253
[07:27:27.090] TRAIN: iteration 897 : loss : 0.178859, loss_ce: 0.008848, loss_dice: 0.348869
[07:27:29.540] TRAIN: iteration 898 : loss : 0.190920, loss_ce: 0.008655, loss_dice: 0.373184
[07:27:31.616] TRAIN: iteration 899 : loss : 0.177548, loss_ce: 0.010548, loss_dice: 0.344548
[07:27:34.909] TRAIN: iteration 900 : loss : 0.249475, loss_ce: 0.007538, loss_dice: 0.491412
[07:27:35.149] TRAIN: iteration 901 : loss : 0.148295, loss_ce: 0.007871, loss_dice: 0.288720
[07:27:39.160] TRAIN: iteration 902 : loss : 0.208444, loss_ce: 0.007947, loss_dice: 0.408940
[07:27:39.368] TRAIN: iteration 903 : loss : 0.190458, loss_ce: 0.011757, loss_dice: 0.369159
[07:27:39.574] TRAIN: iteration 904 : loss : 0.232216, loss_ce: 0.008444, loss_dice: 0.455989
[07:27:39.781] TRAIN: iteration 905 : loss : 0.230000, loss_ce: 0.009508, loss_dice: 0.450491
[07:27:41.811] TRAIN: iteration 906 : loss : 0.136082, loss_ce: 0.007298, loss_dice: 0.264866
[07:27:43.544] TRAIN: iteration 907 : loss : 0.253098, loss_ce: 0.005888, loss_dice: 0.500308
[07:27:47.090] TRAIN: iteration 908 : loss : 0.253022, loss_ce: 0.005782, loss_dice: 0.500262
[07:27:47.296] TRAIN: iteration 909 : loss : 0.249317, loss_ce: 0.005488, loss_dice: 0.493147
[07:27:51.695] TRAIN: iteration 910 : loss : 0.187738, loss_ce: 0.004796, loss_dice: 0.370680
[07:27:51.902] TRAIN: iteration 911 : loss : 0.174331, loss_ce: 0.012100, loss_dice: 0.336562
[07:27:52.110] TRAIN: iteration 912 : loss : 0.232302, loss_ce: 0.006866, loss_dice: 0.457738
[07:27:52.319] TRAIN: iteration 913 : loss : 0.233794, loss_ce: 0.005043, loss_dice: 0.462544
[07:27:52.616] TRAIN: iteration 914 : loss : 0.184902, loss_ce: 0.006378, loss_dice: 0.363425
[07:27:56.356] TRAIN: iteration 915 : loss : 0.234812, loss_ce: 0.004745, loss_dice: 0.464880
[07:27:58.497] TRAIN: iteration 916 : loss : 0.204193, loss_ce: 0.004912, loss_dice: 0.403474
[07:28:00.061] TRAIN: iteration 917 : loss : 0.128581, loss_ce: 0.006433, loss_dice: 0.250730
[07:28:04.140] TRAIN: iteration 918 : loss : 0.187504, loss_ce: 0.004627, loss_dice: 0.370380
[07:28:04.348] TRAIN: iteration 919 : loss : 0.210950, loss_ce: 0.005337, loss_dice: 0.416564
[07:28:04.556] TRAIN: iteration 920 : loss : 0.143670, loss_ce: 0.010916, loss_dice: 0.276424
[07:28:04.557] NaN or Inf found in input tensor.
[07:28:04.774] TRAIN: iteration 921 : loss : 0.157805, loss_ce: 0.015165, loss_dice: 0.300445
[07:28:05.511] TRAIN: iteration 922 : loss : 0.216268, loss_ce: 0.005716, loss_dice: 0.426821
[07:28:08.765] TRAIN: iteration 923 : loss : 0.124224, loss_ce: 0.010080, loss_dice: 0.238367
[07:28:11.250] TRAIN: iteration 924 : loss : 0.194081, loss_ce: 0.008616, loss_dice: 0.379546
[07:28:11.843] TRAIN: iteration 925 : loss : 0.155011, loss_ce: 0.008063, loss_dice: 0.301959
[07:28:16.076] TRAIN: iteration 926 : loss : 0.248193, loss_ce: 0.009846, loss_dice: 0.486541
[07:28:16.282] TRAIN: iteration 927 : loss : 0.255067, loss_ce: 0.011707, loss_dice: 0.498427
[07:28:16.490] TRAIN: iteration 928 : loss : 0.198062, loss_ce: 0.009647, loss_dice: 0.386477
[07:28:16.699] TRAIN: iteration 929 : loss : 0.237418, loss_ce: 0.027236, loss_dice: 0.447601
[07:28:17.690] TRAIN: iteration 930 : loss : 0.186113, loss_ce: 0.013878, loss_dice: 0.358347
[07:28:21.119] TRAIN: iteration 931 : loss : 0.249858, loss_ce: 0.012046, loss_dice: 0.487669
[07:28:23.291] TRAIN: iteration 932 : loss : 0.253536, loss_ce: 0.007076, loss_dice: 0.499995
[07:28:24.506] TRAIN: iteration 933 : loss : 0.225031, loss_ce: 0.006729, loss_dice: 0.443332
[07:28:29.566] TRAIN: iteration 934 : loss : 0.180141, loss_ce: 0.005919, loss_dice: 0.354364
[07:28:29.773] TRAIN: iteration 935 : loss : 0.251880, loss_ce: 0.010919, loss_dice: 0.492840
[07:28:29.981] TRAIN: iteration 936 : loss : 0.262347, loss_ce: 0.040994, loss_dice: 0.483700
[07:28:30.187] TRAIN: iteration 937 : loss : 0.248030, loss_ce: 0.003827, loss_dice: 0.492233
[07:28:31.358] TRAIN: iteration 938 : loss : 0.249045, loss_ce: 0.003401, loss_dice: 0.494690
[07:28:34.212] TRAIN: iteration 939 : loss : 0.191298, loss_ce: 0.004739, loss_dice: 0.377858
[07:28:36.938] TRAIN: iteration 940 : loss : 0.239748, loss_ce: 0.003866, loss_dice: 0.475630
[07:28:37.636] TRAIN: iteration 941 : loss : 0.232759, loss_ce: 0.004848, loss_dice: 0.460671
[07:28:42.352] TRAIN: iteration 942 : loss : 0.242760, loss_ce: 0.004113, loss_dice: 0.481406
[07:28:42.558] TRAIN: iteration 943 : loss : 0.186692, loss_ce: 0.008258, loss_dice: 0.365125
[07:28:42.765] TRAIN: iteration 944 : loss : 0.252524, loss_ce: 0.004943, loss_dice: 0.500105
[07:28:42.972] TRAIN: iteration 945 : loss : 0.182156, loss_ce: 0.006904, loss_dice: 0.357408
[07:28:43.204] TRAIN: iteration 946 : loss : 0.253188, loss_ce: 0.007008, loss_dice: 0.499368
[07:28:49.118] TRAIN: iteration 947 : loss : 0.178868, loss_ce: 0.011037, loss_dice: 0.346700
[07:28:49.330] TRAIN: iteration 948 : loss : 0.202039, loss_ce: 0.009093, loss_dice: 0.394985
[07:28:49.978] TRAIN: iteration 949 : loss : 0.160575, loss_ce: 0.010756, loss_dice: 0.310394
[07:28:54.892] TRAIN: iteration 950 : loss : 0.199082, loss_ce: 0.008648, loss_dice: 0.389517
[07:28:55.098] TRAIN: iteration 951 : loss : 0.254370, loss_ce: 0.008317, loss_dice: 0.500422
[07:28:55.304] TRAIN: iteration 952 : loss : 0.205136, loss_ce: 0.012938, loss_dice: 0.397335
[07:28:55.510] TRAIN: iteration 953 : loss : 0.251815, loss_ce: 0.006978, loss_dice: 0.496651
[07:28:55.716] TRAIN: iteration 954 : loss : 0.207621, loss_ce: 0.006669, loss_dice: 0.408572
[07:29:02.590] TRAIN: iteration 955 : loss : 0.244107, loss_ce: 0.008400, loss_dice: 0.479815
[07:29:02.803] TRAIN: iteration 956 : loss : 0.142797, loss_ce: 0.005598, loss_dice: 0.279996
[07:29:03.009] TRAIN: iteration 957 : loss : 0.168314, loss_ce: 0.008606, loss_dice: 0.328022
[07:29:08.087] TRAIN: iteration 958 : loss : 0.227880, loss_ce: 0.003668, loss_dice: 0.452091
[07:29:08.293] TRAIN: iteration 959 : loss : 0.248148, loss_ce: 0.007134, loss_dice: 0.489162
[07:29:08.499] TRAIN: iteration 960 : loss : 0.196093, loss_ce: 0.004970, loss_dice: 0.387216
[07:29:08.733] TRAIN: iteration 961 : loss : 0.241738, loss_ce: 0.004603, loss_dice: 0.478872
[07:29:08.939] TRAIN: iteration 962 : loss : 0.159041, loss_ce: 0.003971, loss_dice: 0.314110
[07:29:14.058] TRAIN: iteration 963 : loss : 0.171913, loss_ce: 0.004504, loss_dice: 0.339321
[07:29:14.270] TRAIN: iteration 964 : loss : 0.214894, loss_ce: 0.006389, loss_dice: 0.423398
[07:29:14.514] TRAIN: iteration 965 : loss : 0.245693, loss_ce: 0.006027, loss_dice: 0.485359
[07:29:20.638] TRAIN: iteration 966 : loss : 0.175951, loss_ce: 0.012408, loss_dice: 0.339494
[07:29:20.844] TRAIN: iteration 967 : loss : 0.236155, loss_ce: 0.012832, loss_dice: 0.459478
[07:29:21.051] TRAIN: iteration 968 : loss : 0.118607, loss_ce: 0.006390, loss_dice: 0.230823
[07:29:21.257] TRAIN: iteration 969 : loss : 0.254456, loss_ce: 0.046622, loss_dice: 0.462290
[07:29:21.463] TRAIN: iteration 970 : loss : 0.224862, loss_ce: 0.011898, loss_dice: 0.437825
[07:29:27.182] TRAIN: iteration 971 : loss : 0.190591, loss_ce: 0.008086, loss_dice: 0.373096
[07:29:27.387] TRAIN: iteration 972 : loss : 0.188685, loss_ce: 0.007418, loss_dice: 0.369951
[07:29:27.594] TRAIN: iteration 973 : loss : 0.139411, loss_ce: 0.010709, loss_dice: 0.268113
[07:29:32.370] TRAIN: iteration 974 : loss : 0.183469, loss_ce: 0.008487, loss_dice: 0.358452
[07:29:32.587] TRAIN: iteration 975 : loss : 0.249568, loss_ce: 0.010547, loss_dice: 0.488589
[07:29:32.793] TRAIN: iteration 976 : loss : 0.140661, loss_ce: 0.012769, loss_dice: 0.268553
[07:29:33.000] TRAIN: iteration 977 : loss : 0.126590, loss_ce: 0.009117, loss_dice: 0.244062
[07:29:33.206] TRAIN: iteration 978 : loss : 0.237616, loss_ce: 0.019520, loss_dice: 0.455713
[07:29:40.025] TRAIN: iteration 979 : loss : 0.253449, loss_ce: 0.007475, loss_dice: 0.499423
[07:29:40.233] TRAIN: iteration 980 : loss : 0.178790, loss_ce: 0.028290, loss_dice: 0.329291
[07:29:40.470] TRAIN: iteration 981 : loss : 0.138541, loss_ce: 0.012080, loss_dice: 0.265002
[07:29:45.230] TRAIN: iteration 982 : loss : 0.189549, loss_ce: 0.007541, loss_dice: 0.371556
[07:29:45.443] TRAIN: iteration 983 : loss : 0.253738, loss_ce: 0.007087, loss_dice: 0.500388
[07:29:45.650] TRAIN: iteration 984 : loss : 0.231112, loss_ce: 0.009420, loss_dice: 0.452804
[07:29:45.857] TRAIN: iteration 985 : loss : 0.245786, loss_ce: 0.007886, loss_dice: 0.483687
[07:29:46.065] TRAIN: iteration 986 : loss : 0.222434, loss_ce: 0.008655, loss_dice: 0.436213
[07:29:54.021] TRAIN: iteration 987 : loss : 0.225258, loss_ce: 0.015142, loss_dice: 0.435375
[07:29:54.230] TRAIN: iteration 988 : loss : 0.174265, loss_ce: 0.012223, loss_dice: 0.336307
[07:29:54.436] TRAIN: iteration 989 : loss : 0.192111, loss_ce: 0.020505, loss_dice: 0.363717
[07:29:57.666] TRAIN: iteration 990 : loss : 0.202334, loss_ce: 0.008957, loss_dice: 0.395711
[07:29:57.874] TRAIN: iteration 991 : loss : 0.192028, loss_ce: 0.009998, loss_dice: 0.374058
[07:29:58.081] TRAIN: iteration 992 : loss : 0.254984, loss_ce: 0.009399, loss_dice: 0.500570
[07:29:58.288] TRAIN: iteration 993 : loss : 0.225839, loss_ce: 0.009978, loss_dice: 0.441700
[07:29:58.494] TRAIN: iteration 994 : loss : 0.135468, loss_ce: 0.011608, loss_dice: 0.259327
[07:30:06.143] TRAIN: iteration 995 : loss : 0.254277, loss_ce: 0.008307, loss_dice: 0.500248
[07:30:06.355] TRAIN: iteration 996 : loss : 0.254085, loss_ce: 0.007775, loss_dice: 0.500395
[07:30:06.568] TRAIN: iteration 997 : loss : 0.215514, loss_ce: 0.015013, loss_dice: 0.416015
[07:30:09.996] TRAIN: iteration 998 : loss : 0.226448, loss_ce: 0.009547, loss_dice: 0.443350
[07:30:10.208] TRAIN: iteration 999 : loss : 0.240262, loss_ce: 0.013319, loss_dice: 0.467206
[07:30:10.415] TRAIN: iteration 1000 : loss : 0.253321, loss_ce: 0.006385, loss_dice: 0.500258
[07:30:10.652] TRAIN: iteration 1001 : loss : 0.226423, loss_ce: 0.006790, loss_dice: 0.446056
[07:30:10.858] TRAIN: iteration 1002 : loss : 0.191737, loss_ce: 0.018826, loss_dice: 0.364648
[07:30:18.064] TRAIN: iteration 1003 : loss : 0.186778, loss_ce: 0.011262, loss_dice: 0.362294
[07:30:18.269] TRAIN: iteration 1004 : loss : 0.238561, loss_ce: 0.008513, loss_dice: 0.468610
[07:30:18.475] TRAIN: iteration 1005 : loss : 0.162127, loss_ce: 0.009872, loss_dice: 0.314381
[07:30:22.931] TRAIN: iteration 1006 : loss : 0.242152, loss_ce: 0.009199, loss_dice: 0.475106
[07:30:23.137] TRAIN: iteration 1007 : loss : 0.175173, loss_ce: 0.022110, loss_dice: 0.328235
[07:30:23.343] TRAIN: iteration 1008 : loss : 0.176977, loss_ce: 0.007961, loss_dice: 0.345994
[07:30:23.548] TRAIN: iteration 1009 : loss : 0.194882, loss_ce: 0.009128, loss_dice: 0.380636
[07:30:23.756] TRAIN: iteration 1010 : loss : 0.215905, loss_ce: 0.014761, loss_dice: 0.417050
[07:30:30.874] TRAIN: iteration 1011 : loss : 0.214483, loss_ce: 0.007774, loss_dice: 0.421192
[07:30:31.087] TRAIN: iteration 1012 : loss : 0.253652, loss_ce: 0.007032, loss_dice: 0.500273
[07:30:31.295] TRAIN: iteration 1013 : loss : 0.237519, loss_ce: 0.015402, loss_dice: 0.459636
[07:30:35.744] TRAIN: iteration 1014 : loss : 0.168697, loss_ce: 0.012531, loss_dice: 0.324862
[07:30:35.952] TRAIN: iteration 1015 : loss : 0.259409, loss_ce: 0.029505, loss_dice: 0.489313
[07:30:36.158] TRAIN: iteration 1016 : loss : 0.240029, loss_ce: 0.007261, loss_dice: 0.472797
[07:30:36.364] TRAIN: iteration 1017 : loss : 0.217013, loss_ce: 0.035924, loss_dice: 0.398103
[07:30:36.570] TRAIN: iteration 1018 : loss : 0.236128, loss_ce: 0.008495, loss_dice: 0.463761
[07:30:42.670] TRAIN: iteration 1019 : loss : 0.212963, loss_ce: 0.014237, loss_dice: 0.411690
[07:30:42.875] TRAIN: iteration 1020 : loss : 0.145342, loss_ce: 0.008686, loss_dice: 0.281999
[07:30:43.107] TRAIN: iteration 1021 : loss : 0.157596, loss_ce: 0.014353, loss_dice: 0.300839
[07:30:47.646] TRAIN: iteration 1022 : loss : 0.191268, loss_ce: 0.007685, loss_dice: 0.374851
[07:30:47.853] TRAIN: iteration 1023 : loss : 0.164418, loss_ce: 0.010855, loss_dice: 0.317980
[07:30:48.059] TRAIN: iteration 1024 : loss : 0.241464, loss_ce: 0.010506, loss_dice: 0.472422
[07:30:48.264] TRAIN: iteration 1025 : loss : 0.126688, loss_ce: 0.009585, loss_dice: 0.243791
[07:30:48.470] TRAIN: iteration 1026 : loss : 0.244791, loss_ce: 0.006665, loss_dice: 0.482918
[07:30:54.726] TRAIN: iteration 1027 : loss : 0.249810, loss_ce: 0.006700, loss_dice: 0.492920
[07:30:54.931] TRAIN: iteration 1028 : loss : 0.192518, loss_ce: 0.013434, loss_dice: 0.371602
[07:30:55.138] TRAIN: iteration 1029 : loss : 0.161034, loss_ce: 0.006837, loss_dice: 0.315231
[07:31:00.769] TRAIN: iteration 1030 : loss : 0.189898, loss_ce: 0.010611, loss_dice: 0.369184
[07:31:00.976] TRAIN: iteration 1031 : loss : 0.253276, loss_ce: 0.006304, loss_dice: 0.500248
[07:31:01.183] TRAIN: iteration 1032 : loss : 0.143876, loss_ce: 0.008642, loss_dice: 0.279109
[07:31:01.393] TRAIN: iteration 1033 : loss : 0.235509, loss_ce: 0.008559, loss_dice: 0.462459
[07:31:01.600] TRAIN: iteration 1034 : loss : 0.170523, loss_ce: 0.007638, loss_dice: 0.333407
[07:31:05.830] TRAIN: iteration 1035 : loss : 0.138455, loss_ce: 0.007118, loss_dice: 0.269792
[07:31:06.039] TRAIN: iteration 1036 : loss : 0.174640, loss_ce: 0.010516, loss_dice: 0.338765
[07:31:06.245] TRAIN: iteration 1037 : loss : 0.223742, loss_ce: 0.012776, loss_dice: 0.434708
[07:31:13.729] TRAIN: iteration 1038 : loss : 0.136667, loss_ce: 0.007059, loss_dice: 0.266275
[07:31:13.939] TRAIN: iteration 1039 : loss : 0.229713, loss_ce: 0.005817, loss_dice: 0.453609
[07:31:14.146] TRAIN: iteration 1040 : loss : 0.171116, loss_ce: 0.012194, loss_dice: 0.330038
[07:31:14.377] TRAIN: iteration 1041 : loss : 0.151751, loss_ce: 0.007764, loss_dice: 0.295737
[07:31:14.583] TRAIN: iteration 1042 : loss : 0.174003, loss_ce: 0.005118, loss_dice: 0.342887
[07:31:19.761] TRAIN: iteration 1043 : loss : 0.244855, loss_ce: 0.004835, loss_dice: 0.484875
[07:31:19.971] TRAIN: iteration 1044 : loss : 0.155328, loss_ce: 0.007417, loss_dice: 0.303240
[07:31:20.176] TRAIN: iteration 1045 : loss : 0.146078, loss_ce: 0.009125, loss_dice: 0.283031
[07:31:26.649] TRAIN: iteration 1046 : loss : 0.179732, loss_ce: 0.005495, loss_dice: 0.353969
[07:31:26.854] TRAIN: iteration 1047 : loss : 0.237729, loss_ce: 0.007929, loss_dice: 0.467530
[07:31:27.060] TRAIN: iteration 1048 : loss : 0.253058, loss_ce: 0.006069, loss_dice: 0.500047
[07:31:27.265] TRAIN: iteration 1049 : loss : 0.156859, loss_ce: 0.005598, loss_dice: 0.308119
[07:31:27.471] TRAIN: iteration 1050 : loss : 0.201544, loss_ce: 0.016555, loss_dice: 0.386534
[07:31:32.511] TRAIN: iteration 1051 : loss : 0.200970, loss_ce: 0.036250, loss_dice: 0.365689
[07:31:32.718] TRAIN: iteration 1052 : loss : 0.198367, loss_ce: 0.009303, loss_dice: 0.387430
[07:31:32.923] TRAIN: iteration 1053 : loss : 0.145653, loss_ce: 0.009342, loss_dice: 0.281965
[07:31:38.904] TRAIN: iteration 1054 : loss : 0.237838, loss_ce: 0.011504, loss_dice: 0.464173
[07:31:39.111] TRAIN: iteration 1055 : loss : 0.176202, loss_ce: 0.016306, loss_dice: 0.336097
[07:31:39.319] TRAIN: iteration 1056 : loss : 0.194955, loss_ce: 0.013740, loss_dice: 0.376169
[07:31:39.526] TRAIN: iteration 1057 : loss : 0.227588, loss_ce: 0.008343, loss_dice: 0.446833
[07:31:39.731] TRAIN: iteration 1058 : loss : 0.166609, loss_ce: 0.008363, loss_dice: 0.324856
[07:31:45.821] TRAIN: iteration 1059 : loss : 0.233849, loss_ce: 0.009981, loss_dice: 0.457716
[07:31:46.028] TRAIN: iteration 1060 : loss : 0.228795, loss_ce: 0.016079, loss_dice: 0.441510
[07:31:46.028] NaN or Inf found in input tensor.
[07:31:46.243] TRAIN: iteration 1061 : loss : 0.235231, loss_ce: 0.012469, loss_dice: 0.457994
[07:31:50.867] TRAIN: iteration 1062 : loss : 0.171680, loss_ce: 0.009782, loss_dice: 0.333578
[07:31:51.076] TRAIN: iteration 1063 : loss : 0.253490, loss_ce: 0.007990, loss_dice: 0.498990
[07:31:51.284] TRAIN: iteration 1064 : loss : 0.204212, loss_ce: 0.009563, loss_dice: 0.398861
[07:31:51.490] TRAIN: iteration 1065 : loss : 0.225652, loss_ce: 0.006758, loss_dice: 0.444547
[07:31:51.696] TRAIN: iteration 1066 : loss : 0.169622, loss_ce: 0.008839, loss_dice: 0.330406
[07:31:57.583] TRAIN: iteration 1067 : loss : 0.242238, loss_ce: 0.009083, loss_dice: 0.475394
[07:31:57.790] TRAIN: iteration 1068 : loss : 0.207466, loss_ce: 0.006683, loss_dice: 0.408249
[07:31:57.996] TRAIN: iteration 1069 : loss : 0.188219, loss_ce: 0.005338, loss_dice: 0.371100
[07:32:02.938] TRAIN: iteration 1070 : loss : 0.251982, loss_ce: 0.003803, loss_dice: 0.500161
[07:32:03.144] TRAIN: iteration 1071 : loss : 0.252458, loss_ce: 0.009206, loss_dice: 0.495711
[07:32:03.352] TRAIN: iteration 1072 : loss : 0.247846, loss_ce: 0.010045, loss_dice: 0.485648
[07:32:03.557] TRAIN: iteration 1073 : loss : 0.243171, loss_ce: 0.002790, loss_dice: 0.483553
[07:32:03.762] TRAIN: iteration 1074 : loss : 0.249484, loss_ce: 0.024157, loss_dice: 0.474811
[07:32:09.688] TRAIN: iteration 1075 : loss : 0.242632, loss_ce: 0.009259, loss_dice: 0.476005
[07:32:09.894] TRAIN: iteration 1076 : loss : 0.247969, loss_ce: 0.007433, loss_dice: 0.488505
[07:32:10.103] TRAIN: iteration 1077 : loss : 0.252752, loss_ce: 0.013695, loss_dice: 0.491809
[07:32:15.073] TRAIN: iteration 1078 : loss : 0.240982, loss_ce: 0.002584, loss_dice: 0.479379
[07:32:15.283] TRAIN: iteration 1079 : loss : 0.226937, loss_ce: 0.005987, loss_dice: 0.447887
[07:32:15.488] TRAIN: iteration 1080 : loss : 0.241922, loss_ce: 0.002424, loss_dice: 0.481420
[07:32:15.728] TRAIN: iteration 1081 : loss : 0.195331, loss_ce: 0.006829, loss_dice: 0.383832
[07:32:15.933] TRAIN: iteration 1082 : loss : 0.224694, loss_ce: 0.003544, loss_dice: 0.445843
[07:32:22.225] TRAIN: iteration 1083 : loss : 0.222637, loss_ce: 0.017674, loss_dice: 0.427601
[07:32:22.430] TRAIN: iteration 1084 : loss : 0.224199, loss_ce: 0.010560, loss_dice: 0.437838
[07:32:22.637] TRAIN: iteration 1085 : loss : 0.191047, loss_ce: 0.013948, loss_dice: 0.368146
[07:32:27.437] TRAIN: iteration 1086 : loss : 0.251338, loss_ce: 0.007098, loss_dice: 0.495578
[07:32:27.642] TRAIN: iteration 1087 : loss : 0.186445, loss_ce: 0.010371, loss_dice: 0.362518
[07:32:27.848] TRAIN: iteration 1088 : loss : 0.238332, loss_ce: 0.009612, loss_dice: 0.467053
[07:32:28.053] TRAIN: iteration 1089 : loss : 0.255587, loss_ce: 0.010721, loss_dice: 0.500453
[07:32:28.266] TRAIN: iteration 1090 : loss : 0.144212, loss_ce: 0.012383, loss_dice: 0.276042
[07:32:34.271] TRAIN: iteration 1091 : loss : 0.155221, loss_ce: 0.013603, loss_dice: 0.296839
[07:32:34.480] TRAIN: iteration 1092 : loss : 0.197175, loss_ce: 0.011420, loss_dice: 0.382930
[07:32:34.687] TRAIN: iteration 1093 : loss : 0.189075, loss_ce: 0.012505, loss_dice: 0.365645
[07:32:39.890] TRAIN: iteration 1094 : loss : 0.157607, loss_ce: 0.011349, loss_dice: 0.303865
[07:32:40.096] TRAIN: iteration 1095 : loss : 0.182627, loss_ce: 0.023241, loss_dice: 0.342014
[07:32:40.305] TRAIN: iteration 1096 : loss : 0.231833, loss_ce: 0.023593, loss_dice: 0.440073
[07:32:40.511] TRAIN: iteration 1097 : loss : 0.201310, loss_ce: 0.008124, loss_dice: 0.394497
[07:32:40.717] TRAIN: iteration 1098 : loss : 0.254072, loss_ce: 0.007769, loss_dice: 0.500376
[07:32:45.507] TRAIN: iteration 1099 : loss : 0.179295, loss_ce: 0.008754, loss_dice: 0.349836
[07:32:45.713] TRAIN: iteration 1100 : loss : 0.217590, loss_ce: 0.007179, loss_dice: 0.428001
[07:32:45.950] TRAIN: iteration 1101 : loss : 0.246177, loss_ce: 0.006317, loss_dice: 0.486037
[07:32:51.524] TRAIN: iteration 1102 : loss : 0.218444, loss_ce: 0.006412, loss_dice: 0.430476
[07:32:51.731] TRAIN: iteration 1103 : loss : 0.120320, loss_ce: 0.006513, loss_dice: 0.234127
[07:32:51.937] TRAIN: iteration 1104 : loss : 0.252713, loss_ce: 0.005185, loss_dice: 0.500242
[07:32:52.142] TRAIN: iteration 1105 : loss : 0.242474, loss_ce: 0.004639, loss_dice: 0.480310
[07:32:52.348] TRAIN: iteration 1106 : loss : 0.238434, loss_ce: 0.004095, loss_dice: 0.472772
[07:32:57.518] TRAIN: iteration 1107 : loss : 0.237073, loss_ce: 0.006928, loss_dice: 0.467218
[07:32:57.725] TRAIN: iteration 1108 : loss : 0.251651, loss_ce: 0.003193, loss_dice: 0.500109
[07:32:57.931] TRAIN: iteration 1109 : loss : 0.228318, loss_ce: 0.015117, loss_dice: 0.441520
[07:33:03.438] TRAIN: iteration 1110 : loss : 0.251333, loss_ce: 0.002615, loss_dice: 0.500051
[07:33:03.643] TRAIN: iteration 1111 : loss : 0.197593, loss_ce: 0.007932, loss_dice: 0.387254
[07:33:03.849] TRAIN: iteration 1112 : loss : 0.154801, loss_ce: 0.003080, loss_dice: 0.306522
[07:33:04.054] TRAIN: iteration 1113 : loss : 0.227091, loss_ce: 0.009841, loss_dice: 0.444341
[07:33:04.260] TRAIN: iteration 1114 : loss : 0.120662, loss_ce: 0.004699, loss_dice: 0.236625
[07:33:10.026] TRAIN: iteration 1115 : loss : 0.212490, loss_ce: 0.004173, loss_dice: 0.420807
[07:33:10.232] TRAIN: iteration 1116 : loss : 0.168349, loss_ce: 0.015817, loss_dice: 0.320881
[07:33:10.438] TRAIN: iteration 1117 : loss : 0.182123, loss_ce: 0.009754, loss_dice: 0.354492
[07:33:16.329] TRAIN: iteration 1118 : loss : 0.219782, loss_ce: 0.018542, loss_dice: 0.421022
[07:33:16.534] TRAIN: iteration 1119 : loss : 0.143188, loss_ce: 0.007904, loss_dice: 0.278471
[07:33:16.739] TRAIN: iteration 1120 : loss : 0.121415, loss_ce: 0.014313, loss_dice: 0.228518
[07:33:16.976] TRAIN: iteration 1121 : loss : 0.249930, loss_ce: 0.008041, loss_dice: 0.491818
[07:33:17.182] TRAIN: iteration 1122 : loss : 0.224345, loss_ce: 0.009344, loss_dice: 0.439346
[07:33:22.842] TRAIN: iteration 1123 : loss : 0.237124, loss_ce: 0.010567, loss_dice: 0.463680
[07:33:23.052] TRAIN: iteration 1124 : loss : 0.168438, loss_ce: 0.011335, loss_dice: 0.325540
[07:33:23.260] TRAIN: iteration 1125 : loss : 0.171706, loss_ce: 0.015743, loss_dice: 0.327668
[07:33:28.154] TRAIN: iteration 1126 : loss : 0.236074, loss_ce: 0.021140, loss_dice: 0.451008
[07:33:28.365] TRAIN: iteration 1127 : loss : 0.147894, loss_ce: 0.011015, loss_dice: 0.284773
[07:33:28.572] TRAIN: iteration 1128 : loss : 0.179966, loss_ce: 0.013862, loss_dice: 0.346070
[07:33:28.779] TRAIN: iteration 1129 : loss : 0.206653, loss_ce: 0.012281, loss_dice: 0.401024
[07:33:28.985] TRAIN: iteration 1130 : loss : 0.250234, loss_ce: 0.011707, loss_dice: 0.488761
[07:33:34.842] TRAIN: iteration 1131 : loss : 0.176298, loss_ce: 0.012314, loss_dice: 0.340282
[07:33:35.049] TRAIN: iteration 1132 : loss : 0.255750, loss_ce: 0.011045, loss_dice: 0.500455
[07:33:35.255] TRAIN: iteration 1133 : loss : 0.185290, loss_ce: 0.031095, loss_dice: 0.339485
[07:33:40.227] TRAIN: iteration 1134 : loss : 0.222188, loss_ce: 0.014853, loss_dice: 0.429523
[07:33:40.432] TRAIN: iteration 1135 : loss : 0.213360, loss_ce: 0.009671, loss_dice: 0.417048
[07:33:40.638] TRAIN: iteration 1136 : loss : 0.248093, loss_ce: 0.023975, loss_dice: 0.472211
[07:33:40.843] TRAIN: iteration 1137 : loss : 0.144271, loss_ce: 0.011627, loss_dice: 0.276916
[07:33:41.049] TRAIN: iteration 1138 : loss : 0.254249, loss_ce: 0.028946, loss_dice: 0.479553
[07:33:47.385] TRAIN: iteration 1139 : loss : 0.253865, loss_ce: 0.008712, loss_dice: 0.499017
[07:33:47.592] TRAIN: iteration 1140 : loss : 0.252014, loss_ce: 0.011455, loss_dice: 0.492572
[07:33:47.825] TRAIN: iteration 1141 : loss : 0.189069, loss_ce: 0.007663, loss_dice: 0.370476
[07:33:52.844] TRAIN: iteration 1142 : loss : 0.253748, loss_ce: 0.007096, loss_dice: 0.500400
[07:33:53.051] TRAIN: iteration 1143 : loss : 0.254088, loss_ce: 0.007728, loss_dice: 0.500448
[07:33:53.257] TRAIN: iteration 1144 : loss : 0.190607, loss_ce: 0.010908, loss_dice: 0.370305
[07:33:53.462] TRAIN: iteration 1145 : loss : 0.125572, loss_ce: 0.012386, loss_dice: 0.238757
[07:33:53.672] TRAIN: iteration 1146 : loss : 0.191490, loss_ce: 0.008626, loss_dice: 0.374353
[07:33:59.554] TRAIN: iteration 1147 : loss : 0.211122, loss_ce: 0.010791, loss_dice: 0.411453
[07:33:59.764] TRAIN: iteration 1148 : loss : 0.253647, loss_ce: 0.006981, loss_dice: 0.500314
[07:33:59.971] TRAIN: iteration 1149 : loss : 0.195067, loss_ce: 0.011109, loss_dice: 0.379025
[07:34:04.774] TRAIN: iteration 1150 : loss : 0.219740, loss_ce: 0.009410, loss_dice: 0.430069
[07:34:04.980] TRAIN: iteration 1151 : loss : 0.219978, loss_ce: 0.011880, loss_dice: 0.428077
[07:34:05.186] TRAIN: iteration 1152 : loss : 0.225060, loss_ce: 0.007801, loss_dice: 0.442318
[07:34:05.393] TRAIN: iteration 1153 : loss : 0.152740, loss_ce: 0.008093, loss_dice: 0.297387
[07:34:05.599] TRAIN: iteration 1154 : loss : 0.128196, loss_ce: 0.010059, loss_dice: 0.246334
[07:34:12.735] TRAIN: iteration 1155 : loss : 0.208272, loss_ce: 0.007346, loss_dice: 0.409199
[07:34:12.940] TRAIN: iteration 1156 : loss : 0.150262, loss_ce: 0.013294, loss_dice: 0.287230
[07:34:13.147] TRAIN: iteration 1157 : loss : 0.246145, loss_ce: 0.008398, loss_dice: 0.483893
[07:34:16.446] TRAIN: iteration 1158 : loss : 0.173844, loss_ce: 0.005645, loss_dice: 0.342043
[07:34:16.652] TRAIN: iteration 1159 : loss : 0.229629, loss_ce: 0.005018, loss_dice: 0.454240
[07:34:16.858] TRAIN: iteration 1160 : loss : 0.186229, loss_ce: 0.008128, loss_dice: 0.364331
[07:34:17.095] TRAIN: iteration 1161 : loss : 0.252226, loss_ce: 0.004286, loss_dice: 0.500166
[07:34:17.301] TRAIN: iteration 1162 : loss : 0.252103, loss_ce: 0.004082, loss_dice: 0.500124
[07:34:24.413] TRAIN: iteration 1163 : loss : 0.244921, loss_ce: 0.004313, loss_dice: 0.485528
[07:34:24.618] TRAIN: iteration 1164 : loss : 0.251787, loss_ce: 0.003480, loss_dice: 0.500095
[07:34:24.823] TRAIN: iteration 1165 : loss : 0.251872, loss_ce: 0.003610, loss_dice: 0.500134
[07:34:29.598] TRAIN: iteration 1166 : loss : 0.167680, loss_ce: 0.003646, loss_dice: 0.331713
[07:34:29.808] TRAIN: iteration 1167 : loss : 0.251631, loss_ce: 0.003161, loss_dice: 0.500102
[07:34:30.015] TRAIN: iteration 1168 : loss : 0.200214, loss_ce: 0.003641, loss_dice: 0.396787
[07:34:30.220] TRAIN: iteration 1169 : loss : 0.214634, loss_ce: 0.004081, loss_dice: 0.425188
[07:34:30.426] TRAIN: iteration 1170 : loss : 0.213690, loss_ce: 0.005051, loss_dice: 0.422329
[07:34:37.237] TRAIN: iteration 1171 : loss : 0.211527, loss_ce: 0.005322, loss_dice: 0.417733
[07:34:37.446] TRAIN: iteration 1172 : loss : 0.144084, loss_ce: 0.005816, loss_dice: 0.282352
[07:34:37.651] TRAIN: iteration 1173 : loss : 0.251833, loss_ce: 0.003524, loss_dice: 0.500143
[07:34:41.579] TRAIN: iteration 1174 : loss : 0.137034, loss_ce: 0.004666, loss_dice: 0.269402
[07:34:41.787] TRAIN: iteration 1175 : loss : 0.145423, loss_ce: 0.008787, loss_dice: 0.282059
[07:34:41.994] TRAIN: iteration 1176 : loss : 0.189593, loss_ce: 0.005255, loss_dice: 0.373931
[07:34:42.203] TRAIN: iteration 1177 : loss : 0.149977, loss_ce: 0.008806, loss_dice: 0.291148
[07:34:42.468] TRAIN: iteration 1178 : loss : 0.118913, loss_ce: 0.006940, loss_dice: 0.230886
[07:34:49.382] TRAIN: iteration 1179 : loss : 0.202926, loss_ce: 0.006446, loss_dice: 0.399406
[07:34:49.587] TRAIN: iteration 1180 : loss : 0.196186, loss_ce: 0.008613, loss_dice: 0.383759
[07:34:49.821] TRAIN: iteration 1181 : loss : 0.160937, loss_ce: 0.012102, loss_dice: 0.309772
[07:34:54.035] TRAIN: iteration 1182 : loss : 0.238617, loss_ce: 0.017069, loss_dice: 0.460164
[07:34:54.245] TRAIN: iteration 1183 : loss : 0.187689, loss_ce: 0.006546, loss_dice: 0.368832
[07:34:54.451] TRAIN: iteration 1184 : loss : 0.253653, loss_ce: 0.007126, loss_dice: 0.500181
[07:34:54.657] TRAIN: iteration 1185 : loss : 0.129713, loss_ce: 0.010500, loss_dice: 0.248927
[07:34:54.865] TRAIN: iteration 1186 : loss : 0.212829, loss_ce: 0.006977, loss_dice: 0.418680
[07:35:01.179] TRAIN: iteration 1187 : loss : 0.190429, loss_ce: 0.032742, loss_dice: 0.348116
[07:35:01.385] TRAIN: iteration 1188 : loss : 0.208083, loss_ce: 0.007948, loss_dice: 0.408217
[07:35:01.590] TRAIN: iteration 1189 : loss : 0.124282, loss_ce: 0.006421, loss_dice: 0.242144
[07:35:07.376] TRAIN: iteration 1190 : loss : 0.242869, loss_ce: 0.012857, loss_dice: 0.472882
[07:35:07.589] TRAIN: iteration 1191 : loss : 0.228799, loss_ce: 0.007568, loss_dice: 0.450031
[07:35:07.796] TRAIN: iteration 1192 : loss : 0.253544, loss_ce: 0.008560, loss_dice: 0.498528
[07:35:08.004] TRAIN: iteration 1193 : loss : 0.127528, loss_ce: 0.013996, loss_dice: 0.241060
[07:35:08.211] TRAIN: iteration 1194 : loss : 0.216627, loss_ce: 0.008712, loss_dice: 0.424541
[07:35:13.511] TRAIN: iteration 1195 : loss : 0.158838, loss_ce: 0.009281, loss_dice: 0.308394
[07:35:13.717] TRAIN: iteration 1196 : loss : 0.253442, loss_ce: 0.006526, loss_dice: 0.500357
[07:35:13.923] TRAIN: iteration 1197 : loss : 0.178646, loss_ce: 0.006166, loss_dice: 0.351126
[07:35:20.258] TRAIN: iteration 1198 : loss : 0.243230, loss_ce: 0.010364, loss_dice: 0.476096
[07:35:20.468] TRAIN: iteration 1199 : loss : 0.172723, loss_ce: 0.008796, loss_dice: 0.336650
[07:35:20.675] TRAIN: iteration 1200 : loss : 0.152995, loss_ce: 0.011267, loss_dice: 0.294724
[07:35:20.912] TRAIN: iteration 1201 : loss : 0.195232, loss_ce: 0.006654, loss_dice: 0.383810
[07:35:21.118] TRAIN: iteration 1202 : loss : 0.193426, loss_ce: 0.010265, loss_dice: 0.376586
[07:35:26.287] TRAIN: iteration 1203 : loss : 0.192709, loss_ce: 0.007746, loss_dice: 0.377671
[07:35:26.498] TRAIN: iteration 1204 : loss : 0.190766, loss_ce: 0.008243, loss_dice: 0.373289
[07:35:26.704] TRAIN: iteration 1205 : loss : 0.157691, loss_ce: 0.006517, loss_dice: 0.308865
[07:35:33.212] TRAIN: iteration 1206 : loss : 0.128250, loss_ce: 0.009266, loss_dice: 0.247234
[07:35:33.419] TRAIN: iteration 1207 : loss : 0.252824, loss_ce: 0.005851, loss_dice: 0.499797
[07:35:33.625] TRAIN: iteration 1208 : loss : 0.183081, loss_ce: 0.005631, loss_dice: 0.360531
[07:35:33.832] TRAIN: iteration 1209 : loss : 0.252164, loss_ce: 0.004407, loss_dice: 0.499920
[07:35:34.039] TRAIN: iteration 1210 : loss : 0.177786, loss_ce: 0.004002, loss_dice: 0.351571
[07:35:38.609] TRAIN: iteration 1211 : loss : 0.242762, loss_ce: 0.005040, loss_dice: 0.480483
[07:35:38.815] TRAIN: iteration 1212 : loss : 0.257770, loss_ce: 0.018809, loss_dice: 0.496732
[07:35:39.022] TRAIN: iteration 1213 : loss : 0.250375, loss_ce: 0.002965, loss_dice: 0.497784
[07:35:46.711] TRAIN: iteration 1214 : loss : 0.234584, loss_ce: 0.005579, loss_dice: 0.463589
[07:35:46.921] TRAIN: iteration 1215 : loss : 0.251314, loss_ce: 0.028805, loss_dice: 0.473823
[07:35:47.130] TRAIN: iteration 1216 : loss : 0.250909, loss_ce: 0.001803, loss_dice: 0.500015
[07:35:47.335] TRAIN: iteration 1217 : loss : 0.243002, loss_ce: 0.010480, loss_dice: 0.475524
[07:35:47.541] TRAIN: iteration 1218 : loss : 0.234401, loss_ce: 0.016153, loss_dice: 0.452650
[07:35:50.735] TRAIN: iteration 1219 : loss : 0.224257, loss_ce: 0.008447, loss_dice: 0.440066
[07:35:50.941] TRAIN: iteration 1220 : loss : 0.251081, loss_ce: 0.002122, loss_dice: 0.500041
[07:35:51.174] TRAIN: iteration 1221 : loss : 0.253527, loss_ce: 0.011165, loss_dice: 0.495890
[07:35:59.486] TRAIN: iteration 1222 : loss : 0.202309, loss_ce: 0.007367, loss_dice: 0.397252
[07:35:59.696] TRAIN: iteration 1223 : loss : 0.247326, loss_ce: 0.003341, loss_dice: 0.491311
[07:35:59.902] TRAIN: iteration 1224 : loss : 0.230764, loss_ce: 0.015927, loss_dice: 0.445601
[07:36:00.108] TRAIN: iteration 1225 : loss : 0.251855, loss_ce: 0.003602, loss_dice: 0.500107
[07:36:00.316] TRAIN: iteration 1226 : loss : 0.187235, loss_ce: 0.006562, loss_dice: 0.367909
[07:36:02.179] TRAIN: iteration 1227 : loss : 0.242160, loss_ce: 0.005646, loss_dice: 0.478675
[07:36:02.387] TRAIN: iteration 1228 : loss : 0.252710, loss_ce: 0.005203, loss_dice: 0.500217
[07:36:02.593] TRAIN: iteration 1229 : loss : 0.193304, loss_ce: 0.012560, loss_dice: 0.374049
[07:36:11.850] TRAIN: iteration 1230 : loss : 0.210174, loss_ce: 0.007489, loss_dice: 0.412859
[07:36:12.058] TRAIN: iteration 1231 : loss : 0.236685, loss_ce: 0.007726, loss_dice: 0.465644
[07:36:12.264] TRAIN: iteration 1232 : loss : 0.253867, loss_ce: 0.007435, loss_dice: 0.500298
[07:36:12.471] TRAIN: iteration 1233 : loss : 0.210900, loss_ce: 0.012249, loss_dice: 0.409551
[07:36:12.677] TRAIN: iteration 1234 : loss : 0.179696, loss_ce: 0.009712, loss_dice: 0.349679
[07:36:15.040] TRAIN: iteration 1235 : loss : 0.137750, loss_ce: 0.009863, loss_dice: 0.265638
[07:36:15.247] TRAIN: iteration 1236 : loss : 0.227932, loss_ce: 0.016557, loss_dice: 0.439308
[07:36:15.452] TRAIN: iteration 1237 : loss : 0.232110, loss_ce: 0.010037, loss_dice: 0.454184
[07:36:24.442] TRAIN: iteration 1238 : loss : 0.247829, loss_ce: 0.009474, loss_dice: 0.486184
[07:36:24.648] TRAIN: iteration 1239 : loss : 0.183963, loss_ce: 0.011416, loss_dice: 0.356510
[07:36:24.853] TRAIN: iteration 1240 : loss : 0.190477, loss_ce: 0.013733, loss_dice: 0.367220
[07:36:25.088] TRAIN: iteration 1241 : loss : 0.180983, loss_ce: 0.029078, loss_dice: 0.332888
[07:36:25.298] TRAIN: iteration 1242 : loss : 0.194559, loss_ce: 0.009168, loss_dice: 0.379951
[07:36:27.408] TRAIN: iteration 1243 : loss : 0.157165, loss_ce: 0.018116, loss_dice: 0.296214
[07:36:27.620] TRAIN: iteration 1244 : loss : 0.132031, loss_ce: 0.012022, loss_dice: 0.252040
[07:36:27.826] TRAIN: iteration 1245 : loss : 0.234801, loss_ce: 0.012678, loss_dice: 0.456923
[07:36:36.584] TRAIN: iteration 1246 : loss : 0.192298, loss_ce: 0.039170, loss_dice: 0.345427
[07:36:36.790] TRAIN: iteration 1247 : loss : 0.254858, loss_ce: 0.009230, loss_dice: 0.500486
[07:36:36.996] TRAIN: iteration 1248 : loss : 0.197489, loss_ce: 0.017070, loss_dice: 0.377907
[07:36:37.203] TRAIN: iteration 1249 : loss : 0.101721, loss_ce: 0.010712, loss_dice: 0.192731
[07:36:37.409] TRAIN: iteration 1250 : loss : 0.155473, loss_ce: 0.009236, loss_dice: 0.301709
[07:36:39.428] TRAIN: iteration 1251 : loss : 0.223520, loss_ce: 0.013575, loss_dice: 0.433465
[07:36:41.191] TRAIN: iteration 1252 : loss : 0.250341, loss_ce: 0.012147, loss_dice: 0.488535
[07:36:41.396] TRAIN: iteration 1253 : loss : 0.186854, loss_ce: 0.009924, loss_dice: 0.363785
[07:36:49.038] TRAIN: iteration 1254 : loss : 0.177510, loss_ce: 0.008720, loss_dice: 0.346300
[07:36:49.244] TRAIN: iteration 1255 : loss : 0.210089, loss_ce: 0.008625, loss_dice: 0.411552
[07:36:49.452] TRAIN: iteration 1256 : loss : 0.194511, loss_ce: 0.007921, loss_dice: 0.381101
[07:36:49.658] TRAIN: iteration 1257 : loss : 0.249129, loss_ce: 0.006398, loss_dice: 0.491860
[07:36:49.864] TRAIN: iteration 1258 : loss : 0.252973, loss_ce: 0.005688, loss_dice: 0.500258
[07:36:51.093] TRAIN: iteration 1259 : loss : 0.226778, loss_ce: 0.005173, loss_dice: 0.448383
[07:36:53.470] TRAIN: iteration 1260 : loss : 0.251138, loss_ce: 0.004490, loss_dice: 0.497786
[07:36:53.704] TRAIN: iteration 1261 : loss : 0.225022, loss_ce: 0.006169, loss_dice: 0.443875
[07:37:01.534] TRAIN: iteration 1262 : loss : 0.251157, loss_ce: 0.003743, loss_dice: 0.498572
[07:37:01.740] TRAIN: iteration 1263 : loss : 0.211268, loss_ce: 0.003784, loss_dice: 0.418751
[07:37:01.946] TRAIN: iteration 1264 : loss : 0.207842, loss_ce: 0.010083, loss_dice: 0.405601
[07:37:02.152] TRAIN: iteration 1265 : loss : 0.233199, loss_ce: 0.003772, loss_dice: 0.462626
[07:37:02.358] TRAIN: iteration 1266 : loss : 0.132989, loss_ce: 0.004604, loss_dice: 0.261375
[07:37:03.010] TRAIN: iteration 1267 : loss : 0.251759, loss_ce: 0.003435, loss_dice: 0.500084
[07:37:06.872] TRAIN: iteration 1268 : loss : 0.140721, loss_ce: 0.004200, loss_dice: 0.277242
[07:37:07.077] TRAIN: iteration 1269 : loss : 0.187253, loss_ce: 0.006365, loss_dice: 0.368142
[07:37:13.516] TRAIN: iteration 1270 : loss : 0.174700, loss_ce: 0.004829, loss_dice: 0.344571
[07:37:13.722] TRAIN: iteration 1271 : loss : 0.212537, loss_ce: 0.004590, loss_dice: 0.420484
[07:37:13.928] TRAIN: iteration 1272 : loss : 0.209632, loss_ce: 0.007160, loss_dice: 0.412103
[07:37:14.134] TRAIN: iteration 1273 : loss : 0.145253, loss_ce: 0.006217, loss_dice: 0.284289
[07:37:14.340] TRAIN: iteration 1274 : loss : 0.250363, loss_ce: 0.005707, loss_dice: 0.495019
[07:37:14.749] TRAIN: iteration 1275 : loss : 0.197477, loss_ce: 0.005336, loss_dice: 0.389619
[07:37:18.852] TRAIN: iteration 1276 : loss : 0.250778, loss_ce: 0.005651, loss_dice: 0.495904
[07:37:19.065] TRAIN: iteration 1277 : loss : 0.107216, loss_ce: 0.005427, loss_dice: 0.209005
[07:37:25.433] TRAIN: iteration 1278 : loss : 0.202548, loss_ce: 0.007430, loss_dice: 0.397666
[07:37:25.643] TRAIN: iteration 1279 : loss : 0.233208, loss_ce: 0.020588, loss_dice: 0.445827
[07:37:25.854] TRAIN: iteration 1280 : loss : 0.167828, loss_ce: 0.005648, loss_dice: 0.330009
[07:37:26.090] TRAIN: iteration 1281 : loss : 0.153619, loss_ce: 0.005355, loss_dice: 0.301883
[07:37:26.296] TRAIN: iteration 1282 : loss : 0.179691, loss_ce: 0.005741, loss_dice: 0.353642
[07:37:27.547] TRAIN: iteration 1283 : loss : 0.221048, loss_ce: 0.012177, loss_dice: 0.429920
[07:37:30.734] TRAIN: iteration 1284 : loss : 0.253652, loss_ce: 0.009491, loss_dice: 0.497812
[07:37:30.985] TRAIN: iteration 1285 : loss : 0.252065, loss_ce: 0.016880, loss_dice: 0.487251
[07:37:36.704] TRAIN: iteration 1286 : loss : 0.235033, loss_ce: 0.011134, loss_dice: 0.458932
[07:37:36.910] TRAIN: iteration 1287 : loss : 0.169294, loss_ce: 0.005978, loss_dice: 0.332611
[07:37:37.115] TRAIN: iteration 1288 : loss : 0.176233, loss_ce: 0.005789, loss_dice: 0.346677
[07:37:37.321] TRAIN: iteration 1289 : loss : 0.197202, loss_ce: 0.024081, loss_dice: 0.370324
[07:37:37.530] TRAIN: iteration 1290 : loss : 0.157290, loss_ce: 0.007534, loss_dice: 0.307045
[07:37:39.939] TRAIN: iteration 1291 : loss : 0.113368, loss_ce: 0.008914, loss_dice: 0.217822
[07:37:43.232] TRAIN: iteration 1292 : loss : 0.190032, loss_ce: 0.016956, loss_dice: 0.363108
[07:37:44.104] TRAIN: iteration 1293 : loss : 0.165503, loss_ce: 0.014574, loss_dice: 0.316432
[07:37:49.284] TRAIN: iteration 1294 : loss : 0.206749, loss_ce: 0.012097, loss_dice: 0.401401
[07:37:49.491] TRAIN: iteration 1295 : loss : 0.163739, loss_ce: 0.029153, loss_dice: 0.298325
[07:37:49.697] TRAIN: iteration 1296 : loss : 0.187344, loss_ce: 0.012152, loss_dice: 0.362536
[07:37:49.904] TRAIN: iteration 1297 : loss : 0.217959, loss_ce: 0.016414, loss_dice: 0.419503
[07:37:50.110] TRAIN: iteration 1298 : loss : 0.185598, loss_ce: 0.012953, loss_dice: 0.358242
[07:37:52.155] TRAIN: iteration 1299 : loss : 0.256701, loss_ce: 0.012696, loss_dice: 0.500705
[07:37:55.604] TRAIN: iteration 1300 : loss : 0.208426, loss_ce: 0.013411, loss_dice: 0.403440
[07:37:57.424] TRAIN: iteration 1301 : loss : 0.245295, loss_ce: 0.014805, loss_dice: 0.475784
[07:38:02.042] TRAIN: iteration 1302 : loss : 0.227580, loss_ce: 0.012466, loss_dice: 0.442693
[07:38:02.249] TRAIN: iteration 1303 : loss : 0.231909, loss_ce: 0.011891, loss_dice: 0.451928
[07:38:02.454] TRAIN: iteration 1304 : loss : 0.255769, loss_ce: 0.010947, loss_dice: 0.500591
[07:38:02.660] TRAIN: iteration 1305 : loss : 0.195034, loss_ce: 0.011294, loss_dice: 0.378774
[07:38:02.865] TRAIN: iteration 1306 : loss : 0.254998, loss_ce: 0.009535, loss_dice: 0.500461
[07:38:05.086] TRAIN: iteration 1307 : loss : 0.182688, loss_ce: 0.010098, loss_dice: 0.355278
[07:38:07.667] TRAIN: iteration 1308 : loss : 0.189887, loss_ce: 0.011732, loss_dice: 0.368042
[07:38:10.588] TRAIN: iteration 1309 : loss : 0.157069, loss_ce: 0.019446, loss_dice: 0.294692
[07:38:14.947] TRAIN: iteration 1310 : loss : 0.220884, loss_ce: 0.007325, loss_dice: 0.434444
[07:38:15.153] TRAIN: iteration 1311 : loss : 0.251285, loss_ce: 0.007040, loss_dice: 0.495530
[07:38:15.490] TRAIN: iteration 1312 : loss : 0.253382, loss_ce: 0.006512, loss_dice: 0.500251
[07:38:15.696] TRAIN: iteration 1313 : loss : 0.187062, loss_ce: 0.007178, loss_dice: 0.366947
[07:38:15.901] TRAIN: iteration 1314 : loss : 0.253121, loss_ce: 0.005987, loss_dice: 0.500255
[07:38:16.914] TRAIN: iteration 1315 : loss : 0.252884, loss_ce: 0.005554, loss_dice: 0.500214
[07:38:22.072] TRAIN: iteration 1316 : loss : 0.175207, loss_ce: 0.005403, loss_dice: 0.345011
[07:38:22.965] TRAIN: iteration 1317 : loss : 0.252491, loss_ce: 0.004784, loss_dice: 0.500199
[07:38:26.758] TRAIN: iteration 1318 : loss : 0.191218, loss_ce: 0.006188, loss_dice: 0.376247
[07:38:26.964] TRAIN: iteration 1319 : loss : 0.190865, loss_ce: 0.007910, loss_dice: 0.373820
[07:38:27.171] TRAIN: iteration 1320 : loss : 0.178712, loss_ce: 0.004549, loss_dice: 0.352875
[07:38:27.407] TRAIN: iteration 1321 : loss : 0.230294, loss_ce: 0.011397, loss_dice: 0.449190
[07:38:27.612] TRAIN: iteration 1322 : loss : 0.158622, loss_ce: 0.006433, loss_dice: 0.310811
[07:38:30.346] TRAIN: iteration 1323 : loss : 0.251837, loss_ce: 0.003523, loss_dice: 0.500152
[07:38:33.848] TRAIN: iteration 1324 : loss : 0.163442, loss_ce: 0.010972, loss_dice: 0.315913
[07:38:35.474] TRAIN: iteration 1325 : loss : 0.234237, loss_ce: 0.006688, loss_dice: 0.461785
[07:38:39.400] TRAIN: iteration 1326 : loss : 0.213600, loss_ce: 0.007487, loss_dice: 0.419714
[07:38:39.606] TRAIN: iteration 1327 : loss : 0.187910, loss_ce: 0.005774, loss_dice: 0.370047
[07:38:39.810] TRAIN: iteration 1328 : loss : 0.252740, loss_ce: 0.006173, loss_dice: 0.499308
[07:38:40.015] TRAIN: iteration 1329 : loss : 0.245121, loss_ce: 0.006606, loss_dice: 0.483637
[07:38:40.224] TRAIN: iteration 1330 : loss : 0.235186, loss_ce: 0.006838, loss_dice: 0.463535
[07:38:42.952] TRAIN: iteration 1331 : loss : 0.190698, loss_ce: 0.010366, loss_dice: 0.371031
[07:38:46.436] TRAIN: iteration 1332 : loss : 0.125292, loss_ce: 0.006259, loss_dice: 0.244325
[07:38:48.682] TRAIN: iteration 1333 : loss : 0.218427, loss_ce: 0.006792, loss_dice: 0.430062
[07:38:51.824] TRAIN: iteration 1334 : loss : 0.206673, loss_ce: 0.007952, loss_dice: 0.405393
[07:38:52.031] TRAIN: iteration 1335 : loss : 0.169421, loss_ce: 0.006197, loss_dice: 0.332644
[07:38:52.237] TRAIN: iteration 1336 : loss : 0.196423, loss_ce: 0.007101, loss_dice: 0.385745
[07:38:52.443] TRAIN: iteration 1337 : loss : 0.216156, loss_ce: 0.006294, loss_dice: 0.426018
[07:38:52.648] TRAIN: iteration 1338 : loss : 0.159202, loss_ce: 0.005150, loss_dice: 0.313255
[07:38:55.552] TRAIN: iteration 1339 : loss : 0.123159, loss_ce: 0.007143, loss_dice: 0.239175
[07:38:59.923] TRAIN: iteration 1340 : loss : 0.132433, loss_ce: 0.004454, loss_dice: 0.260411
[07:39:01.698] TRAIN: iteration 1341 : loss : 0.267975, loss_ce: 0.066505, loss_dice: 0.469446
[07:39:03.385] TRAIN: iteration 1342 : loss : 0.209306, loss_ce: 0.012351, loss_dice: 0.406260
[07:39:03.591] TRAIN: iteration 1343 : loss : 0.119990, loss_ce: 0.007726, loss_dice: 0.232254
[07:39:03.796] TRAIN: iteration 1344 : loss : 0.128396, loss_ce: 0.004861, loss_dice: 0.251931
[07:39:04.004] TRAIN: iteration 1345 : loss : 0.185031, loss_ce: 0.006341, loss_dice: 0.363721
[07:39:04.210] TRAIN: iteration 1346 : loss : 0.252276, loss_ce: 0.007813, loss_dice: 0.496739
[07:39:08.324] TRAIN: iteration 1347 : loss : 0.219124, loss_ce: 0.010407, loss_dice: 0.427840
[07:39:12.466] TRAIN: iteration 1348 : loss : 0.187318, loss_ce: 0.007024, loss_dice: 0.367613
[07:39:14.649] TRAIN: iteration 1349 : loss : 0.253290, loss_ce: 0.006217, loss_dice: 0.500363
[07:39:16.624] TRAIN: iteration 1350 : loss : 0.253462, loss_ce: 0.006557, loss_dice: 0.500367
[07:39:16.829] TRAIN: iteration 1351 : loss : 0.164927, loss_ce: 0.009751, loss_dice: 0.320102
[07:39:17.035] TRAIN: iteration 1352 : loss : 0.111232, loss_ce: 0.011090, loss_dice: 0.211374
[07:39:17.240] TRAIN: iteration 1353 : loss : 0.191935, loss_ce: 0.012763, loss_dice: 0.371108
[07:39:17.444] TRAIN: iteration 1354 : loss : 0.240477, loss_ce: 0.026171, loss_dice: 0.454783
[07:39:21.241] TRAIN: iteration 1355 : loss : 0.153981, loss_ce: 0.009358, loss_dice: 0.298603
[07:39:25.069] TRAIN: iteration 1356 : loss : 0.254078, loss_ce: 0.008762, loss_dice: 0.499395
[07:39:26.528] TRAIN: iteration 1357 : loss : 0.179222, loss_ce: 0.008001, loss_dice: 0.350444
[07:39:28.883] TRAIN: iteration 1358 : loss : 0.139752, loss_ce: 0.008144, loss_dice: 0.271360
[07:39:29.094] TRAIN: iteration 1359 : loss : 0.240711, loss_ce: 0.016744, loss_dice: 0.464678
[07:39:29.299] TRAIN: iteration 1360 : loss : 0.254374, loss_ce: 0.008387, loss_dice: 0.500361
[07:39:29.311] NaN or Inf found in input tensor.
[07:39:29.530] TRAIN: iteration 1361 : loss : 0.172857, loss_ce: 0.013673, loss_dice: 0.332040
[07:39:29.735] TRAIN: iteration 1362 : loss : 0.154335, loss_ce: 0.007987, loss_dice: 0.300683
[07:39:33.116] TRAIN: iteration 1363 : loss : 0.198874, loss_ce: 0.009408, loss_dice: 0.388339
[07:39:38.083] TRAIN: iteration 1364 : loss : 0.253889, loss_ce: 0.007346, loss_dice: 0.500431
[07:39:39.247] TRAIN: iteration 1365 : loss : 0.249587, loss_ce: 0.006671, loss_dice: 0.492503
[07:39:40.595] TRAIN: iteration 1366 : loss : 0.215568, loss_ce: 0.006218, loss_dice: 0.424919
[07:39:40.801] TRAIN: iteration 1367 : loss : 0.079265, loss_ce: 0.005328, loss_dice: 0.153202
[07:39:41.007] TRAIN: iteration 1368 : loss : 0.124039, loss_ce: 0.005681, loss_dice: 0.242398
[07:39:41.213] TRAIN: iteration 1369 : loss : 0.228226, loss_ce: 0.012654, loss_dice: 0.443798
[07:39:41.419] TRAIN: iteration 1370 : loss : 0.238819, loss_ce: 0.005891, loss_dice: 0.471747
[07:39:45.868] TRAIN: iteration 1371 : loss : 0.146207, loss_ce: 0.005385, loss_dice: 0.287029
[07:39:51.195] TRAIN: iteration 1372 : loss : 0.152014, loss_ce: 0.008063, loss_dice: 0.295965
[07:39:51.602] TRAIN: iteration 1373 : loss : 0.238235, loss_ce: 0.003652, loss_dice: 0.472818
[07:39:52.723] TRAIN: iteration 1374 : loss : 0.248590, loss_ce: 0.025902, loss_dice: 0.471278
[07:39:52.929] TRAIN: iteration 1375 : loss : 0.217700, loss_ce: 0.004767, loss_dice: 0.430632
[07:39:53.135] TRAIN: iteration 1376 : loss : 0.161162, loss_ce: 0.005597, loss_dice: 0.316726
[07:39:53.340] TRAIN: iteration 1377 : loss : 0.131920, loss_ce: 0.003529, loss_dice: 0.260311
[07:39:53.545] TRAIN: iteration 1378 : loss : 0.237519, loss_ce: 0.010058, loss_dice: 0.464980
[07:39:59.053] TRAIN: iteration 1379 : loss : 0.201300, loss_ce: 0.004892, loss_dice: 0.397708
[07:40:03.798] TRAIN: iteration 1380 : loss : 0.212042, loss_ce: 0.008102, loss_dice: 0.415983
[07:40:05.058] TRAIN: iteration 1381 : loss : 0.248675, loss_ce: 0.047996, loss_dice: 0.449354
[07:40:05.264] TRAIN: iteration 1382 : loss : 0.251999, loss_ce: 0.003884, loss_dice: 0.500115
[07:40:05.469] TRAIN: iteration 1383 : loss : 0.170792, loss_ce: 0.017019, loss_dice: 0.324566
[07:40:05.674] TRAIN: iteration 1384 : loss : 0.211622, loss_ce: 0.009389, loss_dice: 0.413855
[07:40:05.981] TRAIN: iteration 1385 : loss : 0.252716, loss_ce: 0.005249, loss_dice: 0.500183
[07:40:06.186] TRAIN: iteration 1386 : loss : 0.232466, loss_ce: 0.006040, loss_dice: 0.458892
[07:40:13.050] TRAIN: iteration 1387 : loss : 0.161011, loss_ce: 0.006560, loss_dice: 0.315461
[07:40:16.755] TRAIN: iteration 1388 : loss : 0.253426, loss_ce: 0.007055, loss_dice: 0.499797
[07:40:16.962] TRAIN: iteration 1389 : loss : 0.162414, loss_ce: 0.011098, loss_dice: 0.313730
[07:40:17.169] TRAIN: iteration 1390 : loss : 0.247782, loss_ce: 0.008752, loss_dice: 0.486813
[07:40:17.375] TRAIN: iteration 1391 : loss : 0.197653, loss_ce: 0.007011, loss_dice: 0.388294
[07:40:17.581] TRAIN: iteration 1392 : loss : 0.253884, loss_ce: 0.007373, loss_dice: 0.500395
[07:40:17.786] TRAIN: iteration 1393 : loss : 0.191519, loss_ce: 0.008060, loss_dice: 0.374979
[07:40:17.994] TRAIN: iteration 1394 : loss : 0.201749, loss_ce: 0.007663, loss_dice: 0.395835
[07:40:24.655] TRAIN: iteration 1395 : loss : 0.165862, loss_ce: 0.007370, loss_dice: 0.324354
[07:40:29.400] TRAIN: iteration 1396 : loss : 0.205549, loss_ce: 0.007163, loss_dice: 0.403936
[07:40:29.606] TRAIN: iteration 1397 : loss : 0.225527, loss_ce: 0.023828, loss_dice: 0.427226
[07:40:29.815] TRAIN: iteration 1398 : loss : 0.170788, loss_ce: 0.005141, loss_dice: 0.336434
[07:40:30.020] TRAIN: iteration 1399 : loss : 0.167088, loss_ce: 0.005462, loss_dice: 0.328713
[07:40:30.226] TRAIN: iteration 1400 : loss : 0.168026, loss_ce: 0.009622, loss_dice: 0.326430
[07:40:30.464] TRAIN: iteration 1401 : loss : 0.177860, loss_ce: 0.010494, loss_dice: 0.345225
[07:40:30.669] TRAIN: iteration 1402 : loss : 0.204774, loss_ce: 0.006553, loss_dice: 0.402994
[07:40:37.237] TRAIN: iteration 1403 : loss : 0.176497, loss_ce: 0.006115, loss_dice: 0.346879
[07:40:41.725] TRAIN: iteration 1404 : loss : 0.248114, loss_ce: 0.004591, loss_dice: 0.491637
[07:40:41.931] TRAIN: iteration 1405 : loss : 0.252221, loss_ce: 0.004281, loss_dice: 0.500161
[07:40:42.181] TRAIN: iteration 1406 : loss : 0.248071, loss_ce: 0.007193, loss_dice: 0.488948
[07:40:42.387] TRAIN: iteration 1407 : loss : 0.246213, loss_ce: 0.004248, loss_dice: 0.488177
[07:40:42.592] TRAIN: iteration 1408 : loss : 0.194244, loss_ce: 0.004317, loss_dice: 0.384172
[07:40:42.798] TRAIN: iteration 1409 : loss : 0.189433, loss_ce: 0.008907, loss_dice: 0.369960
[07:40:43.005] TRAIN: iteration 1410 : loss : 0.245052, loss_ce: 0.008906, loss_dice: 0.481198
[07:40:50.013] TRAIN: iteration 1411 : loss : 0.176801, loss_ce: 0.006042, loss_dice: 0.347559
[07:40:53.576] TRAIN: iteration 1412 : loss : 0.165002, loss_ce: 0.006822, loss_dice: 0.323181
[07:40:53.896] TRAIN: iteration 1413 : loss : 0.244784, loss_ce: 0.005327, loss_dice: 0.484242
[07:40:54.102] TRAIN: iteration 1414 : loss : 0.251838, loss_ce: 0.003541, loss_dice: 0.500136
[07:40:54.310] TRAIN: iteration 1415 : loss : 0.162228, loss_ce: 0.015138, loss_dice: 0.309319
[07:40:54.515] TRAIN: iteration 1416 : loss : 0.251081, loss_ce: 0.004170, loss_dice: 0.497992
[07:40:54.721] TRAIN: iteration 1417 : loss : 0.168970, loss_ce: 0.007252, loss_dice: 0.330689
[07:40:54.927] TRAIN: iteration 1418 : loss : 0.195293, loss_ce: 0.008707, loss_dice: 0.381879
[07:41:01.677] TRAIN: iteration 1419 : loss : 0.186514, loss_ce: 0.005120, loss_dice: 0.367909
[07:41:06.355] TRAIN: iteration 1420 : loss : 0.164306, loss_ce: 0.005043, loss_dice: 0.323570
[07:41:07.101] TRAIN: iteration 1421 : loss : 0.159243, loss_ce: 0.010786, loss_dice: 0.307701
[07:41:07.307] TRAIN: iteration 1422 : loss : 0.196768, loss_ce: 0.006267, loss_dice: 0.387269
[07:41:07.512] TRAIN: iteration 1423 : loss : 0.194467, loss_ce: 0.005806, loss_dice: 0.383127
[07:41:07.717] TRAIN: iteration 1424 : loss : 0.252817, loss_ce: 0.005306, loss_dice: 0.500327
[07:41:07.923] TRAIN: iteration 1425 : loss : 0.234287, loss_ce: 0.005769, loss_dice: 0.462806
[07:41:08.128] TRAIN: iteration 1426 : loss : 0.173275, loss_ce: 0.034950, loss_dice: 0.311599
[07:41:13.935] TRAIN: iteration 1427 : loss : 0.128704, loss_ce: 0.010042, loss_dice: 0.247366
[07:41:19.702] TRAIN: iteration 1428 : loss : 0.251314, loss_ce: 0.006482, loss_dice: 0.496146
[07:41:19.907] TRAIN: iteration 1429 : loss : 0.114788, loss_ce: 0.006488, loss_dice: 0.223089
[07:41:20.113] TRAIN: iteration 1430 : loss : 0.253938, loss_ce: 0.008773, loss_dice: 0.499104
[07:41:20.318] TRAIN: iteration 1431 : loss : 0.125539, loss_ce: 0.012258, loss_dice: 0.238820
[07:41:20.524] TRAIN: iteration 1432 : loss : 0.239870, loss_ce: 0.008141, loss_dice: 0.471598
[07:41:20.730] TRAIN: iteration 1433 : loss : 0.253265, loss_ce: 0.006184, loss_dice: 0.500345
[07:41:20.936] TRAIN: iteration 1434 : loss : 0.155581, loss_ce: 0.007453, loss_dice: 0.303710
[07:41:26.242] TRAIN: iteration 1435 : loss : 0.253444, loss_ce: 0.006538, loss_dice: 0.500351
[07:41:32.023] TRAIN: iteration 1436 : loss : 0.193114, loss_ce: 0.018351, loss_dice: 0.367877
[07:41:32.233] TRAIN: iteration 1437 : loss : 0.170881, loss_ce: 0.008228, loss_dice: 0.333534
[07:41:32.442] TRAIN: iteration 1438 : loss : 0.235401, loss_ce: 0.007358, loss_dice: 0.463445
[07:41:32.649] TRAIN: iteration 1439 : loss : 0.146781, loss_ce: 0.011000, loss_dice: 0.282563
[07:41:32.855] TRAIN: iteration 1440 : loss : 0.132545, loss_ce: 0.011407, loss_dice: 0.253683
[07:41:33.090] TRAIN: iteration 1441 : loss : 0.200904, loss_ce: 0.008339, loss_dice: 0.393469
[07:41:33.297] TRAIN: iteration 1442 : loss : 0.120683, loss_ce: 0.009109, loss_dice: 0.232256
[07:41:38.136] TRAIN: iteration 1443 : loss : 0.157270, loss_ce: 0.007341, loss_dice: 0.307200
[07:41:45.017] TRAIN: iteration 1444 : loss : 0.193640, loss_ce: 0.016068, loss_dice: 0.371212
[07:41:45.228] TRAIN: iteration 1445 : loss : 0.254542, loss_ce: 0.010968, loss_dice: 0.498116
[07:41:45.434] TRAIN: iteration 1446 : loss : 0.178815, loss_ce: 0.006219, loss_dice: 0.351410
[07:41:45.641] TRAIN: iteration 1447 : loss : 0.175692, loss_ce: 0.005246, loss_dice: 0.346139
[07:41:45.847] TRAIN: iteration 1448 : loss : 0.087194, loss_ce: 0.005537, loss_dice: 0.168851
[07:41:46.053] TRAIN: iteration 1449 : loss : 0.154159, loss_ce: 0.008550, loss_dice: 0.299769
[07:41:46.261] TRAIN: iteration 1450 : loss : 0.146196, loss_ce: 0.005981, loss_dice: 0.286411
[07:41:49.569] TRAIN: iteration 1451 : loss : 0.135259, loss_ce: 0.009103, loss_dice: 0.261415
[07:41:56.357] TRAIN: iteration 1452 : loss : 0.174499, loss_ce: 0.005659, loss_dice: 0.343340
[07:41:56.564] TRAIN: iteration 1453 : loss : 0.197049, loss_ce: 0.006729, loss_dice: 0.387369
[07:41:56.770] TRAIN: iteration 1454 : loss : 0.248437, loss_ce: 0.007260, loss_dice: 0.489614
[07:41:56.976] TRAIN: iteration 1455 : loss : 0.139739, loss_ce: 0.006054, loss_dice: 0.273423
[07:41:57.182] TRAIN: iteration 1456 : loss : 0.253300, loss_ce: 0.007012, loss_dice: 0.499587
[07:41:57.388] TRAIN: iteration 1457 : loss : 0.251679, loss_ce: 0.003475, loss_dice: 0.499883
[07:41:57.595] TRAIN: iteration 1458 : loss : 0.251798, loss_ce: 0.003474, loss_dice: 0.500122
[07:42:01.652] TRAIN: iteration 1459 : loss : 0.177534, loss_ce: 0.009948, loss_dice: 0.345120
[07:42:10.539] TRAIN: iteration 1460 : loss : 0.245666, loss_ce: 0.003444, loss_dice: 0.487888
[07:42:10.776] TRAIN: iteration 1461 : loss : 0.167812, loss_ce: 0.005017, loss_dice: 0.330607
[07:42:10.982] TRAIN: iteration 1462 : loss : 0.252136, loss_ce: 0.006266, loss_dice: 0.498005
[07:42:11.190] TRAIN: iteration 1463 : loss : 0.252125, loss_ce: 0.004052, loss_dice: 0.500197
[07:42:11.398] TRAIN: iteration 1464 : loss : 0.221629, loss_ce: 0.017707, loss_dice: 0.425552
[07:42:11.613] TRAIN: iteration 1465 : loss : 0.224606, loss_ce: 0.029573, loss_dice: 0.419640
[07:42:11.822] TRAIN: iteration 1466 : loss : 0.117891, loss_ce: 0.007733, loss_dice: 0.228049
[07:42:15.020] TRAIN: iteration 1467 : loss : 0.225057, loss_ce: 0.012890, loss_dice: 0.437224
[07:42:22.277] TRAIN: iteration 1468 : loss : 0.176228, loss_ce: 0.006585, loss_dice: 0.345871
[07:42:22.483] TRAIN: iteration 1469 : loss : 0.092732, loss_ce: 0.005832, loss_dice: 0.179632
[07:42:22.690] TRAIN: iteration 1470 : loss : 0.179215, loss_ce: 0.005826, loss_dice: 0.352605
[07:42:22.896] TRAIN: iteration 1471 : loss : 0.188968, loss_ce: 0.007881, loss_dice: 0.370054
[07:42:23.101] TRAIN: iteration 1472 : loss : 0.160016, loss_ce: 0.012359, loss_dice: 0.307673
[07:42:23.308] TRAIN: iteration 1473 : loss : 0.239537, loss_ce: 0.006241, loss_dice: 0.472834
[07:42:23.516] TRAIN: iteration 1474 : loss : 0.253225, loss_ce: 0.006129, loss_dice: 0.500322
[07:42:27.659] TRAIN: iteration 1475 : loss : 0.149044, loss_ce: 0.006902, loss_dice: 0.291187
[07:42:33.987] TRAIN: iteration 1476 : loss : 0.253275, loss_ce: 0.006218, loss_dice: 0.500332
[07:42:34.200] TRAIN: iteration 1477 : loss : 0.182259, loss_ce: 0.008894, loss_dice: 0.355623
[07:42:34.407] TRAIN: iteration 1478 : loss : 0.126069, loss_ce: 0.005701, loss_dice: 0.246438
[07:42:34.614] TRAIN: iteration 1479 : loss : 0.162278, loss_ce: 0.008412, loss_dice: 0.316144
[07:42:34.820] TRAIN: iteration 1480 : loss : 0.233319, loss_ce: 0.009867, loss_dice: 0.456770
[07:42:35.054] TRAIN: iteration 1481 : loss : 0.236503, loss_ce: 0.005981, loss_dice: 0.467026
[07:42:35.261] TRAIN: iteration 1482 : loss : 0.251832, loss_ce: 0.012105, loss_dice: 0.491559
[07:42:41.519] TRAIN: iteration 1483 : loss : 0.240137, loss_ce: 0.007768, loss_dice: 0.472506
[07:42:46.400] TRAIN: iteration 1484 : loss : 0.179382, loss_ce: 0.006472, loss_dice: 0.352292
[07:42:46.606] TRAIN: iteration 1485 : loss : 0.175864, loss_ce: 0.015566, loss_dice: 0.336161
[07:42:46.812] TRAIN: iteration 1486 : loss : 0.163487, loss_ce: 0.007920, loss_dice: 0.319054
[07:42:47.018] TRAIN: iteration 1487 : loss : 0.148808, loss_ce: 0.014642, loss_dice: 0.282974
[07:42:47.231] TRAIN: iteration 1488 : loss : 0.253011, loss_ce: 0.005748, loss_dice: 0.500274
[07:42:47.436] TRAIN: iteration 1489 : loss : 0.253242, loss_ce: 0.006205, loss_dice: 0.500279
[07:42:47.643] TRAIN: iteration 1490 : loss : 0.170718, loss_ce: 0.006266, loss_dice: 0.335171
[07:42:54.401] TRAIN: iteration 1491 : loss : 0.253319, loss_ce: 0.006355, loss_dice: 0.500284
[07:42:59.384] TRAIN: iteration 1492 : loss : 0.198390, loss_ce: 0.016206, loss_dice: 0.380573
[07:42:59.591] TRAIN: iteration 1493 : loss : 0.161407, loss_ce: 0.015903, loss_dice: 0.306911
[07:42:59.798] TRAIN: iteration 1494 : loss : 0.245437, loss_ce: 0.006600, loss_dice: 0.484273
[07:43:00.005] TRAIN: iteration 1495 : loss : 0.171026, loss_ce: 0.008581, loss_dice: 0.333470
[07:43:00.212] TRAIN: iteration 1496 : loss : 0.196634, loss_ce: 0.006639, loss_dice: 0.386629
[07:43:00.418] TRAIN: iteration 1497 : loss : 0.154233, loss_ce: 0.007766, loss_dice: 0.300700
[07:43:00.624] TRAIN: iteration 1498 : loss : 0.131093, loss_ce: 0.006965, loss_dice: 0.255220
[07:43:06.910] TRAIN: iteration 1499 : loss : 0.150881, loss_ce: 0.007478, loss_dice: 0.294283
[07:43:11.334] TRAIN: iteration 1500 : loss : 0.119641, loss_ce: 0.005198, loss_dice: 0.234085
[07:43:11.571] TRAIN: iteration 1501 : loss : 0.156681, loss_ce: 0.004534, loss_dice: 0.308828
[07:43:11.777] TRAIN: iteration 1502 : loss : 0.251125, loss_ce: 0.004255, loss_dice: 0.497996
[07:43:11.985] TRAIN: iteration 1503 : loss : 0.217922, loss_ce: 0.004550, loss_dice: 0.431295
[07:43:12.191] TRAIN: iteration 1504 : loss : 0.233308, loss_ce: 0.005808, loss_dice: 0.460808
[07:43:12.398] TRAIN: iteration 1505 : loss : 0.244586, loss_ce: 0.005727, loss_dice: 0.483446
[07:43:12.605] TRAIN: iteration 1506 : loss : 0.251005, loss_ce: 0.001981, loss_dice: 0.500028
[07:43:20.337] TRAIN: iteration 1507 : loss : 0.168644, loss_ce: 0.002225, loss_dice: 0.335064
[07:43:24.171] TRAIN: iteration 1508 : loss : 0.174318, loss_ce: 0.006130, loss_dice: 0.342506
[07:43:24.377] TRAIN: iteration 1509 : loss : 0.247076, loss_ce: 0.006734, loss_dice: 0.487418
[07:43:24.584] TRAIN: iteration 1510 : loss : 0.221648, loss_ce: 0.007306, loss_dice: 0.435990
[07:43:24.789] TRAIN: iteration 1511 : loss : 0.248574, loss_ce: 0.007087, loss_dice: 0.490060
[07:43:24.994] TRAIN: iteration 1512 : loss : 0.235647, loss_ce: 0.006530, loss_dice: 0.464765
[07:43:25.199] TRAIN: iteration 1513 : loss : 0.135158, loss_ce: 0.003355, loss_dice: 0.266960
[07:43:25.404] TRAIN: iteration 1514 : loss : 0.244274, loss_ce: 0.003393, loss_dice: 0.485154
[07:43:32.630] TRAIN: iteration 1515 : loss : 0.190237, loss_ce: 0.007171, loss_dice: 0.373302
[07:43:37.040] TRAIN: iteration 1516 : loss : 0.174643, loss_ce: 0.009925, loss_dice: 0.339361
[07:43:37.247] TRAIN: iteration 1517 : loss : 0.223712, loss_ce: 0.005066, loss_dice: 0.442358
[07:43:37.453] TRAIN: iteration 1518 : loss : 0.159595, loss_ce: 0.006247, loss_dice: 0.312944
[07:43:37.658] TRAIN: iteration 1519 : loss : 0.234221, loss_ce: 0.004360, loss_dice: 0.464082
[07:43:37.864] TRAIN: iteration 1520 : loss : 0.168713, loss_ce: 0.004085, loss_dice: 0.333341
[07:43:38.097] TRAIN: iteration 1521 : loss : 0.166009, loss_ce: 0.004168, loss_dice: 0.327850
[07:43:38.304] TRAIN: iteration 1522 : loss : 0.169064, loss_ce: 0.006297, loss_dice: 0.331831
[07:43:45.532] TRAIN: iteration 1523 : loss : 0.251102, loss_ce: 0.041940, loss_dice: 0.460265
[07:43:50.286] TRAIN: iteration 1524 : loss : 0.259599, loss_ce: 0.020112, loss_dice: 0.499086
[07:43:50.491] TRAIN: iteration 1525 : loss : 0.214325, loss_ce: 0.007809, loss_dice: 0.420842
[07:43:50.698] TRAIN: iteration 1526 : loss : 0.231159, loss_ce: 0.004457, loss_dice: 0.457862
[07:43:50.903] TRAIN: iteration 1527 : loss : 0.251828, loss_ce: 0.003526, loss_dice: 0.500130
[07:43:51.111] TRAIN: iteration 1528 : loss : 0.230482, loss_ce: 0.042633, loss_dice: 0.418331
[07:43:51.316] TRAIN: iteration 1529 : loss : 0.147058, loss_ce: 0.011182, loss_dice: 0.282935
[07:43:51.521] TRAIN: iteration 1530 : loss : 0.221398, loss_ce: 0.005255, loss_dice: 0.437541
[07:43:56.845] TRAIN: iteration 1531 : loss : 0.197692, loss_ce: 0.006027, loss_dice: 0.389357
[07:44:02.131] TRAIN: iteration 1532 : loss : 0.161018, loss_ce: 0.012305, loss_dice: 0.309730
[07:44:02.337] TRAIN: iteration 1533 : loss : 0.252719, loss_ce: 0.005173, loss_dice: 0.500266
[07:44:02.544] TRAIN: iteration 1534 : loss : 0.222442, loss_ce: 0.006161, loss_dice: 0.438723
[07:44:02.749] TRAIN: iteration 1535 : loss : 0.231914, loss_ce: 0.006701, loss_dice: 0.457128
[07:44:02.955] TRAIN: iteration 1536 : loss : 0.146350, loss_ce: 0.009126, loss_dice: 0.283574
[07:44:03.160] TRAIN: iteration 1537 : loss : 0.154807, loss_ce: 0.009917, loss_dice: 0.299698
[07:44:03.367] TRAIN: iteration 1538 : loss : 0.098647, loss_ce: 0.009461, loss_dice: 0.187833
[07:44:10.240] TRAIN: iteration 1539 : loss : 0.253673, loss_ce: 0.007678, loss_dice: 0.499667
[07:44:14.440] TRAIN: iteration 1540 : loss : 0.153704, loss_ce: 0.012613, loss_dice: 0.294794
[07:44:14.670] TRAIN: iteration 1541 : loss : 0.252604, loss_ce: 0.007950, loss_dice: 0.497259
[07:44:14.876] TRAIN: iteration 1542 : loss : 0.254204, loss_ce: 0.007956, loss_dice: 0.500452
[07:44:15.082] TRAIN: iteration 1543 : loss : 0.190737, loss_ce: 0.015423, loss_dice: 0.366051
[07:44:15.289] TRAIN: iteration 1544 : loss : 0.170741, loss_ce: 0.016224, loss_dice: 0.325257
[07:44:15.494] TRAIN: iteration 1545 : loss : 0.124125, loss_ce: 0.011265, loss_dice: 0.236984
[07:44:15.700] TRAIN: iteration 1546 : loss : 0.259822, loss_ce: 0.019601, loss_dice: 0.500042
[07:44:22.504] TRAIN: iteration 1547 : loss : 0.146663, loss_ce: 0.010324, loss_dice: 0.283001
[07:44:27.017] TRAIN: iteration 1548 : loss : 0.231624, loss_ce: 0.015028, loss_dice: 0.448219
[07:44:27.229] TRAIN: iteration 1549 : loss : 0.191506, loss_ce: 0.010442, loss_dice: 0.372570
[07:44:27.435] TRAIN: iteration 1550 : loss : 0.146429, loss_ce: 0.010963, loss_dice: 0.281895
[07:44:27.640] TRAIN: iteration 1551 : loss : 0.211433, loss_ce: 0.008460, loss_dice: 0.414407
[07:44:27.845] TRAIN: iteration 1552 : loss : 0.215859, loss_ce: 0.017375, loss_dice: 0.414344
[07:44:28.051] TRAIN: iteration 1553 : loss : 0.181042, loss_ce: 0.018522, loss_dice: 0.343562
[07:44:28.256] TRAIN: iteration 1554 : loss : 0.254317, loss_ce: 0.008145, loss_dice: 0.500489
[07:44:35.578] TRAIN: iteration 1555 : loss : 0.201857, loss_ce: 0.009355, loss_dice: 0.394359
[07:44:39.372] TRAIN: iteration 1556 : loss : 0.194109, loss_ce: 0.025034, loss_dice: 0.363184
[07:44:39.578] TRAIN: iteration 1557 : loss : 0.190740, loss_ce: 0.008686, loss_dice: 0.372795
[07:44:39.784] TRAIN: iteration 1558 : loss : 0.164051, loss_ce: 0.008271, loss_dice: 0.319831
[07:44:39.989] TRAIN: iteration 1559 : loss : 0.157488, loss_ce: 0.007148, loss_dice: 0.307829
[07:44:40.195] TRAIN: iteration 1560 : loss : 0.123558, loss_ce: 0.006742, loss_dice: 0.240374
[07:44:40.433] TRAIN: iteration 1561 : loss : 0.181574, loss_ce: 0.007144, loss_dice: 0.356005
[07:44:40.639] TRAIN: iteration 1562 : loss : 0.197745, loss_ce: 0.005487, loss_dice: 0.390004
[07:44:47.600] TRAIN: iteration 1563 : loss : 0.138663, loss_ce: 0.005137, loss_dice: 0.272188
[07:44:50.921] TRAIN: iteration 1564 : loss : 0.222885, loss_ce: 0.006039, loss_dice: 0.439731
[07:44:51.442] TRAIN: iteration 1565 : loss : 0.231096, loss_ce: 0.011847, loss_dice: 0.450345
[07:44:51.647] TRAIN: iteration 1566 : loss : 0.228729, loss_ce: 0.006998, loss_dice: 0.450460
[07:44:51.854] TRAIN: iteration 1567 : loss : 0.130669, loss_ce: 0.004313, loss_dice: 0.257025
[07:44:52.060] TRAIN: iteration 1568 : loss : 0.157113, loss_ce: 0.006130, loss_dice: 0.308097
[07:44:52.269] TRAIN: iteration 1569 : loss : 0.224767, loss_ce: 0.021230, loss_dice: 0.428304
[07:44:52.475] TRAIN: iteration 1570 : loss : 0.200073, loss_ce: 0.008869, loss_dice: 0.391276
[07:44:59.253] TRAIN: iteration 1571 : loss : 0.180419, loss_ce: 0.004575, loss_dice: 0.356262
[07:45:03.470] TRAIN: iteration 1572 : loss : 0.213009, loss_ce: 0.021958, loss_dice: 0.404061
[07:45:03.820] TRAIN: iteration 1573 : loss : 0.202179, loss_ce: 0.030575, loss_dice: 0.373783
[07:45:04.026] TRAIN: iteration 1574 : loss : 0.234060, loss_ce: 0.008334, loss_dice: 0.459787
[07:45:04.232] TRAIN: iteration 1575 : loss : 0.206179, loss_ce: 0.009724, loss_dice: 0.402634
[07:45:04.442] TRAIN: iteration 1576 : loss : 0.143845, loss_ce: 0.015977, loss_dice: 0.271713
[07:45:04.648] TRAIN: iteration 1577 : loss : 0.240043, loss_ce: 0.013952, loss_dice: 0.466135
[07:45:04.853] TRAIN: iteration 1578 : loss : 0.237491, loss_ce: 0.008857, loss_dice: 0.466124
[07:45:11.726] TRAIN: iteration 1579 : loss : 0.215448, loss_ce: 0.011100, loss_dice: 0.419796
[07:45:16.498] TRAIN: iteration 1580 : loss : 0.223654, loss_ce: 0.013164, loss_dice: 0.434145
[07:45:17.012] TRAIN: iteration 1581 : loss : 0.176754, loss_ce: 0.017979, loss_dice: 0.335530
[07:45:17.219] TRAIN: iteration 1582 : loss : 0.218244, loss_ce: 0.010682, loss_dice: 0.425807
[07:45:17.425] TRAIN: iteration 1583 : loss : 0.149562, loss_ce: 0.012795, loss_dice: 0.286329
[07:45:17.634] TRAIN: iteration 1584 : loss : 0.134828, loss_ce: 0.013168, loss_dice: 0.256487
[07:45:17.839] TRAIN: iteration 1585 : loss : 0.111818, loss_ce: 0.009259, loss_dice: 0.214377
[07:45:18.044] TRAIN: iteration 1586 : loss : 0.180587, loss_ce: 0.012313, loss_dice: 0.348860
[07:45:24.146] TRAIN: iteration 1587 : loss : 0.163970, loss_ce: 0.010924, loss_dice: 0.317016
[07:45:29.339] TRAIN: iteration 1588 : loss : 0.174735, loss_ce: 0.010763, loss_dice: 0.338706
[07:45:29.548] TRAIN: iteration 1589 : loss : 0.254792, loss_ce: 0.009063, loss_dice: 0.500520
[07:45:29.754] TRAIN: iteration 1590 : loss : 0.198097, loss_ce: 0.008747, loss_dice: 0.387446
[07:45:29.959] TRAIN: iteration 1591 : loss : 0.165335, loss_ce: 0.007659, loss_dice: 0.323012
[07:45:30.166] TRAIN: iteration 1592 : loss : 0.235563, loss_ce: 0.007184, loss_dice: 0.463942
[07:45:30.373] TRAIN: iteration 1593 : loss : 0.211425, loss_ce: 0.006486, loss_dice: 0.416365
[07:45:30.580] TRAIN: iteration 1594 : loss : 0.195631, loss_ce: 0.007383, loss_dice: 0.383878
[07:45:36.038] TRAIN: iteration 1595 : loss : 0.192624, loss_ce: 0.024582, loss_dice: 0.360667
[07:45:40.657] TRAIN: iteration 1596 : loss : 0.217085, loss_ce: 0.048589, loss_dice: 0.385581
[07:45:42.350] TRAIN: iteration 1597 : loss : 0.241592, loss_ce: 0.004931, loss_dice: 0.478253
[07:45:42.556] TRAIN: iteration 1598 : loss : 0.208832, loss_ce: 0.004687, loss_dice: 0.412977
[07:45:42.762] TRAIN: iteration 1599 : loss : 0.161424, loss_ce: 0.006725, loss_dice: 0.316124
[07:45:42.978] TRAIN: iteration 1600 : loss : 0.252191, loss_ce: 0.004188, loss_dice: 0.500195
[07:45:42.979] NaN or Inf found in input tensor.
[07:45:43.191] TRAIN: iteration 1601 : loss : 0.177271, loss_ce: 0.007869, loss_dice: 0.346673
[07:45:43.398] TRAIN: iteration 1602 : loss : 0.130013, loss_ce: 0.005203, loss_dice: 0.254823
[07:45:48.945] TRAIN: iteration 1603 : loss : 0.154889, loss_ce: 0.009416, loss_dice: 0.300362
[07:45:52.818] TRAIN: iteration 1604 : loss : 0.186296, loss_ce: 0.009018, loss_dice: 0.363574
[07:45:55.902] TRAIN: iteration 1605 : loss : 0.152974, loss_ce: 0.016889, loss_dice: 0.289059
[07:45:56.114] TRAIN: iteration 1606 : loss : 0.207677, loss_ce: 0.041683, loss_dice: 0.373670
[07:45:56.319] TRAIN: iteration 1607 : loss : 0.252744, loss_ce: 0.005232, loss_dice: 0.500256
[07:45:56.525] TRAIN: iteration 1608 : loss : 0.185349, loss_ce: 0.006078, loss_dice: 0.364620
[07:45:56.732] TRAIN: iteration 1609 : loss : 0.187617, loss_ce: 0.009496, loss_dice: 0.365739
[07:45:56.939] TRAIN: iteration 1610 : loss : 0.241670, loss_ce: 0.011853, loss_dice: 0.471486
[07:46:01.445] TRAIN: iteration 1611 : loss : 0.154794, loss_ce: 0.007704, loss_dice: 0.301884
[07:46:05.033] TRAIN: iteration 1612 : loss : 0.162340, loss_ce: 0.007481, loss_dice: 0.317199
[07:46:07.789] TRAIN: iteration 1613 : loss : 0.165241, loss_ce: 0.010136, loss_dice: 0.320346
[07:46:07.994] TRAIN: iteration 1614 : loss : 0.133775, loss_ce: 0.007791, loss_dice: 0.259760
[07:46:08.201] TRAIN: iteration 1615 : loss : 0.252665, loss_ce: 0.007572, loss_dice: 0.497757
[07:46:08.407] TRAIN: iteration 1616 : loss : 0.199641, loss_ce: 0.011355, loss_dice: 0.387927
[07:46:08.613] TRAIN: iteration 1617 : loss : 0.244923, loss_ce: 0.007723, loss_dice: 0.482123
[07:46:08.818] TRAIN: iteration 1618 : loss : 0.220449, loss_ce: 0.013537, loss_dice: 0.427360
[07:46:14.006] TRAIN: iteration 1619 : loss : 0.188270, loss_ce: 0.024347, loss_dice: 0.352193
[07:46:18.107] TRAIN: iteration 1620 : loss : 0.149617, loss_ce: 0.010694, loss_dice: 0.288540
[07:46:19.664] TRAIN: iteration 1621 : loss : 0.254101, loss_ce: 0.007725, loss_dice: 0.500478
[07:46:19.870] TRAIN: iteration 1622 : loss : 0.183677, loss_ce: 0.021774, loss_dice: 0.345581
[07:46:20.078] TRAIN: iteration 1623 : loss : 0.219175, loss_ce: 0.008575, loss_dice: 0.429774
[07:46:20.284] TRAIN: iteration 1624 : loss : 0.252373, loss_ce: 0.011780, loss_dice: 0.492966
[07:46:20.489] TRAIN: iteration 1625 : loss : 0.209034, loss_ce: 0.008035, loss_dice: 0.410034
[07:46:20.694] TRAIN: iteration 1626 : loss : 0.143033, loss_ce: 0.007344, loss_dice: 0.278722
[07:46:26.512] TRAIN: iteration 1627 : loss : 0.198495, loss_ce: 0.007611, loss_dice: 0.389378
[07:46:30.259] TRAIN: iteration 1628 : loss : 0.203563, loss_ce: 0.012006, loss_dice: 0.395119
[07:46:31.723] TRAIN: iteration 1629 : loss : 0.163599, loss_ce: 0.007097, loss_dice: 0.320100
[07:46:31.929] TRAIN: iteration 1630 : loss : 0.166532, loss_ce: 0.014726, loss_dice: 0.318339
[07:46:32.135] TRAIN: iteration 1631 : loss : 0.130093, loss_ce: 0.009554, loss_dice: 0.250633
[07:46:32.341] TRAIN: iteration 1632 : loss : 0.253206, loss_ce: 0.006057, loss_dice: 0.500355
[07:46:32.548] TRAIN: iteration 1633 : loss : 0.253408, loss_ce: 0.006402, loss_dice: 0.500414
[07:46:32.758] TRAIN: iteration 1634 : loss : 0.192784, loss_ce: 0.008406, loss_dice: 0.377163
[07:46:39.591] TRAIN: iteration 1635 : loss : 0.225923, loss_ce: 0.007863, loss_dice: 0.443982
[07:46:42.775] TRAIN: iteration 1636 : loss : 0.182996, loss_ce: 0.006652, loss_dice: 0.359340
[07:46:43.903] TRAIN: iteration 1637 : loss : 0.216959, loss_ce: 0.008461, loss_dice: 0.425457
[07:46:44.113] TRAIN: iteration 1638 : loss : 0.237569, loss_ce: 0.012046, loss_dice: 0.463091
[07:46:44.319] TRAIN: iteration 1639 : loss : 0.252344, loss_ce: 0.004450, loss_dice: 0.500238
[07:46:44.525] TRAIN: iteration 1640 : loss : 0.136806, loss_ce: 0.004534, loss_dice: 0.269078
[07:46:44.759] TRAIN: iteration 1641 : loss : 0.252165, loss_ce: 0.004120, loss_dice: 0.500210
[07:46:44.964] TRAIN: iteration 1642 : loss : 0.158023, loss_ce: 0.007381, loss_dice: 0.308665
[07:46:51.642] TRAIN: iteration 1643 : loss : 0.251838, loss_ce: 0.003538, loss_dice: 0.500137
[07:46:55.444] TRAIN: iteration 1644 : loss : 0.191848, loss_ce: 0.020475, loss_dice: 0.363222
[07:46:56.168] TRAIN: iteration 1645 : loss : 0.197098, loss_ce: 0.004979, loss_dice: 0.389216
[07:46:56.374] TRAIN: iteration 1646 : loss : 0.113589, loss_ce: 0.004769, loss_dice: 0.222408
[07:46:56.582] TRAIN: iteration 1647 : loss : 0.193517, loss_ce: 0.007038, loss_dice: 0.379996
[07:46:56.788] TRAIN: iteration 1648 : loss : 0.140152, loss_ce: 0.004451, loss_dice: 0.275852
[07:46:56.998] TRAIN: iteration 1649 : loss : 0.167742, loss_ce: 0.006675, loss_dice: 0.328808
[07:46:57.204] TRAIN: iteration 1650 : loss : 0.252061, loss_ce: 0.003945, loss_dice: 0.500177
[07:47:03.506] TRAIN: iteration 1651 : loss : 0.183177, loss_ce: 0.004605, loss_dice: 0.361749
[07:47:08.388] TRAIN: iteration 1652 : loss : 0.171168, loss_ce: 0.005071, loss_dice: 0.337265
[07:47:08.595] TRAIN: iteration 1653 : loss : 0.252037, loss_ce: 0.003877, loss_dice: 0.500196
[07:47:08.801] TRAIN: iteration 1654 : loss : 0.180077, loss_ce: 0.003582, loss_dice: 0.356573
[07:47:09.008] TRAIN: iteration 1655 : loss : 0.147502, loss_ce: 0.010345, loss_dice: 0.284659
[07:47:09.213] TRAIN: iteration 1656 : loss : 0.178569, loss_ce: 0.010252, loss_dice: 0.346886
[07:47:09.419] TRAIN: iteration 1657 : loss : 0.208053, loss_ce: 0.003557, loss_dice: 0.412549
[07:47:09.628] TRAIN: iteration 1658 : loss : 0.170834, loss_ce: 0.010167, loss_dice: 0.331501
[07:47:16.367] TRAIN: iteration 1659 : loss : 0.210421, loss_ce: 0.008576, loss_dice: 0.412265
[07:47:20.217] TRAIN: iteration 1660 : loss : 0.226304, loss_ce: 0.008267, loss_dice: 0.444341
[07:47:20.450] TRAIN: iteration 1661 : loss : 0.201511, loss_ce: 0.011431, loss_dice: 0.391590
[07:47:20.656] TRAIN: iteration 1662 : loss : 0.220997, loss_ce: 0.011988, loss_dice: 0.430006
[07:47:20.865] TRAIN: iteration 1663 : loss : 0.146784, loss_ce: 0.005495, loss_dice: 0.288072
[07:47:21.073] TRAIN: iteration 1664 : loss : 0.226028, loss_ce: 0.006318, loss_dice: 0.445738
[07:47:21.280] TRAIN: iteration 1665 : loss : 0.138768, loss_ce: 0.007489, loss_dice: 0.270046
[07:47:21.486] TRAIN: iteration 1666 : loss : 0.108985, loss_ce: 0.006163, loss_dice: 0.211806
[07:47:29.166] TRAIN: iteration 1667 : loss : 0.180504, loss_ce: 0.007553, loss_dice: 0.353456
[07:47:32.510] TRAIN: iteration 1668 : loss : 0.214534, loss_ce: 0.027705, loss_dice: 0.401364
[07:47:32.739] TRAIN: iteration 1669 : loss : 0.221760, loss_ce: 0.022321, loss_dice: 0.421198
[07:47:32.946] TRAIN: iteration 1670 : loss : 0.156888, loss_ce: 0.019052, loss_dice: 0.294723
[07:47:33.152] TRAIN: iteration 1671 : loss : 0.252364, loss_ce: 0.006696, loss_dice: 0.498032
[07:47:33.357] TRAIN: iteration 1672 : loss : 0.151145, loss_ce: 0.012685, loss_dice: 0.289605
[07:47:33.563] TRAIN: iteration 1673 : loss : 0.200168, loss_ce: 0.008180, loss_dice: 0.392156
[07:47:33.769] TRAIN: iteration 1674 : loss : 0.142613, loss_ce: 0.007263, loss_dice: 0.277964
[07:47:40.366] TRAIN: iteration 1675 : loss : 0.253689, loss_ce: 0.007413, loss_dice: 0.499966
[07:47:43.880] TRAIN: iteration 1676 : loss : 0.211492, loss_ce: 0.009810, loss_dice: 0.413174
[07:47:46.041] TRAIN: iteration 1677 : loss : 0.151226, loss_ce: 0.008523, loss_dice: 0.293929
[07:47:46.254] TRAIN: iteration 1678 : loss : 0.217419, loss_ce: 0.014227, loss_dice: 0.420611
[07:47:46.460] TRAIN: iteration 1679 : loss : 0.247593, loss_ce: 0.007249, loss_dice: 0.487937
[07:47:46.667] TRAIN: iteration 1680 : loss : 0.175453, loss_ce: 0.007906, loss_dice: 0.342999
[07:47:46.899] TRAIN: iteration 1681 : loss : 0.219675, loss_ce: 0.033136, loss_dice: 0.406213
[07:47:47.105] TRAIN: iteration 1682 : loss : 0.219720, loss_ce: 0.007138, loss_dice: 0.432301
[07:47:53.119] TRAIN: iteration 1683 : loss : 0.164001, loss_ce: 0.009164, loss_dice: 0.318838
[07:47:56.941] TRAIN: iteration 1684 : loss : 0.155993, loss_ce: 0.017516, loss_dice: 0.294470
[07:47:59.067] TRAIN: iteration 1685 : loss : 0.118163, loss_ce: 0.006356, loss_dice: 0.229971
[07:47:59.272] TRAIN: iteration 1686 : loss : 0.241243, loss_ce: 0.009413, loss_dice: 0.473073
[07:47:59.478] TRAIN: iteration 1687 : loss : 0.250917, loss_ce: 0.019169, loss_dice: 0.482664
[07:47:59.684] TRAIN: iteration 1688 : loss : 0.245814, loss_ce: 0.006446, loss_dice: 0.485182
[07:47:59.891] TRAIN: iteration 1689 : loss : 0.249733, loss_ce: 0.012418, loss_dice: 0.487049
[07:48:00.098] TRAIN: iteration 1690 : loss : 0.152580, loss_ce: 0.006759, loss_dice: 0.298402
[07:48:06.704] TRAIN: iteration 1691 : loss : 0.202153, loss_ce: 0.009746, loss_dice: 0.394561
[07:48:09.793] TRAIN: iteration 1692 : loss : 0.085967, loss_ce: 0.006332, loss_dice: 0.165603
[07:48:10.874] TRAIN: iteration 1693 : loss : 0.135346, loss_ce: 0.015781, loss_dice: 0.254911
[07:48:11.079] TRAIN: iteration 1694 : loss : 0.179160, loss_ce: 0.006478, loss_dice: 0.351842
[07:48:11.287] TRAIN: iteration 1695 : loss : 0.175333, loss_ce: 0.006334, loss_dice: 0.344331
[07:48:11.494] TRAIN: iteration 1696 : loss : 0.253039, loss_ce: 0.005863, loss_dice: 0.500215
[07:48:11.701] TRAIN: iteration 1697 : loss : 0.252826, loss_ce: 0.005424, loss_dice: 0.500228
[07:48:11.907] TRAIN: iteration 1698 : loss : 0.202968, loss_ce: 0.009893, loss_dice: 0.396042
[07:48:18.557] TRAIN: iteration 1699 : loss : 0.221224, loss_ce: 0.005396, loss_dice: 0.437051
[07:48:23.167] TRAIN: iteration 1700 : loss : 0.233421, loss_ce: 0.021490, loss_dice: 0.445352
[07:48:24.164] TRAIN: iteration 1701 : loss : 0.264219, loss_ce: 0.033371, loss_dice: 0.495068
[07:48:24.370] TRAIN: iteration 1702 : loss : 0.143258, loss_ce: 0.009795, loss_dice: 0.276722
[07:48:24.575] TRAIN: iteration 1703 : loss : 0.180181, loss_ce: 0.005694, loss_dice: 0.354668
[07:48:24.781] TRAIN: iteration 1704 : loss : 0.152866, loss_ce: 0.007017, loss_dice: 0.298715
[07:48:24.987] TRAIN: iteration 1705 : loss : 0.203064, loss_ce: 0.012299, loss_dice: 0.393828
[07:48:25.196] TRAIN: iteration 1706 : loss : 0.135906, loss_ce: 0.008484, loss_dice: 0.263328
[07:48:32.025] TRAIN: iteration 1707 : loss : 0.253000, loss_ce: 0.005697, loss_dice: 0.500304
[07:48:35.633] TRAIN: iteration 1708 : loss : 0.198803, loss_ce: 0.011499, loss_dice: 0.386108
[07:48:36.799] TRAIN: iteration 1709 : loss : 0.217583, loss_ce: 0.013108, loss_dice: 0.422059
[07:48:37.006] TRAIN: iteration 1710 : loss : 0.225006, loss_ce: 0.029324, loss_dice: 0.420688
[07:48:37.212] TRAIN: iteration 1711 : loss : 0.232785, loss_ce: 0.019105, loss_dice: 0.446465
[07:48:37.420] TRAIN: iteration 1712 : loss : 0.147013, loss_ce: 0.008543, loss_dice: 0.285484
[07:48:37.627] TRAIN: iteration 1713 : loss : 0.213092, loss_ce: 0.007634, loss_dice: 0.418549
[07:48:37.833] TRAIN: iteration 1714 : loss : 0.245173, loss_ce: 0.007533, loss_dice: 0.482812
[07:48:42.962] TRAIN: iteration 1715 : loss : 0.149417, loss_ce: 0.010943, loss_dice: 0.287890
[07:48:47.876] TRAIN: iteration 1716 : loss : 0.169215, loss_ce: 0.010249, loss_dice: 0.328182
[07:48:49.807] TRAIN: iteration 1717 : loss : 0.171743, loss_ce: 0.008667, loss_dice: 0.334819
[07:48:50.016] TRAIN: iteration 1718 : loss : 0.193294, loss_ce: 0.009085, loss_dice: 0.377503
[07:48:50.226] TRAIN: iteration 1719 : loss : 0.196236, loss_ce: 0.009614, loss_dice: 0.382859
[07:48:50.432] TRAIN: iteration 1720 : loss : 0.116133, loss_ce: 0.006072, loss_dice: 0.226194
[07:48:50.670] TRAIN: iteration 1721 : loss : 0.224405, loss_ce: 0.012399, loss_dice: 0.436411
[07:48:50.877] TRAIN: iteration 1722 : loss : 0.186965, loss_ce: 0.005705, loss_dice: 0.368225
[07:48:56.642] TRAIN: iteration 1723 : loss : 0.253020, loss_ce: 0.005741, loss_dice: 0.500298
[07:49:00.350] TRAIN: iteration 1724 : loss : 0.228099, loss_ce: 0.026939, loss_dice: 0.429259
[07:49:03.011] TRAIN: iteration 1725 : loss : 0.136765, loss_ce: 0.008698, loss_dice: 0.264831
[07:49:03.219] TRAIN: iteration 1726 : loss : 0.144175, loss_ce: 0.012176, loss_dice: 0.276174
[07:49:03.425] TRAIN: iteration 1727 : loss : 0.130300, loss_ce: 0.006952, loss_dice: 0.253649
[07:49:03.632] TRAIN: iteration 1728 : loss : 0.149987, loss_ce: 0.006891, loss_dice: 0.293083
[07:49:03.837] TRAIN: iteration 1729 : loss : 0.201230, loss_ce: 0.010329, loss_dice: 0.392131
[07:49:04.043] TRAIN: iteration 1730 : loss : 0.217641, loss_ce: 0.011355, loss_dice: 0.423927
[07:49:09.651] TRAIN: iteration 1731 : loss : 0.252510, loss_ce: 0.004780, loss_dice: 0.500240
[07:49:12.191] TRAIN: iteration 1732 : loss : 0.163335, loss_ce: 0.012459, loss_dice: 0.314212
[07:49:15.197] TRAIN: iteration 1733 : loss : 0.145053, loss_ce: 0.010051, loss_dice: 0.280056
[07:49:15.404] TRAIN: iteration 1734 : loss : 0.192958, loss_ce: 0.011266, loss_dice: 0.374650
[07:49:15.791] TRAIN: iteration 1735 : loss : 0.187194, loss_ce: 0.023119, loss_dice: 0.351269
[08:01:22.528] VALIDATION: iteration 0 : loss : 0.188136, loss_ce: 0.009618, loss_dice: 0.366655
[08:01:24.909] TRAIN: iteration 1736 : loss : 0.215684, loss_ce: 0.007362, loss_dice: 0.424005
[08:01:25.116] TRAIN: iteration 1737 : loss : 0.149489, loss_ce: 0.009114, loss_dice: 0.289864
[08:01:25.328] TRAIN: iteration 1738 : loss : 0.209653, loss_ce: 0.014399, loss_dice: 0.404908
[08:01:25.535] TRAIN: iteration 1739 : loss : 0.249674, loss_ce: 0.013281, loss_dice: 0.486067
[08:01:25.751] TRAIN: iteration 1740 : loss : 0.255128, loss_ce: 0.009692, loss_dice: 0.500563
[08:01:26.004] TRAIN: iteration 1741 : loss : 0.235326, loss_ce: 0.010747, loss_dice: 0.459906
[08:01:26.212] TRAIN: iteration 1742 : loss : 0.161955, loss_ce: 0.010764, loss_dice: 0.313147
[08:01:26.420] TRAIN: iteration 1743 : loss : 0.255549, loss_ce: 0.010452, loss_dice: 0.500646
[08:01:26.632] TRAIN: iteration 1744 : loss : 0.121416, loss_ce: 0.015348, loss_dice: 0.227484
[08:01:26.845] TRAIN: iteration 1745 : loss : 0.215634, loss_ce: 0.010680, loss_dice: 0.420589
[08:01:27.052] TRAIN: iteration 1746 : loss : 0.255916, loss_ce: 0.011105, loss_dice: 0.500727
[08:01:27.259] TRAIN: iteration 1747 : loss : 0.224719, loss_ce: 0.010399, loss_dice: 0.439038
[08:01:27.465] TRAIN: iteration 1748 : loss : 0.258236, loss_ce: 0.015618, loss_dice: 0.500855
[08:01:27.677] TRAIN: iteration 1749 : loss : 0.227460, loss_ce: 0.013226, loss_dice: 0.441693
[08:01:27.891] TRAIN: iteration 1750 : loss : 0.252269, loss_ce: 0.010140, loss_dice: 0.494398
[08:01:28.100] TRAIN: iteration 1751 : loss : 0.231491, loss_ce: 0.011646, loss_dice: 0.451337
[08:01:28.310] TRAIN: iteration 1752 : loss : 0.210545, loss_ce: 0.010848, loss_dice: 0.410241
[08:01:28.519] TRAIN: iteration 1753 : loss : 0.209027, loss_ce: 0.012352, loss_dice: 0.405703
[08:01:28.729] TRAIN: iteration 1754 : loss : 0.225005, loss_ce: 0.010953, loss_dice: 0.439057
[08:01:28.936] TRAIN: iteration 1755 : loss : 0.206206, loss_ce: 0.009252, loss_dice: 0.403159
[08:01:29.145] TRAIN: iteration 1756 : loss : 0.237103, loss_ce: 0.009304, loss_dice: 0.464902
[08:01:29.355] TRAIN: iteration 1757 : loss : 0.129216, loss_ce: 0.010482, loss_dice: 0.247951
[08:01:29.566] TRAIN: iteration 1758 : loss : 0.254150, loss_ce: 0.007797, loss_dice: 0.500504
[08:01:29.774] TRAIN: iteration 1759 : loss : 0.146207, loss_ce: 0.018238, loss_dice: 0.274175
[08:01:29.991] TRAIN: iteration 1760 : loss : 0.200087, loss_ce: 0.030412, loss_dice: 0.369762
[08:01:30.229] TRAIN: iteration 1761 : loss : 0.174958, loss_ce: 0.010495, loss_dice: 0.339421
[08:01:30.442] TRAIN: iteration 1762 : loss : 0.250964, loss_ce: 0.007324, loss_dice: 0.494604
[08:01:30.651] TRAIN: iteration 1763 : loss : 0.189821, loss_ce: 0.006995, loss_dice: 0.372647
[08:01:30.863] TRAIN: iteration 1764 : loss : 0.135292, loss_ce: 0.007669, loss_dice: 0.262915
[08:01:31.074] TRAIN: iteration 1765 : loss : 0.193646, loss_ce: 0.006868, loss_dice: 0.380424
[08:01:31.283] TRAIN: iteration 1766 : loss : 0.251490, loss_ce: 0.005468, loss_dice: 0.497512
[08:01:31.495] TRAIN: iteration 1767 : loss : 0.150481, loss_ce: 0.006372, loss_dice: 0.294590
[08:01:31.704] TRAIN: iteration 1768 : loss : 0.146560, loss_ce: 0.008428, loss_dice: 0.284692
[08:01:31.911] TRAIN: iteration 1769 : loss : 0.201516, loss_ce: 0.013882, loss_dice: 0.389151
[08:01:32.126] TRAIN: iteration 1770 : loss : 0.107065, loss_ce: 0.005577, loss_dice: 0.208554
[08:01:32.336] TRAIN: iteration 1771 : loss : 0.245885, loss_ce: 0.005410, loss_dice: 0.486359
[08:01:32.542] TRAIN: iteration 1772 : loss : 0.252726, loss_ce: 0.005163, loss_dice: 0.500288
[08:01:32.750] TRAIN: iteration 1773 : loss : 0.252482, loss_ce: 0.004718, loss_dice: 0.500245
[08:01:32.956] TRAIN: iteration 1774 : loss : 0.148801, loss_ce: 0.009505, loss_dice: 0.288096
[08:01:33.171] TRAIN: iteration 1775 : loss : 0.196934, loss_ce: 0.006917, loss_dice: 0.386950
[08:01:33.383] TRAIN: iteration 1776 : loss : 0.214190, loss_ce: 0.005273, loss_dice: 0.423107
[08:01:33.597] TRAIN: iteration 1777 : loss : 0.125561, loss_ce: 0.009391, loss_dice: 0.241731
[08:01:33.805] TRAIN: iteration 1778 : loss : 0.121642, loss_ce: 0.006857, loss_dice: 0.236427
[08:01:34.019] TRAIN: iteration 1779 : loss : 0.233324, loss_ce: 0.006530, loss_dice: 0.460119
[08:01:34.230] TRAIN: iteration 1780 : loss : 0.240338, loss_ce: 0.036027, loss_dice: 0.444648
[08:01:34.481] TRAIN: iteration 1781 : loss : 0.166675, loss_ce: 0.008115, loss_dice: 0.325234
[08:01:34.688] TRAIN: iteration 1782 : loss : 0.214687, loss_ce: 0.006707, loss_dice: 0.422668
[08:01:34.895] TRAIN: iteration 1783 : loss : 0.228991, loss_ce: 0.015282, loss_dice: 0.442700
[08:01:35.103] TRAIN: iteration 1784 : loss : 0.206384, loss_ce: 0.005590, loss_dice: 0.407179
[08:01:35.312] TRAIN: iteration 1785 : loss : 0.179361, loss_ce: 0.010280, loss_dice: 0.348443
[08:01:35.522] TRAIN: iteration 1786 : loss : 0.126177, loss_ce: 0.007492, loss_dice: 0.244862
[08:01:35.737] TRAIN: iteration 1787 : loss : 0.179297, loss_ce: 0.006412, loss_dice: 0.352182
[08:01:35.958] TRAIN: iteration 1788 : loss : 0.252954, loss_ce: 0.005629, loss_dice: 0.500280
[08:01:36.168] TRAIN: iteration 1789 : loss : 0.136978, loss_ce: 0.010077, loss_dice: 0.263880
[08:01:36.376] TRAIN: iteration 1790 : loss : 0.199713, loss_ce: 0.007122, loss_dice: 0.392304
[08:01:36.584] TRAIN: iteration 1791 : loss : 0.143024, loss_ce: 0.008450, loss_dice: 0.277599
[08:01:36.791] TRAIN: iteration 1792 : loss : 0.150328, loss_ce: 0.013660, loss_dice: 0.286996
[08:01:36.998] TRAIN: iteration 1793 : loss : 0.131324, loss_ce: 0.009532, loss_dice: 0.253115
[08:01:37.207] TRAIN: iteration 1794 : loss : 0.214681, loss_ce: 0.012846, loss_dice: 0.416515
[08:01:37.415] TRAIN: iteration 1795 : loss : 0.149663, loss_ce: 0.005918, loss_dice: 0.293408
[08:01:37.628] TRAIN: iteration 1796 : loss : 0.193002, loss_ce: 0.006679, loss_dice: 0.379326
[08:01:37.835] TRAIN: iteration 1797 : loss : 0.179261, loss_ce: 0.005427, loss_dice: 0.353095
[08:01:38.041] TRAIN: iteration 1798 : loss : 0.208667, loss_ce: 0.008874, loss_dice: 0.408460
[08:01:38.248] TRAIN: iteration 1799 : loss : 0.187467, loss_ce: 0.006102, loss_dice: 0.368831
[08:01:38.454] TRAIN: iteration 1800 : loss : 0.252769, loss_ce: 0.005287, loss_dice: 0.500252
[08:01:38.691] TRAIN: iteration 1801 : loss : 0.249937, loss_ce: 0.004996, loss_dice: 0.494878
[08:01:38.899] TRAIN: iteration 1802 : loss : 0.181170, loss_ce: 0.004829, loss_dice: 0.357511
[08:01:39.105] TRAIN: iteration 1803 : loss : 0.134528, loss_ce: 0.006704, loss_dice: 0.262352
[08:01:39.319] TRAIN: iteration 1804 : loss : 0.252139, loss_ce: 0.004115, loss_dice: 0.500163
[08:01:39.527] TRAIN: iteration 1805 : loss : 0.134698, loss_ce: 0.006807, loss_dice: 0.262588
[08:01:39.736] TRAIN: iteration 1806 : loss : 0.162609, loss_ce: 0.005579, loss_dice: 0.319640
[08:01:39.943] TRAIN: iteration 1807 : loss : 0.221299, loss_ce: 0.012540, loss_dice: 0.430059
[08:01:40.151] TRAIN: iteration 1808 : loss : 0.252056, loss_ce: 0.003958, loss_dice: 0.500153
[08:01:40.553] TRAIN: iteration 1809 : loss : 0.137012, loss_ce: 0.004721, loss_dice: 0.269303
[08:01:40.763] TRAIN: iteration 1810 : loss : 0.219483, loss_ce: 0.005183, loss_dice: 0.433783
[08:01:40.972] TRAIN: iteration 1811 : loss : 0.215737, loss_ce: 0.021724, loss_dice: 0.409751
[08:01:41.180] TRAIN: iteration 1812 : loss : 0.229976, loss_ce: 0.006004, loss_dice: 0.453947
[08:01:41.388] TRAIN: iteration 1813 : loss : 0.162761, loss_ce: 0.017418, loss_dice: 0.308104
[08:01:41.595] TRAIN: iteration 1814 : loss : 0.177922, loss_ce: 0.005410, loss_dice: 0.350434
[08:01:41.813] TRAIN: iteration 1815 : loss : 0.252931, loss_ce: 0.005533, loss_dice: 0.500329
[08:01:42.025] TRAIN: iteration 1816 : loss : 0.195088, loss_ce: 0.009827, loss_dice: 0.380349
[08:01:42.240] TRAIN: iteration 1817 : loss : 0.253052, loss_ce: 0.006021, loss_dice: 0.500083
[08:01:42.447] TRAIN: iteration 1818 : loss : 0.196980, loss_ce: 0.008181, loss_dice: 0.385778
[08:01:42.654] TRAIN: iteration 1819 : loss : 0.236488, loss_ce: 0.006907, loss_dice: 0.466068
[08:01:42.861] TRAIN: iteration 1820 : loss : 0.253294, loss_ce: 0.006213, loss_dice: 0.500376
[08:01:43.107] TRAIN: iteration 1821 : loss : 0.185210, loss_ce: 0.007777, loss_dice: 0.362644
[08:01:43.315] TRAIN: iteration 1822 : loss : 0.206422, loss_ce: 0.006909, loss_dice: 0.405936
[08:01:43.523] TRAIN: iteration 1823 : loss : 0.243041, loss_ce: 0.007050, loss_dice: 0.479033
[08:01:43.730] TRAIN: iteration 1824 : loss : 0.133806, loss_ce: 0.010367, loss_dice: 0.257246
[08:01:43.936] TRAIN: iteration 1825 : loss : 0.249598, loss_ce: 0.006258, loss_dice: 0.492938
[08:01:44.146] TRAIN: iteration 1826 : loss : 0.199645, loss_ce: 0.006694, loss_dice: 0.392596
[08:01:44.352] TRAIN: iteration 1827 : loss : 0.162069, loss_ce: 0.008531, loss_dice: 0.315608
[08:01:44.567] TRAIN: iteration 1828 : loss : 0.167762, loss_ce: 0.006143, loss_dice: 0.329382
[08:01:44.775] TRAIN: iteration 1829 : loss : 0.202643, loss_ce: 0.006567, loss_dice: 0.398719
[08:01:44.988] TRAIN: iteration 1830 : loss : 0.217249, loss_ce: 0.005551, loss_dice: 0.428947
[08:01:45.200] TRAIN: iteration 1831 : loss : 0.237274, loss_ce: 0.004862, loss_dice: 0.469686
[08:01:45.409] TRAIN: iteration 1832 : loss : 0.191696, loss_ce: 0.009803, loss_dice: 0.373588
[08:01:45.623] TRAIN: iteration 1833 : loss : 0.132526, loss_ce: 0.010923, loss_dice: 0.254130
[08:01:45.833] TRAIN: iteration 1834 : loss : 0.088700, loss_ce: 0.005514, loss_dice: 0.171887
[08:01:46.047] TRAIN: iteration 1835 : loss : 0.126205, loss_ce: 0.004704, loss_dice: 0.247705
[08:01:46.270] TRAIN: iteration 1836 : loss : 0.159263, loss_ce: 0.008462, loss_dice: 0.310064
[08:01:46.479] TRAIN: iteration 1837 : loss : 0.212518, loss_ce: 0.006913, loss_dice: 0.418123
[08:01:46.689] TRAIN: iteration 1838 : loss : 0.235405, loss_ce: 0.012251, loss_dice: 0.458558
[08:01:46.898] TRAIN: iteration 1839 : loss : 0.252538, loss_ce: 0.004786, loss_dice: 0.500290
[08:01:47.105] TRAIN: iteration 1840 : loss : 0.179344, loss_ce: 0.005935, loss_dice: 0.352754
[08:01:47.357] TRAIN: iteration 1841 : loss : 0.228655, loss_ce: 0.004836, loss_dice: 0.452475
[08:01:47.566] TRAIN: iteration 1842 : loss : 0.128739, loss_ce: 0.006464, loss_dice: 0.251014
[08:01:47.773] TRAIN: iteration 1843 : loss : 0.252752, loss_ce: 0.005197, loss_dice: 0.500308
[08:01:47.982] TRAIN: iteration 1844 : loss : 0.222435, loss_ce: 0.005076, loss_dice: 0.439794
[08:01:48.191] TRAIN: iteration 1845 : loss : 0.136287, loss_ce: 0.004690, loss_dice: 0.267883
[08:01:48.398] TRAIN: iteration 1846 : loss : 0.242903, loss_ce: 0.004135, loss_dice: 0.481670
[08:01:48.607] TRAIN: iteration 1847 : loss : 0.195441, loss_ce: 0.006197, loss_dice: 0.384686
[08:01:48.816] TRAIN: iteration 1848 : loss : 0.162280, loss_ce: 0.006025, loss_dice: 0.318534
[08:01:49.024] TRAIN: iteration 1849 : loss : 0.252035, loss_ce: 0.003882, loss_dice: 0.500188
[08:01:49.232] TRAIN: iteration 1850 : loss : 0.243552, loss_ce: 0.004118, loss_dice: 0.482987
[08:01:49.441] TRAIN: iteration 1851 : loss : 0.236674, loss_ce: 0.056746, loss_dice: 0.416603
[08:01:49.650] TRAIN: iteration 1852 : loss : 0.214615, loss_ce: 0.008800, loss_dice: 0.420430
[08:01:49.858] TRAIN: iteration 1853 : loss : 0.161063, loss_ce: 0.004483, loss_dice: 0.317644
[08:01:50.071] TRAIN: iteration 1854 : loss : 0.236241, loss_ce: 0.021310, loss_dice: 0.451171
[08:01:50.279] TRAIN: iteration 1855 : loss : 0.145034, loss_ce: 0.003666, loss_dice: 0.286403
[08:01:50.493] TRAIN: iteration 1856 : loss : 0.158618, loss_ce: 0.019559, loss_dice: 0.297677
[08:01:50.701] TRAIN: iteration 1857 : loss : 0.207679, loss_ce: 0.003673, loss_dice: 0.411686
[08:01:50.909] TRAIN: iteration 1858 : loss : 0.207938, loss_ce: 0.009348, loss_dice: 0.406527
[08:01:51.119] TRAIN: iteration 1859 : loss : 0.217135, loss_ce: 0.004023, loss_dice: 0.430247
[08:01:51.326] TRAIN: iteration 1860 : loss : 0.251785, loss_ce: 0.003442, loss_dice: 0.500129
[08:01:52.211] TRAIN: iteration 1861 : loss : 0.132019, loss_ce: 0.005092, loss_dice: 0.258945
[08:01:52.416] TRAIN: iteration 1862 : loss : 0.200388, loss_ce: 0.005822, loss_dice: 0.394955
[08:01:52.623] TRAIN: iteration 1863 : loss : 0.234935, loss_ce: 0.004326, loss_dice: 0.465545
[08:01:52.831] TRAIN: iteration 1864 : loss : 0.188810, loss_ce: 0.005465, loss_dice: 0.372155
[08:01:53.042] TRAIN: iteration 1865 : loss : 0.120345, loss_ce: 0.004407, loss_dice: 0.236283
[08:01:53.252] TRAIN: iteration 1866 : loss : 0.129425, loss_ce: 0.004515, loss_dice: 0.254335
[08:01:53.466] TRAIN: iteration 1867 : loss : 0.209480, loss_ce: 0.008677, loss_dice: 0.410284
[08:01:53.674] TRAIN: iteration 1868 : loss : 0.122005, loss_ce: 0.004776, loss_dice: 0.239234
[08:01:54.056] TRAIN: iteration 1869 : loss : 0.139546, loss_ce: 0.004435, loss_dice: 0.274657
[08:01:54.264] TRAIN: iteration 1870 : loss : 0.257107, loss_ce: 0.018645, loss_dice: 0.495569
[08:01:54.472] TRAIN: iteration 1871 : loss : 0.252144, loss_ce: 0.004136, loss_dice: 0.500153
[08:01:54.678] TRAIN: iteration 1872 : loss : 0.117306, loss_ce: 0.005031, loss_dice: 0.229582
[08:01:54.889] TRAIN: iteration 1873 : loss : 0.160028, loss_ce: 0.008458, loss_dice: 0.311598
[08:01:55.105] TRAIN: iteration 1874 : loss : 0.237264, loss_ce: 0.008501, loss_dice: 0.466027
[08:01:55.312] TRAIN: iteration 1875 : loss : 0.252031, loss_ce: 0.003885, loss_dice: 0.500177
[08:01:55.519] TRAIN: iteration 1876 : loss : 0.234968, loss_ce: 0.006533, loss_dice: 0.463403
[08:01:55.729] TRAIN: iteration 1877 : loss : 0.080423, loss_ce: 0.005181, loss_dice: 0.155666
[08:01:55.936] TRAIN: iteration 1878 : loss : 0.250867, loss_ce: 0.004530, loss_dice: 0.497204
[08:01:56.152] TRAIN: iteration 1879 : loss : 0.216771, loss_ce: 0.026868, loss_dice: 0.406674
[08:01:56.366] TRAIN: iteration 1880 : loss : 0.160710, loss_ce: 0.009597, loss_dice: 0.311823
[08:01:56.604] TRAIN: iteration 1881 : loss : 0.171032, loss_ce: 0.006794, loss_dice: 0.335271
[08:01:56.815] TRAIN: iteration 1882 : loss : 0.252809, loss_ce: 0.005354, loss_dice: 0.500264
[08:01:57.024] TRAIN: iteration 1883 : loss : 0.164974, loss_ce: 0.009496, loss_dice: 0.320452
[08:01:57.232] TRAIN: iteration 1884 : loss : 0.201214, loss_ce: 0.013167, loss_dice: 0.389261
[08:01:57.463] TRAIN: iteration 1885 : loss : 0.200550, loss_ce: 0.011445, loss_dice: 0.389656
[08:01:57.670] TRAIN: iteration 1886 : loss : 0.212728, loss_ce: 0.007052, loss_dice: 0.418404
[08:01:57.879] TRAIN: iteration 1887 : loss : 0.130265, loss_ce: 0.006989, loss_dice: 0.253541
[08:01:58.090] TRAIN: iteration 1888 : loss : 0.188446, loss_ce: 0.007430, loss_dice: 0.369463
[08:01:58.302] TRAIN: iteration 1889 : loss : 0.254035, loss_ce: 0.007645, loss_dice: 0.500425
[08:01:58.516] TRAIN: iteration 1890 : loss : 0.217555, loss_ce: 0.007289, loss_dice: 0.427821
[08:01:58.732] TRAIN: iteration 1891 : loss : 0.253729, loss_ce: 0.007072, loss_dice: 0.500386
[08:01:58.943] TRAIN: iteration 1892 : loss : 0.145118, loss_ce: 0.006336, loss_dice: 0.283900
[08:01:59.154] TRAIN: iteration 1893 : loss : 0.159884, loss_ce: 0.008855, loss_dice: 0.310912
[08:01:59.362] TRAIN: iteration 1894 : loss : 0.127743, loss_ce: 0.010782, loss_dice: 0.244704
[08:01:59.570] TRAIN: iteration 1895 : loss : 0.224861, loss_ce: 0.005680, loss_dice: 0.444041
[08:01:59.780] TRAIN: iteration 1896 : loss : 0.210988, loss_ce: 0.006939, loss_dice: 0.415037
[08:01:59.988] TRAIN: iteration 1897 : loss : 0.251998, loss_ce: 0.005598, loss_dice: 0.498399
[08:02:00.199] TRAIN: iteration 1898 : loss : 0.252516, loss_ce: 0.004777, loss_dice: 0.500254
[08:02:00.410] TRAIN: iteration 1899 : loss : 0.121416, loss_ce: 0.012335, loss_dice: 0.230496
[08:02:00.625] TRAIN: iteration 1900 : loss : 0.253017, loss_ce: 0.006264, loss_dice: 0.499770
[08:02:00.866] TRAIN: iteration 1901 : loss : 0.156928, loss_ce: 0.016809, loss_dice: 0.297047
[08:02:01.080] TRAIN: iteration 1902 : loss : 0.147866, loss_ce: 0.007076, loss_dice: 0.288657
[08:02:01.295] TRAIN: iteration 1903 : loss : 0.252916, loss_ce: 0.007319, loss_dice: 0.498513
[08:02:01.502] TRAIN: iteration 1904 : loss : 0.244494, loss_ce: 0.005344, loss_dice: 0.483645
[08:02:01.712] TRAIN: iteration 1905 : loss : 0.252376, loss_ce: 0.005266, loss_dice: 0.499485
[08:02:01.920] TRAIN: iteration 1906 : loss : 0.253166, loss_ce: 0.005980, loss_dice: 0.500353
[08:02:02.128] TRAIN: iteration 1907 : loss : 0.203215, loss_ce: 0.006201, loss_dice: 0.400228
[08:02:02.336] TRAIN: iteration 1908 : loss : 0.176506, loss_ce: 0.005505, loss_dice: 0.347507
[08:02:02.544] TRAIN: iteration 1909 : loss : 0.234438, loss_ce: 0.010801, loss_dice: 0.458074
[08:02:02.758] TRAIN: iteration 1910 : loss : 0.163693, loss_ce: 0.004824, loss_dice: 0.322562
[08:02:02.976] TRAIN: iteration 1911 : loss : 0.139882, loss_ce: 0.004710, loss_dice: 0.275053
[08:02:03.189] TRAIN: iteration 1912 : loss : 0.236547, loss_ce: 0.004429, loss_dice: 0.468664
[08:02:03.396] TRAIN: iteration 1913 : loss : 0.217269, loss_ce: 0.005484, loss_dice: 0.429055
[08:02:03.604] TRAIN: iteration 1914 : loss : 0.251756, loss_ce: 0.003332, loss_dice: 0.500180
[08:02:03.813] TRAIN: iteration 1915 : loss : 0.232150, loss_ce: 0.003951, loss_dice: 0.460349
[08:02:04.021] TRAIN: iteration 1916 : loss : 0.201215, loss_ce: 0.005996, loss_dice: 0.396435
[08:02:04.230] TRAIN: iteration 1917 : loss : 0.129203, loss_ce: 0.005772, loss_dice: 0.252633
[08:02:04.438] TRAIN: iteration 1918 : loss : 0.211482, loss_ce: 0.013974, loss_dice: 0.408990
[08:02:04.646] TRAIN: iteration 1919 : loss : 0.104096, loss_ce: 0.004999, loss_dice: 0.203193
[08:02:04.856] TRAIN: iteration 1920 : loss : 0.252097, loss_ce: 0.004012, loss_dice: 0.500181
[08:02:05.102] TRAIN: iteration 1921 : loss : 0.252286, loss_ce: 0.004386, loss_dice: 0.500186
[08:02:05.313] TRAIN: iteration 1922 : loss : 0.160174, loss_ce: 0.006145, loss_dice: 0.314203
[08:02:05.523] TRAIN: iteration 1923 : loss : 0.162921, loss_ce: 0.005699, loss_dice: 0.320142
[08:02:05.730] TRAIN: iteration 1924 : loss : 0.189934, loss_ce: 0.011447, loss_dice: 0.368421
[08:02:05.938] TRAIN: iteration 1925 : loss : 0.252975, loss_ce: 0.005689, loss_dice: 0.500261
[08:02:06.183] TRAIN: iteration 1926 : loss : 0.209647, loss_ce: 0.008510, loss_dice: 0.410783
[08:02:06.392] TRAIN: iteration 1927 : loss : 0.206933, loss_ce: 0.005654, loss_dice: 0.408212
[08:02:06.599] TRAIN: iteration 1928 : loss : 0.244659, loss_ce: 0.005903, loss_dice: 0.483415
[08:02:06.807] TRAIN: iteration 1929 : loss : 0.151691, loss_ce: 0.011948, loss_dice: 0.291434
[08:02:07.014] TRAIN: iteration 1930 : loss : 0.148057, loss_ce: 0.007203, loss_dice: 0.288910
[08:02:07.222] TRAIN: iteration 1931 : loss : 0.188271, loss_ce: 0.008088, loss_dice: 0.368454
[08:02:07.429] TRAIN: iteration 1932 : loss : 0.248768, loss_ce: 0.007179, loss_dice: 0.490356
[08:02:07.638] TRAIN: iteration 1933 : loss : 0.165314, loss_ce: 0.007148, loss_dice: 0.323479
[08:02:07.846] TRAIN: iteration 1934 : loss : 0.116694, loss_ce: 0.008369, loss_dice: 0.225019
[08:02:08.054] TRAIN: iteration 1935 : loss : 0.205920, loss_ce: 0.020073, loss_dice: 0.391767
[08:02:08.263] TRAIN: iteration 1936 : loss : 0.179166, loss_ce: 0.006429, loss_dice: 0.351903
[08:02:08.471] TRAIN: iteration 1937 : loss : 0.252646, loss_ce: 0.005027, loss_dice: 0.500264
[08:02:08.694] TRAIN: iteration 1938 : loss : 0.159456, loss_ce: 0.006654, loss_dice: 0.312257
[08:02:08.901] TRAIN: iteration 1939 : loss : 0.118985, loss_ce: 0.006183, loss_dice: 0.231786
[08:02:09.111] TRAIN: iteration 1940 : loss : 0.119034, loss_ce: 0.007947, loss_dice: 0.230121
[08:02:09.347] TRAIN: iteration 1941 : loss : 0.252370, loss_ce: 0.005163, loss_dice: 0.499577
[08:02:09.562] TRAIN: iteration 1942 : loss : 0.176118, loss_ce: 0.010805, loss_dice: 0.341430
[08:02:09.771] TRAIN: iteration 1943 : loss : 0.182611, loss_ce: 0.014395, loss_dice: 0.350828
[08:02:09.979] TRAIN: iteration 1944 : loss : 0.188613, loss_ce: 0.010399, loss_dice: 0.366828
[08:02:10.186] TRAIN: iteration 1945 : loss : 0.217718, loss_ce: 0.005803, loss_dice: 0.429633
[08:02:10.393] TRAIN: iteration 1946 : loss : 0.114451, loss_ce: 0.005324, loss_dice: 0.223578
[08:02:10.608] TRAIN: iteration 1947 : loss : 0.252459, loss_ce: 0.004675, loss_dice: 0.500244
[08:02:10.818] TRAIN: iteration 1948 : loss : 0.239143, loss_ce: 0.006659, loss_dice: 0.471627
[08:02:11.025] TRAIN: iteration 1949 : loss : 0.185141, loss_ce: 0.005493, loss_dice: 0.364789
[08:02:11.232] TRAIN: iteration 1950 : loss : 0.141852, loss_ce: 0.005548, loss_dice: 0.278156
[08:02:11.439] TRAIN: iteration 1951 : loss : 0.238401, loss_ce: 0.005145, loss_dice: 0.471657
[08:02:11.651] TRAIN: iteration 1952 : loss : 0.130139, loss_ce: 0.005353, loss_dice: 0.254925
[08:02:11.864] TRAIN: iteration 1953 : loss : 0.110406, loss_ce: 0.005559, loss_dice: 0.215253
[08:02:12.074] TRAIN: iteration 1954 : loss : 0.157533, loss_ce: 0.005331, loss_dice: 0.309735
[08:02:12.282] TRAIN: iteration 1955 : loss : 0.246345, loss_ce: 0.003997, loss_dice: 0.488693
[08:02:12.495] TRAIN: iteration 1956 : loss : 0.127871, loss_ce: 0.003743, loss_dice: 0.251999
[08:02:12.704] TRAIN: iteration 1957 : loss : 0.217749, loss_ce: 0.014543, loss_dice: 0.420955
[08:02:12.914] TRAIN: iteration 1958 : loss : 0.209915, loss_ce: 0.007566, loss_dice: 0.412263
[08:02:13.126] TRAIN: iteration 1959 : loss : 0.203863, loss_ce: 0.004696, loss_dice: 0.403029
[08:02:13.337] TRAIN: iteration 1960 : loss : 0.128356, loss_ce: 0.003319, loss_dice: 0.253392
[08:02:13.573] TRAIN: iteration 1961 : loss : 0.156787, loss_ce: 0.007002, loss_dice: 0.306572
[08:02:13.780] TRAIN: iteration 1962 : loss : 0.190521, loss_ce: 0.008549, loss_dice: 0.372492
[08:02:13.987] TRAIN: iteration 1963 : loss : 0.209942, loss_ce: 0.011661, loss_dice: 0.408222
[08:02:14.198] TRAIN: iteration 1964 : loss : 0.209444, loss_ce: 0.009612, loss_dice: 0.409277
[08:02:14.404] TRAIN: iteration 1965 : loss : 0.125520, loss_ce: 0.004257, loss_dice: 0.246783
[08:02:14.611] TRAIN: iteration 1966 : loss : 0.221140, loss_ce: 0.006891, loss_dice: 0.435389
[08:02:14.819] TRAIN: iteration 1967 : loss : 0.208540, loss_ce: 0.005182, loss_dice: 0.411898
[08:02:15.034] TRAIN: iteration 1968 : loss : 0.230074, loss_ce: 0.005853, loss_dice: 0.454294
[08:02:15.251] TRAIN: iteration 1969 : loss : 0.252859, loss_ce: 0.005439, loss_dice: 0.500280
[08:02:15.464] TRAIN: iteration 1970 : loss : 0.176074, loss_ce: 0.005877, loss_dice: 0.346272
[08:02:15.671] TRAIN: iteration 1971 : loss : 0.147300, loss_ce: 0.006176, loss_dice: 0.288425
[08:02:15.882] TRAIN: iteration 1972 : loss : 0.252964, loss_ce: 0.005598, loss_dice: 0.500331
[08:02:16.091] TRAIN: iteration 1973 : loss : 0.220756, loss_ce: 0.006286, loss_dice: 0.435226
[08:02:16.299] TRAIN: iteration 1974 : loss : 0.252445, loss_ce: 0.004627, loss_dice: 0.500262
[08:02:16.507] TRAIN: iteration 1975 : loss : 0.164646, loss_ce: 0.006265, loss_dice: 0.323028
[08:02:16.718] TRAIN: iteration 1976 : loss : 0.229239, loss_ce: 0.005548, loss_dice: 0.452930
[08:02:16.926] TRAIN: iteration 1977 : loss : 0.234476, loss_ce: 0.007225, loss_dice: 0.461727
[08:02:17.133] TRAIN: iteration 1978 : loss : 0.180434, loss_ce: 0.005772, loss_dice: 0.355095
[08:02:17.340] TRAIN: iteration 1979 : loss : 0.131161, loss_ce: 0.007625, loss_dice: 0.254696
[08:02:17.550] TRAIN: iteration 1980 : loss : 0.144330, loss_ce: 0.008118, loss_dice: 0.280542
[08:02:17.795] TRAIN: iteration 1981 : loss : 0.214043, loss_ce: 0.005046, loss_dice: 0.423039
[08:02:18.002] TRAIN: iteration 1982 : loss : 0.129752, loss_ce: 0.006463, loss_dice: 0.253040
[08:02:18.213] TRAIN: iteration 1983 : loss : 0.170422, loss_ce: 0.005720, loss_dice: 0.335124
[08:02:18.426] TRAIN: iteration 1984 : loss : 0.203685, loss_ce: 0.010154, loss_dice: 0.397216
[08:02:18.635] TRAIN: iteration 1985 : loss : 0.111237, loss_ce: 0.008843, loss_dice: 0.213631
[08:02:18.845] TRAIN: iteration 1986 : loss : 0.159286, loss_ce: 0.005007, loss_dice: 0.313566
[08:02:19.056] TRAIN: iteration 1987 : loss : 0.218450, loss_ce: 0.004980, loss_dice: 0.431919
[08:02:19.604] TRAIN: iteration 1988 : loss : 0.252002, loss_ce: 0.007491, loss_dice: 0.496513
[08:02:19.813] TRAIN: iteration 1989 : loss : 0.140616, loss_ce: 0.005463, loss_dice: 0.275770
[08:02:20.035] TRAIN: iteration 1990 : loss : 0.086186, loss_ce: 0.004600, loss_dice: 0.167771
[08:02:20.244] TRAIN: iteration 1991 : loss : 0.182941, loss_ce: 0.013519, loss_dice: 0.352363
[08:02:20.459] TRAIN: iteration 1992 : loss : 0.215959, loss_ce: 0.006462, loss_dice: 0.425456
[08:02:20.669] TRAIN: iteration 1993 : loss : 0.212878, loss_ce: 0.024439, loss_dice: 0.401317
[08:02:20.877] TRAIN: iteration 1994 : loss : 0.212529, loss_ce: 0.008688, loss_dice: 0.416370
[08:02:21.085] TRAIN: iteration 1995 : loss : 0.234694, loss_ce: 0.015856, loss_dice: 0.453531
[08:02:21.298] TRAIN: iteration 1996 : loss : 0.110087, loss_ce: 0.007142, loss_dice: 0.213032
[08:02:21.506] TRAIN: iteration 1997 : loss : 0.233568, loss_ce: 0.005092, loss_dice: 0.462044
[08:02:21.714] TRAIN: iteration 1998 : loss : 0.168316, loss_ce: 0.005659, loss_dice: 0.330972
[08:02:21.922] TRAIN: iteration 1999 : loss : 0.207741, loss_ce: 0.006047, loss_dice: 0.409436
[08:02:22.134] TRAIN: iteration 2000 : loss : 0.151528, loss_ce: 0.008357, loss_dice: 0.294698
[08:02:22.371] TRAIN: iteration 2001 : loss : 0.252116, loss_ce: 0.007793, loss_dice: 0.496440
[08:02:22.579] TRAIN: iteration 2002 : loss : 0.253221, loss_ce: 0.006060, loss_dice: 0.500382
[08:02:22.787] TRAIN: iteration 2003 : loss : 0.215251, loss_ce: 0.010410, loss_dice: 0.420091
[08:02:23.114] TRAIN: iteration 2004 : loss : 0.173972, loss_ce: 0.006180, loss_dice: 0.341765
[08:02:23.323] TRAIN: iteration 2005 : loss : 0.097871, loss_ce: 0.006508, loss_dice: 0.189234
[08:02:23.531] TRAIN: iteration 2006 : loss : 0.124584, loss_ce: 0.006708, loss_dice: 0.242460
[08:02:23.739] TRAIN: iteration 2007 : loss : 0.251778, loss_ce: 0.005247, loss_dice: 0.498309
[08:02:23.952] TRAIN: iteration 2008 : loss : 0.252885, loss_ce: 0.005897, loss_dice: 0.499873
[08:02:24.162] TRAIN: iteration 2009 : loss : 0.171371, loss_ce: 0.004579, loss_dice: 0.338164
[08:02:24.369] TRAIN: iteration 2010 : loss : 0.252522, loss_ce: 0.004808, loss_dice: 0.500236
[08:02:24.575] TRAIN: iteration 2011 : loss : 0.136705, loss_ce: 0.004424, loss_dice: 0.268985
[08:02:24.782] TRAIN: iteration 2012 : loss : 0.153114, loss_ce: 0.005873, loss_dice: 0.300354
[08:02:24.998] TRAIN: iteration 2013 : loss : 0.115412, loss_ce: 0.003715, loss_dice: 0.227108
[08:02:25.213] TRAIN: iteration 2014 : loss : 0.101485, loss_ce: 0.004871, loss_dice: 0.198099
[08:02:25.420] TRAIN: iteration 2015 : loss : 0.196204, loss_ce: 0.006794, loss_dice: 0.385613
[08:02:25.629] TRAIN: iteration 2016 : loss : 0.135667, loss_ce: 0.005420, loss_dice: 0.265914
[08:02:25.842] TRAIN: iteration 2017 : loss : 0.131062, loss_ce: 0.003583, loss_dice: 0.258541
[08:02:26.052] TRAIN: iteration 2018 : loss : 0.251369, loss_ce: 0.002605, loss_dice: 0.500133
[08:02:26.261] TRAIN: iteration 2019 : loss : 0.251290, loss_ce: 0.002449, loss_dice: 0.500130
[08:02:26.471] TRAIN: iteration 2020 : loss : 0.245447, loss_ce: 0.003772, loss_dice: 0.487122
[08:02:26.711] TRAIN: iteration 2021 : loss : 0.199930, loss_ce: 0.020615, loss_dice: 0.379244
[08:02:26.919] TRAIN: iteration 2022 : loss : 0.217478, loss_ce: 0.002621, loss_dice: 0.432335
[08:02:27.126] TRAIN: iteration 2023 : loss : 0.248787, loss_ce: 0.002021, loss_dice: 0.495553
[08:02:27.334] TRAIN: iteration 2024 : loss : 0.206287, loss_ce: 0.002260, loss_dice: 0.410314
[08:02:27.542] TRAIN: iteration 2025 : loss : 0.141197, loss_ce: 0.002315, loss_dice: 0.280079
[08:02:28.777] TRAIN: iteration 2026 : loss : 0.078201, loss_ce: 0.002026, loss_dice: 0.154375
[08:02:28.985] TRAIN: iteration 2027 : loss : 0.252747, loss_ce: 0.008603, loss_dice: 0.496891
[08:02:29.192] TRAIN: iteration 2028 : loss : 0.251033, loss_ce: 0.001984, loss_dice: 0.500082
[08:02:29.398] TRAIN: iteration 2029 : loss : 0.172687, loss_ce: 0.004492, loss_dice: 0.340881
[08:02:29.605] TRAIN: iteration 2030 : loss : 0.248899, loss_ce: 0.002171, loss_dice: 0.495627
[08:02:29.812] TRAIN: iteration 2031 : loss : 0.192864, loss_ce: 0.008576, loss_dice: 0.377152
[08:02:30.020] TRAIN: iteration 2032 : loss : 0.168109, loss_ce: 0.005394, loss_dice: 0.330825
[08:02:30.227] TRAIN: iteration 2033 : loss : 0.144724, loss_ce: 0.005715, loss_dice: 0.283733
[08:02:30.476] TRAIN: iteration 2034 : loss : 0.230825, loss_ce: 0.008967, loss_dice: 0.452683
[08:02:30.694] TRAIN: iteration 2035 : loss : 0.205368, loss_ce: 0.014325, loss_dice: 0.396410
[08:02:30.902] TRAIN: iteration 2036 : loss : 0.251653, loss_ce: 0.003506, loss_dice: 0.499800
[08:02:31.109] TRAIN: iteration 2037 : loss : 0.220436, loss_ce: 0.041890, loss_dice: 0.398982
[08:02:31.316] TRAIN: iteration 2038 : loss : 0.162995, loss_ce: 0.007569, loss_dice: 0.318421
[08:02:31.523] TRAIN: iteration 2039 : loss : 0.239429, loss_ce: 0.005076, loss_dice: 0.473783
[08:02:31.735] TRAIN: iteration 2040 : loss : 0.146981, loss_ce: 0.006040, loss_dice: 0.287923
[08:02:31.977] TRAIN: iteration 2041 : loss : 0.107313, loss_ce: 0.006093, loss_dice: 0.208532
[08:02:32.185] TRAIN: iteration 2042 : loss : 0.187931, loss_ce: 0.005969, loss_dice: 0.369893
[08:02:32.392] TRAIN: iteration 2043 : loss : 0.105092, loss_ce: 0.005983, loss_dice: 0.204202
[08:02:32.663] TRAIN: iteration 2044 : loss : 0.235638, loss_ce: 0.021684, loss_dice: 0.449591
[08:02:32.871] TRAIN: iteration 2045 : loss : 0.225765, loss_ce: 0.010208, loss_dice: 0.441322
[08:02:33.078] TRAIN: iteration 2046 : loss : 0.210661, loss_ce: 0.012253, loss_dice: 0.409069
[08:02:33.285] TRAIN: iteration 2047 : loss : 0.247246, loss_ce: 0.006685, loss_dice: 0.487806
[08:02:33.494] TRAIN: iteration 2048 : loss : 0.182546, loss_ce: 0.006995, loss_dice: 0.358098
[08:02:33.708] TRAIN: iteration 2049 : loss : 0.227285, loss_ce: 0.014645, loss_dice: 0.439924
[08:02:33.921] TRAIN: iteration 2050 : loss : 0.153888, loss_ce: 0.010782, loss_dice: 0.296994
[08:02:34.130] TRAIN: iteration 2051 : loss : 0.208167, loss_ce: 0.008297, loss_dice: 0.408036
[08:02:34.338] TRAIN: iteration 2052 : loss : 0.253805, loss_ce: 0.007185, loss_dice: 0.500425
[08:02:34.548] TRAIN: iteration 2053 : loss : 0.250801, loss_ce: 0.006699, loss_dice: 0.494902
[08:02:34.763] TRAIN: iteration 2054 : loss : 0.156426, loss_ce: 0.006934, loss_dice: 0.305918
[08:02:34.984] TRAIN: iteration 2055 : loss : 0.144868, loss_ce: 0.007488, loss_dice: 0.282248
[08:02:35.193] TRAIN: iteration 2056 : loss : 0.177251, loss_ce: 0.007009, loss_dice: 0.347493
[08:02:35.401] TRAIN: iteration 2057 : loss : 0.167386, loss_ce: 0.006862, loss_dice: 0.327910
[08:02:35.610] TRAIN: iteration 2058 : loss : 0.235885, loss_ce: 0.013858, loss_dice: 0.457913
[08:02:35.819] TRAIN: iteration 2059 : loss : 0.236285, loss_ce: 0.009911, loss_dice: 0.462659
[08:02:36.029] TRAIN: iteration 2060 : loss : 0.192268, loss_ce: 0.008223, loss_dice: 0.376313
[08:02:36.267] TRAIN: iteration 2061 : loss : 0.213739, loss_ce: 0.014316, loss_dice: 0.413161
[08:02:36.474] TRAIN: iteration 2062 : loss : 0.208807, loss_ce: 0.005201, loss_dice: 0.412413
[08:02:36.725] TRAIN: iteration 2063 : loss : 0.251531, loss_ce: 0.005980, loss_dice: 0.497082
[08:02:36.941] TRAIN: iteration 2064 : loss : 0.216238, loss_ce: 0.005720, loss_dice: 0.426756
[08:02:37.149] TRAIN: iteration 2065 : loss : 0.164816, loss_ce: 0.005606, loss_dice: 0.324026
[08:02:37.359] TRAIN: iteration 2066 : loss : 0.173863, loss_ce: 0.004465, loss_dice: 0.343261
[08:02:37.567] TRAIN: iteration 2067 : loss : 0.216547, loss_ce: 0.009515, loss_dice: 0.423578
[08:02:37.778] TRAIN: iteration 2068 : loss : 0.220416, loss_ce: 0.003912, loss_dice: 0.436920
[08:02:37.985] TRAIN: iteration 2069 : loss : 0.188388, loss_ce: 0.004335, loss_dice: 0.372440
[08:02:38.192] TRAIN: iteration 2070 : loss : 0.238467, loss_ce: 0.003643, loss_dice: 0.473291
[08:02:38.400] TRAIN: iteration 2071 : loss : 0.128171, loss_ce: 0.004799, loss_dice: 0.251543
[08:02:38.609] TRAIN: iteration 2072 : loss : 0.154116, loss_ce: 0.009361, loss_dice: 0.298870
[08:02:38.820] TRAIN: iteration 2073 : loss : 0.168357, loss_ce: 0.003550, loss_dice: 0.333165
[08:02:39.034] TRAIN: iteration 2074 : loss : 0.145565, loss_ce: 0.004893, loss_dice: 0.286236
[08:02:39.241] TRAIN: iteration 2075 : loss : 0.239625, loss_ce: 0.004234, loss_dice: 0.475016
[08:02:39.448] TRAIN: iteration 2076 : loss : 0.195325, loss_ce: 0.008168, loss_dice: 0.382482
[08:02:39.655] TRAIN: iteration 2077 : loss : 0.226009, loss_ce: 0.016336, loss_dice: 0.435681
[08:02:39.871] TRAIN: iteration 2078 : loss : 0.210318, loss_ce: 0.008891, loss_dice: 0.411744
[08:02:40.080] TRAIN: iteration 2079 : loss : 0.145284, loss_ce: 0.007486, loss_dice: 0.283081
[08:02:40.288] TRAIN: iteration 2080 : loss : 0.137331, loss_ce: 0.007990, loss_dice: 0.266673
[08:02:40.524] TRAIN: iteration 2081 : loss : 0.195295, loss_ce: 0.006057, loss_dice: 0.384533
[08:02:40.729] TRAIN: iteration 2082 : loss : 0.118084, loss_ce: 0.013216, loss_dice: 0.222952
[08:02:40.938] TRAIN: iteration 2083 : loss : 0.208346, loss_ce: 0.005655, loss_dice: 0.411037
[08:02:41.151] TRAIN: iteration 2084 : loss : 0.253026, loss_ce: 0.006056, loss_dice: 0.499995
[08:02:41.358] TRAIN: iteration 2085 : loss : 0.251476, loss_ce: 0.006663, loss_dice: 0.496288
[08:02:41.564] TRAIN: iteration 2086 : loss : 0.133548, loss_ce: 0.007193, loss_dice: 0.259902
[08:02:41.773] TRAIN: iteration 2087 : loss : 0.233760, loss_ce: 0.009509, loss_dice: 0.458011
[08:02:41.981] TRAIN: iteration 2088 : loss : 0.189378, loss_ce: 0.040410, loss_dice: 0.338347
[08:02:42.189] TRAIN: iteration 2089 : loss : 0.118810, loss_ce: 0.007970, loss_dice: 0.229650
[08:02:42.397] TRAIN: iteration 2090 : loss : 0.151677, loss_ce: 0.008476, loss_dice: 0.294879
[08:02:42.604] TRAIN: iteration 2091 : loss : 0.253319, loss_ce: 0.006242, loss_dice: 0.500395
[08:02:42.811] TRAIN: iteration 2092 : loss : 0.231424, loss_ce: 0.007679, loss_dice: 0.455169
[08:02:43.019] TRAIN: iteration 2093 : loss : 0.238586, loss_ce: 0.006470, loss_dice: 0.470703
[08:02:43.226] TRAIN: iteration 2094 : loss : 0.184303, loss_ce: 0.007261, loss_dice: 0.361345
[08:02:43.433] TRAIN: iteration 2095 : loss : 0.178280, loss_ce: 0.009622, loss_dice: 0.346939
[08:02:43.641] TRAIN: iteration 2096 : loss : 0.191802, loss_ce: 0.006816, loss_dice: 0.376788
[08:02:43.850] TRAIN: iteration 2097 : loss : 0.253311, loss_ce: 0.006215, loss_dice: 0.500407
[08:02:44.066] TRAIN: iteration 2098 : loss : 0.231533, loss_ce: 0.013267, loss_dice: 0.449799
[08:02:44.280] TRAIN: iteration 2099 : loss : 0.257311, loss_ce: 0.024598, loss_dice: 0.490023
[08:02:44.495] TRAIN: iteration 2100 : loss : 0.239765, loss_ce: 0.005291, loss_dice: 0.474239
[08:02:44.737] TRAIN: iteration 2101 : loss : 0.112235, loss_ce: 0.008047, loss_dice: 0.216424
[08:02:44.961] TRAIN: iteration 2102 : loss : 0.110340, loss_ce: 0.006975, loss_dice: 0.213705
[08:02:45.168] TRAIN: iteration 2103 : loss : 0.233674, loss_ce: 0.005444, loss_dice: 0.461903
[08:02:45.375] TRAIN: iteration 2104 : loss : 0.252746, loss_ce: 0.005207, loss_dice: 0.500285
[08:02:45.588] TRAIN: iteration 2105 : loss : 0.163206, loss_ce: 0.014914, loss_dice: 0.311497
[08:02:45.796] TRAIN: iteration 2106 : loss : 0.120068, loss_ce: 0.007767, loss_dice: 0.232369
[08:02:46.006] TRAIN: iteration 2107 : loss : 0.219564, loss_ce: 0.011900, loss_dice: 0.427228
[08:02:46.215] TRAIN: iteration 2108 : loss : 0.222205, loss_ce: 0.007477, loss_dice: 0.436932
[08:02:46.427] TRAIN: iteration 2109 : loss : 0.187646, loss_ce: 0.007783, loss_dice: 0.367509
[08:02:46.633] TRAIN: iteration 2110 : loss : 0.159114, loss_ce: 0.009080, loss_dice: 0.309147
[08:02:46.842] TRAIN: iteration 2111 : loss : 0.189483, loss_ce: 0.010412, loss_dice: 0.368554
[08:02:47.097] TRAIN: iteration 2112 : loss : 0.224662, loss_ce: 0.009175, loss_dice: 0.440148
[08:02:47.307] TRAIN: iteration 2113 : loss : 0.138099, loss_ce: 0.015162, loss_dice: 0.261037
[08:02:47.734] TRAIN: iteration 2114 : loss : 0.246735, loss_ce: 0.006708, loss_dice: 0.486762
[08:02:47.946] TRAIN: iteration 2115 : loss : 0.243843, loss_ce: 0.007013, loss_dice: 0.480673
[08:02:48.154] TRAIN: iteration 2116 : loss : 0.152062, loss_ce: 0.007952, loss_dice: 0.296173
[08:02:48.362] TRAIN: iteration 2117 : loss : 0.179830, loss_ce: 0.009252, loss_dice: 0.350408
[08:02:48.568] TRAIN: iteration 2118 : loss : 0.245589, loss_ce: 0.007316, loss_dice: 0.483863
[08:02:48.774] TRAIN: iteration 2119 : loss : 0.236730, loss_ce: 0.006756, loss_dice: 0.466705
[08:02:48.981] TRAIN: iteration 2120 : loss : 0.236246, loss_ce: 0.008101, loss_dice: 0.464390
[08:02:49.223] TRAIN: iteration 2121 : loss : 0.212788, loss_ce: 0.007679, loss_dice: 0.417897
[08:02:49.431] TRAIN: iteration 2122 : loss : 0.165496, loss_ce: 0.007540, loss_dice: 0.323452
[08:02:49.639] TRAIN: iteration 2123 : loss : 0.247191, loss_ce: 0.016877, loss_dice: 0.477504
[08:02:49.852] TRAIN: iteration 2124 : loss : 0.177208, loss_ce: 0.034788, loss_dice: 0.319629
[08:02:50.060] TRAIN: iteration 2125 : loss : 0.162774, loss_ce: 0.029555, loss_dice: 0.295993
[08:02:50.266] TRAIN: iteration 2126 : loss : 0.151235, loss_ce: 0.015072, loss_dice: 0.287399
[08:02:50.484] TRAIN: iteration 2127 : loss : 0.199458, loss_ce: 0.009132, loss_dice: 0.389783
[08:02:50.691] TRAIN: iteration 2128 : loss : 0.238545, loss_ce: 0.008615, loss_dice: 0.468475
[08:02:50.898] TRAIN: iteration 2129 : loss : 0.217530, loss_ce: 0.009387, loss_dice: 0.425673
[08:02:51.109] TRAIN: iteration 2130 : loss : 0.254868, loss_ce: 0.009170, loss_dice: 0.500566
[08:02:51.317] TRAIN: iteration 2131 : loss : 0.244530, loss_ce: 0.010297, loss_dice: 0.478763
[08:02:51.524] TRAIN: iteration 2132 : loss : 0.150791, loss_ce: 0.024297, loss_dice: 0.277286
[08:02:51.735] TRAIN: iteration 2133 : loss : 0.162062, loss_ce: 0.009918, loss_dice: 0.314206
[08:02:52.889] TRAIN: iteration 2134 : loss : 0.231237, loss_ce: 0.010385, loss_dice: 0.452089
[08:02:53.096] TRAIN: iteration 2135 : loss : 0.099889, loss_ce: 0.009603, loss_dice: 0.190175
[08:02:53.311] TRAIN: iteration 2136 : loss : 0.236693, loss_ce: 0.010277, loss_dice: 0.463109
[08:02:53.518] TRAIN: iteration 2137 : loss : 0.217984, loss_ce: 0.012221, loss_dice: 0.423747
[08:02:53.726] TRAIN: iteration 2138 : loss : 0.179851, loss_ce: 0.009665, loss_dice: 0.350037
[08:02:53.931] TRAIN: iteration 2139 : loss : 0.205377, loss_ce: 0.008859, loss_dice: 0.401895
[08:02:54.138] TRAIN: iteration 2140 : loss : 0.254532, loss_ce: 0.008574, loss_dice: 0.500489
[08:02:54.379] TRAIN: iteration 2141 : loss : 0.122035, loss_ce: 0.009935, loss_dice: 0.234135
[08:02:54.585] TRAIN: iteration 2142 : loss : 0.154921, loss_ce: 0.007373, loss_dice: 0.302468
[08:02:54.797] TRAIN: iteration 2143 : loss : 0.181138, loss_ce: 0.011811, loss_dice: 0.350465
[08:02:55.009] TRAIN: iteration 2144 : loss : 0.217725, loss_ce: 0.007527, loss_dice: 0.427923
[08:02:55.218] TRAIN: iteration 2145 : loss : 0.162797, loss_ce: 0.011377, loss_dice: 0.314216
[08:02:55.424] TRAIN: iteration 2146 : loss : 0.237335, loss_ce: 0.007017, loss_dice: 0.467653
[08:02:55.630] TRAIN: iteration 2147 : loss : 0.252486, loss_ce: 0.006166, loss_dice: 0.498807
[08:02:55.837] TRAIN: iteration 2148 : loss : 0.189182, loss_ce: 0.013442, loss_dice: 0.364921
[08:02:56.044] TRAIN: iteration 2149 : loss : 0.252958, loss_ce: 0.005885, loss_dice: 0.500031
[08:02:56.250] TRAIN: iteration 2150 : loss : 0.250971, loss_ce: 0.006605, loss_dice: 0.495338
[08:02:56.458] TRAIN: iteration 2151 : loss : 0.141640, loss_ce: 0.005682, loss_dice: 0.277599
[08:02:56.666] TRAIN: iteration 2152 : loss : 0.167655, loss_ce: 0.022534, loss_dice: 0.312777
[08:02:56.874] TRAIN: iteration 2153 : loss : 0.149570, loss_ce: 0.004967, loss_dice: 0.294172
[08:02:57.083] TRAIN: iteration 2154 : loss : 0.248368, loss_ce: 0.004916, loss_dice: 0.491820
[08:02:57.290] TRAIN: iteration 2155 : loss : 0.140580, loss_ce: 0.007810, loss_dice: 0.273350
[08:02:57.498] TRAIN: iteration 2156 : loss : 0.199323, loss_ce: 0.004937, loss_dice: 0.393708
[08:02:57.706] TRAIN: iteration 2157 : loss : 0.181853, loss_ce: 0.004602, loss_dice: 0.359105
[08:02:57.915] TRAIN: iteration 2158 : loss : 0.127181, loss_ce: 0.008512, loss_dice: 0.245850
[08:02:58.130] TRAIN: iteration 2159 : loss : 0.225049, loss_ce: 0.006962, loss_dice: 0.443136
[08:02:58.342] TRAIN: iteration 2160 : loss : 0.249739, loss_ce: 0.003971, loss_dice: 0.495506
[08:02:58.579] TRAIN: iteration 2161 : loss : 0.165154, loss_ce: 0.013372, loss_dice: 0.316936
[08:02:58.787] TRAIN: iteration 2162 : loss : 0.252087, loss_ce: 0.003995, loss_dice: 0.500178
[08:02:58.995] TRAIN: iteration 2163 : loss : 0.209474, loss_ce: 0.013433, loss_dice: 0.405516
[08:02:59.202] TRAIN: iteration 2164 : loss : 0.131458, loss_ce: 0.006264, loss_dice: 0.256652
[08:02:59.411] TRAIN: iteration 2165 : loss : 0.222520, loss_ce: 0.005063, loss_dice: 0.439976
[08:02:59.620] TRAIN: iteration 2166 : loss : 0.148198, loss_ce: 0.006897, loss_dice: 0.289500
[08:02:59.828] TRAIN: iteration 2167 : loss : 0.130501, loss_ce: 0.008206, loss_dice: 0.252795
[08:03:00.035] TRAIN: iteration 2168 : loss : 0.168360, loss_ce: 0.006007, loss_dice: 0.330712
[08:03:00.243] TRAIN: iteration 2169 : loss : 0.169011, loss_ce: 0.005975, loss_dice: 0.332047
[08:03:00.450] TRAIN: iteration 2170 : loss : 0.237921, loss_ce: 0.006115, loss_dice: 0.469727
[08:03:00.661] TRAIN: iteration 2171 : loss : 0.132328, loss_ce: 0.005472, loss_dice: 0.259185
[08:03:00.875] TRAIN: iteration 2172 : loss : 0.131564, loss_ce: 0.004827, loss_dice: 0.258302
[08:03:01.087] TRAIN: iteration 2173 : loss : 0.222464, loss_ce: 0.005823, loss_dice: 0.439105
[08:03:01.293] TRAIN: iteration 2174 : loss : 0.142922, loss_ce: 0.007985, loss_dice: 0.277859
[08:03:01.502] TRAIN: iteration 2175 : loss : 0.146944, loss_ce: 0.004747, loss_dice: 0.289141
[08:03:01.709] TRAIN: iteration 2176 : loss : 0.198041, loss_ce: 0.004779, loss_dice: 0.391303
[08:03:01.923] TRAIN: iteration 2177 : loss : 0.216154, loss_ce: 0.038212, loss_dice: 0.394095
[08:03:02.149] TRAIN: iteration 2178 : loss : 0.154574, loss_ce: 0.004178, loss_dice: 0.304970
[08:03:02.356] TRAIN: iteration 2179 : loss : 0.251896, loss_ce: 0.003644, loss_dice: 0.500148
[08:03:02.571] TRAIN: iteration 2180 : loss : 0.242008, loss_ce: 0.010476, loss_dice: 0.473540
[08:03:02.812] TRAIN: iteration 2181 : loss : 0.247529, loss_ce: 0.031656, loss_dice: 0.463402
[08:03:03.021] TRAIN: iteration 2182 : loss : 0.148031, loss_ce: 0.004619, loss_dice: 0.291443
[08:03:03.228] TRAIN: iteration 2183 : loss : 0.104476, loss_ce: 0.006249, loss_dice: 0.202703
[08:03:03.435] TRAIN: iteration 2184 : loss : 0.146987, loss_ce: 0.013043, loss_dice: 0.280931
[08:03:03.643] TRAIN: iteration 2185 : loss : 0.165081, loss_ce: 0.008608, loss_dice: 0.321555
[08:03:03.850] TRAIN: iteration 2186 : loss : 0.234201, loss_ce: 0.005317, loss_dice: 0.463084
[08:03:04.057] TRAIN: iteration 2187 : loss : 0.239893, loss_ce: 0.005359, loss_dice: 0.474427
[08:03:04.267] TRAIN: iteration 2188 : loss : 0.245445, loss_ce: 0.007936, loss_dice: 0.482954
[08:03:04.476] TRAIN: iteration 2189 : loss : 0.178537, loss_ce: 0.006454, loss_dice: 0.350621
[08:03:04.684] TRAIN: iteration 2190 : loss : 0.207897, loss_ce: 0.010235, loss_dice: 0.405558
[08:03:04.897] TRAIN: iteration 2191 : loss : 0.221975, loss_ce: 0.011013, loss_dice: 0.432936
[08:03:05.106] TRAIN: iteration 2192 : loss : 0.087587, loss_ce: 0.007567, loss_dice: 0.167608
[08:03:05.316] TRAIN: iteration 2193 : loss : 0.182824, loss_ce: 0.009518, loss_dice: 0.356131
[08:03:05.524] TRAIN: iteration 2194 : loss : 0.253755, loss_ce: 0.007073, loss_dice: 0.500438
[08:03:05.737] TRAIN: iteration 2195 : loss : 0.151734, loss_ce: 0.016022, loss_dice: 0.287447
[08:03:05.952] TRAIN: iteration 2196 : loss : 0.152350, loss_ce: 0.010823, loss_dice: 0.293878
[08:03:06.163] TRAIN: iteration 2197 : loss : 0.204801, loss_ce: 0.008086, loss_dice: 0.401516
[08:03:06.372] TRAIN: iteration 2198 : loss : 0.251877, loss_ce: 0.009066, loss_dice: 0.494687
[08:03:06.578] TRAIN: iteration 2199 : loss : 0.254137, loss_ce: 0.007754, loss_dice: 0.500520
[08:03:06.801] TRAIN: iteration 2200 : loss : 0.235544, loss_ce: 0.010173, loss_dice: 0.460914
[08:03:07.048] TRAIN: iteration 2201 : loss : 0.173906, loss_ce: 0.027198, loss_dice: 0.320615
[08:03:07.254] TRAIN: iteration 2202 : loss : 0.234596, loss_ce: 0.009934, loss_dice: 0.459258
[08:03:07.462] TRAIN: iteration 2203 : loss : 0.217428, loss_ce: 0.013168, loss_dice: 0.421688
[08:03:07.670] TRAIN: iteration 2204 : loss : 0.136892, loss_ce: 0.009354, loss_dice: 0.264431
[08:03:08.209] TRAIN: iteration 2205 : loss : 0.204138, loss_ce: 0.008706, loss_dice: 0.399569
[08:03:08.415] TRAIN: iteration 2206 : loss : 0.219596, loss_ce: 0.008204, loss_dice: 0.430987
[08:03:08.624] TRAIN: iteration 2207 : loss : 0.253378, loss_ce: 0.008277, loss_dice: 0.498478
[08:03:08.866] TRAIN: iteration 2208 : loss : 0.236427, loss_ce: 0.007614, loss_dice: 0.465241
[08:03:09.073] TRAIN: iteration 2209 : loss : 0.143976, loss_ce: 0.008533, loss_dice: 0.279419
[08:03:09.287] TRAIN: iteration 2210 : loss : 0.200170, loss_ce: 0.007421, loss_dice: 0.392919
[08:03:09.495] TRAIN: iteration 2211 : loss : 0.182059, loss_ce: 0.006196, loss_dice: 0.357923
[08:03:09.702] TRAIN: iteration 2212 : loss : 0.195107, loss_ce: 0.013085, loss_dice: 0.377128
[08:03:09.911] TRAIN: iteration 2213 : loss : 0.137458, loss_ce: 0.007027, loss_dice: 0.267889
[08:03:10.756] TRAIN: iteration 2214 : loss : 0.240162, loss_ce: 0.010554, loss_dice: 0.469770
[08:03:10.966] TRAIN: iteration 2215 : loss : 0.151897, loss_ce: 0.007069, loss_dice: 0.296725
[08:03:11.173] TRAIN: iteration 2216 : loss : 0.228194, loss_ce: 0.039648, loss_dice: 0.416741
[08:03:11.389] TRAIN: iteration 2217 : loss : 0.169020, loss_ce: 0.005436, loss_dice: 0.332604
[08:03:11.598] TRAIN: iteration 2218 : loss : 0.132663, loss_ce: 0.008426, loss_dice: 0.256899
[08:03:11.809] TRAIN: iteration 2219 : loss : 0.222467, loss_ce: 0.004784, loss_dice: 0.440150
[08:03:12.018] TRAIN: iteration 2220 : loss : 0.248860, loss_ce: 0.016068, loss_dice: 0.481652
[08:03:12.264] TRAIN: iteration 2221 : loss : 0.118290, loss_ce: 0.009338, loss_dice: 0.227242
[08:03:12.477] TRAIN: iteration 2222 : loss : 0.161986, loss_ce: 0.010096, loss_dice: 0.313876
[08:03:12.687] TRAIN: iteration 2223 : loss : 0.121249, loss_ce: 0.005601, loss_dice: 0.236897
[08:03:12.894] TRAIN: iteration 2224 : loss : 0.137776, loss_ce: 0.009117, loss_dice: 0.266434
[08:03:13.103] TRAIN: iteration 2225 : loss : 0.127752, loss_ce: 0.004629, loss_dice: 0.250874
[08:03:13.318] TRAIN: iteration 2226 : loss : 0.194490, loss_ce: 0.011284, loss_dice: 0.377696
[08:03:13.524] TRAIN: iteration 2227 : loss : 0.243600, loss_ce: 0.017417, loss_dice: 0.469784
[08:03:13.731] TRAIN: iteration 2228 : loss : 0.109989, loss_ce: 0.005908, loss_dice: 0.214069
[08:03:13.940] TRAIN: iteration 2229 : loss : 0.098421, loss_ce: 0.004759, loss_dice: 0.192083
[08:03:16.212] TRAIN: iteration 2230 : loss : 0.159187, loss_ce: 0.008100, loss_dice: 0.310274
[08:03:16.419] TRAIN: iteration 2231 : loss : 0.238362, loss_ce: 0.005587, loss_dice: 0.471137
[08:03:16.625] TRAIN: iteration 2232 : loss : 0.139828, loss_ce: 0.008064, loss_dice: 0.271592
[08:03:16.832] TRAIN: iteration 2233 : loss : 0.136368, loss_ce: 0.006172, loss_dice: 0.266563
[08:03:17.051] TRAIN: iteration 2234 : loss : 0.220467, loss_ce: 0.005139, loss_dice: 0.435794
[08:03:17.258] TRAIN: iteration 2235 : loss : 0.112556, loss_ce: 0.005777, loss_dice: 0.219336
[08:03:17.465] TRAIN: iteration 2236 : loss : 0.147565, loss_ce: 0.008208, loss_dice: 0.286923
[08:03:17.672] TRAIN: iteration 2237 : loss : 0.252766, loss_ce: 0.005260, loss_dice: 0.500272
[08:03:17.880] TRAIN: iteration 2238 : loss : 0.209667, loss_ce: 0.009067, loss_dice: 0.410267
[08:03:18.088] TRAIN: iteration 2239 : loss : 0.107636, loss_ce: 0.006588, loss_dice: 0.208683
[08:03:18.295] TRAIN: iteration 2240 : loss : 0.127507, loss_ce: 0.007807, loss_dice: 0.247207
[08:03:18.539] TRAIN: iteration 2241 : loss : 0.245937, loss_ce: 0.006203, loss_dice: 0.485671
[08:03:18.753] TRAIN: iteration 2242 : loss : 0.166405, loss_ce: 0.005426, loss_dice: 0.327384
[08:03:18.960] TRAIN: iteration 2243 : loss : 0.239363, loss_ce: 0.006056, loss_dice: 0.472669
[08:03:19.167] TRAIN: iteration 2244 : loss : 0.151554, loss_ce: 0.009358, loss_dice: 0.293751
[08:03:19.381] TRAIN: iteration 2245 : loss : 0.230965, loss_ce: 0.005119, loss_dice: 0.456811
[08:03:19.588] TRAIN: iteration 2246 : loss : 0.250881, loss_ce: 0.005706, loss_dice: 0.496055
[08:03:19.801] TRAIN: iteration 2247 : loss : 0.209649, loss_ce: 0.004818, loss_dice: 0.414480
[08:03:20.008] TRAIN: iteration 2248 : loss : 0.154182, loss_ce: 0.005432, loss_dice: 0.302931
[08:03:20.216] TRAIN: iteration 2249 : loss : 0.239682, loss_ce: 0.008733, loss_dice: 0.470630
[08:03:20.431] TRAIN: iteration 2250 : loss : 0.160733, loss_ce: 0.020602, loss_dice: 0.300864
[08:03:20.638] TRAIN: iteration 2251 : loss : 0.244927, loss_ce: 0.004547, loss_dice: 0.485308
[08:03:20.845] TRAIN: iteration 2252 : loss : 0.228979, loss_ce: 0.005646, loss_dice: 0.452311
[08:03:21.052] TRAIN: iteration 2253 : loss : 0.231977, loss_ce: 0.006975, loss_dice: 0.456979
[08:03:21.260] TRAIN: iteration 2254 : loss : 0.207702, loss_ce: 0.008971, loss_dice: 0.406434
[08:03:21.498] TRAIN: iteration 2255 : loss : 0.192908, loss_ce: 0.005216, loss_dice: 0.380599
[08:03:21.705] TRAIN: iteration 2256 : loss : 0.252657, loss_ce: 0.005036, loss_dice: 0.500278
[08:03:21.911] TRAIN: iteration 2257 : loss : 0.240442, loss_ce: 0.031599, loss_dice: 0.449285
[08:03:22.118] TRAIN: iteration 2258 : loss : 0.108155, loss_ce: 0.004339, loss_dice: 0.211971
[08:03:22.326] TRAIN: iteration 2259 : loss : 0.092113, loss_ce: 0.003887, loss_dice: 0.180340
[08:03:22.533] TRAIN: iteration 2260 : loss : 0.124400, loss_ce: 0.005535, loss_dice: 0.243265
[08:03:22.775] TRAIN: iteration 2261 : loss : 0.106210, loss_ce: 0.004757, loss_dice: 0.207663
[08:03:22.985] TRAIN: iteration 2262 : loss : 0.094461, loss_ce: 0.004065, loss_dice: 0.184858
[08:03:23.196] TRAIN: iteration 2263 : loss : 0.147584, loss_ce: 0.007501, loss_dice: 0.287667
[08:03:23.408] TRAIN: iteration 2264 : loss : 0.251862, loss_ce: 0.003564, loss_dice: 0.500161
[08:03:23.621] TRAIN: iteration 2265 : loss : 0.141995, loss_ce: 0.011203, loss_dice: 0.272788
[08:03:23.831] TRAIN: iteration 2266 : loss : 0.170324, loss_ce: 0.007896, loss_dice: 0.332753
[08:03:24.047] TRAIN: iteration 2267 : loss : 0.157913, loss_ce: 0.019468, loss_dice: 0.296358
[08:03:24.255] TRAIN: iteration 2268 : loss : 0.252241, loss_ce: 0.004265, loss_dice: 0.500218
[08:03:24.462] TRAIN: iteration 2269 : loss : 0.082982, loss_ce: 0.004738, loss_dice: 0.161226
[08:03:24.669] TRAIN: iteration 2270 : loss : 0.189786, loss_ce: 0.005489, loss_dice: 0.374082
[08:03:24.875] TRAIN: iteration 2271 : loss : 0.181510, loss_ce: 0.008630, loss_dice: 0.354390
[08:03:25.083] TRAIN: iteration 2272 : loss : 0.162826, loss_ce: 0.005922, loss_dice: 0.319731
[08:03:25.289] TRAIN: iteration 2273 : loss : 0.153379, loss_ce: 0.008970, loss_dice: 0.297788
[08:03:25.497] TRAIN: iteration 2274 : loss : 0.252383, loss_ce: 0.005880, loss_dice: 0.498886
[08:03:26.455] TRAIN: iteration 2275 : loss : 0.117651, loss_ce: 0.007937, loss_dice: 0.227365
[08:03:26.668] TRAIN: iteration 2276 : loss : 0.228937, loss_ce: 0.004896, loss_dice: 0.452979
[08:03:26.876] TRAIN: iteration 2277 : loss : 0.253036, loss_ce: 0.005691, loss_dice: 0.500381
[08:03:27.084] TRAIN: iteration 2278 : loss : 0.226060, loss_ce: 0.006099, loss_dice: 0.446020
[08:03:27.295] TRAIN: iteration 2279 : loss : 0.172030, loss_ce: 0.005013, loss_dice: 0.339047
[08:03:27.503] TRAIN: iteration 2280 : loss : 0.084768, loss_ce: 0.004687, loss_dice: 0.164849
[08:03:27.740] TRAIN: iteration 2281 : loss : 0.210825, loss_ce: 0.007917, loss_dice: 0.413734
[08:03:27.946] TRAIN: iteration 2282 : loss : 0.189670, loss_ce: 0.014510, loss_dice: 0.364830
[08:03:28.279] TRAIN: iteration 2283 : loss : 0.253259, loss_ce: 0.006095, loss_dice: 0.500422
[08:03:28.486] TRAIN: iteration 2284 : loss : 0.067198, loss_ce: 0.004227, loss_dice: 0.130169
[08:03:28.692] TRAIN: iteration 2285 : loss : 0.205062, loss_ce: 0.007604, loss_dice: 0.402520
[08:03:28.899] TRAIN: iteration 2286 : loss : 0.239569, loss_ce: 0.006393, loss_dice: 0.472746
[08:03:29.108] TRAIN: iteration 2287 : loss : 0.159921, loss_ce: 0.005440, loss_dice: 0.314402
[08:03:29.315] TRAIN: iteration 2288 : loss : 0.240499, loss_ce: 0.005176, loss_dice: 0.475822
[08:03:29.522] TRAIN: iteration 2289 : loss : 0.069404, loss_ce: 0.003275, loss_dice: 0.135533
[08:03:29.729] TRAIN: iteration 2290 : loss : 0.146941, loss_ce: 0.005826, loss_dice: 0.288056
[08:03:31.492] TRAIN: iteration 2291 : loss : 0.228830, loss_ce: 0.046971, loss_dice: 0.410689
[08:03:31.700] TRAIN: iteration 2292 : loss : 0.103532, loss_ce: 0.008539, loss_dice: 0.198524
[08:03:31.906] TRAIN: iteration 2293 : loss : 0.218977, loss_ce: 0.004656, loss_dice: 0.433298
[08:03:32.113] TRAIN: iteration 2294 : loss : 0.114536, loss_ce: 0.004542, loss_dice: 0.224530
[08:03:32.320] TRAIN: iteration 2295 : loss : 0.139132, loss_ce: 0.015129, loss_dice: 0.263134
[08:03:32.527] TRAIN: iteration 2296 : loss : 0.080633, loss_ce: 0.003546, loss_dice: 0.157721
[08:03:32.734] TRAIN: iteration 2297 : loss : 0.116060, loss_ce: 0.008978, loss_dice: 0.223142
[08:03:32.941] TRAIN: iteration 2298 : loss : 0.247816, loss_ce: 0.004692, loss_dice: 0.490940
[08:03:33.168] TRAIN: iteration 2299 : loss : 0.096316, loss_ce: 0.003828, loss_dice: 0.188804
[08:03:33.375] TRAIN: iteration 2300 : loss : 0.129663, loss_ce: 0.014537, loss_dice: 0.244789
[08:03:33.615] TRAIN: iteration 2301 : loss : 0.144918, loss_ce: 0.004419, loss_dice: 0.285416
[08:03:33.821] TRAIN: iteration 2302 : loss : 0.251269, loss_ce: 0.004513, loss_dice: 0.498026
[08:03:34.028] TRAIN: iteration 2303 : loss : 0.248245, loss_ce: 0.005886, loss_dice: 0.490604
[08:03:34.234] TRAIN: iteration 2304 : loss : 0.175396, loss_ce: 0.006977, loss_dice: 0.343815
[08:03:34.452] TRAIN: iteration 2305 : loss : 0.126995, loss_ce: 0.006754, loss_dice: 0.247236
[08:03:34.660] TRAIN: iteration 2306 : loss : 0.252151, loss_ce: 0.011610, loss_dice: 0.492692
[08:03:34.868] TRAIN: iteration 2307 : loss : 0.252068, loss_ce: 0.003947, loss_dice: 0.500189
[08:03:35.079] TRAIN: iteration 2308 : loss : 0.203645, loss_ce: 0.043790, loss_dice: 0.363501
[08:03:36.552] TRAIN: iteration 2309 : loss : 0.130095, loss_ce: 0.004639, loss_dice: 0.255551
[08:03:36.759] TRAIN: iteration 2310 : loss : 0.081344, loss_ce: 0.005441, loss_dice: 0.157247
[08:03:36.969] TRAIN: iteration 2311 : loss : 0.252114, loss_ce: 0.005570, loss_dice: 0.498658
[08:03:37.184] TRAIN: iteration 2312 : loss : 0.082815, loss_ce: 0.004768, loss_dice: 0.160861
[08:03:37.398] TRAIN: iteration 2313 : loss : 0.140875, loss_ce: 0.011245, loss_dice: 0.270504
[08:03:37.605] TRAIN: iteration 2314 : loss : 0.242814, loss_ce: 0.009498, loss_dice: 0.476130
[08:03:37.815] TRAIN: iteration 2315 : loss : 0.160311, loss_ce: 0.006276, loss_dice: 0.314347
[08:03:38.025] TRAIN: iteration 2316 : loss : 0.152378, loss_ce: 0.005605, loss_dice: 0.299152
[08:03:38.235] TRAIN: iteration 2317 : loss : 0.136976, loss_ce: 0.014212, loss_dice: 0.259740
[08:03:38.446] TRAIN: iteration 2318 : loss : 0.103387, loss_ce: 0.005853, loss_dice: 0.200922
[08:03:38.657] TRAIN: iteration 2319 : loss : 0.195186, loss_ce: 0.034539, loss_dice: 0.355832
[08:03:38.864] TRAIN: iteration 2320 : loss : 0.199004, loss_ce: 0.006622, loss_dice: 0.391385
[08:03:39.095] TRAIN: iteration 2321 : loss : 0.136348, loss_ce: 0.009771, loss_dice: 0.262926
[08:03:39.303] TRAIN: iteration 2322 : loss : 0.246957, loss_ce: 0.006885, loss_dice: 0.487028
[08:03:39.511] TRAIN: iteration 2323 : loss : 0.252402, loss_ce: 0.004552, loss_dice: 0.500252
[08:03:39.717] TRAIN: iteration 2324 : loss : 0.237283, loss_ce: 0.006244, loss_dice: 0.468322
[08:03:39.925] TRAIN: iteration 2325 : loss : 0.252858, loss_ce: 0.006936, loss_dice: 0.498779
[08:03:40.134] TRAIN: iteration 2326 : loss : 0.141004, loss_ce: 0.009320, loss_dice: 0.272687
[08:03:40.342] TRAIN: iteration 2327 : loss : 0.186389, loss_ce: 0.010719, loss_dice: 0.362059
[08:03:40.550] TRAIN: iteration 2328 : loss : 0.182218, loss_ce: 0.007188, loss_dice: 0.357248
[08:03:40.761] TRAIN: iteration 2329 : loss : 0.251937, loss_ce: 0.006047, loss_dice: 0.497828
[08:03:40.968] TRAIN: iteration 2330 : loss : 0.168782, loss_ce: 0.008134, loss_dice: 0.329430
[08:03:41.175] TRAIN: iteration 2331 : loss : 0.201528, loss_ce: 0.015669, loss_dice: 0.387388
[08:03:41.385] TRAIN: iteration 2332 : loss : 0.252018, loss_ce: 0.017730, loss_dice: 0.486305
[08:03:41.621] TRAIN: iteration 2333 : loss : 0.249219, loss_ce: 0.009460, loss_dice: 0.488979
[08:03:41.829] TRAIN: iteration 2334 : loss : 0.137044, loss_ce: 0.006389, loss_dice: 0.267698
[08:03:42.036] TRAIN: iteration 2335 : loss : 0.115006, loss_ce: 0.009994, loss_dice: 0.220017
[08:03:42.244] TRAIN: iteration 2336 : loss : 0.253213, loss_ce: 0.006071, loss_dice: 0.500354
[08:03:42.483] TRAIN: iteration 2337 : loss : 0.156741, loss_ce: 0.006963, loss_dice: 0.306519
[08:03:42.691] TRAIN: iteration 2338 : loss : 0.158555, loss_ce: 0.017781, loss_dice: 0.299329
[08:03:42.897] TRAIN: iteration 2339 : loss : 0.170179, loss_ce: 0.031205, loss_dice: 0.309153
[08:03:44.015] TRAIN: iteration 2340 : loss : 0.182179, loss_ce: 0.007042, loss_dice: 0.357317
[08:03:44.252] TRAIN: iteration 2341 : loss : 0.199392, loss_ce: 0.006094, loss_dice: 0.392691
[08:03:44.461] TRAIN: iteration 2342 : loss : 0.153333, loss_ce: 0.007323, loss_dice: 0.299343
[08:03:44.669] TRAIN: iteration 2343 : loss : 0.253363, loss_ce: 0.007252, loss_dice: 0.499473
[08:03:44.876] TRAIN: iteration 2344 : loss : 0.252060, loss_ce: 0.008203, loss_dice: 0.495916
[08:03:45.084] TRAIN: iteration 2345 : loss : 0.214312, loss_ce: 0.011814, loss_dice: 0.416811
[08:03:45.293] TRAIN: iteration 2346 : loss : 0.222588, loss_ce: 0.009397, loss_dice: 0.435778
[08:03:45.502] TRAIN: iteration 2347 : loss : 0.117876, loss_ce: 0.005408, loss_dice: 0.230344
[08:03:46.298] TRAIN: iteration 2348 : loss : 0.143014, loss_ce: 0.006869, loss_dice: 0.279159
[08:03:46.506] TRAIN: iteration 2349 : loss : 0.158302, loss_ce: 0.005250, loss_dice: 0.311355
[08:03:46.714] TRAIN: iteration 2350 : loss : 0.169096, loss_ce: 0.010218, loss_dice: 0.327974
[08:03:46.924] TRAIN: iteration 2351 : loss : 0.222019, loss_ce: 0.006151, loss_dice: 0.437888
[08:03:47.131] TRAIN: iteration 2352 : loss : 0.201193, loss_ce: 0.009114, loss_dice: 0.393273
[08:03:47.341] TRAIN: iteration 2353 : loss : 0.140732, loss_ce: 0.005706, loss_dice: 0.275757
[08:03:47.551] TRAIN: iteration 2354 : loss : 0.144486, loss_ce: 0.005276, loss_dice: 0.283696
[08:03:47.760] TRAIN: iteration 2355 : loss : 0.230741, loss_ce: 0.005706, loss_dice: 0.455777
[08:03:47.966] TRAIN: iteration 2356 : loss : 0.197207, loss_ce: 0.004429, loss_dice: 0.389986
[08:03:48.178] TRAIN: iteration 2357 : loss : 0.248297, loss_ce: 0.005490, loss_dice: 0.491104
[08:03:48.385] TRAIN: iteration 2358 : loss : 0.251775, loss_ce: 0.005898, loss_dice: 0.497652
[08:03:48.597] TRAIN: iteration 2359 : loss : 0.122075, loss_ce: 0.005935, loss_dice: 0.238215
[08:03:48.804] TRAIN: iteration 2360 : loss : 0.155112, loss_ce: 0.008317, loss_dice: 0.301908
[08:03:49.060] TRAIN: iteration 2361 : loss : 0.236211, loss_ce: 0.007571, loss_dice: 0.464851
[08:03:49.270] TRAIN: iteration 2362 : loss : 0.241964, loss_ce: 0.011348, loss_dice: 0.472579
[08:03:49.476] TRAIN: iteration 2363 : loss : 0.146067, loss_ce: 0.004789, loss_dice: 0.287346
[08:03:51.321] TRAIN: iteration 2364 : loss : 0.162610, loss_ce: 0.004271, loss_dice: 0.320948
[08:03:51.528] TRAIN: iteration 2365 : loss : 0.248737, loss_ce: 0.022932, loss_dice: 0.474542
[08:03:51.735] TRAIN: iteration 2366 : loss : 0.250624, loss_ce: 0.011154, loss_dice: 0.490093
[08:03:51.944] TRAIN: iteration 2367 : loss : 0.097834, loss_ce: 0.006384, loss_dice: 0.189284
[08:03:52.155] TRAIN: iteration 2368 : loss : 0.252311, loss_ce: 0.004410, loss_dice: 0.500213
[08:03:52.363] TRAIN: iteration 2369 : loss : 0.246383, loss_ce: 0.033395, loss_dice: 0.459371
[08:03:52.571] TRAIN: iteration 2370 : loss : 0.242485, loss_ce: 0.031675, loss_dice: 0.453294
[08:03:52.778] TRAIN: iteration 2371 : loss : 0.180719, loss_ce: 0.013057, loss_dice: 0.348381
[08:03:54.358] TRAIN: iteration 2372 : loss : 0.132959, loss_ce: 0.007604, loss_dice: 0.258314
[08:03:54.565] TRAIN: iteration 2373 : loss : 0.107189, loss_ce: 0.004830, loss_dice: 0.209549
[08:03:54.812] TRAIN: iteration 2374 : loss : 0.253662, loss_ce: 0.006936, loss_dice: 0.500388
[08:03:55.025] TRAIN: iteration 2375 : loss : 0.143783, loss_ce: 0.006427, loss_dice: 0.281139
[08:03:55.238] TRAIN: iteration 2376 : loss : 0.177903, loss_ce: 0.007836, loss_dice: 0.347970
[08:03:55.444] TRAIN: iteration 2377 : loss : 0.191708, loss_ce: 0.015735, loss_dice: 0.367681
[08:03:55.652] TRAIN: iteration 2378 : loss : 0.149808, loss_ce: 0.012042, loss_dice: 0.287575
[08:03:55.859] TRAIN: iteration 2379 : loss : 0.193638, loss_ce: 0.007238, loss_dice: 0.380038
[08:03:56.068] TRAIN: iteration 2380 : loss : 0.113120, loss_ce: 0.005945, loss_dice: 0.220295
[08:03:56.315] TRAIN: iteration 2381 : loss : 0.243645, loss_ce: 0.017178, loss_dice: 0.470111
[08:03:56.523] TRAIN: iteration 2382 : loss : 0.253330, loss_ce: 0.006343, loss_dice: 0.500316
[08:03:56.739] TRAIN: iteration 2383 : loss : 0.253332, loss_ce: 0.006320, loss_dice: 0.500344
[08:03:56.950] TRAIN: iteration 2384 : loss : 0.117483, loss_ce: 0.005836, loss_dice: 0.229130
[08:03:57.156] TRAIN: iteration 2385 : loss : 0.253031, loss_ce: 0.005750, loss_dice: 0.500312
[08:03:57.363] TRAIN: iteration 2386 : loss : 0.202190, loss_ce: 0.006565, loss_dice: 0.397815
[08:03:57.570] TRAIN: iteration 2387 : loss : 0.112845, loss_ce: 0.006018, loss_dice: 0.219673
[08:03:57.778] TRAIN: iteration 2388 : loss : 0.217617, loss_ce: 0.028802, loss_dice: 0.406432
[08:03:57.986] TRAIN: iteration 2389 : loss : 0.145150, loss_ce: 0.006959, loss_dice: 0.283341
[08:03:58.194] TRAIN: iteration 2390 : loss : 0.251687, loss_ce: 0.007986, loss_dice: 0.495388
[08:03:58.402] TRAIN: iteration 2391 : loss : 0.205765, loss_ce: 0.004381, loss_dice: 0.407150
[08:03:58.611] TRAIN: iteration 2392 : loss : 0.248864, loss_ce: 0.005134, loss_dice: 0.492594
[08:03:58.823] TRAIN: iteration 2393 : loss : 0.241345, loss_ce: 0.004266, loss_dice: 0.478425
[08:03:59.031] TRAIN: iteration 2394 : loss : 0.208509, loss_ce: 0.013613, loss_dice: 0.403406
[08:03:59.246] TRAIN: iteration 2395 : loss : 0.246297, loss_ce: 0.003681, loss_dice: 0.488913
[08:03:59.457] TRAIN: iteration 2396 : loss : 0.251768, loss_ce: 0.003462, loss_dice: 0.500073
[08:03:59.671] TRAIN: iteration 2397 : loss : 0.236355, loss_ce: 0.004914, loss_dice: 0.467797
[08:03:59.877] TRAIN: iteration 2398 : loss : 0.219368, loss_ce: 0.004880, loss_dice: 0.433856
[08:04:00.085] TRAIN: iteration 2399 : loss : 0.124061, loss_ce: 0.004124, loss_dice: 0.243998
[08:04:00.299] TRAIN: iteration 2400 : loss : 0.139594, loss_ce: 0.007776, loss_dice: 0.271411
[08:04:00.538] TRAIN: iteration 2401 : loss : 0.173978, loss_ce: 0.006548, loss_dice: 0.341409
[08:04:00.748] TRAIN: iteration 2402 : loss : 0.122061, loss_ce: 0.006079, loss_dice: 0.238043
[08:04:00.954] TRAIN: iteration 2403 : loss : 0.197191, loss_ce: 0.004714, loss_dice: 0.389669
[08:04:01.167] TRAIN: iteration 2404 : loss : 0.166082, loss_ce: 0.012211, loss_dice: 0.319954
[08:04:01.375] TRAIN: iteration 2405 : loss : 0.158545, loss_ce: 0.005173, loss_dice: 0.311917
[08:04:03.233] TRAIN: iteration 2406 : loss : 0.170538, loss_ce: 0.005442, loss_dice: 0.335634
[08:04:03.443] TRAIN: iteration 2407 : loss : 0.228178, loss_ce: 0.010021, loss_dice: 0.446335
[08:04:03.661] TRAIN: iteration 2408 : loss : 0.252932, loss_ce: 0.005539, loss_dice: 0.500326
[08:04:03.869] TRAIN: iteration 2409 : loss : 0.110655, loss_ce: 0.007544, loss_dice: 0.213766
[08:04:04.086] TRAIN: iteration 2410 : loss : 0.208133, loss_ce: 0.021280, loss_dice: 0.394985
[08:04:04.293] TRAIN: iteration 2411 : loss : 0.109726, loss_ce: 0.006770, loss_dice: 0.212683
[08:04:04.499] TRAIN: iteration 2412 : loss : 0.089919, loss_ce: 0.008113, loss_dice: 0.171725
[08:04:04.712] TRAIN: iteration 2413 : loss : 0.254904, loss_ce: 0.010227, loss_dice: 0.499580
[08:04:04.922] TRAIN: iteration 2414 : loss : 0.112732, loss_ce: 0.005784, loss_dice: 0.219680
[08:04:05.130] TRAIN: iteration 2415 : loss : 0.246150, loss_ce: 0.008348, loss_dice: 0.483953
[08:04:05.337] TRAIN: iteration 2416 : loss : 0.177818, loss_ce: 0.006752, loss_dice: 0.348884
[08:04:05.545] TRAIN: iteration 2417 : loss : 0.124756, loss_ce: 0.012276, loss_dice: 0.237236
[08:04:05.753] TRAIN: iteration 2418 : loss : 0.224336, loss_ce: 0.006399, loss_dice: 0.442272
[08:04:06.554] TRAIN: iteration 2419 : loss : 0.121243, loss_ce: 0.005844, loss_dice: 0.236642
[08:04:06.762] TRAIN: iteration 2420 : loss : 0.143771, loss_ce: 0.005653, loss_dice: 0.281889
[08:04:07.001] TRAIN: iteration 2421 : loss : 0.174075, loss_ce: 0.007898, loss_dice: 0.340253
[08:04:07.208] TRAIN: iteration 2422 : loss : 0.181800, loss_ce: 0.004778, loss_dice: 0.358822
[08:04:07.416] TRAIN: iteration 2423 : loss : 0.197023, loss_ce: 0.006828, loss_dice: 0.387218
[08:04:07.624] TRAIN: iteration 2424 : loss : 0.154306, loss_ce: 0.006065, loss_dice: 0.302548
[08:04:07.832] TRAIN: iteration 2425 : loss : 0.137390, loss_ce: 0.009367, loss_dice: 0.265413
[08:04:08.051] TRAIN: iteration 2426 : loss : 0.126919, loss_ce: 0.004074, loss_dice: 0.249764
[08:04:08.259] TRAIN: iteration 2427 : loss : 0.095762, loss_ce: 0.005115, loss_dice: 0.186408
[08:04:08.468] TRAIN: iteration 2428 : loss : 0.187432, loss_ce: 0.009849, loss_dice: 0.365014
[08:04:08.675] TRAIN: iteration 2429 : loss : 0.096411, loss_ce: 0.005272, loss_dice: 0.187551
[08:04:08.883] TRAIN: iteration 2430 : loss : 0.227359, loss_ce: 0.007162, loss_dice: 0.447555
[08:04:09.090] TRAIN: iteration 2431 : loss : 0.251700, loss_ce: 0.003217, loss_dice: 0.500182
[08:04:09.298] TRAIN: iteration 2432 : loss : 0.251033, loss_ce: 0.003954, loss_dice: 0.498113
[08:04:09.503] TRAIN: iteration 2433 : loss : 0.110831, loss_ce: 0.003518, loss_dice: 0.218144
[08:04:09.711] TRAIN: iteration 2434 : loss : 0.166030, loss_ce: 0.008711, loss_dice: 0.323348
[08:04:09.927] TRAIN: iteration 2435 : loss : 0.150612, loss_ce: 0.005545, loss_dice: 0.295678
[08:04:10.184] TRAIN: iteration 2436 : loss : 0.209579, loss_ce: 0.025377, loss_dice: 0.393781
[08:04:11.153] TRAIN: iteration 2437 : loss : 0.172855, loss_ce: 0.006654, loss_dice: 0.339056
[08:04:11.359] TRAIN: iteration 2438 : loss : 0.251597, loss_ce: 0.003068, loss_dice: 0.500126
[08:04:11.571] TRAIN: iteration 2439 : loss : 0.173731, loss_ce: 0.020806, loss_dice: 0.326655
[08:04:11.783] TRAIN: iteration 2440 : loss : 0.197013, loss_ce: 0.005316, loss_dice: 0.388711
[08:04:12.015] TRAIN: iteration 2441 : loss : 0.252427, loss_ce: 0.004866, loss_dice: 0.499988
[08:04:12.228] TRAIN: iteration 2442 : loss : 0.147955, loss_ce: 0.005501, loss_dice: 0.290408
[08:04:12.438] TRAIN: iteration 2443 : loss : 0.252369, loss_ce: 0.004441, loss_dice: 0.500298
[08:04:12.646] TRAIN: iteration 2444 : loss : 0.252612, loss_ce: 0.004933, loss_dice: 0.500292
[08:04:12.860] TRAIN: iteration 2445 : loss : 0.221227, loss_ce: 0.007939, loss_dice: 0.434516
[08:04:13.068] TRAIN: iteration 2446 : loss : 0.092797, loss_ce: 0.005765, loss_dice: 0.179829
[08:04:13.275] TRAIN: iteration 2447 : loss : 0.166277, loss_ce: 0.008611, loss_dice: 0.323942
[08:04:13.482] TRAIN: iteration 2448 : loss : 0.131011, loss_ce: 0.004715, loss_dice: 0.257307
[08:04:13.688] TRAIN: iteration 2449 : loss : 0.150333, loss_ce: 0.017851, loss_dice: 0.282815
[08:04:13.895] TRAIN: iteration 2450 : loss : 0.119397, loss_ce: 0.006828, loss_dice: 0.231966
[08:04:14.102] TRAIN: iteration 2451 : loss : 0.202560, loss_ce: 0.006805, loss_dice: 0.398316
[08:04:14.309] TRAIN: iteration 2452 : loss : 0.200499, loss_ce: 0.009333, loss_dice: 0.391666
[08:04:14.516] TRAIN: iteration 2453 : loss : 0.201365, loss_ce: 0.006520, loss_dice: 0.396211
[08:04:14.723] TRAIN: iteration 2454 : loss : 0.185084, loss_ce: 0.006091, loss_dice: 0.364076
[08:04:14.930] TRAIN: iteration 2455 : loss : 0.159653, loss_ce: 0.006017, loss_dice: 0.313288
[08:04:16.120] TRAIN: iteration 2456 : loss : 0.199912, loss_ce: 0.006456, loss_dice: 0.393367
[08:04:16.328] TRAIN: iteration 2457 : loss : 0.245913, loss_ce: 0.005363, loss_dice: 0.486462
[08:04:16.537] TRAIN: iteration 2458 : loss : 0.165136, loss_ce: 0.014596, loss_dice: 0.315675
[08:04:16.750] TRAIN: iteration 2459 : loss : 0.167635, loss_ce: 0.006121, loss_dice: 0.329149
[08:04:16.958] TRAIN: iteration 2460 : loss : 0.196490, loss_ce: 0.011044, loss_dice: 0.381936
[08:04:17.195] TRAIN: iteration 2461 : loss : 0.208142, loss_ce: 0.005679, loss_dice: 0.410605
[08:04:17.404] TRAIN: iteration 2462 : loss : 0.253083, loss_ce: 0.005799, loss_dice: 0.500367
[08:04:17.612] TRAIN: iteration 2463 : loss : 0.175480, loss_ce: 0.017137, loss_dice: 0.333824
[08:04:18.892] TRAIN: iteration 2464 : loss : 0.195438, loss_ce: 0.013523, loss_dice: 0.377354
[08:04:19.099] TRAIN: iteration 2465 : loss : 0.253152, loss_ce: 0.005969, loss_dice: 0.500336
[08:04:19.305] TRAIN: iteration 2466 : loss : 0.158424, loss_ce: 0.008834, loss_dice: 0.308014
[08:04:19.517] TRAIN: iteration 2467 : loss : 0.121346, loss_ce: 0.009583, loss_dice: 0.233109
[08:04:19.726] TRAIN: iteration 2468 : loss : 0.155926, loss_ce: 0.006724, loss_dice: 0.305128
[08:04:19.932] TRAIN: iteration 2469 : loss : 0.207070, loss_ce: 0.008722, loss_dice: 0.405419
[08:04:20.142] TRAIN: iteration 2470 : loss : 0.232628, loss_ce: 0.009400, loss_dice: 0.455856
[08:04:20.350] TRAIN: iteration 2471 : loss : 0.164750, loss_ce: 0.008656, loss_dice: 0.320843
[08:04:20.556] TRAIN: iteration 2472 : loss : 0.253474, loss_ce: 0.006550, loss_dice: 0.500398
[08:04:20.763] TRAIN: iteration 2473 : loss : 0.161323, loss_ce: 0.007129, loss_dice: 0.315518
[08:04:20.973] TRAIN: iteration 2474 : loss : 0.161643, loss_ce: 0.027116, loss_dice: 0.296170
[08:04:21.182] TRAIN: iteration 2475 : loss : 0.244874, loss_ce: 0.011920, loss_dice: 0.477827
[08:04:21.391] TRAIN: iteration 2476 : loss : 0.227078, loss_ce: 0.006778, loss_dice: 0.447377
[08:04:21.606] TRAIN: iteration 2477 : loss : 0.206285, loss_ce: 0.007022, loss_dice: 0.405549
[08:04:21.819] TRAIN: iteration 2478 : loss : 0.157504, loss_ce: 0.005620, loss_dice: 0.309388
[08:04:22.025] TRAIN: iteration 2479 : loss : 0.117211, loss_ce: 0.006294, loss_dice: 0.228129
[08:04:22.232] TRAIN: iteration 2480 : loss : 0.207817, loss_ce: 0.019949, loss_dice: 0.395685
[08:04:22.471] TRAIN: iteration 2481 : loss : 0.252112, loss_ce: 0.005816, loss_dice: 0.498408
[08:04:22.679] TRAIN: iteration 2482 : loss : 0.236643, loss_ce: 0.007874, loss_dice: 0.465412
[08:04:22.886] TRAIN: iteration 2483 : loss : 0.129856, loss_ce: 0.012556, loss_dice: 0.247156
[08:04:23.093] TRAIN: iteration 2484 : loss : 0.171565, loss_ce: 0.005508, loss_dice: 0.337621
[08:04:23.306] TRAIN: iteration 2485 : loss : 0.250215, loss_ce: 0.005166, loss_dice: 0.495265
[08:04:23.541] TRAIN: iteration 2486 : loss : 0.137720, loss_ce: 0.010749, loss_dice: 0.264691
[08:04:23.750] TRAIN: iteration 2487 : loss : 0.151444, loss_ce: 0.007731, loss_dice: 0.295157
[08:04:23.959] TRAIN: iteration 2488 : loss : 0.217564, loss_ce: 0.010360, loss_dice: 0.424767
[08:04:24.177] TRAIN: iteration 2489 : loss : 0.248039, loss_ce: 0.005479, loss_dice: 0.490598
[08:04:24.384] TRAIN: iteration 2490 : loss : 0.094240, loss_ce: 0.006854, loss_dice: 0.181626
[08:04:24.591] TRAIN: iteration 2491 : loss : 0.241616, loss_ce: 0.018952, loss_dice: 0.464280
[08:04:24.802] TRAIN: iteration 2492 : loss : 0.253052, loss_ce: 0.005786, loss_dice: 0.500317
[08:04:25.017] TRAIN: iteration 2493 : loss : 0.074959, loss_ce: 0.004540, loss_dice: 0.145379
[08:04:25.228] TRAIN: iteration 2494 : loss : 0.172474, loss_ce: 0.024745, loss_dice: 0.320204
[08:04:25.435] TRAIN: iteration 2495 : loss : 0.084008, loss_ce: 0.007431, loss_dice: 0.160585
[08:04:25.651] TRAIN: iteration 2496 : loss : 0.249927, loss_ce: 0.011098, loss_dice: 0.488755
[08:04:25.858] TRAIN: iteration 2497 : loss : 0.244757, loss_ce: 0.007792, loss_dice: 0.481722
[08:04:26.067] TRAIN: iteration 2498 : loss : 0.216256, loss_ce: 0.006704, loss_dice: 0.425809
[08:04:26.281] TRAIN: iteration 2499 : loss : 0.179999, loss_ce: 0.007159, loss_dice: 0.352839
[08:04:26.898] TRAIN: iteration 2500 : loss : 0.158279, loss_ce: 0.017887, loss_dice: 0.298671
[08:04:27.136] TRAIN: iteration 2501 : loss : 0.096586, loss_ce: 0.006854, loss_dice: 0.186319
[08:04:27.343] TRAIN: iteration 2502 : loss : 0.138407, loss_ce: 0.009024, loss_dice: 0.267790
[08:04:27.553] TRAIN: iteration 2503 : loss : 0.220569, loss_ce: 0.007965, loss_dice: 0.433173
[08:04:27.760] TRAIN: iteration 2504 : loss : 0.097791, loss_ce: 0.007808, loss_dice: 0.187774
[08:04:27.967] TRAIN: iteration 2505 : loss : 0.151526, loss_ce: 0.008290, loss_dice: 0.294763
[08:04:28.177] TRAIN: iteration 2506 : loss : 0.170259, loss_ce: 0.013981, loss_dice: 0.326536
[08:04:28.383] TRAIN: iteration 2507 : loss : 0.245102, loss_ce: 0.008306, loss_dice: 0.481898
[08:04:28.616] TRAIN: iteration 2508 : loss : 0.216635, loss_ce: 0.011779, loss_dice: 0.421492
[08:04:28.823] TRAIN: iteration 2509 : loss : 0.189722, loss_ce: 0.011180, loss_dice: 0.368263
[08:04:29.031] TRAIN: iteration 2510 : loss : 0.177671, loss_ce: 0.006359, loss_dice: 0.348982
[08:04:29.240] TRAIN: iteration 2511 : loss : 0.130947, loss_ce: 0.005708, loss_dice: 0.256186
[08:04:29.448] TRAIN: iteration 2512 : loss : 0.194840, loss_ce: 0.029926, loss_dice: 0.359755
[08:04:29.655] TRAIN: iteration 2513 : loss : 0.242456, loss_ce: 0.006966, loss_dice: 0.477946
[08:04:29.885] TRAIN: iteration 2514 : loss : 0.223297, loss_ce: 0.008485, loss_dice: 0.438109
[08:04:30.092] TRAIN: iteration 2515 : loss : 0.197769, loss_ce: 0.007108, loss_dice: 0.388430
[08:04:31.119] TRAIN: iteration 2516 : loss : 0.234374, loss_ce: 0.019157, loss_dice: 0.449591
[08:04:31.327] TRAIN: iteration 2517 : loss : 0.108033, loss_ce: 0.007136, loss_dice: 0.208929
[08:04:31.533] TRAIN: iteration 2518 : loss : 0.252375, loss_ce: 0.004595, loss_dice: 0.500154
[08:04:31.741] TRAIN: iteration 2519 : loss : 0.213041, loss_ce: 0.021130, loss_dice: 0.404952
[08:04:31.948] TRAIN: iteration 2520 : loss : 0.150835, loss_ce: 0.009839, loss_dice: 0.291832
[08:04:32.193] TRAIN: iteration 2521 : loss : 0.138127, loss_ce: 0.005953, loss_dice: 0.270302
[08:04:32.402] TRAIN: iteration 2522 : loss : 0.151962, loss_ce: 0.009482, loss_dice: 0.294442
[08:04:32.609] TRAIN: iteration 2523 : loss : 0.160787, loss_ce: 0.007920, loss_dice: 0.313653
[08:04:34.201] TRAIN: iteration 2524 : loss : 0.150308, loss_ce: 0.008235, loss_dice: 0.292382
[08:04:34.408] TRAIN: iteration 2525 : loss : 0.206261, loss_ce: 0.007411, loss_dice: 0.405112
[08:04:34.615] TRAIN: iteration 2526 : loss : 0.178493, loss_ce: 0.006640, loss_dice: 0.350347
[08:04:34.822] TRAIN: iteration 2527 : loss : 0.135551, loss_ce: 0.006383, loss_dice: 0.264719
[08:04:35.029] TRAIN: iteration 2528 : loss : 0.153878, loss_ce: 0.008610, loss_dice: 0.299146
[08:04:35.237] TRAIN: iteration 2529 : loss : 0.252654, loss_ce: 0.005046, loss_dice: 0.500262
[08:04:35.444] TRAIN: iteration 2530 : loss : 0.116303, loss_ce: 0.007775, loss_dice: 0.224831
[08:04:35.651] TRAIN: iteration 2531 : loss : 0.157789, loss_ce: 0.006371, loss_dice: 0.309208
[08:04:36.259] TRAIN: iteration 2532 : loss : 0.173847, loss_ce: 0.006621, loss_dice: 0.341073
[08:04:36.467] TRAIN: iteration 2533 : loss : 0.111212, loss_ce: 0.004474, loss_dice: 0.217949
[08:04:36.674] TRAIN: iteration 2534 : loss : 0.129908, loss_ce: 0.009142, loss_dice: 0.250674
[08:04:36.883] TRAIN: iteration 2535 : loss : 0.182187, loss_ce: 0.006669, loss_dice: 0.357705
[08:04:37.092] TRAIN: iteration 2536 : loss : 0.121755, loss_ce: 0.008935, loss_dice: 0.234574
[08:04:37.299] TRAIN: iteration 2537 : loss : 0.109959, loss_ce: 0.009187, loss_dice: 0.210731
[08:04:37.512] TRAIN: iteration 2538 : loss : 0.248991, loss_ce: 0.011273, loss_dice: 0.486708
[08:04:37.719] TRAIN: iteration 2539 : loss : 0.150721, loss_ce: 0.005318, loss_dice: 0.296124
[08:04:37.927] TRAIN: iteration 2540 : loss : 0.178980, loss_ce: 0.012323, loss_dice: 0.345637
[08:04:38.165] TRAIN: iteration 2541 : loss : 0.196670, loss_ce: 0.016471, loss_dice: 0.376869
[08:04:38.374] TRAIN: iteration 2542 : loss : 0.167876, loss_ce: 0.011662, loss_dice: 0.324090
[08:04:38.580] TRAIN: iteration 2543 : loss : 0.145701, loss_ce: 0.008478, loss_dice: 0.282925
[08:04:39.363] TRAIN: iteration 2544 : loss : 0.109489, loss_ce: 0.006050, loss_dice: 0.212927
[08:04:39.575] TRAIN: iteration 2545 : loss : 0.252443, loss_ce: 0.005587, loss_dice: 0.499299
[08:04:39.781] TRAIN: iteration 2546 : loss : 0.156343, loss_ce: 0.004884, loss_dice: 0.307801
[08:04:40.054] TRAIN: iteration 2547 : loss : 0.195100, loss_ce: 0.012044, loss_dice: 0.378157
[08:04:40.262] TRAIN: iteration 2548 : loss : 0.197806, loss_ce: 0.006369, loss_dice: 0.389243
[08:04:40.470] TRAIN: iteration 2549 : loss : 0.252705, loss_ce: 0.005108, loss_dice: 0.500302
[08:04:40.677] TRAIN: iteration 2550 : loss : 0.217000, loss_ce: 0.005616, loss_dice: 0.428384
[08:04:40.885] TRAIN: iteration 2551 : loss : 0.083536, loss_ce: 0.007183, loss_dice: 0.159889
[08:04:41.093] TRAIN: iteration 2552 : loss : 0.252694, loss_ce: 0.005098, loss_dice: 0.500290
[08:04:41.306] TRAIN: iteration 2553 : loss : 0.188506, loss_ce: 0.006507, loss_dice: 0.370504
[08:04:41.513] TRAIN: iteration 2554 : loss : 0.137333, loss_ce: 0.007154, loss_dice: 0.267511
[08:04:41.722] TRAIN: iteration 2555 : loss : 0.204230, loss_ce: 0.006711, loss_dice: 0.401750
[08:04:41.928] TRAIN: iteration 2556 : loss : 0.215977, loss_ce: 0.005390, loss_dice: 0.426563
[08:04:42.145] TRAIN: iteration 2557 : loss : 0.155004, loss_ce: 0.006636, loss_dice: 0.303373
[08:04:42.352] TRAIN: iteration 2558 : loss : 0.237010, loss_ce: 0.005499, loss_dice: 0.468520
[08:04:42.564] TRAIN: iteration 2559 : loss : 0.154814, loss_ce: 0.016401, loss_dice: 0.293228
[08:04:42.772] TRAIN: iteration 2560 : loss : 0.252895, loss_ce: 0.005426, loss_dice: 0.500364
[08:04:42.773] NaN or Inf found in input tensor.
[08:04:43.001] TRAIN: iteration 2561 : loss : 0.152483, loss_ce: 0.010348, loss_dice: 0.294618
[08:04:43.208] TRAIN: iteration 2562 : loss : 0.152011, loss_ce: 0.008048, loss_dice: 0.295974
[08:04:43.416] TRAIN: iteration 2563 : loss : 0.251287, loss_ce: 0.005193, loss_dice: 0.497381
[08:04:43.623] TRAIN: iteration 2564 : loss : 0.183528, loss_ce: 0.006327, loss_dice: 0.360730
[08:04:43.832] TRAIN: iteration 2565 : loss : 0.122360, loss_ce: 0.006375, loss_dice: 0.238344
[08:04:44.038] TRAIN: iteration 2566 : loss : 0.140964, loss_ce: 0.005842, loss_dice: 0.276085
[08:04:44.244] TRAIN: iteration 2567 : loss : 0.239480, loss_ce: 0.004757, loss_dice: 0.474202
[08:04:44.452] TRAIN: iteration 2568 : loss : 0.240743, loss_ce: 0.016767, loss_dice: 0.464719
[08:04:44.660] TRAIN: iteration 2569 : loss : 0.192633, loss_ce: 0.006830, loss_dice: 0.378436
[08:04:44.868] TRAIN: iteration 2570 : loss : 0.251575, loss_ce: 0.006330, loss_dice: 0.496820
[08:04:45.161] TRAIN: iteration 2571 : loss : 0.251822, loss_ce: 0.004768, loss_dice: 0.498877
[08:04:45.368] TRAIN: iteration 2572 : loss : 0.167465, loss_ce: 0.005707, loss_dice: 0.329222
[08:04:45.582] TRAIN: iteration 2573 : loss : 0.250827, loss_ce: 0.004770, loss_dice: 0.496885
[08:04:45.794] TRAIN: iteration 2574 : loss : 0.130050, loss_ce: 0.005955, loss_dice: 0.254145
[08:04:46.028] TRAIN: iteration 2575 : loss : 0.250996, loss_ce: 0.004782, loss_dice: 0.497210
[08:04:46.237] TRAIN: iteration 2576 : loss : 0.085568, loss_ce: 0.005158, loss_dice: 0.165977
[08:04:46.445] TRAIN: iteration 2577 : loss : 0.107879, loss_ce: 0.009382, loss_dice: 0.206375
[08:04:46.659] TRAIN: iteration 2578 : loss : 0.250693, loss_ce: 0.058722, loss_dice: 0.442663
[08:04:46.873] TRAIN: iteration 2579 : loss : 0.074950, loss_ce: 0.005915, loss_dice: 0.143986
[08:04:47.082] TRAIN: iteration 2580 : loss : 0.206938, loss_ce: 0.008027, loss_dice: 0.405849
[08:04:47.318] TRAIN: iteration 2581 : loss : 0.253185, loss_ce: 0.005996, loss_dice: 0.500373
[08:04:47.525] TRAIN: iteration 2582 : loss : 0.111698, loss_ce: 0.006981, loss_dice: 0.216414
[08:04:47.735] TRAIN: iteration 2583 : loss : 0.253230, loss_ce: 0.006085, loss_dice: 0.500375
[08:04:47.949] TRAIN: iteration 2584 : loss : 0.243723, loss_ce: 0.006640, loss_dice: 0.480807
[08:04:48.158] TRAIN: iteration 2585 : loss : 0.231160, loss_ce: 0.007057, loss_dice: 0.455264
[08:04:48.367] TRAIN: iteration 2586 : loss : 0.174244, loss_ce: 0.008933, loss_dice: 0.339555
[08:04:48.581] TRAIN: iteration 2587 : loss : 0.253717, loss_ce: 0.006999, loss_dice: 0.500434
[08:04:48.787] TRAIN: iteration 2588 : loss : 0.191381, loss_ce: 0.007761, loss_dice: 0.375001
[08:04:48.994] TRAIN: iteration 2589 : loss : 0.235447, loss_ce: 0.008495, loss_dice: 0.462399
[08:04:49.202] TRAIN: iteration 2590 : loss : 0.251393, loss_ce: 0.007243, loss_dice: 0.495543
[08:04:50.081] TRAIN: iteration 2591 : loss : 0.252537, loss_ce: 0.007917, loss_dice: 0.497157
[08:04:50.288] TRAIN: iteration 2592 : loss : 0.120733, loss_ce: 0.011431, loss_dice: 0.230034
[08:04:50.497] TRAIN: iteration 2593 : loss : 0.113804, loss_ce: 0.007147, loss_dice: 0.220460
[08:04:50.708] TRAIN: iteration 2594 : loss : 0.186515, loss_ce: 0.006723, loss_dice: 0.366306
[08:04:50.916] TRAIN: iteration 2595 : loss : 0.252978, loss_ce: 0.006464, loss_dice: 0.499491
[08:04:51.125] TRAIN: iteration 2596 : loss : 0.253615, loss_ce: 0.006808, loss_dice: 0.500422
[08:04:51.335] TRAIN: iteration 2597 : loss : 0.223752, loss_ce: 0.007162, loss_dice: 0.440343
[08:04:51.541] TRAIN: iteration 2598 : loss : 0.246972, loss_ce: 0.006497, loss_dice: 0.487448
[08:04:52.776] TRAIN: iteration 2599 : loss : 0.129352, loss_ce: 0.008255, loss_dice: 0.250448
[08:04:52.984] TRAIN: iteration 2600 : loss : 0.149268, loss_ce: 0.005586, loss_dice: 0.292951
[08:04:53.220] TRAIN: iteration 2601 : loss : 0.113465, loss_ce: 0.007970, loss_dice: 0.218960
[08:04:53.431] TRAIN: iteration 2602 : loss : 0.253029, loss_ce: 0.005707, loss_dice: 0.500351
[08:04:53.661] TRAIN: iteration 2603 : loss : 0.190564, loss_ce: 0.007621, loss_dice: 0.373507
[08:04:53.868] TRAIN: iteration 2604 : loss : 0.226123, loss_ce: 0.005894, loss_dice: 0.446352
[08:04:54.077] TRAIN: iteration 2605 : loss : 0.146241, loss_ce: 0.010855, loss_dice: 0.281628
[08:04:54.284] TRAIN: iteration 2606 : loss : 0.182449, loss_ce: 0.005694, loss_dice: 0.359205
[08:04:54.491] TRAIN: iteration 2607 : loss : 0.208811, loss_ce: 0.006399, loss_dice: 0.411222
[08:04:54.886] TRAIN: iteration 2608 : loss : 0.194491, loss_ce: 0.013351, loss_dice: 0.375630
[08:04:55.094] TRAIN: iteration 2609 : loss : 0.206991, loss_ce: 0.005737, loss_dice: 0.408244
[08:04:55.302] TRAIN: iteration 2610 : loss : 0.175995, loss_ce: 0.006455, loss_dice: 0.345535
[08:04:55.510] TRAIN: iteration 2611 : loss : 0.105967, loss_ce: 0.007303, loss_dice: 0.204631
[08:04:55.720] TRAIN: iteration 2612 : loss : 0.254333, loss_ce: 0.009066, loss_dice: 0.499600
[08:04:55.929] TRAIN: iteration 2613 : loss : 0.214504, loss_ce: 0.005096, loss_dice: 0.423913
[08:04:56.136] TRAIN: iteration 2614 : loss : 0.130820, loss_ce: 0.005058, loss_dice: 0.256582
[08:04:56.348] TRAIN: iteration 2615 : loss : 0.155877, loss_ce: 0.005462, loss_dice: 0.306293
[08:04:58.315] TRAIN: iteration 2616 : loss : 0.172396, loss_ce: 0.005956, loss_dice: 0.338836
[08:04:59.770] TRAIN: iteration 2617 : loss : 0.126892, loss_ce: 0.008411, loss_dice: 0.245373
[08:04:59.979] TRAIN: iteration 2618 : loss : 0.140863, loss_ce: 0.010326, loss_dice: 0.271400
[08:05:00.192] TRAIN: iteration 2619 : loss : 0.251908, loss_ce: 0.003624, loss_dice: 0.500192
[08:05:00.401] TRAIN: iteration 2620 : loss : 0.229889, loss_ce: 0.004159, loss_dice: 0.455620
[08:05:00.639] TRAIN: iteration 2621 : loss : 0.141999, loss_ce: 0.003385, loss_dice: 0.280612
[08:05:00.848] TRAIN: iteration 2622 : loss : 0.118348, loss_ce: 0.006769, loss_dice: 0.229927
[08:05:01.058] TRAIN: iteration 2623 : loss : 0.206173, loss_ce: 0.009796, loss_dice: 0.402550
[08:05:01.269] TRAIN: iteration 2624 : loss : 0.173627, loss_ce: 0.008558, loss_dice: 0.338697
[08:05:01.480] TRAIN: iteration 2625 : loss : 0.124090, loss_ce: 0.003724, loss_dice: 0.244456
[08:05:01.688] TRAIN: iteration 2626 : loss : 0.195006, loss_ce: 0.007904, loss_dice: 0.382107
[08:05:01.897] TRAIN: iteration 2627 : loss : 0.116793, loss_ce: 0.003741, loss_dice: 0.229846
[08:05:02.104] TRAIN: iteration 2628 : loss : 0.133991, loss_ce: 0.004709, loss_dice: 0.263273
[08:05:02.312] TRAIN: iteration 2629 : loss : 0.251760, loss_ce: 0.003358, loss_dice: 0.500163
[08:05:02.526] TRAIN: iteration 2630 : loss : 0.251399, loss_ce: 0.004057, loss_dice: 0.498741
[08:05:02.735] TRAIN: iteration 2631 : loss : 0.217395, loss_ce: 0.003670, loss_dice: 0.431121
[08:05:02.943] TRAIN: iteration 2632 : loss : 0.251235, loss_ce: 0.003958, loss_dice: 0.498512
[08:05:03.153] TRAIN: iteration 2633 : loss : 0.077144, loss_ce: 0.004002, loss_dice: 0.150285
[08:05:03.359] TRAIN: iteration 2634 : loss : 0.243645, loss_ce: 0.007884, loss_dice: 0.479406
[08:05:03.567] TRAIN: iteration 2635 : loss : 0.128807, loss_ce: 0.006720, loss_dice: 0.250894
[08:05:03.781] TRAIN: iteration 2636 : loss : 0.122920, loss_ce: 0.010001, loss_dice: 0.235838
[08:05:03.988] TRAIN: iteration 2637 : loss : 0.250101, loss_ce: 0.007817, loss_dice: 0.492386
[08:05:04.195] TRAIN: iteration 2638 : loss : 0.109470, loss_ce: 0.003564, loss_dice: 0.215376
[08:05:04.401] TRAIN: iteration 2639 : loss : 0.247240, loss_ce: 0.010505, loss_dice: 0.483975
[08:05:04.608] TRAIN: iteration 2640 : loss : 0.169999, loss_ce: 0.005604, loss_dice: 0.334394
[08:05:04.850] TRAIN: iteration 2641 : loss : 0.106479, loss_ce: 0.010071, loss_dice: 0.202886
[08:05:05.056] TRAIN: iteration 2642 : loss : 0.124484, loss_ce: 0.008631, loss_dice: 0.240338
[08:05:05.268] TRAIN: iteration 2643 : loss : 0.120426, loss_ce: 0.008972, loss_dice: 0.231880
[08:05:06.003] TRAIN: iteration 2644 : loss : 0.220645, loss_ce: 0.005849, loss_dice: 0.435441
[08:05:06.211] TRAIN: iteration 2645 : loss : 0.198946, loss_ce: 0.015271, loss_dice: 0.382620
[08:05:06.418] TRAIN: iteration 2646 : loss : 0.191115, loss_ce: 0.006340, loss_dice: 0.375890
[08:05:06.625] TRAIN: iteration 2647 : loss : 0.133264, loss_ce: 0.008207, loss_dice: 0.258322
[08:05:06.833] TRAIN: iteration 2648 : loss : 0.042626, loss_ce: 0.003111, loss_dice: 0.082141
[08:05:07.040] TRAIN: iteration 2649 : loss : 0.225329, loss_ce: 0.008167, loss_dice: 0.442491
[08:05:07.249] TRAIN: iteration 2650 : loss : 0.259576, loss_ce: 0.022322, loss_dice: 0.496831
[08:05:07.455] TRAIN: iteration 2651 : loss : 0.119654, loss_ce: 0.007639, loss_dice: 0.231670
[08:05:08.435] TRAIN: iteration 2652 : loss : 0.152473, loss_ce: 0.007052, loss_dice: 0.297894
[08:05:08.642] TRAIN: iteration 2653 : loss : 0.104760, loss_ce: 0.005864, loss_dice: 0.203657
[08:05:08.848] TRAIN: iteration 2654 : loss : 0.231628, loss_ce: 0.006763, loss_dice: 0.456493
[08:05:09.054] TRAIN: iteration 2655 : loss : 0.086841, loss_ce: 0.005527, loss_dice: 0.168155
[08:05:09.263] TRAIN: iteration 2656 : loss : 0.188170, loss_ce: 0.007328, loss_dice: 0.369013
[08:05:09.475] TRAIN: iteration 2657 : loss : 0.094832, loss_ce: 0.007376, loss_dice: 0.182288
[08:05:09.686] TRAIN: iteration 2658 : loss : 0.252889, loss_ce: 0.005435, loss_dice: 0.500344
[08:05:09.892] TRAIN: iteration 2659 : loss : 0.127193, loss_ce: 0.006044, loss_dice: 0.248342
[08:05:11.658] TRAIN: iteration 2660 : loss : 0.222615, loss_ce: 0.004677, loss_dice: 0.440553
[08:05:11.895] TRAIN: iteration 2661 : loss : 0.152600, loss_ce: 0.008980, loss_dice: 0.296219
[08:05:12.107] TRAIN: iteration 2662 : loss : 0.128796, loss_ce: 0.004350, loss_dice: 0.253241
[08:05:12.314] TRAIN: iteration 2663 : loss : 0.239659, loss_ce: 0.004273, loss_dice: 0.475044
[08:05:12.527] TRAIN: iteration 2664 : loss : 0.230812, loss_ce: 0.004565, loss_dice: 0.457059
[08:05:12.737] TRAIN: iteration 2665 : loss : 0.158609, loss_ce: 0.011003, loss_dice: 0.306215
[08:05:12.948] TRAIN: iteration 2666 : loss : 0.225039, loss_ce: 0.006100, loss_dice: 0.443978
[08:05:13.155] TRAIN: iteration 2667 : loss : 0.124506, loss_ce: 0.004542, loss_dice: 0.244470
[08:05:13.790] TRAIN: iteration 2668 : loss : 0.232133, loss_ce: 0.003447, loss_dice: 0.460819
[08:05:13.998] TRAIN: iteration 2669 : loss : 0.150042, loss_ce: 0.011930, loss_dice: 0.288154
[08:05:14.206] TRAIN: iteration 2670 : loss : 0.158317, loss_ce: 0.006717, loss_dice: 0.309917
[08:05:14.413] TRAIN: iteration 2671 : loss : 0.242817, loss_ce: 0.004162, loss_dice: 0.481471
[08:05:14.621] TRAIN: iteration 2672 : loss : 0.199973, loss_ce: 0.005222, loss_dice: 0.394725
[08:05:14.829] TRAIN: iteration 2673 : loss : 0.220782, loss_ce: 0.004642, loss_dice: 0.436922
[08:05:15.035] TRAIN: iteration 2674 : loss : 0.147664, loss_ce: 0.004653, loss_dice: 0.290675
[08:05:15.611] TRAIN: iteration 2675 : loss : 0.207509, loss_ce: 0.008024, loss_dice: 0.406994
[08:05:16.608] TRAIN: iteration 2676 : loss : 0.173875, loss_ce: 0.008801, loss_dice: 0.338950
[08:05:16.815] TRAIN: iteration 2677 : loss : 0.116070, loss_ce: 0.007044, loss_dice: 0.225097
[08:05:17.025] TRAIN: iteration 2678 : loss : 0.123992, loss_ce: 0.008189, loss_dice: 0.239795
[08:05:17.235] TRAIN: iteration 2679 : loss : 0.244182, loss_ce: 0.006205, loss_dice: 0.482158
[08:05:17.443] TRAIN: iteration 2680 : loss : 0.241441, loss_ce: 0.006018, loss_dice: 0.476864
[08:05:17.679] TRAIN: iteration 2681 : loss : 0.253355, loss_ce: 0.006281, loss_dice: 0.500430
[08:05:17.887] TRAIN: iteration 2682 : loss : 0.200717, loss_ce: 0.008178, loss_dice: 0.393255
[08:05:18.104] TRAIN: iteration 2683 : loss : 0.215286, loss_ce: 0.008732, loss_dice: 0.421840
[08:05:18.313] TRAIN: iteration 2684 : loss : 0.085268, loss_ce: 0.008514, loss_dice: 0.162021
[08:05:18.527] TRAIN: iteration 2685 : loss : 0.156338, loss_ce: 0.007083, loss_dice: 0.305592
[08:05:18.733] TRAIN: iteration 2686 : loss : 0.208972, loss_ce: 0.005727, loss_dice: 0.412217
[08:05:18.939] TRAIN: iteration 2687 : loss : 0.178715, loss_ce: 0.023842, loss_dice: 0.333588
[08:05:19.146] TRAIN: iteration 2688 : loss : 0.078098, loss_ce: 0.004589, loss_dice: 0.151607
[08:05:19.354] TRAIN: iteration 2689 : loss : 0.160603, loss_ce: 0.008148, loss_dice: 0.313059
[08:05:19.575] TRAIN: iteration 2690 : loss : 0.124077, loss_ce: 0.006972, loss_dice: 0.241182
[08:05:19.782] TRAIN: iteration 2691 : loss : 0.177667, loss_ce: 0.024441, loss_dice: 0.330892
[08:05:19.988] TRAIN: iteration 2692 : loss : 0.093185, loss_ce: 0.006587, loss_dice: 0.179783
[08:05:20.196] TRAIN: iteration 2693 : loss : 0.136447, loss_ce: 0.005508, loss_dice: 0.267386
[08:05:20.403] TRAIN: iteration 2694 : loss : 0.254259, loss_ce: 0.008594, loss_dice: 0.499925
[08:05:20.611] TRAIN: iteration 2695 : loss : 0.246211, loss_ce: 0.006854, loss_dice: 0.485569
[08:05:20.821] TRAIN: iteration 2696 : loss : 0.099385, loss_ce: 0.010907, loss_dice: 0.187862
[08:05:21.033] TRAIN: iteration 2697 : loss : 0.108786, loss_ce: 0.007920, loss_dice: 0.209652
[08:05:21.240] TRAIN: iteration 2698 : loss : 0.131069, loss_ce: 0.006205, loss_dice: 0.255933
[08:05:22.241] TRAIN: iteration 2699 : loss : 0.063681, loss_ce: 0.004580, loss_dice: 0.122783
[08:05:22.448] TRAIN: iteration 2700 : loss : 0.171992, loss_ce: 0.006444, loss_dice: 0.337540
[08:05:22.689] TRAIN: iteration 2701 : loss : 0.152993, loss_ce: 0.006630, loss_dice: 0.299357
[08:05:22.896] TRAIN: iteration 2702 : loss : 0.120281, loss_ce: 0.005871, loss_dice: 0.234691
[08:05:23.104] TRAIN: iteration 2703 : loss : 0.168301, loss_ce: 0.014091, loss_dice: 0.322512
[08:05:23.313] TRAIN: iteration 2704 : loss : 0.201784, loss_ce: 0.005993, loss_dice: 0.397576
[08:05:23.523] TRAIN: iteration 2705 : loss : 0.237587, loss_ce: 0.004852, loss_dice: 0.470321
[08:05:23.731] TRAIN: iteration 2706 : loss : 0.149407, loss_ce: 0.006405, loss_dice: 0.292409
[08:05:24.998] TRAIN: iteration 2707 : loss : 0.198689, loss_ce: 0.020930, loss_dice: 0.376448
[08:05:25.205] TRAIN: iteration 2708 : loss : 0.202321, loss_ce: 0.029211, loss_dice: 0.375431
[08:05:25.412] TRAIN: iteration 2709 : loss : 0.119345, loss_ce: 0.004178, loss_dice: 0.234512
[08:05:25.622] TRAIN: iteration 2710 : loss : 0.226450, loss_ce: 0.007732, loss_dice: 0.445168
[08:05:25.830] TRAIN: iteration 2711 : loss : 0.178274, loss_ce: 0.005275, loss_dice: 0.351272
[08:05:26.036] TRAIN: iteration 2712 : loss : 0.246597, loss_ce: 0.005267, loss_dice: 0.487928
[08:05:26.244] TRAIN: iteration 2713 : loss : 0.219676, loss_ce: 0.003951, loss_dice: 0.435402
[08:05:26.454] TRAIN: iteration 2714 : loss : 0.182653, loss_ce: 0.020589, loss_dice: 0.344716
[08:05:28.273] TRAIN: iteration 2715 : loss : 0.078392, loss_ce: 0.004677, loss_dice: 0.152107
[08:05:28.479] TRAIN: iteration 2716 : loss : 0.168804, loss_ce: 0.008878, loss_dice: 0.328729
[08:05:28.692] TRAIN: iteration 2717 : loss : 0.252041, loss_ce: 0.003941, loss_dice: 0.500140
[08:05:28.899] TRAIN: iteration 2718 : loss : 0.079598, loss_ce: 0.004319, loss_dice: 0.154877
[08:05:29.105] TRAIN: iteration 2719 : loss : 0.239218, loss_ce: 0.007341, loss_dice: 0.471094
[08:05:29.312] TRAIN: iteration 2720 : loss : 0.165260, loss_ce: 0.006563, loss_dice: 0.323958
[08:05:29.550] TRAIN: iteration 2721 : loss : 0.221384, loss_ce: 0.005660, loss_dice: 0.437108
[08:05:29.762] TRAIN: iteration 2722 : loss : 0.236443, loss_ce: 0.024404, loss_dice: 0.448482
[08:05:30.535] TRAIN: iteration 2723 : loss : 0.142989, loss_ce: 0.005709, loss_dice: 0.280270
[08:05:30.744] TRAIN: iteration 2724 : loss : 0.229944, loss_ce: 0.007852, loss_dice: 0.452035
[08:05:31.018] TRAIN: iteration 2725 : loss : 0.252160, loss_ce: 0.006022, loss_dice: 0.498298
[08:05:31.227] TRAIN: iteration 2726 : loss : 0.168821, loss_ce: 0.006982, loss_dice: 0.330661
[08:05:31.439] TRAIN: iteration 2727 : loss : 0.110800, loss_ce: 0.009819, loss_dice: 0.211780
[08:05:31.646] TRAIN: iteration 2728 : loss : 0.253053, loss_ce: 0.006141, loss_dice: 0.499964
[08:05:31.855] TRAIN: iteration 2729 : loss : 0.096536, loss_ce: 0.008278, loss_dice: 0.184794
[08:05:32.064] TRAIN: iteration 2730 : loss : 0.194530, loss_ce: 0.015037, loss_dice: 0.374024
[08:05:32.745] TRAIN: iteration 2731 : loss : 0.179501, loss_ce: 0.010340, loss_dice: 0.348662
[08:05:32.953] TRAIN: iteration 2732 : loss : 0.112274, loss_ce: 0.011517, loss_dice: 0.213030
[08:05:33.160] TRAIN: iteration 2733 : loss : 0.090048, loss_ce: 0.007115, loss_dice: 0.172981
[08:05:33.368] TRAIN: iteration 2734 : loss : 0.124699, loss_ce: 0.008289, loss_dice: 0.241110
[08:05:33.575] TRAIN: iteration 2735 : loss : 0.120384, loss_ce: 0.007551, loss_dice: 0.233216
[08:05:33.783] TRAIN: iteration 2736 : loss : 0.133212, loss_ce: 0.007656, loss_dice: 0.258769
[08:05:33.992] TRAIN: iteration 2737 : loss : 0.179598, loss_ce: 0.007935, loss_dice: 0.351261
[08:05:34.199] TRAIN: iteration 2738 : loss : 0.203788, loss_ce: 0.017741, loss_dice: 0.389835
[08:05:35.205] TRAIN: iteration 2739 : loss : 0.151474, loss_ce: 0.007524, loss_dice: 0.295425
[08:05:35.764] TRAIN: iteration 2740 : loss : 0.227087, loss_ce: 0.014730, loss_dice: 0.439444
[08:05:36.002] TRAIN: iteration 2741 : loss : 0.137580, loss_ce: 0.013108, loss_dice: 0.262052
[08:05:36.209] TRAIN: iteration 2742 : loss : 0.248246, loss_ce: 0.007086, loss_dice: 0.489406
[08:05:36.418] TRAIN: iteration 2743 : loss : 0.142071, loss_ce: 0.005205, loss_dice: 0.278936
[08:05:36.718] TRAIN: iteration 2744 : loss : 0.250533, loss_ce: 0.005968, loss_dice: 0.495098
[08:05:36.925] TRAIN: iteration 2745 : loss : 0.152168, loss_ce: 0.007839, loss_dice: 0.296497
[08:05:37.134] TRAIN: iteration 2746 : loss : 0.166980, loss_ce: 0.005235, loss_dice: 0.328725
[08:05:37.341] TRAIN: iteration 2747 : loss : 0.209743, loss_ce: 0.021411, loss_dice: 0.398074
[08:05:37.802] TRAIN: iteration 2748 : loss : 0.206081, loss_ce: 0.007507, loss_dice: 0.404655
[08:05:38.012] TRAIN: iteration 2749 : loss : 0.153200, loss_ce: 0.007381, loss_dice: 0.299019
[08:05:38.218] TRAIN: iteration 2750 : loss : 0.117555, loss_ce: 0.005646, loss_dice: 0.229465
[08:05:38.431] TRAIN: iteration 2751 : loss : 0.212969, loss_ce: 0.014689, loss_dice: 0.411248
[08:05:38.644] TRAIN: iteration 2752 : loss : 0.220981, loss_ce: 0.005023, loss_dice: 0.436939
[08:05:38.857] TRAIN: iteration 2753 : loss : 0.187935, loss_ce: 0.008242, loss_dice: 0.367627
[08:05:39.071] TRAIN: iteration 2754 : loss : 0.167122, loss_ce: 0.009614, loss_dice: 0.324630
[08:05:39.277] TRAIN: iteration 2755 : loss : 0.169181, loss_ce: 0.004822, loss_dice: 0.333539
[08:05:41.295] TRAIN: iteration 2756 : loss : 0.183541, loss_ce: 0.006840, loss_dice: 0.360242
[08:05:41.505] TRAIN: iteration 2757 : loss : 0.174645, loss_ce: 0.009282, loss_dice: 0.340009
[08:05:41.711] TRAIN: iteration 2758 : loss : 0.179933, loss_ce: 0.005073, loss_dice: 0.354794
[08:05:41.918] TRAIN: iteration 2759 : loss : 0.149928, loss_ce: 0.004105, loss_dice: 0.295751
[08:05:42.126] TRAIN: iteration 2760 : loss : 0.185081, loss_ce: 0.039497, loss_dice: 0.330665
[08:05:42.362] TRAIN: iteration 2761 : loss : 0.176737, loss_ce: 0.006235, loss_dice: 0.347238
[08:05:42.568] TRAIN: iteration 2762 : loss : 0.191260, loss_ce: 0.004823, loss_dice: 0.377698
[08:05:42.775] TRAIN: iteration 2763 : loss : 0.223724, loss_ce: 0.004701, loss_dice: 0.442747
[08:05:42.983] TRAIN: iteration 2764 : loss : 0.251727, loss_ce: 0.004927, loss_dice: 0.498527
[08:05:43.190] TRAIN: iteration 2765 : loss : 0.164334, loss_ce: 0.004921, loss_dice: 0.323746
[08:05:43.398] TRAIN: iteration 2766 : loss : 0.149217, loss_ce: 0.004408, loss_dice: 0.294027
[08:05:43.605] TRAIN: iteration 2767 : loss : 0.252012, loss_ce: 0.003824, loss_dice: 0.500199
[08:05:43.812] TRAIN: iteration 2768 : loss : 0.251857, loss_ce: 0.003585, loss_dice: 0.500130
[08:05:44.024] TRAIN: iteration 2769 : loss : 0.191467, loss_ce: 0.004019, loss_dice: 0.378916
[08:05:44.230] TRAIN: iteration 2770 : loss : 0.132802, loss_ce: 0.003707, loss_dice: 0.261897
[08:05:44.441] TRAIN: iteration 2771 : loss : 0.250870, loss_ce: 0.003653, loss_dice: 0.498086
[08:05:44.657] TRAIN: iteration 2772 : loss : 0.148467, loss_ce: 0.005212, loss_dice: 0.291721
[08:05:44.864] TRAIN: iteration 2773 : loss : 0.209224, loss_ce: 0.003817, loss_dice: 0.414630
[08:05:45.070] TRAIN: iteration 2774 : loss : 0.135845, loss_ce: 0.009923, loss_dice: 0.261767
[08:05:45.278] TRAIN: iteration 2775 : loss : 0.228759, loss_ce: 0.015886, loss_dice: 0.441632
[08:05:45.491] TRAIN: iteration 2776 : loss : 0.198487, loss_ce: 0.003280, loss_dice: 0.393694
[08:05:46.023] TRAIN: iteration 2777 : loss : 0.251656, loss_ce: 0.003159, loss_dice: 0.500154
[08:05:47.207] TRAIN: iteration 2778 : loss : 0.148254, loss_ce: 0.004259, loss_dice: 0.292249
[08:05:47.421] TRAIN: iteration 2779 : loss : 0.128753, loss_ce: 0.007743, loss_dice: 0.249763
[08:05:47.627] TRAIN: iteration 2780 : loss : 0.115163, loss_ce: 0.005792, loss_dice: 0.224534
[08:05:47.859] TRAIN: iteration 2781 : loss : 0.199734, loss_ce: 0.019158, loss_dice: 0.380311
[08:05:48.070] TRAIN: iteration 2782 : loss : 0.086827, loss_ce: 0.003603, loss_dice: 0.170052
[08:05:48.277] TRAIN: iteration 2783 : loss : 0.191648, loss_ce: 0.008647, loss_dice: 0.374649
[08:05:48.487] TRAIN: iteration 2784 : loss : 0.190269, loss_ce: 0.009723, loss_dice: 0.370814
[08:05:48.694] TRAIN: iteration 2785 : loss : 0.252450, loss_ce: 0.004601, loss_dice: 0.500299
[08:05:50.375] TRAIN: iteration 2786 : loss : 0.109653, loss_ce: 0.006429, loss_dice: 0.212877
[08:05:50.588] TRAIN: iteration 2787 : loss : 0.131085, loss_ce: 0.005516, loss_dice: 0.256654
[08:05:50.794] TRAIN: iteration 2788 : loss : 0.228995, loss_ce: 0.007235, loss_dice: 0.450754
[08:05:51.001] TRAIN: iteration 2789 : loss : 0.056764, loss_ce: 0.004114, loss_dice: 0.109413
[08:05:51.216] TRAIN: iteration 2790 : loss : 0.115134, loss_ce: 0.008284, loss_dice: 0.221983
[08:05:51.429] TRAIN: iteration 2791 : loss : 0.248745, loss_ce: 0.006974, loss_dice: 0.490516
[08:05:51.638] TRAIN: iteration 2792 : loss : 0.165310, loss_ce: 0.009452, loss_dice: 0.321169
[08:05:51.845] TRAIN: iteration 2793 : loss : 0.103724, loss_ce: 0.008361, loss_dice: 0.199087
[08:05:52.553] TRAIN: iteration 2794 : loss : 0.132773, loss_ce: 0.008722, loss_dice: 0.256825
[08:05:52.761] TRAIN: iteration 2795 : loss : 0.161242, loss_ce: 0.007011, loss_dice: 0.315473
[08:05:52.968] TRAIN: iteration 2796 : loss : 0.247564, loss_ce: 0.008137, loss_dice: 0.486990
[08:05:53.174] TRAIN: iteration 2797 : loss : 0.169708, loss_ce: 0.013774, loss_dice: 0.325642
[08:05:53.386] TRAIN: iteration 2798 : loss : 0.164259, loss_ce: 0.007307, loss_dice: 0.321212
[08:05:53.596] TRAIN: iteration 2799 : loss : 0.091628, loss_ce: 0.005072, loss_dice: 0.178183
[08:05:53.805] TRAIN: iteration 2800 : loss : 0.253010, loss_ce: 0.005643, loss_dice: 0.500376
[08:05:54.040] TRAIN: iteration 2801 : loss : 0.081422, loss_ce: 0.004343, loss_dice: 0.158501
[08:05:54.872] TRAIN: iteration 2802 : loss : 0.236524, loss_ce: 0.006193, loss_dice: 0.466855
[08:05:55.079] TRAIN: iteration 2803 : loss : 0.156706, loss_ce: 0.007781, loss_dice: 0.305630
[08:05:55.287] TRAIN: iteration 2804 : loss : 0.252624, loss_ce: 0.004932, loss_dice: 0.500316
[08:05:55.495] TRAIN: iteration 2805 : loss : 0.171934, loss_ce: 0.004997, loss_dice: 0.338871
[08:05:55.703] TRAIN: iteration 2806 : loss : 0.252252, loss_ce: 0.004242, loss_dice: 0.500262
[08:05:55.910] TRAIN: iteration 2807 : loss : 0.055767, loss_ce: 0.003117, loss_dice: 0.108417
[08:05:56.118] TRAIN: iteration 2808 : loss : 0.197705, loss_ce: 0.030918, loss_dice: 0.364492
[08:05:56.325] TRAIN: iteration 2809 : loss : 0.180117, loss_ce: 0.004840, loss_dice: 0.355394
[08:05:56.983] TRAIN: iteration 2810 : loss : 0.174160, loss_ce: 0.003936, loss_dice: 0.344384
[08:05:57.190] TRAIN: iteration 2811 : loss : 0.167039, loss_ce: 0.014494, loss_dice: 0.319584
[08:05:57.404] TRAIN: iteration 2812 : loss : 0.246330, loss_ce: 0.003804, loss_dice: 0.488856
[08:05:57.617] TRAIN: iteration 2813 : loss : 0.183256, loss_ce: 0.004197, loss_dice: 0.362316
[08:05:57.823] TRAIN: iteration 2814 : loss : 0.216942, loss_ce: 0.005118, loss_dice: 0.428766
[08:05:58.032] TRAIN: iteration 2815 : loss : 0.251585, loss_ce: 0.003010, loss_dice: 0.500160
[08:05:58.241] TRAIN: iteration 2816 : loss : 0.140527, loss_ce: 0.008961, loss_dice: 0.272093
[08:05:58.448] TRAIN: iteration 2817 : loss : 0.166931, loss_ce: 0.007667, loss_dice: 0.326195
[08:06:00.241] TRAIN: iteration 2818 : loss : 0.126428, loss_ce: 0.004736, loss_dice: 0.248120
[08:06:00.447] TRAIN: iteration 2819 : loss : 0.251548, loss_ce: 0.002964, loss_dice: 0.500132
[08:06:00.656] TRAIN: iteration 2820 : loss : 0.182771, loss_ce: 0.003710, loss_dice: 0.361833
[08:06:00.876] TRAIN: iteration 2821 : loss : 0.142432, loss_ce: 0.006557, loss_dice: 0.278306
[08:06:01.086] TRAIN: iteration 2822 : loss : 0.130472, loss_ce: 0.003303, loss_dice: 0.257642
[08:06:01.293] TRAIN: iteration 2823 : loss : 0.154777, loss_ce: 0.004853, loss_dice: 0.304702
[08:06:01.502] TRAIN: iteration 2824 : loss : 0.205772, loss_ce: 0.005318, loss_dice: 0.406227
[08:06:01.713] TRAIN: iteration 2825 : loss : 0.171798, loss_ce: 0.007597, loss_dice: 0.335999
[08:06:02.014] TRAIN: iteration 2826 : loss : 0.202096, loss_ce: 0.007614, loss_dice: 0.396577
[08:06:02.224] TRAIN: iteration 2827 : loss : 0.137807, loss_ce: 0.003959, loss_dice: 0.271655
[08:06:02.432] TRAIN: iteration 2828 : loss : 0.251500, loss_ce: 0.002853, loss_dice: 0.500146
[08:06:02.674] TRAIN: iteration 2829 : loss : 0.103701, loss_ce: 0.004733, loss_dice: 0.202668
[08:06:02.884] TRAIN: iteration 2830 : loss : 0.168164, loss_ce: 0.003677, loss_dice: 0.332651
[08:06:05.195] TRAIN: iteration 2831 : loss : 0.165854, loss_ce: 0.006657, loss_dice: 0.325052
[08:06:05.407] TRAIN: iteration 2832 : loss : 0.134545, loss_ce: 0.009669, loss_dice: 0.259421
[08:06:05.613] TRAIN: iteration 2833 : loss : 0.231174, loss_ce: 0.003261, loss_dice: 0.459087
[08:06:05.819] TRAIN: iteration 2834 : loss : 0.251635, loss_ce: 0.003073, loss_dice: 0.500197
[08:06:06.030] TRAIN: iteration 2835 : loss : 0.246441, loss_ce: 0.004595, loss_dice: 0.488287
[08:06:06.239] TRAIN: iteration 2836 : loss : 0.257833, loss_ce: 0.018382, loss_dice: 0.497285
[08:06:06.447] TRAIN: iteration 2837 : loss : 0.176252, loss_ce: 0.008012, loss_dice: 0.344492
[08:06:06.655] TRAIN: iteration 2838 : loss : 0.233420, loss_ce: 0.007676, loss_dice: 0.459165
[08:06:08.539] TRAIN: iteration 2839 : loss : 0.250646, loss_ce: 0.005281, loss_dice: 0.496011
[08:06:08.746] TRAIN: iteration 2840 : loss : 0.251935, loss_ce: 0.004410, loss_dice: 0.499460
[08:06:08.983] TRAIN: iteration 2841 : loss : 0.189183, loss_ce: 0.005377, loss_dice: 0.372990
[08:06:09.190] TRAIN: iteration 2842 : loss : 0.109791, loss_ce: 0.006205, loss_dice: 0.213376
[08:06:09.396] TRAIN: iteration 2843 : loss : 0.246601, loss_ce: 0.019631, loss_dice: 0.473570
[08:06:09.603] TRAIN: iteration 2844 : loss : 0.125968, loss_ce: 0.004918, loss_dice: 0.247017
[08:06:09.811] TRAIN: iteration 2845 : loss : 0.172533, loss_ce: 0.004780, loss_dice: 0.340287
[08:06:10.018] TRAIN: iteration 2846 : loss : 0.121312, loss_ce: 0.004354, loss_dice: 0.238269
[08:06:11.532] TRAIN: iteration 2847 : loss : 0.098692, loss_ce: 0.006841, loss_dice: 0.190544
[08:06:11.741] TRAIN: iteration 2848 : loss : 0.176922, loss_ce: 0.005696, loss_dice: 0.348148
[08:06:11.949] TRAIN: iteration 2849 : loss : 0.142024, loss_ce: 0.003667, loss_dice: 0.280381
[08:06:12.186] TRAIN: iteration 2850 : loss : 0.246438, loss_ce: 0.004652, loss_dice: 0.488223
[08:06:12.394] TRAIN: iteration 2851 : loss : 0.217557, loss_ce: 0.004418, loss_dice: 0.430696
[08:06:12.604] TRAIN: iteration 2852 : loss : 0.251657, loss_ce: 0.003205, loss_dice: 0.500109
[08:06:12.814] TRAIN: iteration 2853 : loss : 0.216877, loss_ce: 0.005233, loss_dice: 0.428520
[08:06:13.022] TRAIN: iteration 2854 : loss : 0.116808, loss_ce: 0.004155, loss_dice: 0.229460
[08:06:13.233] TRAIN: iteration 2855 : loss : 0.199092, loss_ce: 0.011953, loss_dice: 0.386231
[08:06:13.478] TRAIN: iteration 2856 : loss : 0.240144, loss_ce: 0.009515, loss_dice: 0.470773
[08:06:13.687] TRAIN: iteration 2857 : loss : 0.194787, loss_ce: 0.007122, loss_dice: 0.382451
[08:06:13.894] TRAIN: iteration 2858 : loss : 0.145403, loss_ce: 0.004829, loss_dice: 0.285977
[08:06:14.102] TRAIN: iteration 2859 : loss : 0.214061, loss_ce: 0.010195, loss_dice: 0.417927
[08:06:14.309] TRAIN: iteration 2860 : loss : 0.122707, loss_ce: 0.005077, loss_dice: 0.240337
[08:06:14.545] TRAIN: iteration 2861 : loss : 0.252053, loss_ce: 0.004068, loss_dice: 0.500038
[08:06:15.093] TRAIN: iteration 2862 : loss : 0.129519, loss_ce: 0.004189, loss_dice: 0.254850
[08:06:15.887] TRAIN: iteration 2863 : loss : 0.232293, loss_ce: 0.008276, loss_dice: 0.456311
[08:06:16.094] TRAIN: iteration 2864 : loss : 0.242413, loss_ce: 0.013463, loss_dice: 0.471362
[08:06:16.305] TRAIN: iteration 2865 : loss : 0.250972, loss_ce: 0.004805, loss_dice: 0.497138
[08:06:16.512] TRAIN: iteration 2866 : loss : 0.218053, loss_ce: 0.005492, loss_dice: 0.430614
[08:06:16.719] TRAIN: iteration 2867 : loss : 0.189066, loss_ce: 0.006759, loss_dice: 0.371374
[08:06:16.926] TRAIN: iteration 2868 : loss : 0.244947, loss_ce: 0.005563, loss_dice: 0.484331
[08:06:17.135] TRAIN: iteration 2869 : loss : 0.185336, loss_ce: 0.007569, loss_dice: 0.363104
[08:06:19.685] TRAIN: iteration 2870 : loss : 0.144466, loss_ce: 0.005608, loss_dice: 0.283325
[08:06:19.893] TRAIN: iteration 2871 : loss : 0.221485, loss_ce: 0.006121, loss_dice: 0.436849
[08:06:20.101] TRAIN: iteration 2872 : loss : 0.179876, loss_ce: 0.009794, loss_dice: 0.349958
[08:06:20.307] TRAIN: iteration 2873 : loss : 0.172902, loss_ce: 0.007725, loss_dice: 0.338079
[08:06:20.514] TRAIN: iteration 2874 : loss : 0.253084, loss_ce: 0.005892, loss_dice: 0.500275
[08:06:20.722] TRAIN: iteration 2875 : loss : 0.100497, loss_ce: 0.006674, loss_dice: 0.194320
[08:06:20.930] TRAIN: iteration 2876 : loss : 0.161029, loss_ce: 0.007651, loss_dice: 0.314407
[08:06:21.139] TRAIN: iteration 2877 : loss : 0.231415, loss_ce: 0.006798, loss_dice: 0.456032
[08:06:21.346] TRAIN: iteration 2878 : loss : 0.228252, loss_ce: 0.006955, loss_dice: 0.449549
[08:06:21.554] TRAIN: iteration 2879 : loss : 0.157250, loss_ce: 0.011836, loss_dice: 0.302665
[08:06:21.761] TRAIN: iteration 2880 : loss : 0.214784, loss_ce: 0.007306, loss_dice: 0.422263
[08:06:21.995] TRAIN: iteration 2881 : loss : 0.156608, loss_ce: 0.017261, loss_dice: 0.295954
[08:06:22.209] TRAIN: iteration 2882 : loss : 0.253419, loss_ce: 0.006435, loss_dice: 0.500403
[08:06:22.416] TRAIN: iteration 2883 : loss : 0.252832, loss_ce: 0.005938, loss_dice: 0.499725
[08:06:22.623] TRAIN: iteration 2884 : loss : 0.153157, loss_ce: 0.006555, loss_dice: 0.299759
[08:06:22.835] TRAIN: iteration 2885 : loss : 0.123254, loss_ce: 0.007465, loss_dice: 0.239044
[08:06:23.082] TRAIN: iteration 2886 : loss : 0.224038, loss_ce: 0.006707, loss_dice: 0.441370
[08:06:23.592] TRAIN: iteration 2887 : loss : 0.139042, loss_ce: 0.006143, loss_dice: 0.271942
[08:06:23.799] TRAIN: iteration 2888 : loss : 0.119206, loss_ce: 0.007830, loss_dice: 0.230583
[08:06:26.151] TRAIN: iteration 2889 : loss : 0.144435, loss_ce: 0.008888, loss_dice: 0.279983
[08:06:26.361] TRAIN: iteration 2890 : loss : 0.252985, loss_ce: 0.008059, loss_dice: 0.497911
[08:06:26.569] TRAIN: iteration 2891 : loss : 0.152787, loss_ce: 0.006332, loss_dice: 0.299241
[08:06:26.783] TRAIN: iteration 2892 : loss : 0.252749, loss_ce: 0.005165, loss_dice: 0.500334
[08:06:26.992] TRAIN: iteration 2893 : loss : 0.177399, loss_ce: 0.034802, loss_dice: 0.319996
[08:06:27.203] TRAIN: iteration 2894 : loss : 0.142936, loss_ce: 0.004573, loss_dice: 0.281299
[08:06:27.485] TRAIN: iteration 2895 : loss : 0.188561, loss_ce: 0.009010, loss_dice: 0.368112
[08:06:27.693] TRAIN: iteration 2896 : loss : 0.225441, loss_ce: 0.015135, loss_dice: 0.435747
[08:06:29.530] TRAIN: iteration 2897 : loss : 0.168904, loss_ce: 0.011367, loss_dice: 0.326441
[08:06:29.737] TRAIN: iteration 2898 : loss : 0.166655, loss_ce: 0.017433, loss_dice: 0.315876
[08:06:29.943] TRAIN: iteration 2899 : loss : 0.148806, loss_ce: 0.005116, loss_dice: 0.292495
[08:06:30.150] TRAIN: iteration 2900 : loss : 0.127026, loss_ce: 0.006752, loss_dice: 0.247301
[08:06:30.383] TRAIN: iteration 2901 : loss : 0.106813, loss_ce: 0.002940, loss_dice: 0.210685
[08:06:30.590] TRAIN: iteration 2902 : loss : 0.253177, loss_ce: 0.006626, loss_dice: 0.499728
[08:06:30.863] TRAIN: iteration 2903 : loss : 0.227240, loss_ce: 0.005525, loss_dice: 0.448955
[08:06:31.071] TRAIN: iteration 2904 : loss : 0.148283, loss_ce: 0.005272, loss_dice: 0.291294
[08:06:31.952] TRAIN: iteration 2905 : loss : 0.197725, loss_ce: 0.027350, loss_dice: 0.368100
[08:06:32.161] TRAIN: iteration 2906 : loss : 0.153024, loss_ce: 0.005078, loss_dice: 0.300970
[08:06:32.369] TRAIN: iteration 2907 : loss : 0.111073, loss_ce: 0.004674, loss_dice: 0.217472
[08:06:32.575] TRAIN: iteration 2908 : loss : 0.196756, loss_ce: 0.004949, loss_dice: 0.388564
[08:06:32.782] TRAIN: iteration 2909 : loss : 0.252029, loss_ce: 0.003843, loss_dice: 0.500215
[08:06:32.988] TRAIN: iteration 2910 : loss : 0.060880, loss_ce: 0.004019, loss_dice: 0.117741
[08:06:34.002] TRAIN: iteration 2911 : loss : 0.237067, loss_ce: 0.013181, loss_dice: 0.460954
[08:06:34.213] TRAIN: iteration 2912 : loss : 0.174962, loss_ce: 0.023256, loss_dice: 0.326668
[08:06:35.321] TRAIN: iteration 2913 : loss : 0.156972, loss_ce: 0.008890, loss_dice: 0.305054
[08:06:35.529] TRAIN: iteration 2914 : loss : 0.111016, loss_ce: 0.004740, loss_dice: 0.217292
[08:06:35.740] TRAIN: iteration 2915 : loss : 0.134809, loss_ce: 0.005240, loss_dice: 0.264377
[08:06:35.946] TRAIN: iteration 2916 : loss : 0.245830, loss_ce: 0.009817, loss_dice: 0.481844
[08:06:36.152] TRAIN: iteration 2917 : loss : 0.112154, loss_ce: 0.005211, loss_dice: 0.219097
[08:06:36.365] TRAIN: iteration 2918 : loss : 0.249049, loss_ce: 0.006212, loss_dice: 0.491886
[08:06:36.904] TRAIN: iteration 2919 : loss : 0.087280, loss_ce: 0.005849, loss_dice: 0.168712
[08:06:37.112] TRAIN: iteration 2920 : loss : 0.202738, loss_ce: 0.007725, loss_dice: 0.397751
[08:06:39.124] TRAIN: iteration 2921 : loss : 0.221680, loss_ce: 0.006539, loss_dice: 0.436820
[08:06:39.332] TRAIN: iteration 2922 : loss : 0.138808, loss_ce: 0.004166, loss_dice: 0.273450
[08:06:39.541] TRAIN: iteration 2923 : loss : 0.252264, loss_ce: 0.004319, loss_dice: 0.500210
[08:06:39.751] TRAIN: iteration 2924 : loss : 0.131987, loss_ce: 0.003800, loss_dice: 0.260175
[08:06:39.957] TRAIN: iteration 2925 : loss : 0.178115, loss_ce: 0.009039, loss_dice: 0.347192
[08:06:40.164] TRAIN: iteration 2926 : loss : 0.149180, loss_ce: 0.004306, loss_dice: 0.294053
[08:06:40.374] TRAIN: iteration 2927 : loss : 0.108958, loss_ce: 0.004344, loss_dice: 0.213572
[08:06:40.583] TRAIN: iteration 2928 : loss : 0.184572, loss_ce: 0.011838, loss_dice: 0.357306
[08:06:40.790] TRAIN: iteration 2929 : loss : 0.123385, loss_ce: 0.004687, loss_dice: 0.242084
[08:06:40.996] TRAIN: iteration 2930 : loss : 0.245683, loss_ce: 0.003386, loss_dice: 0.487980
[08:06:41.205] TRAIN: iteration 2931 : loss : 0.208251, loss_ce: 0.011828, loss_dice: 0.404675
[08:06:41.412] TRAIN: iteration 2932 : loss : 0.207351, loss_ce: 0.007102, loss_dice: 0.407600
[08:06:41.618] TRAIN: iteration 2933 : loss : 0.232050, loss_ce: 0.005686, loss_dice: 0.458414
[08:06:41.825] TRAIN: iteration 2934 : loss : 0.252382, loss_ce: 0.004490, loss_dice: 0.500274
[08:06:42.199] TRAIN: iteration 2935 : loss : 0.171619, loss_ce: 0.007236, loss_dice: 0.336002
[08:06:42.407] TRAIN: iteration 2936 : loss : 0.228035, loss_ce: 0.005012, loss_dice: 0.451058
[08:06:42.615] TRAIN: iteration 2937 : loss : 0.223231, loss_ce: 0.004727, loss_dice: 0.441736
[08:06:43.707] TRAIN: iteration 2938 : loss : 0.244925, loss_ce: 0.005277, loss_dice: 0.484573
[08:06:43.913] TRAIN: iteration 2939 : loss : 0.145948, loss_ce: 0.006414, loss_dice: 0.285481
[08:06:44.119] TRAIN: iteration 2940 : loss : 0.252619, loss_ce: 0.004933, loss_dice: 0.500305
[08:06:44.892] TRAIN: iteration 2941 : loss : 0.157173, loss_ce: 0.013803, loss_dice: 0.300542
[08:06:45.099] TRAIN: iteration 2942 : loss : 0.092282, loss_ce: 0.005403, loss_dice: 0.179162
[08:06:45.305] TRAIN: iteration 2943 : loss : 0.134095, loss_ce: 0.005090, loss_dice: 0.263099
[08:06:45.511] TRAIN: iteration 2944 : loss : 0.194536, loss_ce: 0.009157, loss_dice: 0.379915
[08:06:45.718] TRAIN: iteration 2945 : loss : 0.247146, loss_ce: 0.004682, loss_dice: 0.489610
[08:06:51.245] TRAIN: iteration 2946 : loss : 0.247437, loss_ce: 0.014706, loss_dice: 0.480168
[08:06:51.453] TRAIN: iteration 2947 : loss : 0.244346, loss_ce: 0.006235, loss_dice: 0.482457
[08:06:51.660] TRAIN: iteration 2948 : loss : 0.114975, loss_ce: 0.005392, loss_dice: 0.224558
[08:06:51.869] TRAIN: iteration 2949 : loss : 0.111875, loss_ce: 0.007110, loss_dice: 0.216640
[08:06:52.080] TRAIN: iteration 2950 : loss : 0.247668, loss_ce: 0.006812, loss_dice: 0.488523
[08:06:52.294] TRAIN: iteration 2951 : loss : 0.217089, loss_ce: 0.008285, loss_dice: 0.425893
[08:06:52.505] TRAIN: iteration 2952 : loss : 0.159919, loss_ce: 0.005441, loss_dice: 0.314397
[08:06:52.713] TRAIN: iteration 2953 : loss : 0.123830, loss_ce: 0.005262, loss_dice: 0.242398
[08:06:52.925] TRAIN: iteration 2954 : loss : 0.249890, loss_ce: 0.006118, loss_dice: 0.493662
[08:06:53.133] TRAIN: iteration 2955 : loss : 0.161584, loss_ce: 0.020432, loss_dice: 0.302736
[08:06:53.347] TRAIN: iteration 2956 : loss : 0.205811, loss_ce: 0.004459, loss_dice: 0.407162
[08:06:53.555] TRAIN: iteration 2957 : loss : 0.089172, loss_ce: 0.005393, loss_dice: 0.172951
[08:06:53.762] TRAIN: iteration 2958 : loss : 0.173996, loss_ce: 0.006181, loss_dice: 0.341810
[08:06:53.977] TRAIN: iteration 2959 : loss : 0.214161, loss_ce: 0.005524, loss_dice: 0.422799
[08:06:54.186] TRAIN: iteration 2960 : loss : 0.242930, loss_ce: 0.004843, loss_dice: 0.481016
[08:06:54.427] TRAIN: iteration 2961 : loss : 0.251448, loss_ce: 0.004593, loss_dice: 0.498304
[08:06:54.632] TRAIN: iteration 2962 : loss : 0.244097, loss_ce: 0.005123, loss_dice: 0.483071
[08:06:55.270] TRAIN: iteration 2963 : loss : 0.079564, loss_ce: 0.005612, loss_dice: 0.153516
[08:06:55.476] TRAIN: iteration 2964 : loss : 0.112149, loss_ce: 0.005831, loss_dice: 0.218468
[08:06:55.684] TRAIN: iteration 2965 : loss : 0.105843, loss_ce: 0.005548, loss_dice: 0.206137
[08:06:57.526] TRAIN: iteration 2966 : loss : 0.134602, loss_ce: 0.009499, loss_dice: 0.259706
[08:06:57.732] TRAIN: iteration 2967 : loss : 0.093547, loss_ce: 0.005027, loss_dice: 0.182067
[08:06:57.939] TRAIN: iteration 2968 : loss : 0.187087, loss_ce: 0.004938, loss_dice: 0.369236
[08:06:58.153] TRAIN: iteration 2969 : loss : 0.145571, loss_ce: 0.004323, loss_dice: 0.286820
[08:06:58.360] TRAIN: iteration 2970 : loss : 0.180673, loss_ce: 0.006008, loss_dice: 0.355338
[08:06:58.570] TRAIN: iteration 2971 : loss : 0.252469, loss_ce: 0.004682, loss_dice: 0.500256
[08:06:58.782] TRAIN: iteration 2972 : loss : 0.178120, loss_ce: 0.004810, loss_dice: 0.351429
[08:06:59.449] TRAIN: iteration 2973 : loss : 0.251601, loss_ce: 0.004321, loss_dice: 0.498882
[08:07:00.484] TRAIN: iteration 2974 : loss : 0.252034, loss_ce: 0.003848, loss_dice: 0.500221
[08:07:00.691] TRAIN: iteration 2975 : loss : 0.185657, loss_ce: 0.005672, loss_dice: 0.365642
[08:07:00.897] TRAIN: iteration 2976 : loss : 0.145432, loss_ce: 0.013458, loss_dice: 0.277406
[08:07:01.105] TRAIN: iteration 2977 : loss : 0.247417, loss_ce: 0.004793, loss_dice: 0.490041
[08:07:01.313] TRAIN: iteration 2978 : loss : 0.176567, loss_ce: 0.017912, loss_dice: 0.335221
[08:07:01.527] TRAIN: iteration 2979 : loss : 0.249846, loss_ce: 0.003359, loss_dice: 0.496334
[08:07:01.734] TRAIN: iteration 2980 : loss : 0.245326, loss_ce: 0.004780, loss_dice: 0.485872
[08:07:04.606] TRAIN: iteration 2981 : loss : 0.211417, loss_ce: 0.004234, loss_dice: 0.418600
[08:07:04.814] TRAIN: iteration 2982 : loss : 0.098585, loss_ce: 0.004596, loss_dice: 0.192574
[08:07:05.025] TRAIN: iteration 2983 : loss : 0.140638, loss_ce: 0.007663, loss_dice: 0.273614
[08:07:05.231] TRAIN: iteration 2984 : loss : 0.239543, loss_ce: 0.004312, loss_dice: 0.474774
[08:07:05.438] TRAIN: iteration 2985 : loss : 0.208236, loss_ce: 0.016198, loss_dice: 0.400274
[08:07:05.648] TRAIN: iteration 2986 : loss : 0.138609, loss_ce: 0.008042, loss_dice: 0.269177
[08:07:05.857] TRAIN: iteration 2987 : loss : 0.252719, loss_ce: 0.006468, loss_dice: 0.498969
[08:07:06.067] TRAIN: iteration 2988 : loss : 0.229889, loss_ce: 0.004866, loss_dice: 0.454912
[08:07:07.783] TRAIN: iteration 2989 : loss : 0.161409, loss_ce: 0.005450, loss_dice: 0.317368
[08:07:07.989] TRAIN: iteration 2990 : loss : 0.149229, loss_ce: 0.007565, loss_dice: 0.290893
[08:07:08.197] TRAIN: iteration 2991 : loss : 0.241294, loss_ce: 0.048849, loss_dice: 0.433739
[08:07:08.403] TRAIN: iteration 2992 : loss : 0.252782, loss_ce: 0.005239, loss_dice: 0.500324
[08:07:08.610] TRAIN: iteration 2993 : loss : 0.139619, loss_ce: 0.007925, loss_dice: 0.271313
[08:07:08.881] TRAIN: iteration 2994 : loss : 0.179857, loss_ce: 0.008428, loss_dice: 0.351287
[08:07:09.089] TRAIN: iteration 2995 : loss : 0.196226, loss_ce: 0.013483, loss_dice: 0.378968
[08:07:09.295] TRAIN: iteration 2996 : loss : 0.198950, loss_ce: 0.006375, loss_dice: 0.391526
[08:07:14.945] TRAIN: iteration 2997 : loss : 0.189230, loss_ce: 0.007477, loss_dice: 0.370984
[08:07:15.156] TRAIN: iteration 2998 : loss : 0.153418, loss_ce: 0.019915, loss_dice: 0.286921
[08:07:15.373] TRAIN: iteration 2999 : loss : 0.250616, loss_ce: 0.007108, loss_dice: 0.494125
[08:07:15.581] TRAIN: iteration 3000 : loss : 0.149325, loss_ce: 0.006815, loss_dice: 0.291834
[08:07:15.818] TRAIN: iteration 3001 : loss : 0.144287, loss_ce: 0.007227, loss_dice: 0.281347
[08:07:16.025] TRAIN: iteration 3002 : loss : 0.071004, loss_ce: 0.005463, loss_dice: 0.136544
[08:07:16.231] TRAIN: iteration 3003 : loss : 0.230017, loss_ce: 0.016522, loss_dice: 0.443512
[08:07:16.438] TRAIN: iteration 3004 : loss : 0.123613, loss_ce: 0.007100, loss_dice: 0.240126
[08:07:16.645] TRAIN: iteration 3005 : loss : 0.148194, loss_ce: 0.009035, loss_dice: 0.287352
[08:07:16.855] TRAIN: iteration 3006 : loss : 0.144306, loss_ce: 0.008941, loss_dice: 0.279671
[08:07:17.064] TRAIN: iteration 3007 : loss : 0.072491, loss_ce: 0.007144, loss_dice: 0.137838
[08:07:17.280] TRAIN: iteration 3008 : loss : 0.189564, loss_ce: 0.008315, loss_dice: 0.370814
[08:07:17.488] TRAIN: iteration 3009 : loss : 0.219590, loss_ce: 0.009247, loss_dice: 0.429933
[08:07:17.693] TRAIN: iteration 3010 : loss : 0.138282, loss_ce: 0.020512, loss_dice: 0.256053
[08:07:17.903] TRAIN: iteration 3011 : loss : 0.175339, loss_ce: 0.012860, loss_dice: 0.337818
[08:07:18.110] TRAIN: iteration 3012 : loss : 0.174978, loss_ce: 0.009739, loss_dice: 0.340216
[08:07:19.502] TRAIN: iteration 3013 : loss : 0.148408, loss_ce: 0.025414, loss_dice: 0.271402
[08:07:19.707] TRAIN: iteration 3014 : loss : 0.254230, loss_ce: 0.007975, loss_dice: 0.500485
[08:07:19.919] TRAIN: iteration 3015 : loss : 0.121273, loss_ce: 0.006517, loss_dice: 0.236029
[08:07:20.126] TRAIN: iteration 3016 : loss : 0.220231, loss_ce: 0.009403, loss_dice: 0.431059
[08:07:20.332] TRAIN: iteration 3017 : loss : 0.251068, loss_ce: 0.009963, loss_dice: 0.492173
[08:07:20.541] TRAIN: iteration 3018 : loss : 0.161155, loss_ce: 0.009207, loss_dice: 0.313103
[08:07:20.747] TRAIN: iteration 3019 : loss : 0.167416, loss_ce: 0.008120, loss_dice: 0.326711
[08:07:20.959] TRAIN: iteration 3020 : loss : 0.205322, loss_ce: 0.017460, loss_dice: 0.393184
[08:07:21.198] TRAIN: iteration 3021 : loss : 0.254547, loss_ce: 0.008535, loss_dice: 0.500559
[08:07:21.404] TRAIN: iteration 3022 : loss : 0.202751, loss_ce: 0.008810, loss_dice: 0.396692
[08:07:21.612] TRAIN: iteration 3023 : loss : 0.254086, loss_ce: 0.007665, loss_dice: 0.500507
[08:07:21.818] TRAIN: iteration 3024 : loss : 0.123715, loss_ce: 0.008462, loss_dice: 0.238968
[08:07:22.027] TRAIN: iteration 3025 : loss : 0.210366, loss_ce: 0.014032, loss_dice: 0.406699
[08:07:22.234] TRAIN: iteration 3026 : loss : 0.098638, loss_ce: 0.007126, loss_dice: 0.190149
[08:07:22.440] TRAIN: iteration 3027 : loss : 0.237798, loss_ce: 0.034773, loss_dice: 0.440823
[08:07:22.646] TRAIN: iteration 3028 : loss : 0.253754, loss_ce: 0.007061, loss_dice: 0.500447
[08:07:22.852] TRAIN: iteration 3029 : loss : 0.253651, loss_ce: 0.006902, loss_dice: 0.500401
[08:07:23.314] TRAIN: iteration 3030 : loss : 0.216777, loss_ce: 0.007012, loss_dice: 0.426541
[08:07:25.906] TRAIN: iteration 3031 : loss : 0.218264, loss_ce: 0.006239, loss_dice: 0.430288
[08:07:26.112] TRAIN: iteration 3032 : loss : 0.253364, loss_ce: 0.006339, loss_dice: 0.500389
[08:07:26.319] TRAIN: iteration 3033 : loss : 0.235396, loss_ce: 0.007134, loss_dice: 0.463659
[08:07:26.527] TRAIN: iteration 3034 : loss : 0.239838, loss_ce: 0.009048, loss_dice: 0.470628
[08:07:26.738] TRAIN: iteration 3035 : loss : 0.253351, loss_ce: 0.006315, loss_dice: 0.500386
[08:07:26.945] TRAIN: iteration 3036 : loss : 0.106326, loss_ce: 0.005530, loss_dice: 0.207122
[08:07:27.157] TRAIN: iteration 3037 : loss : 0.136923, loss_ce: 0.008388, loss_dice: 0.265458
[08:07:28.787] TRAIN: iteration 3038 : loss : 0.157155, loss_ce: 0.006824, loss_dice: 0.307487
[08:07:28.994] TRAIN: iteration 3039 : loss : 0.090698, loss_ce: 0.004658, loss_dice: 0.176737
[08:07:29.204] TRAIN: iteration 3040 : loss : 0.118074, loss_ce: 0.005178, loss_dice: 0.230970
[08:07:29.205] NaN or Inf found in input tensor.
[08:07:29.421] TRAIN: iteration 3041 : loss : 0.117363, loss_ce: 0.006392, loss_dice: 0.228335
[08:07:29.717] TRAIN: iteration 3042 : loss : 0.227595, loss_ce: 0.005219, loss_dice: 0.449970
[08:07:29.923] TRAIN: iteration 3043 : loss : 0.252479, loss_ce: 0.004646, loss_dice: 0.500312
[08:07:30.131] TRAIN: iteration 3044 : loss : 0.205168, loss_ce: 0.004608, loss_dice: 0.405728
[08:07:30.337] TRAIN: iteration 3045 : loss : 0.235672, loss_ce: 0.005070, loss_dice: 0.466274
[08:07:33.119] TRAIN: iteration 3046 : loss : 0.236250, loss_ce: 0.006788, loss_dice: 0.465712
[08:07:33.416] TRAIN: iteration 3047 : loss : 0.213944, loss_ce: 0.014605, loss_dice: 0.413283
[08:07:33.623] TRAIN: iteration 3048 : loss : 0.187659, loss_ce: 0.005922, loss_dice: 0.369395
[08:07:33.831] TRAIN: iteration 3049 : loss : 0.251958, loss_ce: 0.004227, loss_dice: 0.499688
[08:07:34.038] TRAIN: iteration 3050 : loss : 0.131539, loss_ce: 0.009674, loss_dice: 0.253403
[08:07:34.613] TRAIN: iteration 3051 : loss : 0.182915, loss_ce: 0.022744, loss_dice: 0.343086
[08:07:34.821] TRAIN: iteration 3052 : loss : 0.252534, loss_ce: 0.004767, loss_dice: 0.500301
[08:07:35.029] TRAIN: iteration 3053 : loss : 0.251287, loss_ce: 0.005262, loss_dice: 0.497313
[08:07:37.522] TRAIN: iteration 3054 : loss : 0.241635, loss_ce: 0.009180, loss_dice: 0.474091
[08:07:37.730] TRAIN: iteration 3055 : loss : 0.238465, loss_ce: 0.008461, loss_dice: 0.468469
[08:07:37.938] TRAIN: iteration 3056 : loss : 0.160122, loss_ce: 0.011860, loss_dice: 0.308385
[08:07:38.146] TRAIN: iteration 3057 : loss : 0.253651, loss_ce: 0.007815, loss_dice: 0.499487
[08:07:38.351] TRAIN: iteration 3058 : loss : 0.253210, loss_ce: 0.006068, loss_dice: 0.500351
[08:07:39.011] TRAIN: iteration 3059 : loss : 0.253440, loss_ce: 0.006428, loss_dice: 0.500453
[08:07:39.217] TRAIN: iteration 3060 : loss : 0.149101, loss_ce: 0.021856, loss_dice: 0.276347
[08:07:39.456] TRAIN: iteration 3061 : loss : 0.196754, loss_ce: 0.006649, loss_dice: 0.386859
[08:07:43.985] TRAIN: iteration 3062 : loss : 0.131568, loss_ce: 0.010621, loss_dice: 0.252515
[08:07:44.191] TRAIN: iteration 3063 : loss : 0.109812, loss_ce: 0.007464, loss_dice: 0.212160
[08:07:44.398] TRAIN: iteration 3064 : loss : 0.253812, loss_ce: 0.007123, loss_dice: 0.500500
[08:07:44.610] TRAIN: iteration 3065 : loss : 0.183605, loss_ce: 0.009866, loss_dice: 0.357344
[08:07:44.816] TRAIN: iteration 3066 : loss : 0.181737, loss_ce: 0.007712, loss_dice: 0.355763
[08:07:45.023] TRAIN: iteration 3067 : loss : 0.190093, loss_ce: 0.007991, loss_dice: 0.372195
[08:07:45.231] TRAIN: iteration 3068 : loss : 0.127624, loss_ce: 0.008726, loss_dice: 0.246523
[08:07:45.437] TRAIN: iteration 3069 : loss : 0.134582, loss_ce: 0.008759, loss_dice: 0.260405
[08:07:47.140] TRAIN: iteration 3070 : loss : 0.131015, loss_ce: 0.011380, loss_dice: 0.250649
[08:07:47.349] TRAIN: iteration 3071 : loss : 0.126519, loss_ce: 0.006198, loss_dice: 0.246839
[08:07:47.555] TRAIN: iteration 3072 : loss : 0.196160, loss_ce: 0.006713, loss_dice: 0.385608
[08:07:47.761] TRAIN: iteration 3073 : loss : 0.202601, loss_ce: 0.005903, loss_dice: 0.399300
[08:07:47.966] TRAIN: iteration 3074 : loss : 0.174863, loss_ce: 0.007985, loss_dice: 0.341742
[08:07:48.181] TRAIN: iteration 3075 : loss : 0.253074, loss_ce: 0.005740, loss_dice: 0.500408
[08:07:48.387] TRAIN: iteration 3076 : loss : 0.112253, loss_ce: 0.007640, loss_dice: 0.216865
[08:07:48.593] TRAIN: iteration 3077 : loss : 0.199122, loss_ce: 0.004151, loss_dice: 0.394092
[08:07:51.756] TRAIN: iteration 3078 : loss : 0.232309, loss_ce: 0.007639, loss_dice: 0.456980
[08:07:51.971] TRAIN: iteration 3079 : loss : 0.097349, loss_ce: 0.005815, loss_dice: 0.188883
[08:07:52.179] TRAIN: iteration 3080 : loss : 0.240110, loss_ce: 0.005913, loss_dice: 0.474307
[08:07:52.417] TRAIN: iteration 3081 : loss : 0.095805, loss_ce: 0.004267, loss_dice: 0.187344
[08:07:52.743] TRAIN: iteration 3082 : loss : 0.248562, loss_ce: 0.006619, loss_dice: 0.490505
[08:07:52.951] TRAIN: iteration 3083 : loss : 0.103507, loss_ce: 0.003162, loss_dice: 0.203851
[08:07:53.157] TRAIN: iteration 3084 : loss : 0.135952, loss_ce: 0.007059, loss_dice: 0.264844
[08:07:53.368] TRAIN: iteration 3085 : loss : 0.228002, loss_ce: 0.005824, loss_dice: 0.450180
[08:07:56.395] TRAIN: iteration 3086 : loss : 0.251770, loss_ce: 0.003352, loss_dice: 0.500187
[08:07:56.602] TRAIN: iteration 3087 : loss : 0.116149, loss_ce: 0.004717, loss_dice: 0.227580
[08:07:56.809] TRAIN: iteration 3088 : loss : 0.206277, loss_ce: 0.003442, loss_dice: 0.409112
[08:07:57.016] TRAIN: iteration 3089 : loss : 0.252914, loss_ce: 0.008839, loss_dice: 0.496988
[08:07:57.224] TRAIN: iteration 3090 : loss : 0.143105, loss_ce: 0.003310, loss_dice: 0.282900
[08:07:57.433] TRAIN: iteration 3091 : loss : 0.251711, loss_ce: 0.003249, loss_dice: 0.500172
[08:07:57.639] TRAIN: iteration 3092 : loss : 0.185104, loss_ce: 0.005556, loss_dice: 0.364652
[08:07:57.847] TRAIN: iteration 3093 : loss : 0.186755, loss_ce: 0.003041, loss_dice: 0.370469
[08:07:59.885] TRAIN: iteration 3094 : loss : 0.246678, loss_ce: 0.004461, loss_dice: 0.488895
[08:08:00.093] TRAIN: iteration 3095 : loss : 0.207464, loss_ce: 0.014963, loss_dice: 0.399965
[08:08:00.300] TRAIN: iteration 3096 : loss : 0.209818, loss_ce: 0.006159, loss_dice: 0.413476
[08:08:00.506] TRAIN: iteration 3097 : loss : 0.249045, loss_ce: 0.029597, loss_dice: 0.468492
[08:08:00.713] TRAIN: iteration 3098 : loss : 0.107861, loss_ce: 0.003638, loss_dice: 0.212085
[08:08:00.920] TRAIN: iteration 3099 : loss : 0.247839, loss_ce: 0.003137, loss_dice: 0.492541
[08:08:01.126] TRAIN: iteration 3100 : loss : 0.120962, loss_ce: 0.007270, loss_dice: 0.234654
[08:08:01.473] TRAIN: iteration 3101 : loss : 0.229429, loss_ce: 0.026088, loss_dice: 0.432770
[08:08:03.727] TRAIN: iteration 3102 : loss : 0.121967, loss_ce: 0.003731, loss_dice: 0.240204
[08:08:03.935] TRAIN: iteration 3103 : loss : 0.187040, loss_ce: 0.006283, loss_dice: 0.367797
[08:08:04.144] TRAIN: iteration 3104 : loss : 0.127722, loss_ce: 0.003910, loss_dice: 0.251534
[08:08:04.355] TRAIN: iteration 3105 : loss : 0.229543, loss_ce: 0.008352, loss_dice: 0.450735
[08:08:04.563] TRAIN: iteration 3106 : loss : 0.250029, loss_ce: 0.005417, loss_dice: 0.494640
[08:08:04.769] TRAIN: iteration 3107 : loss : 0.105939, loss_ce: 0.005378, loss_dice: 0.206501
[08:08:04.980] TRAIN: iteration 3108 : loss : 0.191766, loss_ce: 0.005810, loss_dice: 0.377721
[08:08:05.186] TRAIN: iteration 3109 : loss : 0.096811, loss_ce: 0.007118, loss_dice: 0.186504
[08:08:07.325] TRAIN: iteration 3110 : loss : 0.167871, loss_ce: 0.009540, loss_dice: 0.326201
[08:08:07.532] TRAIN: iteration 3111 : loss : 0.252531, loss_ce: 0.004776, loss_dice: 0.500286
[08:08:07.739] TRAIN: iteration 3112 : loss : 0.171136, loss_ce: 0.006307, loss_dice: 0.335965
[08:08:07.946] TRAIN: iteration 3113 : loss : 0.148316, loss_ce: 0.021674, loss_dice: 0.274957
[08:08:08.152] TRAIN: iteration 3114 : loss : 0.150807, loss_ce: 0.008834, loss_dice: 0.292780
[08:08:09.643] TRAIN: iteration 3115 : loss : 0.218282, loss_ce: 0.007362, loss_dice: 0.429202
[08:08:09.850] TRAIN: iteration 3116 : loss : 0.247951, loss_ce: 0.006779, loss_dice: 0.489124
[08:08:10.057] TRAIN: iteration 3117 : loss : 0.198963, loss_ce: 0.008428, loss_dice: 0.389497
[08:08:10.264] TRAIN: iteration 3118 : loss : 0.252896, loss_ce: 0.008445, loss_dice: 0.497346
[08:08:10.472] TRAIN: iteration 3119 : loss : 0.220345, loss_ce: 0.007770, loss_dice: 0.432921
[08:08:10.680] TRAIN: iteration 3120 : loss : 0.223810, loss_ce: 0.008004, loss_dice: 0.439617
[08:08:10.917] TRAIN: iteration 3121 : loss : 0.209649, loss_ce: 0.009502, loss_dice: 0.409796
[08:08:11.124] TRAIN: iteration 3122 : loss : 0.142495, loss_ce: 0.017706, loss_dice: 0.267283
[08:08:13.019] TRAIN: iteration 3123 : loss : 0.172916, loss_ce: 0.007690, loss_dice: 0.338141
[08:08:13.225] TRAIN: iteration 3124 : loss : 0.180533, loss_ce: 0.010083, loss_dice: 0.350984
[08:08:13.434] TRAIN: iteration 3125 : loss : 0.233218, loss_ce: 0.008084, loss_dice: 0.458352
[08:08:13.642] TRAIN: iteration 3126 : loss : 0.231252, loss_ce: 0.012678, loss_dice: 0.449826
[08:08:13.849] TRAIN: iteration 3127 : loss : 0.194774, loss_ce: 0.010257, loss_dice: 0.379292
[08:08:14.055] TRAIN: iteration 3128 : loss : 0.215456, loss_ce: 0.009256, loss_dice: 0.421656
[08:08:14.269] TRAIN: iteration 3129 : loss : 0.247869, loss_ce: 0.008151, loss_dice: 0.487588
[08:08:16.818] TRAIN: iteration 3130 : loss : 0.253139, loss_ce: 0.012015, loss_dice: 0.494263
[08:08:19.015] TRAIN: iteration 3131 : loss : 0.200243, loss_ce: 0.009400, loss_dice: 0.391085
[08:08:19.222] TRAIN: iteration 3132 : loss : 0.248836, loss_ce: 0.006019, loss_dice: 0.491654
[08:08:19.431] TRAIN: iteration 3133 : loss : 0.254706, loss_ce: 0.009206, loss_dice: 0.500205
[08:08:19.637] TRAIN: iteration 3134 : loss : 0.126149, loss_ce: 0.006888, loss_dice: 0.245409
[08:08:19.845] TRAIN: iteration 3135 : loss : 0.234128, loss_ce: 0.009006, loss_dice: 0.459250
[08:08:20.053] TRAIN: iteration 3136 : loss : 0.122839, loss_ce: 0.007542, loss_dice: 0.238136
[08:08:20.261] TRAIN: iteration 3137 : loss : 0.194495, loss_ce: 0.016942, loss_dice: 0.372048
[08:08:23.972] TRAIN: iteration 3138 : loss : 0.071417, loss_ce: 0.003871, loss_dice: 0.138963
[08:08:24.180] TRAIN: iteration 3139 : loss : 0.201023, loss_ce: 0.011792, loss_dice: 0.390255
[08:08:24.389] TRAIN: iteration 3140 : loss : 0.118800, loss_ce: 0.004825, loss_dice: 0.232775
[08:08:24.625] TRAIN: iteration 3141 : loss : 0.163430, loss_ce: 0.010450, loss_dice: 0.316411
[08:08:24.837] TRAIN: iteration 3142 : loss : 0.241241, loss_ce: 0.016321, loss_dice: 0.466161
[08:08:25.045] TRAIN: iteration 3143 : loss : 0.176414, loss_ce: 0.011910, loss_dice: 0.340917
[08:08:25.256] TRAIN: iteration 3144 : loss : 0.252646, loss_ce: 0.005001, loss_dice: 0.500291
[08:08:25.463] TRAIN: iteration 3145 : loss : 0.189769, loss_ce: 0.018213, loss_dice: 0.361325
[08:08:26.697] TRAIN: iteration 3146 : loss : 0.157458, loss_ce: 0.007250, loss_dice: 0.307666
[08:08:30.276] TRAIN: iteration 3147 : loss : 0.252185, loss_ce: 0.008193, loss_dice: 0.496177
[08:08:30.484] TRAIN: iteration 3148 : loss : 0.202901, loss_ce: 0.006951, loss_dice: 0.398851
[08:08:30.691] TRAIN: iteration 3149 : loss : 0.160720, loss_ce: 0.007136, loss_dice: 0.314303
[08:08:30.900] TRAIN: iteration 3150 : loss : 0.141330, loss_ce: 0.008218, loss_dice: 0.274441
[08:08:31.107] TRAIN: iteration 3151 : loss : 0.253261, loss_ce: 0.006209, loss_dice: 0.500313
[08:08:31.315] TRAIN: iteration 3152 : loss : 0.160885, loss_ce: 0.006260, loss_dice: 0.315509
[08:08:31.523] TRAIN: iteration 3153 : loss : 0.095112, loss_ce: 0.005061, loss_dice: 0.185162
[08:08:31.730] TRAIN: iteration 3154 : loss : 0.184756, loss_ce: 0.005386, loss_dice: 0.364126
[08:08:33.029] TRAIN: iteration 3155 : loss : 0.142173, loss_ce: 0.013837, loss_dice: 0.270509
[08:08:33.236] TRAIN: iteration 3156 : loss : 0.139949, loss_ce: 0.016771, loss_dice: 0.263128
[08:08:33.450] TRAIN: iteration 3157 : loss : 0.192101, loss_ce: 0.010929, loss_dice: 0.373272
[08:08:33.657] TRAIN: iteration 3158 : loss : 0.153788, loss_ce: 0.007834, loss_dice: 0.299742
[08:08:34.068] TRAIN: iteration 3159 : loss : 0.248846, loss_ce: 0.009736, loss_dice: 0.487956
[08:08:34.277] TRAIN: iteration 3160 : loss : 0.126316, loss_ce: 0.011735, loss_dice: 0.240898
[08:08:34.516] TRAIN: iteration 3161 : loss : 0.123563, loss_ce: 0.008209, loss_dice: 0.238917
[08:08:36.371] TRAIN: iteration 3162 : loss : 0.155987, loss_ce: 0.013209, loss_dice: 0.298765
[08:08:38.823] TRAIN: iteration 3163 : loss : 0.062299, loss_ce: 0.005600, loss_dice: 0.118998
[08:08:39.034] TRAIN: iteration 3164 : loss : 0.140310, loss_ce: 0.008169, loss_dice: 0.272451
[08:08:39.242] TRAIN: iteration 3165 : loss : 0.152674, loss_ce: 0.009235, loss_dice: 0.296113
[08:08:39.450] TRAIN: iteration 3166 : loss : 0.181942, loss_ce: 0.010410, loss_dice: 0.353473
[08:08:39.659] TRAIN: iteration 3167 : loss : 0.152304, loss_ce: 0.024825, loss_dice: 0.279783
[08:08:39.866] TRAIN: iteration 3168 : loss : 0.218178, loss_ce: 0.018610, loss_dice: 0.417746
[08:08:40.073] TRAIN: iteration 3169 : loss : 0.104321, loss_ce: 0.007233, loss_dice: 0.201410
[08:08:40.280] TRAIN: iteration 3170 : loss : 0.251731, loss_ce: 0.010100, loss_dice: 0.493362
[08:08:43.255] TRAIN: iteration 3171 : loss : 0.254186, loss_ce: 0.008924, loss_dice: 0.499447
[08:08:43.463] TRAIN: iteration 3172 : loss : 0.200355, loss_ce: 0.028877, loss_dice: 0.371834
[08:08:43.670] TRAIN: iteration 3173 : loss : 0.125542, loss_ce: 0.009333, loss_dice: 0.241751
[08:08:43.884] TRAIN: iteration 3174 : loss : 0.138338, loss_ce: 0.009982, loss_dice: 0.266695
[08:08:44.095] TRAIN: iteration 3175 : loss : 0.221191, loss_ce: 0.013877, loss_dice: 0.428506
[08:08:44.302] TRAIN: iteration 3176 : loss : 0.254877, loss_ce: 0.009127, loss_dice: 0.500628
[08:08:44.510] TRAIN: iteration 3177 : loss : 0.088786, loss_ce: 0.008897, loss_dice: 0.168674
[08:08:44.717] TRAIN: iteration 3178 : loss : 0.254894, loss_ce: 0.009151, loss_dice: 0.500637
[08:08:47.842] TRAIN: iteration 3179 : loss : 0.254701, loss_ce: 0.010871, loss_dice: 0.498531
[08:08:48.062] TRAIN: iteration 3180 : loss : 0.255283, loss_ce: 0.009820, loss_dice: 0.500746
[08:08:48.299] TRAIN: iteration 3181 : loss : 0.192374, loss_ce: 0.007429, loss_dice: 0.377318
[08:08:48.506] TRAIN: iteration 3182 : loss : 0.067590, loss_ce: 0.008314, loss_dice: 0.126866
[08:08:48.714] TRAIN: iteration 3183 : loss : 0.250750, loss_ce: 0.009693, loss_dice: 0.491808
[08:08:48.922] TRAIN: iteration 3184 : loss : 0.089104, loss_ce: 0.006789, loss_dice: 0.171418
[08:08:49.703] TRAIN: iteration 3185 : loss : 0.174499, loss_ce: 0.012521, loss_dice: 0.336476
[08:08:49.910] TRAIN: iteration 3186 : loss : 0.116098, loss_ce: 0.009490, loss_dice: 0.222706
[08:08:54.532] TRAIN: iteration 3187 : loss : 0.127087, loss_ce: 0.007764, loss_dice: 0.246409
[08:08:54.742] TRAIN: iteration 3188 : loss : 0.242490, loss_ce: 0.007040, loss_dice: 0.477941
[08:08:54.949] TRAIN: iteration 3189 : loss : 0.175299, loss_ce: 0.015962, loss_dice: 0.334637
[08:08:55.156] TRAIN: iteration 3190 : loss : 0.068724, loss_ce: 0.005405, loss_dice: 0.132042
[08:08:55.363] TRAIN: iteration 3191 : loss : 0.223414, loss_ce: 0.007414, loss_dice: 0.439415
[08:08:55.570] TRAIN: iteration 3192 : loss : 0.109580, loss_ce: 0.007718, loss_dice: 0.211441
[08:08:56.323] TRAIN: iteration 3193 : loss : 0.253279, loss_ce: 0.006087, loss_dice: 0.500471
[08:08:56.531] TRAIN: iteration 3194 : loss : 0.102169, loss_ce: 0.009379, loss_dice: 0.194959
[08:09:01.866] TRAIN: iteration 3195 : loss : 0.230633, loss_ce: 0.005909, loss_dice: 0.455358
[08:09:02.072] TRAIN: iteration 3196 : loss : 0.098724, loss_ce: 0.004009, loss_dice: 0.193440
[08:09:02.277] TRAIN: iteration 3197 : loss : 0.226817, loss_ce: 0.019385, loss_dice: 0.434249
[08:09:02.484] TRAIN: iteration 3198 : loss : 0.245253, loss_ce: 0.004113, loss_dice: 0.486393
[08:09:02.690] TRAIN: iteration 3199 : loss : 0.258085, loss_ce: 0.041848, loss_dice: 0.474321
[08:09:02.897] TRAIN: iteration 3200 : loss : 0.047467, loss_ce: 0.003776, loss_dice: 0.091158
[08:09:03.247] TRAIN: iteration 3201 : loss : 0.236808, loss_ce: 0.005521, loss_dice: 0.468096
[08:09:03.455] TRAIN: iteration 3202 : loss : 0.223938, loss_ce: 0.004145, loss_dice: 0.443730
[08:09:06.865] TRAIN: iteration 3203 : loss : 0.150224, loss_ce: 0.004168, loss_dice: 0.296280
[08:09:07.074] TRAIN: iteration 3204 : loss : 0.191175, loss_ce: 0.011252, loss_dice: 0.371097
[08:09:07.282] TRAIN: iteration 3205 : loss : 0.087862, loss_ce: 0.006282, loss_dice: 0.169441
[08:09:07.489] TRAIN: iteration 3206 : loss : 0.112854, loss_ce: 0.005931, loss_dice: 0.219777
[08:09:07.695] TRAIN: iteration 3207 : loss : 0.148345, loss_ce: 0.005495, loss_dice: 0.291195
[08:09:07.902] TRAIN: iteration 3208 : loss : 0.194460, loss_ce: 0.008003, loss_dice: 0.380918
[08:09:08.811] TRAIN: iteration 3209 : loss : 0.204300, loss_ce: 0.004469, loss_dice: 0.404131
[08:09:09.018] TRAIN: iteration 3210 : loss : 0.250861, loss_ce: 0.005393, loss_dice: 0.496330
[08:09:11.499] TRAIN: iteration 3211 : loss : 0.194902, loss_ce: 0.005376, loss_dice: 0.384429
[08:09:11.705] TRAIN: iteration 3212 : loss : 0.187862, loss_ce: 0.003638, loss_dice: 0.372085
[08:09:11.913] TRAIN: iteration 3213 : loss : 0.111374, loss_ce: 0.004690, loss_dice: 0.218058
[08:09:12.120] TRAIN: iteration 3214 : loss : 0.222808, loss_ce: 0.005716, loss_dice: 0.439900
[08:09:12.327] TRAIN: iteration 3215 : loss : 0.048958, loss_ce: 0.003139, loss_dice: 0.094777
[08:09:12.533] TRAIN: iteration 3216 : loss : 0.152429, loss_ce: 0.011684, loss_dice: 0.293174
[08:09:16.633] TRAIN: iteration 3217 : loss : 0.138830, loss_ce: 0.007469, loss_dice: 0.270192
[08:09:16.848] TRAIN: iteration 3218 : loss : 0.201986, loss_ce: 0.006276, loss_dice: 0.397695
[08:09:17.057] TRAIN: iteration 3219 : loss : 0.224826, loss_ce: 0.004896, loss_dice: 0.444756
[08:09:17.269] TRAIN: iteration 3220 : loss : 0.143374, loss_ce: 0.014130, loss_dice: 0.272618
[08:09:17.506] TRAIN: iteration 3221 : loss : 0.199491, loss_ce: 0.021257, loss_dice: 0.377726
[08:09:17.712] TRAIN: iteration 3222 : loss : 0.252694, loss_ce: 0.007834, loss_dice: 0.497553
[08:09:17.924] TRAIN: iteration 3223 : loss : 0.180472, loss_ce: 0.006469, loss_dice: 0.354474
[08:09:18.134] TRAIN: iteration 3224 : loss : 0.220428, loss_ce: 0.015662, loss_dice: 0.425194
[08:09:24.211] TRAIN: iteration 3225 : loss : 0.219662, loss_ce: 0.015787, loss_dice: 0.423538
[08:09:24.418] TRAIN: iteration 3226 : loss : 0.253349, loss_ce: 0.011062, loss_dice: 0.495636
[08:09:24.625] TRAIN: iteration 3227 : loss : 0.115864, loss_ce: 0.008137, loss_dice: 0.223591
[08:09:24.832] TRAIN: iteration 3228 : loss : 0.126267, loss_ce: 0.006810, loss_dice: 0.245724
[08:09:25.039] TRAIN: iteration 3229 : loss : 0.125694, loss_ce: 0.007026, loss_dice: 0.244362
[08:09:25.246] TRAIN: iteration 3230 : loss : 0.196923, loss_ce: 0.009889, loss_dice: 0.383957
[08:09:25.452] TRAIN: iteration 3231 : loss : 0.100052, loss_ce: 0.008670, loss_dice: 0.191434
[08:09:25.659] TRAIN: iteration 3232 : loss : 0.184805, loss_ce: 0.008160, loss_dice: 0.361450
[08:09:29.369] TRAIN: iteration 3233 : loss : 0.213565, loss_ce: 0.006695, loss_dice: 0.420435
[08:09:29.575] TRAIN: iteration 3234 : loss : 0.135085, loss_ce: 0.008476, loss_dice: 0.261694
[08:09:29.784] TRAIN: iteration 3235 : loss : 0.070398, loss_ce: 0.005680, loss_dice: 0.135117
[08:09:29.991] TRAIN: iteration 3236 : loss : 0.252224, loss_ce: 0.005743, loss_dice: 0.498706
[08:09:30.198] TRAIN: iteration 3237 : loss : 0.168683, loss_ce: 0.011943, loss_dice: 0.325423
[08:09:30.409] TRAIN: iteration 3238 : loss : 0.252748, loss_ce: 0.005227, loss_dice: 0.500270
[08:09:30.616] TRAIN: iteration 3239 : loss : 0.241660, loss_ce: 0.005673, loss_dice: 0.477648
[08:09:30.822] TRAIN: iteration 3240 : loss : 0.216895, loss_ce: 0.004731, loss_dice: 0.429058
[08:09:35.181] TRAIN: iteration 3241 : loss : 0.192299, loss_ce: 0.005059, loss_dice: 0.379538
[08:09:35.388] TRAIN: iteration 3242 : loss : 0.246492, loss_ce: 0.011897, loss_dice: 0.481087
[08:09:35.596] TRAIN: iteration 3243 : loss : 0.253513, loss_ce: 0.021843, loss_dice: 0.485182
[08:09:35.804] TRAIN: iteration 3244 : loss : 0.244410, loss_ce: 0.044273, loss_dice: 0.444547
[08:09:36.014] TRAIN: iteration 3245 : loss : 0.098125, loss_ce: 0.005983, loss_dice: 0.190267
[08:09:36.221] TRAIN: iteration 3246 : loss : 0.205803, loss_ce: 0.016505, loss_dice: 0.395102
[08:09:36.427] TRAIN: iteration 3247 : loss : 0.120250, loss_ce: 0.011705, loss_dice: 0.228794
[08:09:36.636] TRAIN: iteration 3248 : loss : 0.225184, loss_ce: 0.004946, loss_dice: 0.445422
[08:09:43.623] TRAIN: iteration 3249 : loss : 0.067353, loss_ce: 0.004151, loss_dice: 0.130555
[08:09:43.830] TRAIN: iteration 3250 : loss : 0.176374, loss_ce: 0.005776, loss_dice: 0.346972
[08:09:44.037] TRAIN: iteration 3251 : loss : 0.231837, loss_ce: 0.006320, loss_dice: 0.457354
[08:09:44.244] TRAIN: iteration 3252 : loss : 0.111232, loss_ce: 0.005143, loss_dice: 0.217320
[08:09:44.452] TRAIN: iteration 3253 : loss : 0.242443, loss_ce: 0.005707, loss_dice: 0.479179
[08:09:44.660] TRAIN: iteration 3254 : loss : 0.125499, loss_ce: 0.006072, loss_dice: 0.244926
[08:09:44.866] TRAIN: iteration 3255 : loss : 0.142842, loss_ce: 0.008901, loss_dice: 0.276783
[08:09:45.073] TRAIN: iteration 3256 : loss : 0.172461, loss_ce: 0.005440, loss_dice: 0.339482
[08:09:48.572] TRAIN: iteration 3257 : loss : 0.252331, loss_ce: 0.005293, loss_dice: 0.499369
[08:09:48.779] TRAIN: iteration 3258 : loss : 0.089969, loss_ce: 0.007559, loss_dice: 0.172378
[08:09:48.986] TRAIN: iteration 3259 : loss : 0.116701, loss_ce: 0.004870, loss_dice: 0.228532
[08:09:49.195] TRAIN: iteration 3260 : loss : 0.103561, loss_ce: 0.007244, loss_dice: 0.199877
[08:09:49.432] TRAIN: iteration 3261 : loss : 0.132475, loss_ce: 0.005798, loss_dice: 0.259152
[08:09:49.638] TRAIN: iteration 3262 : loss : 0.143036, loss_ce: 0.005904, loss_dice: 0.280168
[08:09:49.844] TRAIN: iteration 3263 : loss : 0.211978, loss_ce: 0.010570, loss_dice: 0.413387
[08:09:50.051] TRAIN: iteration 3264 : loss : 0.212532, loss_ce: 0.028020, loss_dice: 0.397045
[08:09:56.118] TRAIN: iteration 3265 : loss : 0.181319, loss_ce: 0.006580, loss_dice: 0.356058
[08:09:56.324] TRAIN: iteration 3266 : loss : 0.128283, loss_ce: 0.014056, loss_dice: 0.242510
[08:09:56.531] TRAIN: iteration 3267 : loss : 0.123743, loss_ce: 0.006850, loss_dice: 0.240636
[08:09:56.738] TRAIN: iteration 3268 : loss : 0.208374, loss_ce: 0.005898, loss_dice: 0.410850
[08:09:56.946] TRAIN: iteration 3269 : loss : 0.089967, loss_ce: 0.004134, loss_dice: 0.175799
[08:09:57.160] TRAIN: iteration 3270 : loss : 0.123244, loss_ce: 0.005489, loss_dice: 0.240998
[08:09:57.368] TRAIN: iteration 3271 : loss : 0.253246, loss_ce: 0.006067, loss_dice: 0.500424
[08:09:57.575] TRAIN: iteration 3272 : loss : 0.170041, loss_ce: 0.024082, loss_dice: 0.316001
[08:10:04.380] TRAIN: iteration 3273 : loss : 0.252284, loss_ce: 0.007515, loss_dice: 0.497053
[08:10:04.590] TRAIN: iteration 3274 : loss : 0.148479, loss_ce: 0.011812, loss_dice: 0.285146
[08:10:04.797] TRAIN: iteration 3275 : loss : 0.122511, loss_ce: 0.006343, loss_dice: 0.238679
[08:10:05.005] TRAIN: iteration 3276 : loss : 0.240616, loss_ce: 0.008619, loss_dice: 0.472612
[08:10:05.214] TRAIN: iteration 3277 : loss : 0.136872, loss_ce: 0.006346, loss_dice: 0.267397
[08:10:05.423] TRAIN: iteration 3278 : loss : 0.252598, loss_ce: 0.004901, loss_dice: 0.500294
[08:10:05.631] TRAIN: iteration 3279 : loss : 0.092053, loss_ce: 0.006482, loss_dice: 0.177625
[08:10:05.860] TRAIN: iteration 3280 : loss : 0.182366, loss_ce: 0.005099, loss_dice: 0.359633
[08:10:11.286] TRAIN: iteration 3281 : loss : 0.156476, loss_ce: 0.012653, loss_dice: 0.300299
[08:10:11.493] TRAIN: iteration 3282 : loss : 0.148757, loss_ce: 0.014286, loss_dice: 0.283227
[08:10:11.699] TRAIN: iteration 3283 : loss : 0.128158, loss_ce: 0.007327, loss_dice: 0.248989
[08:10:11.906] TRAIN: iteration 3284 : loss : 0.187787, loss_ce: 0.006160, loss_dice: 0.369413
[08:10:12.113] TRAIN: iteration 3285 : loss : 0.170261, loss_ce: 0.005426, loss_dice: 0.335096
[08:10:12.319] TRAIN: iteration 3286 : loss : 0.148319, loss_ce: 0.006113, loss_dice: 0.290526
[08:10:12.535] TRAIN: iteration 3287 : loss : 0.158999, loss_ce: 0.006649, loss_dice: 0.311349
[08:10:12.744] TRAIN: iteration 3288 : loss : 0.177492, loss_ce: 0.026848, loss_dice: 0.328136
[08:10:17.789] TRAIN: iteration 3289 : loss : 0.218042, loss_ce: 0.004947, loss_dice: 0.431138
[08:10:18.000] TRAIN: iteration 3290 : loss : 0.056754, loss_ce: 0.003940, loss_dice: 0.109568
[08:10:18.207] TRAIN: iteration 3291 : loss : 0.139593, loss_ce: 0.003660, loss_dice: 0.275527
[08:10:18.414] TRAIN: iteration 3292 : loss : 0.156204, loss_ce: 0.017019, loss_dice: 0.295388
[08:10:18.625] TRAIN: iteration 3293 : loss : 0.178552, loss_ce: 0.004383, loss_dice: 0.352721
[08:10:18.832] TRAIN: iteration 3294 : loss : 0.244951, loss_ce: 0.005617, loss_dice: 0.484284
[08:10:19.038] TRAIN: iteration 3295 : loss : 0.169299, loss_ce: 0.004063, loss_dice: 0.334535
[08:10:19.247] TRAIN: iteration 3296 : loss : 0.151358, loss_ce: 0.006685, loss_dice: 0.296031
[08:10:22.592] TRAIN: iteration 3297 : loss : 0.166233, loss_ce: 0.014709, loss_dice: 0.317757
[08:10:22.805] TRAIN: iteration 3298 : loss : 0.180231, loss_ce: 0.021483, loss_dice: 0.338980
[08:10:23.012] TRAIN: iteration 3299 : loss : 0.159263, loss_ce: 0.008495, loss_dice: 0.310032
[08:10:23.219] TRAIN: iteration 3300 : loss : 0.186080, loss_ce: 0.004835, loss_dice: 0.367325
[08:10:23.455] TRAIN: iteration 3301 : loss : 0.223076, loss_ce: 0.009455, loss_dice: 0.436696
[08:10:23.662] TRAIN: iteration 3302 : loss : 0.156069, loss_ce: 0.018584, loss_dice: 0.293554
[08:10:24.101] TRAIN: iteration 3303 : loss : 0.114769, loss_ce: 0.003950, loss_dice: 0.225587
[08:10:24.310] TRAIN: iteration 3304 : loss : 0.089528, loss_ce: 0.003968, loss_dice: 0.175088
[08:10:30.307] TRAIN: iteration 3305 : loss : 0.167458, loss_ce: 0.005746, loss_dice: 0.329170
[08:10:30.513] TRAIN: iteration 3306 : loss : 0.124286, loss_ce: 0.008200, loss_dice: 0.240372
[08:10:30.719] TRAIN: iteration 3307 : loss : 0.122391, loss_ce: 0.008357, loss_dice: 0.236425
[08:10:30.926] TRAIN: iteration 3308 : loss : 0.168528, loss_ce: 0.023844, loss_dice: 0.313213
[08:10:31.133] TRAIN: iteration 3309 : loss : 0.197446, loss_ce: 0.007288, loss_dice: 0.387605
[08:10:31.340] TRAIN: iteration 3310 : loss : 0.252950, loss_ce: 0.005551, loss_dice: 0.500349
[08:10:31.546] TRAIN: iteration 3311 : loss : 0.111265, loss_ce: 0.006869, loss_dice: 0.215660
[08:10:31.753] TRAIN: iteration 3312 : loss : 0.251281, loss_ce: 0.008355, loss_dice: 0.494208
[08:10:35.942] TRAIN: iteration 3313 : loss : 0.209821, loss_ce: 0.019094, loss_dice: 0.400547
[08:10:36.149] TRAIN: iteration 3314 : loss : 0.259474, loss_ce: 0.020101, loss_dice: 0.498847
[08:10:36.357] TRAIN: iteration 3315 : loss : 0.219481, loss_ce: 0.010468, loss_dice: 0.428493
[08:10:36.565] TRAIN: iteration 3316 : loss : 0.197955, loss_ce: 0.011833, loss_dice: 0.384076
[08:10:36.772] TRAIN: iteration 3317 : loss : 0.242531, loss_ce: 0.013012, loss_dice: 0.472050
[08:10:36.978] TRAIN: iteration 3318 : loss : 0.255485, loss_ce: 0.010244, loss_dice: 0.500727
[08:10:40.128] TRAIN: iteration 3319 : loss : 0.248087, loss_ce: 0.011364, loss_dice: 0.484811
[08:10:40.336] TRAIN: iteration 3320 : loss : 0.173834, loss_ce: 0.012227, loss_dice: 0.335441
[08:10:43.255] TRAIN: iteration 3321 : loss : 0.253411, loss_ce: 0.013283, loss_dice: 0.493538
[08:10:43.463] TRAIN: iteration 3322 : loss : 0.183229, loss_ce: 0.011021, loss_dice: 0.355436
[08:10:43.670] TRAIN: iteration 3323 : loss : 0.229374, loss_ce: 0.012432, loss_dice: 0.446316
[08:10:43.877] TRAIN: iteration 3324 : loss : 0.254646, loss_ce: 0.010382, loss_dice: 0.498909
[08:10:44.084] TRAIN: iteration 3325 : loss : 0.233093, loss_ce: 0.011643, loss_dice: 0.454542
[08:10:44.292] TRAIN: iteration 3326 : loss : 0.181243, loss_ce: 0.012791, loss_dice: 0.349696
[08:10:45.935] TRAIN: iteration 3327 : loss : 0.123342, loss_ce: 0.008736, loss_dice: 0.237949
[08:10:46.142] TRAIN: iteration 3328 : loss : 0.137857, loss_ce: 0.006976, loss_dice: 0.268737
[08:10:48.800] TRAIN: iteration 3329 : loss : 0.157470, loss_ce: 0.008142, loss_dice: 0.306798
[08:10:49.009] TRAIN: iteration 3330 : loss : 0.149228, loss_ce: 0.011542, loss_dice: 0.286914
[08:10:49.517] TRAIN: iteration 3331 : loss : 0.081459, loss_ce: 0.006104, loss_dice: 0.156814
[08:10:52.027] TRAIN: iteration 3332 : loss : 0.218920, loss_ce: 0.008246, loss_dice: 0.429594
[08:10:52.235] TRAIN: iteration 3333 : loss : 0.214257, loss_ce: 0.006574, loss_dice: 0.421940
[08:10:52.442] TRAIN: iteration 3334 : loss : 0.232643, loss_ce: 0.007279, loss_dice: 0.458008
[08:10:54.294] TRAIN: iteration 3335 : loss : 0.083167, loss_ce: 0.006815, loss_dice: 0.159520
[08:10:54.501] TRAIN: iteration 3336 : loss : 0.178227, loss_ce: 0.004253, loss_dice: 0.352200
[08:10:56.803] TRAIN: iteration 3337 : loss : 0.155893, loss_ce: 0.009397, loss_dice: 0.302388
[08:10:57.015] TRAIN: iteration 3338 : loss : 0.224349, loss_ce: 0.011205, loss_dice: 0.437494
[08:10:58.292] TRAIN: iteration 3339 : loss : 0.117363, loss_ce: 0.004546, loss_dice: 0.230179
[08:11:00.926] TRAIN: iteration 3340 : loss : 0.209279, loss_ce: 0.041834, loss_dice: 0.376723
[08:11:01.162] TRAIN: iteration 3341 : loss : 0.194085, loss_ce: 0.021702, loss_dice: 0.366469
[08:11:01.370] TRAIN: iteration 3342 : loss : 0.083113, loss_ce: 0.004812, loss_dice: 0.161414
[08:11:01.837] TRAIN: iteration 3343 : loss : 0.215352, loss_ce: 0.004232, loss_dice: 0.426473
[08:11:02.045] TRAIN: iteration 3344 : loss : 0.152609, loss_ce: 0.004872, loss_dice: 0.300346
[08:11:04.248] TRAIN: iteration 3345 : loss : 0.198640, loss_ce: 0.031468, loss_dice: 0.365811
[08:11:04.461] TRAIN: iteration 3346 : loss : 0.133587, loss_ce: 0.007615, loss_dice: 0.259560
[08:11:04.833] TRAIN: iteration 3347 : loss : 0.100891, loss_ce: 0.004980, loss_dice: 0.196802
[08:11:09.747] TRAIN: iteration 3348 : loss : 0.190265, loss_ce: 0.007592, loss_dice: 0.372938
[08:11:09.953] TRAIN: iteration 3349 : loss : 0.165456, loss_ce: 0.010164, loss_dice: 0.320748
[08:11:10.160] TRAIN: iteration 3350 : loss : 0.218373, loss_ce: 0.008196, loss_dice: 0.428549
[08:11:12.736] TRAIN: iteration 3351 : loss : 0.161532, loss_ce: 0.006099, loss_dice: 0.316966
[08:11:12.942] TRAIN: iteration 3352 : loss : 0.252946, loss_ce: 0.006402, loss_dice: 0.499490
[08:11:13.152] TRAIN: iteration 3353 : loss : 0.130335, loss_ce: 0.007788, loss_dice: 0.252883
[08:11:13.359] TRAIN: iteration 3354 : loss : 0.152565, loss_ce: 0.006016, loss_dice: 0.299113
[08:11:13.566] TRAIN: iteration 3355 : loss : 0.116207, loss_ce: 0.007629, loss_dice: 0.224786
[08:11:18.344] TRAIN: iteration 3356 : loss : 0.253364, loss_ce: 0.006342, loss_dice: 0.500386
[08:11:18.555] TRAIN: iteration 3357 : loss : 0.181610, loss_ce: 0.011149, loss_dice: 0.352071
[08:11:18.762] TRAIN: iteration 3358 : loss : 0.175886, loss_ce: 0.010053, loss_dice: 0.341719
[08:11:20.714] TRAIN: iteration 3359 : loss : 0.127919, loss_ce: 0.007657, loss_dice: 0.248182
[08:11:20.925] TRAIN: iteration 3360 : loss : 0.253566, loss_ce: 0.006748, loss_dice: 0.500384
[08:11:21.478] TRAIN: iteration 3361 : loss : 0.103827, loss_ce: 0.006632, loss_dice: 0.201022
[08:11:21.685] TRAIN: iteration 3362 : loss : 0.123888, loss_ce: 0.012360, loss_dice: 0.235415
[08:11:21.891] TRAIN: iteration 3363 : loss : 0.244242, loss_ce: 0.005769, loss_dice: 0.482715
[08:11:25.491] TRAIN: iteration 3364 : loss : 0.155109, loss_ce: 0.010822, loss_dice: 0.299396
[08:11:25.701] TRAIN: iteration 3365 : loss : 0.179699, loss_ce: 0.011608, loss_dice: 0.347789
[08:11:25.907] TRAIN: iteration 3366 : loss : 0.133310, loss_ce: 0.012413, loss_dice: 0.254208
[08:11:27.330] TRAIN: iteration 3367 : loss : 0.211734, loss_ce: 0.006124, loss_dice: 0.417345
[08:11:27.538] TRAIN: iteration 3368 : loss : 0.137983, loss_ce: 0.007701, loss_dice: 0.268266
[08:11:28.151] TRAIN: iteration 3369 : loss : 0.253884, loss_ce: 0.007314, loss_dice: 0.500455
[08:11:28.358] TRAIN: iteration 3370 : loss : 0.241451, loss_ce: 0.008378, loss_dice: 0.474525
[08:11:28.566] TRAIN: iteration 3371 : loss : 0.172018, loss_ce: 0.006882, loss_dice: 0.337154
[08:11:33.912] TRAIN: iteration 3372 : loss : 0.122996, loss_ce: 0.007326, loss_dice: 0.238666
[08:11:34.118] TRAIN: iteration 3373 : loss : 0.173207, loss_ce: 0.005552, loss_dice: 0.340863
[08:11:34.327] TRAIN: iteration 3374 : loss : 0.101704, loss_ce: 0.008986, loss_dice: 0.194423
[08:11:35.898] TRAIN: iteration 3375 : loss : 0.243980, loss_ce: 0.006286, loss_dice: 0.481674
[08:11:36.109] TRAIN: iteration 3376 : loss : 0.167907, loss_ce: 0.005498, loss_dice: 0.330317
[08:11:37.128] TRAIN: iteration 3377 : loss : 0.245450, loss_ce: 0.005421, loss_dice: 0.485479
[08:11:37.335] TRAIN: iteration 3378 : loss : 0.224898, loss_ce: 0.007396, loss_dice: 0.442400
[08:11:37.541] TRAIN: iteration 3379 : loss : 0.142944, loss_ce: 0.010304, loss_dice: 0.275584
[08:11:42.790] TRAIN: iteration 3380 : loss : 0.135508, loss_ce: 0.003981, loss_dice: 0.267036
[08:11:43.028] TRAIN: iteration 3381 : loss : 0.098655, loss_ce: 0.005114, loss_dice: 0.192196
[08:11:43.235] TRAIN: iteration 3382 : loss : 0.232394, loss_ce: 0.003933, loss_dice: 0.460854
[08:11:44.263] TRAIN: iteration 3383 : loss : 0.169229, loss_ce: 0.005917, loss_dice: 0.332540
[08:11:44.470] TRAIN: iteration 3384 : loss : 0.119415, loss_ce: 0.007407, loss_dice: 0.231422
[08:11:45.851] TRAIN: iteration 3385 : loss : 0.085325, loss_ce: 0.004082, loss_dice: 0.166568
[08:11:47.403] TRAIN: iteration 3386 : loss : 0.210941, loss_ce: 0.026222, loss_dice: 0.395660
[08:11:47.610] TRAIN: iteration 3387 : loss : 0.252162, loss_ce: 0.004107, loss_dice: 0.500217
[08:11:52.151] TRAIN: iteration 3388 : loss : 0.188234, loss_ce: 0.010487, loss_dice: 0.365981
[08:11:52.358] TRAIN: iteration 3389 : loss : 0.252456, loss_ce: 0.004609, loss_dice: 0.500302
[08:11:52.569] TRAIN: iteration 3390 : loss : 0.090089, loss_ce: 0.004929, loss_dice: 0.175250
[08:11:53.261] TRAIN: iteration 3391 : loss : 0.109550, loss_ce: 0.005430, loss_dice: 0.213671
[08:11:53.472] TRAIN: iteration 3392 : loss : 0.211149, loss_ce: 0.004483, loss_dice: 0.417815
[08:11:54.390] TRAIN: iteration 3393 : loss : 0.138731, loss_ce: 0.004985, loss_dice: 0.272476
[08:11:56.524] TRAIN: iteration 3394 : loss : 0.117411, loss_ce: 0.015930, loss_dice: 0.218893
[08:11:56.734] TRAIN: iteration 3395 : loss : 0.126826, loss_ce: 0.007145, loss_dice: 0.246508
[08:12:02.846] TRAIN: iteration 3396 : loss : 0.219803, loss_ce: 0.006077, loss_dice: 0.433529
[08:12:03.053] TRAIN: iteration 3397 : loss : 0.058533, loss_ce: 0.004351, loss_dice: 0.112714
[08:12:03.260] TRAIN: iteration 3398 : loss : 0.152534, loss_ce: 0.007209, loss_dice: 0.297860
[08:12:05.083] TRAIN: iteration 3399 : loss : 0.245522, loss_ce: 0.007308, loss_dice: 0.483737
[08:12:05.290] TRAIN: iteration 3400 : loss : 0.254496, loss_ce: 0.008821, loss_dice: 0.500170
[08:12:05.290] NaN or Inf found in input tensor.
[08:12:05.503] TRAIN: iteration 3401 : loss : 0.235333, loss_ce: 0.025879, loss_dice: 0.444787
[08:12:06.836] TRAIN: iteration 3402 : loss : 0.136864, loss_ce: 0.020708, loss_dice: 0.253020
[08:12:07.043] TRAIN: iteration 3403 : loss : 0.150783, loss_ce: 0.006507, loss_dice: 0.295059
[08:12:10.987] TRAIN: iteration 3404 : loss : 0.189243, loss_ce: 0.005864, loss_dice: 0.372622
[08:12:11.193] TRAIN: iteration 3405 : loss : 0.180632, loss_ce: 0.006369, loss_dice: 0.354896
[08:12:11.400] TRAIN: iteration 3406 : loss : 0.127442, loss_ce: 0.013224, loss_dice: 0.241659
[08:12:15.194] TRAIN: iteration 3407 : loss : 0.086462, loss_ce: 0.004493, loss_dice: 0.168431
[08:12:15.401] TRAIN: iteration 3408 : loss : 0.253621, loss_ce: 0.006723, loss_dice: 0.500518
[08:12:15.607] TRAIN: iteration 3409 : loss : 0.253296, loss_ce: 0.006377, loss_dice: 0.500216
[08:12:15.813] TRAIN: iteration 3410 : loss : 0.133965, loss_ce: 0.008374, loss_dice: 0.259556
[08:12:16.020] TRAIN: iteration 3411 : loss : 0.098740, loss_ce: 0.005434, loss_dice: 0.192045
[08:12:20.933] TRAIN: iteration 3412 : loss : 0.183839, loss_ce: 0.010932, loss_dice: 0.356745
[08:12:21.145] TRAIN: iteration 3413 : loss : 0.147291, loss_ce: 0.004823, loss_dice: 0.289759
[08:12:21.351] TRAIN: iteration 3414 : loss : 0.141013, loss_ce: 0.005484, loss_dice: 0.276542
[08:12:23.850] TRAIN: iteration 3415 : loss : 0.178736, loss_ce: 0.008495, loss_dice: 0.348978
[08:12:24.056] TRAIN: iteration 3416 : loss : 0.052823, loss_ce: 0.003505, loss_dice: 0.102141
[08:12:24.263] TRAIN: iteration 3417 : loss : 0.249381, loss_ce: 0.005795, loss_dice: 0.492967
[08:12:25.511] TRAIN: iteration 3418 : loss : 0.216528, loss_ce: 0.007952, loss_dice: 0.425104
[08:12:25.718] TRAIN: iteration 3419 : loss : 0.189899, loss_ce: 0.048912, loss_dice: 0.330886
[08:12:27.968] TRAIN: iteration 3420 : loss : 0.182335, loss_ce: 0.009081, loss_dice: 0.355590
[08:12:28.202] TRAIN: iteration 3421 : loss : 0.136274, loss_ce: 0.017517, loss_dice: 0.255031
[08:12:28.409] TRAIN: iteration 3422 : loss : 0.201095, loss_ce: 0.005984, loss_dice: 0.396206
[08:12:33.744] TRAIN: iteration 3423 : loss : 0.138219, loss_ce: 0.007505, loss_dice: 0.268933
[08:12:33.956] TRAIN: iteration 3424 : loss : 0.163761, loss_ce: 0.005382, loss_dice: 0.322139
[08:12:34.163] TRAIN: iteration 3425 : loss : 0.252378, loss_ce: 0.010932, loss_dice: 0.493825
[08:12:36.216] TRAIN: iteration 3426 : loss : 0.133917, loss_ce: 0.015907, loss_dice: 0.251927
[08:12:36.423] TRAIN: iteration 3427 : loss : 0.232290, loss_ce: 0.007540, loss_dice: 0.457040
[08:12:36.631] TRAIN: iteration 3428 : loss : 0.229835, loss_ce: 0.006350, loss_dice: 0.453320
[08:12:36.837] TRAIN: iteration 3429 : loss : 0.117951, loss_ce: 0.014089, loss_dice: 0.221813
[08:12:37.046] TRAIN: iteration 3430 : loss : 0.216467, loss_ce: 0.007982, loss_dice: 0.424952
[08:12:42.800] TRAIN: iteration 3431 : loss : 0.119303, loss_ce: 0.012097, loss_dice: 0.226510
[08:12:43.014] TRAIN: iteration 3432 : loss : 0.251462, loss_ce: 0.007084, loss_dice: 0.495840
[08:12:44.298] TRAIN: iteration 3433 : loss : 0.181536, loss_ce: 0.012526, loss_dice: 0.350547
[08:12:46.821] TRAIN: iteration 3434 : loss : 0.205225, loss_ce: 0.014906, loss_dice: 0.395545
[08:12:47.036] TRAIN: iteration 3435 : loss : 0.138153, loss_ce: 0.009084, loss_dice: 0.267221
[08:12:47.243] TRAIN: iteration 3436 : loss : 0.112125, loss_ce: 0.007815, loss_dice: 0.216434
[08:12:47.450] TRAIN: iteration 3437 : loss : 0.104345, loss_ce: 0.007701, loss_dice: 0.200990
[08:12:47.657] TRAIN: iteration 3438 : loss : 0.138202, loss_ce: 0.009843, loss_dice: 0.266562
[08:12:51.603] TRAIN: iteration 3439 : loss : 0.250699, loss_ce: 0.009732, loss_dice: 0.491666
[08:12:51.810] TRAIN: iteration 3440 : loss : 0.071839, loss_ce: 0.006199, loss_dice: 0.137478
[08:12:55.624] TRAIN: iteration 3441 : loss : 0.156511, loss_ce: 0.009178, loss_dice: 0.303844
[08:12:58.435] TRAIN: iteration 3442 : loss : 0.246270, loss_ce: 0.009523, loss_dice: 0.483018
[08:12:58.642] TRAIN: iteration 3443 : loss : 0.216939, loss_ce: 0.010379, loss_dice: 0.423499
[08:12:58.848] TRAIN: iteration 3444 : loss : 0.184202, loss_ce: 0.010079, loss_dice: 0.358325
[08:12:59.055] TRAIN: iteration 3445 : loss : 0.096089, loss_ce: 0.006930, loss_dice: 0.185248
[08:12:59.262] TRAIN: iteration 3446 : loss : 0.086773, loss_ce: 0.009985, loss_dice: 0.163562
[08:13:01.484] TRAIN: iteration 3447 : loss : 0.146951, loss_ce: 0.007853, loss_dice: 0.286048
[08:13:01.690] TRAIN: iteration 3448 : loss : 0.254625, loss_ce: 0.009777, loss_dice: 0.499473
[08:13:04.477] TRAIN: iteration 3449 : loss : 0.118911, loss_ce: 0.011615, loss_dice: 0.226207
[08:13:08.663] TRAIN: iteration 3450 : loss : 0.103464, loss_ce: 0.006775, loss_dice: 0.200153
[08:13:08.870] TRAIN: iteration 3451 : loss : 0.253497, loss_ce: 0.006561, loss_dice: 0.500434
[08:13:09.077] TRAIN: iteration 3452 : loss : 0.150295, loss_ce: 0.007848, loss_dice: 0.292742
[08:13:09.283] TRAIN: iteration 3453 : loss : 0.240064, loss_ce: 0.006163, loss_dice: 0.473965
[08:13:09.492] TRAIN: iteration 3454 : loss : 0.252988, loss_ce: 0.005638, loss_dice: 0.500339
[08:13:11.501] TRAIN: iteration 3455 : loss : 0.141193, loss_ce: 0.004329, loss_dice: 0.278056
[08:13:11.708] TRAIN: iteration 3456 : loss : 0.117582, loss_ce: 0.005263, loss_dice: 0.229902
[08:13:13.900] TRAIN: iteration 3457 : loss : 0.169227, loss_ce: 0.019547, loss_dice: 0.318908
[08:13:18.667] TRAIN: iteration 3458 : loss : 0.144468, loss_ce: 0.006379, loss_dice: 0.282557
[08:13:18.875] TRAIN: iteration 3459 : loss : 0.236141, loss_ce: 0.004016, loss_dice: 0.468266
[08:13:19.129] TRAIN: iteration 3460 : loss : 0.162952, loss_ce: 0.003599, loss_dice: 0.322305
[08:13:19.130] NaN or Inf found in input tensor.
[08:13:19.348] TRAIN: iteration 3461 : loss : 0.211812, loss_ce: 0.003749, loss_dice: 0.419875
[08:13:19.557] TRAIN: iteration 3462 : loss : 0.248254, loss_ce: 0.006507, loss_dice: 0.490001
[08:13:19.764] TRAIN: iteration 3463 : loss : 0.185265, loss_ce: 0.004873, loss_dice: 0.365657
[08:13:19.971] TRAIN: iteration 3464 : loss : 0.130677, loss_ce: 0.009481, loss_dice: 0.251873
[08:13:25.187] TRAIN: iteration 3465 : loss : 0.164576, loss_ce: 0.012856, loss_dice: 0.316295
[08:13:27.998] TRAIN: iteration 3466 : loss : 0.140207, loss_ce: 0.002919, loss_dice: 0.277494
[08:13:28.204] TRAIN: iteration 3467 : loss : 0.251690, loss_ce: 0.003188, loss_dice: 0.500192
[08:13:28.410] TRAIN: iteration 3468 : loss : 0.224129, loss_ce: 0.005118, loss_dice: 0.443139
[08:13:28.616] TRAIN: iteration 3469 : loss : 0.135809, loss_ce: 0.003708, loss_dice: 0.267911
[08:13:28.716] TRAIN: iteration 3470 : loss : 0.125918, loss_ce: 0.003540, loss_dice: 0.248296
[08:19:39.828] VALIDATION: iteration 1 : loss : 0.180516, loss_ce: 0.007531, loss_dice: 0.353500
[08:19:41.466] TRAIN: iteration 3471 : loss : 0.193346, loss_ce: 0.005933, loss_dice: 0.380758
[08:19:42.665] TRAIN: iteration 3472 : loss : 0.111292, loss_ce: 0.005245, loss_dice: 0.217338
[08:19:42.873] TRAIN: iteration 3473 : loss : 0.192560, loss_ce: 0.010099, loss_dice: 0.375020
[08:19:43.080] TRAIN: iteration 3474 : loss : 0.199349, loss_ce: 0.006415, loss_dice: 0.392282
[08:19:43.290] TRAIN: iteration 3475 : loss : 0.230548, loss_ce: 0.006716, loss_dice: 0.454381
[08:19:43.497] TRAIN: iteration 3476 : loss : 0.214738, loss_ce: 0.005250, loss_dice: 0.424227
[08:19:43.708] TRAIN: iteration 3477 : loss : 0.113878, loss_ce: 0.013142, loss_dice: 0.214615
[08:19:43.921] TRAIN: iteration 3478 : loss : 0.147183, loss_ce: 0.005841, loss_dice: 0.288525
[08:19:44.137] TRAIN: iteration 3479 : loss : 0.156365, loss_ce: 0.006145, loss_dice: 0.306584
[08:19:44.346] TRAIN: iteration 3480 : loss : 0.253076, loss_ce: 0.005787, loss_dice: 0.500364
[08:19:44.586] TRAIN: iteration 3481 : loss : 0.197684, loss_ce: 0.006688, loss_dice: 0.388680
[08:19:44.797] TRAIN: iteration 3482 : loss : 0.249272, loss_ce: 0.006774, loss_dice: 0.491771
[08:19:45.010] TRAIN: iteration 3483 : loss : 0.163453, loss_ce: 0.007662, loss_dice: 0.319244
[08:19:45.218] TRAIN: iteration 3484 : loss : 0.139689, loss_ce: 0.017995, loss_dice: 0.261383
[08:19:45.426] TRAIN: iteration 3485 : loss : 0.145784, loss_ce: 0.006620, loss_dice: 0.284948
[08:19:45.637] TRAIN: iteration 3486 : loss : 0.253400, loss_ce: 0.006396, loss_dice: 0.500404
[08:19:45.845] TRAIN: iteration 3487 : loss : 0.130275, loss_ce: 0.006286, loss_dice: 0.254264
[08:19:46.054] TRAIN: iteration 3488 : loss : 0.081650, loss_ce: 0.008724, loss_dice: 0.154576
[08:19:46.263] TRAIN: iteration 3489 : loss : 0.058626, loss_ce: 0.005306, loss_dice: 0.111946
[08:19:46.479] TRAIN: iteration 3490 : loss : 0.098318, loss_ce: 0.006367, loss_dice: 0.190270
[08:19:46.687] TRAIN: iteration 3491 : loss : 0.218433, loss_ce: 0.010091, loss_dice: 0.426775
[08:19:46.896] TRAIN: iteration 3492 : loss : 0.238021, loss_ce: 0.005157, loss_dice: 0.470884
[08:19:47.113] TRAIN: iteration 3493 : loss : 0.165890, loss_ce: 0.005331, loss_dice: 0.326449
[08:19:47.325] TRAIN: iteration 3494 : loss : 0.197464, loss_ce: 0.007174, loss_dice: 0.387753
[08:19:47.540] TRAIN: iteration 3495 : loss : 0.153709, loss_ce: 0.004358, loss_dice: 0.303060
[08:19:47.752] TRAIN: iteration 3496 : loss : 0.241693, loss_ce: 0.004972, loss_dice: 0.478415
[08:19:47.962] TRAIN: iteration 3497 : loss : 0.136211, loss_ce: 0.009410, loss_dice: 0.263011
[08:19:48.175] TRAIN: iteration 3498 : loss : 0.250340, loss_ce: 0.004824, loss_dice: 0.495855
[08:19:48.382] TRAIN: iteration 3499 : loss : 0.176105, loss_ce: 0.009842, loss_dice: 0.342368
[08:19:48.590] TRAIN: iteration 3500 : loss : 0.071481, loss_ce: 0.005103, loss_dice: 0.137860
[08:19:48.826] TRAIN: iteration 3501 : loss : 0.114238, loss_ce: 0.004429, loss_dice: 0.224047
[08:19:49.052] TRAIN: iteration 3502 : loss : 0.146045, loss_ce: 0.004007, loss_dice: 0.288084
[08:19:49.719] TRAIN: iteration 3503 : loss : 0.094441, loss_ce: 0.006392, loss_dice: 0.182489
[08:19:49.926] TRAIN: iteration 3504 : loss : 0.075624, loss_ce: 0.004560, loss_dice: 0.146687
[08:19:50.136] TRAIN: iteration 3505 : loss : 0.102804, loss_ce: 0.012004, loss_dice: 0.193604
[08:19:50.344] TRAIN: iteration 3506 : loss : 0.158039, loss_ce: 0.017670, loss_dice: 0.298408
[08:19:50.560] TRAIN: iteration 3507 : loss : 0.137300, loss_ce: 0.005955, loss_dice: 0.268646
[08:19:50.768] TRAIN: iteration 3508 : loss : 0.115329, loss_ce: 0.005684, loss_dice: 0.224974
[08:19:50.975] TRAIN: iteration 3509 : loss : 0.116207, loss_ce: 0.005202, loss_dice: 0.227213
[08:19:51.260] TRAIN: iteration 3510 : loss : 0.159889, loss_ce: 0.007415, loss_dice: 0.312363
[08:19:51.467] TRAIN: iteration 3511 : loss : 0.173006, loss_ce: 0.005843, loss_dice: 0.340170
[08:19:51.674] TRAIN: iteration 3512 : loss : 0.252728, loss_ce: 0.006222, loss_dice: 0.499234
[08:19:51.887] TRAIN: iteration 3513 : loss : 0.188057, loss_ce: 0.010447, loss_dice: 0.365668
[08:19:52.096] TRAIN: iteration 3514 : loss : 0.140074, loss_ce: 0.007144, loss_dice: 0.273004
[08:19:52.304] TRAIN: iteration 3515 : loss : 0.160078, loss_ce: 0.009148, loss_dice: 0.311009
[08:19:52.517] TRAIN: iteration 3516 : loss : 0.255590, loss_ce: 0.012806, loss_dice: 0.498375
[08:19:52.725] TRAIN: iteration 3517 : loss : 0.252025, loss_ce: 0.005520, loss_dice: 0.498530
[08:19:52.937] TRAIN: iteration 3518 : loss : 0.235248, loss_ce: 0.007305, loss_dice: 0.463191
[08:19:53.157] TRAIN: iteration 3519 : loss : 0.228108, loss_ce: 0.005712, loss_dice: 0.450503
[08:19:53.367] TRAIN: iteration 3520 : loss : 0.174746, loss_ce: 0.006189, loss_dice: 0.343304
[08:19:53.603] TRAIN: iteration 3521 : loss : 0.234624, loss_ce: 0.005493, loss_dice: 0.463756
[08:19:53.811] TRAIN: iteration 3522 : loss : 0.094797, loss_ce: 0.007586, loss_dice: 0.182009
[08:19:54.020] TRAIN: iteration 3523 : loss : 0.137443, loss_ce: 0.006034, loss_dice: 0.268851
[08:19:54.230] TRAIN: iteration 3524 : loss : 0.245279, loss_ce: 0.008849, loss_dice: 0.481709
[08:19:54.438] TRAIN: iteration 3525 : loss : 0.129972, loss_ce: 0.013166, loss_dice: 0.246778
[08:19:54.645] TRAIN: iteration 3526 : loss : 0.156926, loss_ce: 0.004234, loss_dice: 0.309617
[08:19:54.855] TRAIN: iteration 3527 : loss : 0.089128, loss_ce: 0.006360, loss_dice: 0.171896
[08:19:55.064] TRAIN: iteration 3528 : loss : 0.252873, loss_ce: 0.005375, loss_dice: 0.500372
[08:19:55.272] TRAIN: iteration 3529 : loss : 0.121570, loss_ce: 0.005165, loss_dice: 0.237975
[08:19:55.480] TRAIN: iteration 3530 : loss : 0.090645, loss_ce: 0.003520, loss_dice: 0.177771
[08:19:55.691] TRAIN: iteration 3531 : loss : 0.252301, loss_ce: 0.004353, loss_dice: 0.500249
[08:19:55.899] TRAIN: iteration 3532 : loss : 0.201428, loss_ce: 0.003866, loss_dice: 0.398991
[08:19:56.107] TRAIN: iteration 3533 : loss : 0.240827, loss_ce: 0.009730, loss_dice: 0.471924
[08:19:56.314] TRAIN: iteration 3534 : loss : 0.247496, loss_ce: 0.003669, loss_dice: 0.491323
[08:19:56.529] TRAIN: iteration 3535 : loss : 0.195464, loss_ce: 0.005155, loss_dice: 0.385773
[08:19:56.737] TRAIN: iteration 3536 : loss : 0.251332, loss_ce: 0.002566, loss_dice: 0.500097
[08:19:56.947] TRAIN: iteration 3537 : loss : 0.133842, loss_ce: 0.013127, loss_dice: 0.254557
[08:19:57.157] TRAIN: iteration 3538 : loss : 0.124685, loss_ce: 0.006489, loss_dice: 0.242880
[08:19:57.366] TRAIN: iteration 3539 : loss : 0.159512, loss_ce: 0.003520, loss_dice: 0.315503
[08:19:57.575] TRAIN: iteration 3540 : loss : 0.195126, loss_ce: 0.008386, loss_dice: 0.381867
[08:19:57.813] TRAIN: iteration 3541 : loss : 0.246777, loss_ce: 0.003732, loss_dice: 0.489822
[08:19:58.021] TRAIN: iteration 3542 : loss : 0.252445, loss_ce: 0.004580, loss_dice: 0.500310
[08:19:58.232] TRAIN: iteration 3543 : loss : 0.200447, loss_ce: 0.004423, loss_dice: 0.396470
[08:19:58.441] TRAIN: iteration 3544 : loss : 0.206285, loss_ce: 0.004081, loss_dice: 0.408488
[08:19:58.652] TRAIN: iteration 3545 : loss : 0.151760, loss_ce: 0.003946, loss_dice: 0.299574
[08:19:58.860] TRAIN: iteration 3546 : loss : 0.251997, loss_ce: 0.003779, loss_dice: 0.500215
[08:19:59.070] TRAIN: iteration 3547 : loss : 0.101534, loss_ce: 0.003524, loss_dice: 0.199544
[08:19:59.282] TRAIN: iteration 3548 : loss : 0.195048, loss_ce: 0.004115, loss_dice: 0.385981
[08:19:59.490] TRAIN: iteration 3549 : loss : 0.147653, loss_ce: 0.004742, loss_dice: 0.290565
[08:19:59.699] TRAIN: iteration 3550 : loss : 0.251716, loss_ce: 0.003257, loss_dice: 0.500175
[08:19:59.907] TRAIN: iteration 3551 : loss : 0.165306, loss_ce: 0.003769, loss_dice: 0.326843
[08:20:00.118] TRAIN: iteration 3552 : loss : 0.183001, loss_ce: 0.009432, loss_dice: 0.356569
[08:20:00.326] TRAIN: iteration 3553 : loss : 0.200194, loss_ce: 0.005207, loss_dice: 0.395181
[08:20:00.536] TRAIN: iteration 3554 : loss : 0.129301, loss_ce: 0.006932, loss_dice: 0.251669
[08:20:00.743] TRAIN: iteration 3555 : loss : 0.102559, loss_ce: 0.004364, loss_dice: 0.200754
[08:20:00.950] TRAIN: iteration 3556 : loss : 0.242479, loss_ce: 0.005747, loss_dice: 0.479211
[08:20:01.384] TRAIN: iteration 3557 : loss : 0.249044, loss_ce: 0.006063, loss_dice: 0.492024
[08:20:01.591] TRAIN: iteration 3558 : loss : 0.108792, loss_ce: 0.004418, loss_dice: 0.213165
[08:20:01.799] TRAIN: iteration 3559 : loss : 0.220059, loss_ce: 0.006263, loss_dice: 0.433856
[08:20:02.010] TRAIN: iteration 3560 : loss : 0.168382, loss_ce: 0.006300, loss_dice: 0.330464
[08:20:02.248] TRAIN: iteration 3561 : loss : 0.225746, loss_ce: 0.006750, loss_dice: 0.444741
[08:20:02.456] TRAIN: iteration 3562 : loss : 0.253140, loss_ce: 0.005849, loss_dice: 0.500431
[08:20:02.666] TRAIN: iteration 3563 : loss : 0.162952, loss_ce: 0.006590, loss_dice: 0.319314
[08:20:02.873] TRAIN: iteration 3564 : loss : 0.200353, loss_ce: 0.007272, loss_dice: 0.393434
[08:20:05.084] TRAIN: iteration 3565 : loss : 0.142747, loss_ce: 0.008123, loss_dice: 0.277370
[08:20:05.293] TRAIN: iteration 3566 : loss : 0.118095, loss_ce: 0.005452, loss_dice: 0.230737
[08:20:05.504] TRAIN: iteration 3567 : loss : 0.208826, loss_ce: 0.007382, loss_dice: 0.410270
[08:20:05.718] TRAIN: iteration 3568 : loss : 0.141179, loss_ce: 0.009767, loss_dice: 0.272591
[08:20:05.933] TRAIN: iteration 3569 : loss : 0.151960, loss_ce: 0.007039, loss_dice: 0.296881
[08:20:06.144] TRAIN: iteration 3570 : loss : 0.252983, loss_ce: 0.005585, loss_dice: 0.500382
[08:20:06.353] TRAIN: iteration 3571 : loss : 0.252737, loss_ce: 0.005112, loss_dice: 0.500363
[08:20:06.567] TRAIN: iteration 3572 : loss : 0.247487, loss_ce: 0.012658, loss_dice: 0.482317
[08:20:06.776] TRAIN: iteration 3573 : loss : 0.171306, loss_ce: 0.005465, loss_dice: 0.337146
[08:20:06.985] TRAIN: iteration 3574 : loss : 0.110484, loss_ce: 0.006611, loss_dice: 0.214358
[08:20:07.192] TRAIN: iteration 3575 : loss : 0.107621, loss_ce: 0.004153, loss_dice: 0.211089
[08:20:07.400] TRAIN: iteration 3576 : loss : 0.188221, loss_ce: 0.006709, loss_dice: 0.369734
[08:20:07.610] TRAIN: iteration 3577 : loss : 0.252318, loss_ce: 0.004369, loss_dice: 0.500267
[08:20:07.819] TRAIN: iteration 3578 : loss : 0.076958, loss_ce: 0.003607, loss_dice: 0.150309
[08:20:08.027] TRAIN: iteration 3579 : loss : 0.228036, loss_ce: 0.004503, loss_dice: 0.451569
[08:20:08.235] TRAIN: iteration 3580 : loss : 0.139509, loss_ce: 0.003444, loss_dice: 0.275574
[08:20:08.472] TRAIN: iteration 3581 : loss : 0.147859, loss_ce: 0.004687, loss_dice: 0.291032
[08:20:08.679] TRAIN: iteration 3582 : loss : 0.156152, loss_ce: 0.004241, loss_dice: 0.308062
[08:20:08.888] TRAIN: iteration 3583 : loss : 0.165245, loss_ce: 0.003722, loss_dice: 0.326768
[08:20:09.099] TRAIN: iteration 3584 : loss : 0.246240, loss_ce: 0.018706, loss_dice: 0.473774
[08:20:09.309] TRAIN: iteration 3585 : loss : 0.248550, loss_ce: 0.010445, loss_dice: 0.486655
[08:20:09.517] TRAIN: iteration 3586 : loss : 0.241592, loss_ce: 0.002336, loss_dice: 0.480848
[08:20:09.724] TRAIN: iteration 3587 : loss : 0.167837, loss_ce: 0.008544, loss_dice: 0.327130
[08:20:09.931] TRAIN: iteration 3588 : loss : 0.251190, loss_ce: 0.002293, loss_dice: 0.500087
[08:20:10.141] TRAIN: iteration 3589 : loss : 0.068543, loss_ce: 0.002891, loss_dice: 0.134194
[08:20:10.356] TRAIN: iteration 3590 : loss : 0.149661, loss_ce: 0.006840, loss_dice: 0.292481
[08:20:10.564] TRAIN: iteration 3591 : loss : 0.094954, loss_ce: 0.003655, loss_dice: 0.186254
[08:20:10.778] TRAIN: iteration 3592 : loss : 0.236431, loss_ce: 0.003400, loss_dice: 0.469462
[08:20:10.990] TRAIN: iteration 3593 : loss : 0.155533, loss_ce: 0.007758, loss_dice: 0.303307
[08:20:11.199] TRAIN: iteration 3594 : loss : 0.249773, loss_ce: 0.003996, loss_dice: 0.495550
[08:20:11.410] TRAIN: iteration 3595 : loss : 0.224636, loss_ce: 0.003905, loss_dice: 0.445366
[08:20:11.619] TRAIN: iteration 3596 : loss : 0.110305, loss_ce: 0.004823, loss_dice: 0.215787
[08:20:11.831] TRAIN: iteration 3597 : loss : 0.085633, loss_ce: 0.004624, loss_dice: 0.166642
[08:20:12.039] TRAIN: iteration 3598 : loss : 0.101987, loss_ce: 0.005160, loss_dice: 0.198813
[08:20:12.247] TRAIN: iteration 3599 : loss : 0.150281, loss_ce: 0.005796, loss_dice: 0.294766
[08:20:12.464] TRAIN: iteration 3600 : loss : 0.175840, loss_ce: 0.005342, loss_dice: 0.346339
[08:20:12.699] TRAIN: iteration 3601 : loss : 0.123023, loss_ce: 0.009099, loss_dice: 0.236948
[08:20:12.907] TRAIN: iteration 3602 : loss : 0.192274, loss_ce: 0.010736, loss_dice: 0.373812
[08:20:13.115] TRAIN: iteration 3603 : loss : 0.177597, loss_ce: 0.006859, loss_dice: 0.348335
[08:20:13.321] TRAIN: iteration 3604 : loss : 0.136073, loss_ce: 0.006784, loss_dice: 0.265362
[08:20:13.529] TRAIN: iteration 3605 : loss : 0.101168, loss_ce: 0.008382, loss_dice: 0.193954
[08:20:13.736] TRAIN: iteration 3606 : loss : 0.141700, loss_ce: 0.012716, loss_dice: 0.270684
[08:20:13.950] TRAIN: iteration 3607 : loss : 0.214489, loss_ce: 0.006603, loss_dice: 0.422375
[08:20:14.162] TRAIN: iteration 3608 : loss : 0.171253, loss_ce: 0.019357, loss_dice: 0.323149
[08:20:14.371] TRAIN: iteration 3609 : loss : 0.182545, loss_ce: 0.007467, loss_dice: 0.357623
[08:20:14.581] TRAIN: iteration 3610 : loss : 0.156104, loss_ce: 0.033520, loss_dice: 0.278688
[08:20:14.789] TRAIN: iteration 3611 : loss : 0.204562, loss_ce: 0.008290, loss_dice: 0.400834
[08:20:15.001] TRAIN: iteration 3612 : loss : 0.212780, loss_ce: 0.007573, loss_dice: 0.417986
[08:20:15.210] TRAIN: iteration 3613 : loss : 0.154005, loss_ce: 0.006693, loss_dice: 0.301317
[08:20:15.419] TRAIN: iteration 3614 : loss : 0.106719, loss_ce: 0.009436, loss_dice: 0.204003
[08:20:15.640] TRAIN: iteration 3615 : loss : 0.173086, loss_ce: 0.008704, loss_dice: 0.337469
[08:20:15.848] TRAIN: iteration 3616 : loss : 0.242569, loss_ce: 0.005289, loss_dice: 0.479850
[08:20:16.057] TRAIN: iteration 3617 : loss : 0.154564, loss_ce: 0.005531, loss_dice: 0.303597
[08:20:16.268] TRAIN: iteration 3618 : loss : 0.139094, loss_ce: 0.006049, loss_dice: 0.272140
[08:20:16.482] TRAIN: iteration 3619 : loss : 0.131898, loss_ce: 0.015891, loss_dice: 0.247905
[08:20:16.689] TRAIN: iteration 3620 : loss : 0.217964, loss_ce: 0.015158, loss_dice: 0.420771
[08:20:16.931] TRAIN: iteration 3621 : loss : 0.085210, loss_ce: 0.009090, loss_dice: 0.161330
[08:20:17.148] TRAIN: iteration 3622 : loss : 0.084586, loss_ce: 0.007251, loss_dice: 0.161922
[08:20:17.356] TRAIN: iteration 3623 : loss : 0.089499, loss_ce: 0.005331, loss_dice: 0.173666
[08:20:17.564] TRAIN: iteration 3624 : loss : 0.253892, loss_ce: 0.008124, loss_dice: 0.499660
[08:20:17.782] TRAIN: iteration 3625 : loss : 0.086599, loss_ce: 0.010831, loss_dice: 0.162366
[08:20:17.990] TRAIN: iteration 3626 : loss : 0.163235, loss_ce: 0.006914, loss_dice: 0.319555
[08:20:18.200] TRAIN: iteration 3627 : loss : 0.093429, loss_ce: 0.004198, loss_dice: 0.182660
[08:20:18.411] TRAIN: iteration 3628 : loss : 0.117266, loss_ce: 0.008327, loss_dice: 0.226206
[08:20:18.619] TRAIN: iteration 3629 : loss : 0.181198, loss_ce: 0.004743, loss_dice: 0.357653
[08:20:18.831] TRAIN: iteration 3630 : loss : 0.166537, loss_ce: 0.006029, loss_dice: 0.327044
[08:20:19.039] TRAIN: iteration 3631 : loss : 0.130441, loss_ce: 0.013617, loss_dice: 0.247265
[08:20:19.254] TRAIN: iteration 3632 : loss : 0.193875, loss_ce: 0.009004, loss_dice: 0.378747
[08:20:19.461] TRAIN: iteration 3633 : loss : 0.252567, loss_ce: 0.004814, loss_dice: 0.500319
[08:20:19.669] TRAIN: iteration 3634 : loss : 0.246564, loss_ce: 0.006255, loss_dice: 0.486873
[08:20:19.878] TRAIN: iteration 3635 : loss : 0.118438, loss_ce: 0.004998, loss_dice: 0.231878
[08:20:20.089] TRAIN: iteration 3636 : loss : 0.131702, loss_ce: 0.003925, loss_dice: 0.259480
[08:20:20.296] TRAIN: iteration 3637 : loss : 0.178505, loss_ce: 0.015890, loss_dice: 0.341120
[08:20:20.504] TRAIN: iteration 3638 : loss : 0.145480, loss_ce: 0.008702, loss_dice: 0.282257
[08:20:20.714] TRAIN: iteration 3639 : loss : 0.118035, loss_ce: 0.006871, loss_dice: 0.229199
[08:20:20.923] TRAIN: iteration 3640 : loss : 0.095965, loss_ce: 0.006290, loss_dice: 0.185641
[08:20:21.171] TRAIN: iteration 3641 : loss : 0.231532, loss_ce: 0.006299, loss_dice: 0.456764
[08:20:21.379] TRAIN: iteration 3642 : loss : 0.153864, loss_ce: 0.014170, loss_dice: 0.293558
[08:20:21.586] TRAIN: iteration 3643 : loss : 0.176381, loss_ce: 0.011553, loss_dice: 0.341209
[08:20:21.797] TRAIN: iteration 3644 : loss : 0.134664, loss_ce: 0.005571, loss_dice: 0.263758
[08:20:22.003] TRAIN: iteration 3645 : loss : 0.145975, loss_ce: 0.004655, loss_dice: 0.287296
[08:20:22.220] TRAIN: iteration 3646 : loss : 0.189414, loss_ce: 0.004941, loss_dice: 0.373886
[08:20:22.431] TRAIN: iteration 3647 : loss : 0.136420, loss_ce: 0.006468, loss_dice: 0.266373
[08:20:22.640] TRAIN: iteration 3648 : loss : 0.244199, loss_ce: 0.005596, loss_dice: 0.482801
[08:20:22.850] TRAIN: iteration 3649 : loss : 0.106244, loss_ce: 0.006809, loss_dice: 0.205680
[08:20:23.059] TRAIN: iteration 3650 : loss : 0.073473, loss_ce: 0.003976, loss_dice: 0.142970
[08:20:23.267] TRAIN: iteration 3651 : loss : 0.119080, loss_ce: 0.010910, loss_dice: 0.227250
[08:20:23.475] TRAIN: iteration 3652 : loss : 0.252872, loss_ce: 0.005385, loss_dice: 0.500359
[08:20:24.096] TRAIN: iteration 3653 : loss : 0.252961, loss_ce: 0.005563, loss_dice: 0.500360
[08:20:24.308] TRAIN: iteration 3654 : loss : 0.252817, loss_ce: 0.005294, loss_dice: 0.500340
[08:20:24.521] TRAIN: iteration 3655 : loss : 0.161455, loss_ce: 0.008241, loss_dice: 0.314669
[08:20:24.746] TRAIN: iteration 3656 : loss : 0.183901, loss_ce: 0.005300, loss_dice: 0.362501
[08:20:24.956] TRAIN: iteration 3657 : loss : 0.079651, loss_ce: 0.004860, loss_dice: 0.154442
[08:20:25.165] TRAIN: iteration 3658 : loss : 0.252957, loss_ce: 0.005541, loss_dice: 0.500373
[08:20:25.375] TRAIN: iteration 3659 : loss : 0.161465, loss_ce: 0.004283, loss_dice: 0.318647
[08:20:25.612] TRAIN: iteration 3660 : loss : 0.115626, loss_ce: 0.010312, loss_dice: 0.220939
[08:20:25.969] TRAIN: iteration 3661 : loss : 0.138875, loss_ce: 0.004614, loss_dice: 0.273136
[08:20:26.177] TRAIN: iteration 3662 : loss : 0.252524, loss_ce: 0.004723, loss_dice: 0.500325
[08:20:26.384] TRAIN: iteration 3663 : loss : 0.168638, loss_ce: 0.009525, loss_dice: 0.327751
[08:20:26.590] TRAIN: iteration 3664 : loss : 0.242232, loss_ce: 0.014461, loss_dice: 0.470002
[08:20:26.798] TRAIN: iteration 3665 : loss : 0.106094, loss_ce: 0.005147, loss_dice: 0.207042
[08:20:27.005] TRAIN: iteration 3666 : loss : 0.131049, loss_ce: 0.007147, loss_dice: 0.254951
[08:20:27.213] TRAIN: iteration 3667 : loss : 0.123344, loss_ce: 0.005459, loss_dice: 0.241230
[08:20:27.424] TRAIN: iteration 3668 : loss : 0.148258, loss_ce: 0.005533, loss_dice: 0.290982
[08:20:27.761] TRAIN: iteration 3669 : loss : 0.152218, loss_ce: 0.003137, loss_dice: 0.301300
[08:20:27.992] TRAIN: iteration 3670 : loss : 0.139103, loss_ce: 0.003395, loss_dice: 0.274811
[08:20:28.200] TRAIN: iteration 3671 : loss : 0.134746, loss_ce: 0.006470, loss_dice: 0.263021
[08:20:28.407] TRAIN: iteration 3672 : loss : 0.189074, loss_ce: 0.003850, loss_dice: 0.374298
[08:20:28.620] TRAIN: iteration 3673 : loss : 0.251999, loss_ce: 0.003909, loss_dice: 0.500089
[08:20:28.830] TRAIN: iteration 3674 : loss : 0.251510, loss_ce: 0.003034, loss_dice: 0.499987
[08:20:29.037] TRAIN: iteration 3675 : loss : 0.115022, loss_ce: 0.004429, loss_dice: 0.225615
[08:20:29.247] TRAIN: iteration 3676 : loss : 0.165432, loss_ce: 0.007778, loss_dice: 0.323085
[08:20:29.453] TRAIN: iteration 3677 : loss : 0.253490, loss_ce: 0.008813, loss_dice: 0.498166
[08:20:29.660] TRAIN: iteration 3678 : loss : 0.136028, loss_ce: 0.009733, loss_dice: 0.262324
[08:20:29.868] TRAIN: iteration 3679 : loss : 0.142681, loss_ce: 0.004979, loss_dice: 0.280384
[08:20:30.075] TRAIN: iteration 3680 : loss : 0.234716, loss_ce: 0.016314, loss_dice: 0.453119
[08:20:30.311] TRAIN: iteration 3681 : loss : 0.249646, loss_ce: 0.008169, loss_dice: 0.491123
[08:20:30.519] TRAIN: iteration 3682 : loss : 0.252092, loss_ce: 0.004190, loss_dice: 0.499994
[08:20:30.728] TRAIN: iteration 3683 : loss : 0.184708, loss_ce: 0.008383, loss_dice: 0.361033
[08:20:30.954] TRAIN: iteration 3684 : loss : 0.163359, loss_ce: 0.019561, loss_dice: 0.307157
[08:20:31.161] TRAIN: iteration 3685 : loss : 0.164158, loss_ce: 0.008066, loss_dice: 0.320250
[08:20:31.369] TRAIN: iteration 3686 : loss : 0.183234, loss_ce: 0.005985, loss_dice: 0.360483
[08:20:31.580] TRAIN: iteration 3687 : loss : 0.146446, loss_ce: 0.007242, loss_dice: 0.285650
[08:20:31.789] TRAIN: iteration 3688 : loss : 0.183943, loss_ce: 0.008273, loss_dice: 0.359613
[08:20:31.997] TRAIN: iteration 3689 : loss : 0.115903, loss_ce: 0.005034, loss_dice: 0.226772
[08:20:32.217] TRAIN: iteration 3690 : loss : 0.251277, loss_ce: 0.004162, loss_dice: 0.498393
[08:20:32.426] TRAIN: iteration 3691 : loss : 0.136802, loss_ce: 0.008431, loss_dice: 0.265172
[08:20:32.633] TRAIN: iteration 3692 : loss : 0.108295, loss_ce: 0.004881, loss_dice: 0.211708
[08:20:32.844] TRAIN: iteration 3693 : loss : 0.131683, loss_ce: 0.013373, loss_dice: 0.249993
[08:20:33.053] TRAIN: iteration 3694 : loss : 0.253197, loss_ce: 0.006007, loss_dice: 0.500387
[08:20:33.260] TRAIN: iteration 3695 : loss : 0.253289, loss_ce: 0.006142, loss_dice: 0.500435
[08:20:33.467] TRAIN: iteration 3696 : loss : 0.127592, loss_ce: 0.005447, loss_dice: 0.249737
[08:20:33.674] TRAIN: iteration 3697 : loss : 0.217899, loss_ce: 0.005742, loss_dice: 0.430057
[08:20:33.884] TRAIN: iteration 3698 : loss : 0.251061, loss_ce: 0.006797, loss_dice: 0.495324
[08:20:34.093] TRAIN: iteration 3699 : loss : 0.118899, loss_ce: 0.010036, loss_dice: 0.227762
[08:20:34.302] TRAIN: iteration 3700 : loss : 0.185690, loss_ce: 0.011360, loss_dice: 0.360019
[08:20:34.540] TRAIN: iteration 3701 : loss : 0.072231, loss_ce: 0.005659, loss_dice: 0.138803
[08:20:34.747] TRAIN: iteration 3702 : loss : 0.103555, loss_ce: 0.007158, loss_dice: 0.199952
[08:20:34.957] TRAIN: iteration 3703 : loss : 0.207160, loss_ce: 0.005224, loss_dice: 0.409096
[08:20:35.169] TRAIN: iteration 3704 : loss : 0.079194, loss_ce: 0.004278, loss_dice: 0.154110
[08:20:35.378] TRAIN: iteration 3705 : loss : 0.188088, loss_ce: 0.031988, loss_dice: 0.344187
[08:20:35.588] TRAIN: iteration 3706 : loss : 0.125068, loss_ce: 0.005675, loss_dice: 0.244461
[08:20:35.797] TRAIN: iteration 3707 : loss : 0.156880, loss_ce: 0.004490, loss_dice: 0.309269
[08:20:36.007] TRAIN: iteration 3708 : loss : 0.209020, loss_ce: 0.004266, loss_dice: 0.413774
[08:20:36.221] TRAIN: iteration 3709 : loss : 0.149481, loss_ce: 0.003906, loss_dice: 0.295056
[08:20:36.429] TRAIN: iteration 3710 : loss : 0.120466, loss_ce: 0.003125, loss_dice: 0.237806
[08:20:36.639] TRAIN: iteration 3711 : loss : 0.142426, loss_ce: 0.009176, loss_dice: 0.275676
[08:20:36.851] TRAIN: iteration 3712 : loss : 0.270614, loss_ce: 0.047248, loss_dice: 0.493979
[08:20:37.066] TRAIN: iteration 3713 : loss : 0.084907, loss_ce: 0.002386, loss_dice: 0.167428
[08:20:37.279] TRAIN: iteration 3714 : loss : 0.229510, loss_ce: 0.004442, loss_dice: 0.454579
[08:20:37.488] TRAIN: iteration 3715 : loss : 0.099581, loss_ce: 0.004176, loss_dice: 0.194986
[08:20:37.697] TRAIN: iteration 3716 : loss : 0.250946, loss_ce: 0.001840, loss_dice: 0.500052
[08:20:37.904] TRAIN: iteration 3717 : loss : 0.181299, loss_ce: 0.004260, loss_dice: 0.358338
[08:20:38.114] TRAIN: iteration 3718 : loss : 0.204919, loss_ce: 0.002931, loss_dice: 0.406907
[08:20:38.325] TRAIN: iteration 3719 : loss : 0.187517, loss_ce: 0.005375, loss_dice: 0.369660
[08:20:38.538] TRAIN: iteration 3720 : loss : 0.251600, loss_ce: 0.003021, loss_dice: 0.500179
[08:20:38.783] TRAIN: iteration 3721 : loss : 0.244327, loss_ce: 0.005078, loss_dice: 0.483575
[08:20:38.990] TRAIN: iteration 3722 : loss : 0.250716, loss_ce: 0.004576, loss_dice: 0.496855
[08:20:39.198] TRAIN: iteration 3723 : loss : 0.196754, loss_ce: 0.011909, loss_dice: 0.381599
[08:20:39.405] TRAIN: iteration 3724 : loss : 0.177327, loss_ce: 0.004633, loss_dice: 0.350021
[08:20:39.612] TRAIN: iteration 3725 : loss : 0.128050, loss_ce: 0.009846, loss_dice: 0.246253
[08:20:39.823] TRAIN: iteration 3726 : loss : 0.085360, loss_ce: 0.005466, loss_dice: 0.165255
[08:20:40.030] TRAIN: iteration 3727 : loss : 0.251549, loss_ce: 0.004997, loss_dice: 0.498102
[08:20:40.237] TRAIN: iteration 3728 : loss : 0.169302, loss_ce: 0.011660, loss_dice: 0.326943
[08:20:40.461] TRAIN: iteration 3729 : loss : 0.252889, loss_ce: 0.005423, loss_dice: 0.500355
[08:20:40.668] TRAIN: iteration 3730 : loss : 0.167630, loss_ce: 0.008364, loss_dice: 0.326896
[08:20:40.876] TRAIN: iteration 3731 : loss : 0.062488, loss_ce: 0.004861, loss_dice: 0.120114
[08:20:41.087] TRAIN: iteration 3732 : loss : 0.169174, loss_ce: 0.006778, loss_dice: 0.331569
[08:20:41.293] TRAIN: iteration 3733 : loss : 0.254105, loss_ce: 0.007654, loss_dice: 0.500557
[08:20:41.509] TRAIN: iteration 3734 : loss : 0.226740, loss_ce: 0.007952, loss_dice: 0.445529
[08:20:41.720] TRAIN: iteration 3735 : loss : 0.199699, loss_ce: 0.009474, loss_dice: 0.389924
[08:20:42.880] TRAIN: iteration 3736 : loss : 0.165008, loss_ce: 0.006692, loss_dice: 0.323323
[08:20:43.087] TRAIN: iteration 3737 : loss : 0.080820, loss_ce: 0.006745, loss_dice: 0.154895
[08:20:43.294] TRAIN: iteration 3738 : loss : 0.145808, loss_ce: 0.008782, loss_dice: 0.282835
[08:20:43.501] TRAIN: iteration 3739 : loss : 0.164520, loss_ce: 0.006616, loss_dice: 0.322424
[08:20:43.708] TRAIN: iteration 3740 : loss : 0.093896, loss_ce: 0.005227, loss_dice: 0.182565
[08:20:43.950] TRAIN: iteration 3741 : loss : 0.242994, loss_ce: 0.006876, loss_dice: 0.479113
[08:20:44.161] TRAIN: iteration 3742 : loss : 0.253303, loss_ce: 0.006176, loss_dice: 0.500431
[08:20:44.370] TRAIN: iteration 3743 : loss : 0.249655, loss_ce: 0.006930, loss_dice: 0.492380
[08:20:44.576] TRAIN: iteration 3744 : loss : 0.070890, loss_ce: 0.003620, loss_dice: 0.138160
[08:20:44.783] TRAIN: iteration 3745 : loss : 0.173609, loss_ce: 0.004877, loss_dice: 0.342342
[08:20:44.991] TRAIN: iteration 3746 : loss : 0.203984, loss_ce: 0.011024, loss_dice: 0.396943
[08:20:45.197] TRAIN: iteration 3747 : loss : 0.211510, loss_ce: 0.008253, loss_dice: 0.414766
[08:20:45.404] TRAIN: iteration 3748 : loss : 0.224159, loss_ce: 0.007130, loss_dice: 0.441189
[08:20:45.610] TRAIN: iteration 3749 : loss : 0.251514, loss_ce: 0.012810, loss_dice: 0.490218
[08:20:45.817] TRAIN: iteration 3750 : loss : 0.121896, loss_ce: 0.005857, loss_dice: 0.237935
[08:20:46.036] TRAIN: iteration 3751 : loss : 0.184965, loss_ce: 0.005950, loss_dice: 0.363980
[08:20:46.244] TRAIN: iteration 3752 : loss : 0.100030, loss_ce: 0.006597, loss_dice: 0.193462
[08:20:46.454] TRAIN: iteration 3753 : loss : 0.075908, loss_ce: 0.003673, loss_dice: 0.148142
[08:20:46.660] TRAIN: iteration 3754 : loss : 0.116097, loss_ce: 0.004353, loss_dice: 0.227841
[08:20:46.868] TRAIN: iteration 3755 : loss : 0.089235, loss_ce: 0.003116, loss_dice: 0.175353
[08:20:47.075] TRAIN: iteration 3756 : loss : 0.108240, loss_ce: 0.009537, loss_dice: 0.206944
[08:20:47.283] TRAIN: iteration 3757 : loss : 0.140208, loss_ce: 0.005169, loss_dice: 0.275247
[08:20:47.492] TRAIN: iteration 3758 : loss : 0.183144, loss_ce: 0.007514, loss_dice: 0.358773
[08:20:47.702] TRAIN: iteration 3759 : loss : 0.177753, loss_ce: 0.004087, loss_dice: 0.351420
[08:20:47.914] TRAIN: iteration 3760 : loss : 0.114821, loss_ce: 0.005199, loss_dice: 0.224442
[08:20:48.150] TRAIN: iteration 3761 : loss : 0.165655, loss_ce: 0.006353, loss_dice: 0.324956
[08:20:48.360] TRAIN: iteration 3762 : loss : 0.245372, loss_ce: 0.005230, loss_dice: 0.485514
[08:20:48.570] TRAIN: iteration 3763 : loss : 0.252795, loss_ce: 0.005221, loss_dice: 0.500368
[08:20:49.041] TRAIN: iteration 3764 : loss : 0.136962, loss_ce: 0.011094, loss_dice: 0.262829
[08:20:49.248] TRAIN: iteration 3765 : loss : 0.050893, loss_ce: 0.002987, loss_dice: 0.098799
[08:20:49.456] TRAIN: iteration 3766 : loss : 0.192708, loss_ce: 0.033712, loss_dice: 0.351703
[08:20:49.664] TRAIN: iteration 3767 : loss : 0.263807, loss_ce: 0.028793, loss_dice: 0.498821
[08:20:49.877] TRAIN: iteration 3768 : loss : 0.092626, loss_ce: 0.004388, loss_dice: 0.180864
[08:20:50.094] TRAIN: iteration 3769 : loss : 0.144053, loss_ce: 0.005334, loss_dice: 0.282773
[08:20:50.303] TRAIN: iteration 3770 : loss : 0.143832, loss_ce: 0.008689, loss_dice: 0.278976
[08:20:50.510] TRAIN: iteration 3771 : loss : 0.167874, loss_ce: 0.005198, loss_dice: 0.330549
[08:20:50.717] TRAIN: iteration 3772 : loss : 0.178087, loss_ce: 0.006968, loss_dice: 0.349207
[08:20:50.924] TRAIN: iteration 3773 : loss : 0.178381, loss_ce: 0.009845, loss_dice: 0.346918
[08:20:51.132] TRAIN: iteration 3774 : loss : 0.250928, loss_ce: 0.005124, loss_dice: 0.496732
[08:20:51.341] TRAIN: iteration 3775 : loss : 0.103329, loss_ce: 0.004699, loss_dice: 0.201960
[08:20:51.548] TRAIN: iteration 3776 : loss : 0.225091, loss_ce: 0.016909, loss_dice: 0.433273
[08:20:51.755] TRAIN: iteration 3777 : loss : 0.245437, loss_ce: 0.006208, loss_dice: 0.484665
[08:20:51.961] TRAIN: iteration 3778 : loss : 0.184565, loss_ce: 0.003965, loss_dice: 0.365166
[08:20:52.169] TRAIN: iteration 3779 : loss : 0.161372, loss_ce: 0.012904, loss_dice: 0.309840
[08:20:52.377] TRAIN: iteration 3780 : loss : 0.185653, loss_ce: 0.019273, loss_dice: 0.352033
[08:20:52.613] TRAIN: iteration 3781 : loss : 0.192754, loss_ce: 0.030803, loss_dice: 0.354706
[08:20:52.820] TRAIN: iteration 3782 : loss : 0.194708, loss_ce: 0.003923, loss_dice: 0.385494
[08:20:53.029] TRAIN: iteration 3783 : loss : 0.155130, loss_ce: 0.007568, loss_dice: 0.302692
[08:20:53.238] TRAIN: iteration 3784 : loss : 0.079613, loss_ce: 0.003631, loss_dice: 0.155595
[08:20:53.446] TRAIN: iteration 3785 : loss : 0.251691, loss_ce: 0.003272, loss_dice: 0.500109
[08:20:53.655] TRAIN: iteration 3786 : loss : 0.139734, loss_ce: 0.006983, loss_dice: 0.272486
[08:20:53.863] TRAIN: iteration 3787 : loss : 0.154451, loss_ce: 0.004798, loss_dice: 0.304104
[08:20:54.071] TRAIN: iteration 3788 : loss : 0.161591, loss_ce: 0.004154, loss_dice: 0.319028
[08:20:54.280] TRAIN: iteration 3789 : loss : 0.251002, loss_ce: 0.008531, loss_dice: 0.493473
[08:20:54.577] TRAIN: iteration 3790 : loss : 0.136643, loss_ce: 0.003972, loss_dice: 0.269314
[08:20:54.788] TRAIN: iteration 3791 : loss : 0.053889, loss_ce: 0.002658, loss_dice: 0.105119
[08:20:54.997] TRAIN: iteration 3792 : loss : 0.087564, loss_ce: 0.003805, loss_dice: 0.171324
[08:20:55.205] TRAIN: iteration 3793 : loss : 0.179390, loss_ce: 0.004038, loss_dice: 0.354742
[08:20:55.418] TRAIN: iteration 3794 : loss : 0.251239, loss_ce: 0.002389, loss_dice: 0.500089
[08:20:55.627] TRAIN: iteration 3795 : loss : 0.250165, loss_ce: 0.010703, loss_dice: 0.489627
[08:20:55.834] TRAIN: iteration 3796 : loss : 0.251396, loss_ce: 0.002628, loss_dice: 0.500164
[08:20:56.041] TRAIN: iteration 3797 : loss : 0.243085, loss_ce: 0.017139, loss_dice: 0.469031
[08:20:56.641] TRAIN: iteration 3798 : loss : 0.188769, loss_ce: 0.011998, loss_dice: 0.365540
[08:20:56.848] TRAIN: iteration 3799 : loss : 0.250182, loss_ce: 0.003266, loss_dice: 0.497099
[08:20:57.061] TRAIN: iteration 3800 : loss : 0.217496, loss_ce: 0.005197, loss_dice: 0.429796
[08:20:57.291] TRAIN: iteration 3801 : loss : 0.245484, loss_ce: 0.004384, loss_dice: 0.486583
[08:20:57.501] TRAIN: iteration 3802 : loss : 0.104223, loss_ce: 0.003784, loss_dice: 0.204661
[08:20:57.709] TRAIN: iteration 3803 : loss : 0.071220, loss_ce: 0.003599, loss_dice: 0.138841
[08:20:57.919] TRAIN: iteration 3804 : loss : 0.252386, loss_ce: 0.004480, loss_dice: 0.500292
[08:20:58.128] TRAIN: iteration 3805 : loss : 0.135916, loss_ce: 0.003760, loss_dice: 0.268073
[08:20:58.335] TRAIN: iteration 3806 : loss : 0.203803, loss_ce: 0.006308, loss_dice: 0.401298
[08:20:58.545] TRAIN: iteration 3807 : loss : 0.116215, loss_ce: 0.003478, loss_dice: 0.228952
[08:20:58.754] TRAIN: iteration 3808 : loss : 0.252180, loss_ce: 0.004112, loss_dice: 0.500248
[08:20:59.048] TRAIN: iteration 3809 : loss : 0.251631, loss_ce: 0.004576, loss_dice: 0.498687
[08:20:59.254] TRAIN: iteration 3810 : loss : 0.146417, loss_ce: 0.003596, loss_dice: 0.289238
[08:20:59.462] TRAIN: iteration 3811 : loss : 0.123352, loss_ce: 0.004453, loss_dice: 0.242251
[08:20:59.669] TRAIN: iteration 3812 : loss : 0.251731, loss_ce: 0.003294, loss_dice: 0.500169
[08:20:59.878] TRAIN: iteration 3813 : loss : 0.242055, loss_ce: 0.003290, loss_dice: 0.480819
[08:21:00.090] TRAIN: iteration 3814 : loss : 0.197863, loss_ce: 0.027620, loss_dice: 0.368105
[08:21:00.298] TRAIN: iteration 3815 : loss : 0.194419, loss_ce: 0.007045, loss_dice: 0.381793
[08:21:00.507] TRAIN: iteration 3816 : loss : 0.203694, loss_ce: 0.004579, loss_dice: 0.402809
[08:21:00.717] TRAIN: iteration 3817 : loss : 0.221304, loss_ce: 0.024276, loss_dice: 0.418331
[08:21:01.295] TRAIN: iteration 3818 : loss : 0.187461, loss_ce: 0.009067, loss_dice: 0.365856
[08:21:01.505] TRAIN: iteration 3819 : loss : 0.237513, loss_ce: 0.005392, loss_dice: 0.469634
[08:21:01.714] TRAIN: iteration 3820 : loss : 0.195974, loss_ce: 0.010968, loss_dice: 0.380981
[08:21:01.948] TRAIN: iteration 3821 : loss : 0.142620, loss_ce: 0.006240, loss_dice: 0.279000
[08:21:02.155] TRAIN: iteration 3822 : loss : 0.194682, loss_ce: 0.013135, loss_dice: 0.376228
[08:21:02.369] TRAIN: iteration 3823 : loss : 0.252370, loss_ce: 0.004473, loss_dice: 0.500268
[08:21:02.576] TRAIN: iteration 3824 : loss : 0.251092, loss_ce: 0.004895, loss_dice: 0.497289
[08:21:03.219] TRAIN: iteration 3825 : loss : 0.224145, loss_ce: 0.006153, loss_dice: 0.442137
[08:21:03.491] TRAIN: iteration 3826 : loss : 0.105315, loss_ce: 0.008279, loss_dice: 0.202352
[08:21:03.707] TRAIN: iteration 3827 : loss : 0.126764, loss_ce: 0.004884, loss_dice: 0.248643
[08:21:03.921] TRAIN: iteration 3828 : loss : 0.129842, loss_ce: 0.008062, loss_dice: 0.251622
[08:21:04.129] TRAIN: iteration 3829 : loss : 0.221314, loss_ce: 0.006252, loss_dice: 0.436376
[08:21:04.344] TRAIN: iteration 3830 : loss : 0.169204, loss_ce: 0.023116, loss_dice: 0.315293
[08:21:04.554] TRAIN: iteration 3831 : loss : 0.249583, loss_ce: 0.009165, loss_dice: 0.490000
[08:21:04.769] TRAIN: iteration 3832 : loss : 0.110563, loss_ce: 0.006681, loss_dice: 0.214445
[08:21:04.977] TRAIN: iteration 3833 : loss : 0.161211, loss_ce: 0.020319, loss_dice: 0.302102
[08:21:05.190] TRAIN: iteration 3834 : loss : 0.237719, loss_ce: 0.006468, loss_dice: 0.468970
[08:21:05.403] TRAIN: iteration 3835 : loss : 0.190272, loss_ce: 0.020177, loss_dice: 0.360366
[08:21:05.612] TRAIN: iteration 3836 : loss : 0.207516, loss_ce: 0.008467, loss_dice: 0.406566
[08:21:05.823] TRAIN: iteration 3837 : loss : 0.116194, loss_ce: 0.009325, loss_dice: 0.223064
[08:21:06.036] TRAIN: iteration 3838 : loss : 0.249536, loss_ce: 0.007782, loss_dice: 0.491289
[08:21:06.244] TRAIN: iteration 3839 : loss : 0.239213, loss_ce: 0.042982, loss_dice: 0.435444
[08:21:06.454] TRAIN: iteration 3840 : loss : 0.119082, loss_ce: 0.010391, loss_dice: 0.227774
[08:21:06.692] TRAIN: iteration 3841 : loss : 0.212489, loss_ce: 0.012485, loss_dice: 0.412493
[08:21:06.902] TRAIN: iteration 3842 : loss : 0.177029, loss_ce: 0.008433, loss_dice: 0.345625
[08:21:07.110] TRAIN: iteration 3843 : loss : 0.252866, loss_ce: 0.009099, loss_dice: 0.496633
[08:21:07.322] TRAIN: iteration 3844 : loss : 0.186916, loss_ce: 0.008612, loss_dice: 0.365219
[08:21:07.529] TRAIN: iteration 3845 : loss : 0.186783, loss_ce: 0.007890, loss_dice: 0.365675
[08:21:07.742] TRAIN: iteration 3846 : loss : 0.229864, loss_ce: 0.009480, loss_dice: 0.450248
[08:21:07.949] TRAIN: iteration 3847 : loss : 0.254914, loss_ce: 0.009257, loss_dice: 0.500571
[08:21:08.157] TRAIN: iteration 3848 : loss : 0.254742, loss_ce: 0.008901, loss_dice: 0.500583
[08:21:08.945] TRAIN: iteration 3849 : loss : 0.254060, loss_ce: 0.007645, loss_dice: 0.500476
[08:21:09.152] TRAIN: iteration 3850 : loss : 0.247033, loss_ce: 0.007626, loss_dice: 0.486441
[08:21:09.360] TRAIN: iteration 3851 : loss : 0.139830, loss_ce: 0.006997, loss_dice: 0.272664
[08:21:09.566] TRAIN: iteration 3852 : loss : 0.076953, loss_ce: 0.005076, loss_dice: 0.148831
[08:21:09.774] TRAIN: iteration 3853 : loss : 0.135009, loss_ce: 0.006569, loss_dice: 0.263448
[08:21:09.982] TRAIN: iteration 3854 : loss : 0.134645, loss_ce: 0.006366, loss_dice: 0.262923
[08:21:10.205] TRAIN: iteration 3855 : loss : 0.094753, loss_ce: 0.004626, loss_dice: 0.184880
[08:21:10.412] TRAIN: iteration 3856 : loss : 0.246769, loss_ce: 0.004571, loss_dice: 0.488967
[08:21:12.123] TRAIN: iteration 3857 : loss : 0.252646, loss_ce: 0.005054, loss_dice: 0.500238
[08:21:12.330] TRAIN: iteration 3858 : loss : 0.252335, loss_ce: 0.004393, loss_dice: 0.500276
[08:21:12.537] TRAIN: iteration 3859 : loss : 0.165089, loss_ce: 0.007592, loss_dice: 0.322585
[08:21:12.746] TRAIN: iteration 3860 : loss : 0.121297, loss_ce: 0.002962, loss_dice: 0.239633
[08:21:12.747] NaN or Inf found in input tensor.
[08:21:12.963] TRAIN: iteration 3861 : loss : 0.181222, loss_ce: 0.005479, loss_dice: 0.356964
[08:21:13.170] TRAIN: iteration 3862 : loss : 0.172213, loss_ce: 0.004606, loss_dice: 0.339820
[08:21:13.379] TRAIN: iteration 3863 : loss : 0.185132, loss_ce: 0.003459, loss_dice: 0.366805
[08:21:13.593] TRAIN: iteration 3864 : loss : 0.106563, loss_ce: 0.002730, loss_dice: 0.210396
[08:21:13.800] TRAIN: iteration 3865 : loss : 0.188414, loss_ce: 0.008135, loss_dice: 0.368694
[08:21:14.014] TRAIN: iteration 3866 : loss : 0.251423, loss_ce: 0.002664, loss_dice: 0.500181
[08:21:14.221] TRAIN: iteration 3867 : loss : 0.251127, loss_ce: 0.002150, loss_dice: 0.500104
[08:21:14.428] TRAIN: iteration 3868 : loss : 0.243182, loss_ce: 0.003249, loss_dice: 0.483114
[08:21:14.635] TRAIN: iteration 3869 : loss : 0.219704, loss_ce: 0.008875, loss_dice: 0.430533
[08:21:14.842] TRAIN: iteration 3870 : loss : 0.140763, loss_ce: 0.007349, loss_dice: 0.274177
[08:21:15.049] TRAIN: iteration 3871 : loss : 0.233488, loss_ce: 0.013639, loss_dice: 0.453337
[08:21:15.256] TRAIN: iteration 3872 : loss : 0.175798, loss_ce: 0.008503, loss_dice: 0.343094
[08:21:15.463] TRAIN: iteration 3873 : loss : 0.147909, loss_ce: 0.003957, loss_dice: 0.291861
[08:21:15.672] TRAIN: iteration 3874 : loss : 0.251553, loss_ce: 0.003049, loss_dice: 0.500056
[08:21:15.880] TRAIN: iteration 3875 : loss : 0.181291, loss_ce: 0.018304, loss_dice: 0.344278
[08:21:16.088] TRAIN: iteration 3876 : loss : 0.251780, loss_ce: 0.003403, loss_dice: 0.500157
[08:21:16.295] TRAIN: iteration 3877 : loss : 0.120987, loss_ce: 0.007725, loss_dice: 0.234249
[08:21:16.502] TRAIN: iteration 3878 : loss : 0.174662, loss_ce: 0.003785, loss_dice: 0.345539
[08:21:16.708] TRAIN: iteration 3879 : loss : 0.252335, loss_ce: 0.004383, loss_dice: 0.500287
[08:21:16.915] TRAIN: iteration 3880 : loss : 0.174085, loss_ce: 0.004450, loss_dice: 0.343719
[08:21:17.154] TRAIN: iteration 3881 : loss : 0.066644, loss_ce: 0.003211, loss_dice: 0.130077
[08:21:17.361] TRAIN: iteration 3882 : loss : 0.243789, loss_ce: 0.004739, loss_dice: 0.482840
[08:21:17.568] TRAIN: iteration 3883 : loss : 0.220412, loss_ce: 0.004776, loss_dice: 0.436049
[08:21:17.776] TRAIN: iteration 3884 : loss : 0.190678, loss_ce: 0.013235, loss_dice: 0.368121
[08:21:17.984] TRAIN: iteration 3885 : loss : 0.175216, loss_ce: 0.003495, loss_dice: 0.346937
[08:21:18.196] TRAIN: iteration 3886 : loss : 0.099205, loss_ce: 0.006261, loss_dice: 0.192149
[08:21:18.407] TRAIN: iteration 3887 : loss : 0.151818, loss_ce: 0.006968, loss_dice: 0.296668
[08:21:18.617] TRAIN: iteration 3888 : loss : 0.073219, loss_ce: 0.003136, loss_dice: 0.143302
[08:21:18.827] TRAIN: iteration 3889 : loss : 0.203242, loss_ce: 0.009431, loss_dice: 0.397053
[08:21:19.034] TRAIN: iteration 3890 : loss : 0.198060, loss_ce: 0.041196, loss_dice: 0.354924
[08:21:19.253] TRAIN: iteration 3891 : loss : 0.102797, loss_ce: 0.007358, loss_dice: 0.198236
[08:21:19.459] TRAIN: iteration 3892 : loss : 0.256468, loss_ce: 0.016035, loss_dice: 0.496901
[08:21:19.666] TRAIN: iteration 3893 : loss : 0.209357, loss_ce: 0.007700, loss_dice: 0.411015
[08:21:19.876] TRAIN: iteration 3894 : loss : 0.207896, loss_ce: 0.004901, loss_dice: 0.410891
[08:21:20.088] TRAIN: iteration 3895 : loss : 0.252787, loss_ce: 0.005234, loss_dice: 0.500340
[08:21:20.295] TRAIN: iteration 3896 : loss : 0.225196, loss_ce: 0.008825, loss_dice: 0.441567
[08:21:20.505] TRAIN: iteration 3897 : loss : 0.242496, loss_ce: 0.006568, loss_dice: 0.478425
[08:21:20.713] TRAIN: iteration 3898 : loss : 0.251986, loss_ce: 0.006522, loss_dice: 0.497450
[08:21:20.924] TRAIN: iteration 3899 : loss : 0.043579, loss_ce: 0.004072, loss_dice: 0.083087
[08:21:21.133] TRAIN: iteration 3900 : loss : 0.204109, loss_ce: 0.019823, loss_dice: 0.388395
[08:21:21.133] NaN or Inf found in input tensor.
[08:21:21.349] TRAIN: iteration 3901 : loss : 0.154682, loss_ce: 0.007762, loss_dice: 0.301602
[08:21:21.556] TRAIN: iteration 3902 : loss : 0.253252, loss_ce: 0.006161, loss_dice: 0.500343
[08:21:21.766] TRAIN: iteration 3903 : loss : 0.118412, loss_ce: 0.008806, loss_dice: 0.228019
[08:21:21.972] TRAIN: iteration 3904 : loss : 0.253954, loss_ce: 0.007421, loss_dice: 0.500487
[08:21:22.183] TRAIN: iteration 3905 : loss : 0.244918, loss_ce: 0.007779, loss_dice: 0.482056
[08:21:22.393] TRAIN: iteration 3906 : loss : 0.253374, loss_ce: 0.006348, loss_dice: 0.500399
[08:21:22.603] TRAIN: iteration 3907 : loss : 0.218975, loss_ce: 0.007559, loss_dice: 0.430392
[08:21:22.811] TRAIN: iteration 3908 : loss : 0.126555, loss_ce: 0.006699, loss_dice: 0.246410
[08:21:23.021] TRAIN: iteration 3909 : loss : 0.114318, loss_ce: 0.008318, loss_dice: 0.220319
[08:21:23.236] TRAIN: iteration 3910 : loss : 0.246732, loss_ce: 0.006102, loss_dice: 0.487362
[08:21:23.445] TRAIN: iteration 3911 : loss : 0.045358, loss_ce: 0.003980, loss_dice: 0.086736
[08:21:23.652] TRAIN: iteration 3912 : loss : 0.124889, loss_ce: 0.011663, loss_dice: 0.238116
[08:21:23.859] TRAIN: iteration 3913 : loss : 0.156715, loss_ce: 0.008199, loss_dice: 0.305231
[08:21:24.074] TRAIN: iteration 3914 : loss : 0.142356, loss_ce: 0.018442, loss_dice: 0.266270
[08:21:24.283] TRAIN: iteration 3915 : loss : 0.173688, loss_ce: 0.007007, loss_dice: 0.340369
[08:21:24.490] TRAIN: iteration 3916 : loss : 0.192967, loss_ce: 0.007455, loss_dice: 0.378480
[08:21:24.696] TRAIN: iteration 3917 : loss : 0.235109, loss_ce: 0.006746, loss_dice: 0.463472
[08:21:24.931] TRAIN: iteration 3918 : loss : 0.198538, loss_ce: 0.006299, loss_dice: 0.390778
[08:21:25.146] TRAIN: iteration 3919 : loss : 0.153810, loss_ce: 0.005499, loss_dice: 0.302122
[08:21:25.360] TRAIN: iteration 3920 : loss : 0.252952, loss_ce: 0.005609, loss_dice: 0.500294
[08:21:25.600] TRAIN: iteration 3921 : loss : 0.253156, loss_ce: 0.005942, loss_dice: 0.500370
[08:21:25.806] TRAIN: iteration 3922 : loss : 0.123417, loss_ce: 0.006040, loss_dice: 0.240795
[08:21:26.016] TRAIN: iteration 3923 : loss : 0.167661, loss_ce: 0.006185, loss_dice: 0.329137
[08:21:26.225] TRAIN: iteration 3924 : loss : 0.106213, loss_ce: 0.011273, loss_dice: 0.201152
[08:21:26.432] TRAIN: iteration 3925 : loss : 0.129179, loss_ce: 0.010924, loss_dice: 0.247435
[08:21:26.641] TRAIN: iteration 3926 : loss : 0.137408, loss_ce: 0.006083, loss_dice: 0.268733
[08:21:26.854] TRAIN: iteration 3927 : loss : 0.169904, loss_ce: 0.008351, loss_dice: 0.331458
[08:21:27.062] TRAIN: iteration 3928 : loss : 0.215713, loss_ce: 0.005631, loss_dice: 0.425796
[08:21:27.274] TRAIN: iteration 3929 : loss : 0.157278, loss_ce: 0.020785, loss_dice: 0.293771
[08:21:27.481] TRAIN: iteration 3930 : loss : 0.187631, loss_ce: 0.004868, loss_dice: 0.370395
[08:21:27.688] TRAIN: iteration 3931 : loss : 0.199749, loss_ce: 0.010285, loss_dice: 0.389213
[08:21:27.897] TRAIN: iteration 3932 : loss : 0.216636, loss_ce: 0.007848, loss_dice: 0.425424
[08:21:28.105] TRAIN: iteration 3933 : loss : 0.246105, loss_ce: 0.006207, loss_dice: 0.486003
[08:21:28.315] TRAIN: iteration 3934 : loss : 0.149093, loss_ce: 0.006815, loss_dice: 0.291372
[08:21:28.530] TRAIN: iteration 3935 : loss : 0.200031, loss_ce: 0.007035, loss_dice: 0.393027
[08:21:28.738] TRAIN: iteration 3936 : loss : 0.174287, loss_ce: 0.040815, loss_dice: 0.307759
[08:21:28.946] TRAIN: iteration 3937 : loss : 0.209006, loss_ce: 0.007297, loss_dice: 0.410715
[08:21:29.153] TRAIN: iteration 3938 : loss : 0.232550, loss_ce: 0.006917, loss_dice: 0.458182
[08:21:29.360] TRAIN: iteration 3939 : loss : 0.042498, loss_ce: 0.003977, loss_dice: 0.081018
[08:21:29.837] TRAIN: iteration 3940 : loss : 0.175928, loss_ce: 0.007450, loss_dice: 0.344406
[08:21:30.078] TRAIN: iteration 3941 : loss : 0.253346, loss_ce: 0.006306, loss_dice: 0.500387
[08:21:30.287] TRAIN: iteration 3942 : loss : 0.249877, loss_ce: 0.006591, loss_dice: 0.493162
[08:21:30.494] TRAIN: iteration 3943 : loss : 0.113103, loss_ce: 0.008663, loss_dice: 0.217543
[08:21:30.701] TRAIN: iteration 3944 : loss : 0.253774, loss_ce: 0.007073, loss_dice: 0.500474
[08:21:30.909] TRAIN: iteration 3945 : loss : 0.253980, loss_ce: 0.007471, loss_dice: 0.500489
[08:21:31.116] TRAIN: iteration 3946 : loss : 0.095266, loss_ce: 0.007475, loss_dice: 0.183057
[08:21:31.325] TRAIN: iteration 3947 : loss : 0.247538, loss_ce: 0.007175, loss_dice: 0.487902
[08:21:31.532] TRAIN: iteration 3948 : loss : 0.253308, loss_ce: 0.007350, loss_dice: 0.499265
[08:21:31.739] TRAIN: iteration 3949 : loss : 0.247107, loss_ce: 0.007043, loss_dice: 0.487172
[08:21:31.946] TRAIN: iteration 3950 : loss : 0.122632, loss_ce: 0.006719, loss_dice: 0.238544
[08:21:32.153] TRAIN: iteration 3951 : loss : 0.204785, loss_ce: 0.006782, loss_dice: 0.402788
[08:21:32.360] TRAIN: iteration 3952 : loss : 0.250547, loss_ce: 0.006713, loss_dice: 0.494380
[08:21:32.567] TRAIN: iteration 3953 : loss : 0.141468, loss_ce: 0.012490, loss_dice: 0.270445
[08:21:32.773] TRAIN: iteration 3954 : loss : 0.155211, loss_ce: 0.006497, loss_dice: 0.303925
[08:21:32.980] TRAIN: iteration 3955 : loss : 0.247982, loss_ce: 0.005553, loss_dice: 0.490412
[08:21:33.198] TRAIN: iteration 3956 : loss : 0.096854, loss_ce: 0.005663, loss_dice: 0.188046
[08:21:33.407] TRAIN: iteration 3957 : loss : 0.249813, loss_ce: 0.007242, loss_dice: 0.492385
[08:21:33.614] TRAIN: iteration 3958 : loss : 0.213382, loss_ce: 0.009263, loss_dice: 0.417500
[08:21:33.820] TRAIN: iteration 3959 : loss : 0.203916, loss_ce: 0.023349, loss_dice: 0.384483
[08:21:34.030] TRAIN: iteration 3960 : loss : 0.149874, loss_ce: 0.005409, loss_dice: 0.294339
[08:21:34.303] TRAIN: iteration 3961 : loss : 0.135190, loss_ce: 0.008471, loss_dice: 0.261908
[08:21:34.511] TRAIN: iteration 3962 : loss : 0.110349, loss_ce: 0.007006, loss_dice: 0.213692
[08:21:34.719] TRAIN: iteration 3963 : loss : 0.253013, loss_ce: 0.005705, loss_dice: 0.500322
[08:21:34.928] TRAIN: iteration 3964 : loss : 0.194960, loss_ce: 0.014411, loss_dice: 0.375508
[08:21:35.135] TRAIN: iteration 3965 : loss : 0.238342, loss_ce: 0.006864, loss_dice: 0.469821
[08:21:35.462] TRAIN: iteration 3966 : loss : 0.101389, loss_ce: 0.004594, loss_dice: 0.198185
[08:21:35.670] TRAIN: iteration 3967 : loss : 0.161192, loss_ce: 0.012922, loss_dice: 0.309462
[08:21:35.878] TRAIN: iteration 3968 : loss : 0.179213, loss_ce: 0.006131, loss_dice: 0.352294
[08:21:36.086] TRAIN: iteration 3969 : loss : 0.252273, loss_ce: 0.005782, loss_dice: 0.498764
[08:21:36.297] TRAIN: iteration 3970 : loss : 0.184677, loss_ce: 0.007733, loss_dice: 0.361621
[08:21:36.517] TRAIN: iteration 3971 : loss : 0.252787, loss_ce: 0.005308, loss_dice: 0.500265
[08:21:36.732] TRAIN: iteration 3972 : loss : 0.168716, loss_ce: 0.006108, loss_dice: 0.331325
[08:21:36.962] TRAIN: iteration 3973 : loss : 0.207409, loss_ce: 0.005608, loss_dice: 0.409210
[08:21:37.170] TRAIN: iteration 3974 : loss : 0.121169, loss_ce: 0.008644, loss_dice: 0.233695
[08:21:37.382] TRAIN: iteration 3975 : loss : 0.086332, loss_ce: 0.004027, loss_dice: 0.168637
[08:21:37.591] TRAIN: iteration 3976 : loss : 0.057107, loss_ce: 0.003303, loss_dice: 0.110911
[08:21:37.799] TRAIN: iteration 3977 : loss : 0.137007, loss_ce: 0.006533, loss_dice: 0.267482
[08:21:38.007] TRAIN: iteration 3978 : loss : 0.164706, loss_ce: 0.007505, loss_dice: 0.321906
[08:21:38.218] TRAIN: iteration 3979 : loss : 0.232974, loss_ce: 0.004469, loss_dice: 0.461480
[08:21:38.425] TRAIN: iteration 3980 : loss : 0.251629, loss_ce: 0.003127, loss_dice: 0.500132
[08:21:38.661] TRAIN: iteration 3981 : loss : 0.244480, loss_ce: 0.004987, loss_dice: 0.483973
[08:21:38.869] TRAIN: iteration 3982 : loss : 0.246561, loss_ce: 0.005375, loss_dice: 0.487746
[08:21:39.080] TRAIN: iteration 3983 : loss : 0.178663, loss_ce: 0.011605, loss_dice: 0.345721
[08:21:39.288] TRAIN: iteration 3984 : loss : 0.147801, loss_ce: 0.008330, loss_dice: 0.287272
[08:21:39.496] TRAIN: iteration 3985 : loss : 0.248553, loss_ce: 0.004008, loss_dice: 0.493098
[08:21:39.704] TRAIN: iteration 3986 : loss : 0.252276, loss_ce: 0.004308, loss_dice: 0.500244
[08:21:39.913] TRAIN: iteration 3987 : loss : 0.192622, loss_ce: 0.009060, loss_dice: 0.376184
[08:21:40.245] TRAIN: iteration 3988 : loss : 0.145119, loss_ce: 0.004942, loss_dice: 0.285296
[08:21:40.453] TRAIN: iteration 3989 : loss : 0.129905, loss_ce: 0.005342, loss_dice: 0.254467
[08:21:40.661] TRAIN: iteration 3990 : loss : 0.037116, loss_ce: 0.002574, loss_dice: 0.071658
[08:21:41.245] TRAIN: iteration 3991 : loss : 0.115973, loss_ce: 0.005922, loss_dice: 0.226025
[08:21:41.452] TRAIN: iteration 3992 : loss : 0.148998, loss_ce: 0.006259, loss_dice: 0.291738
[08:21:41.658] TRAIN: iteration 3993 : loss : 0.134151, loss_ce: 0.004278, loss_dice: 0.264023
[08:21:41.869] TRAIN: iteration 3994 : loss : 0.189485, loss_ce: 0.029309, loss_dice: 0.349661
[08:21:42.078] TRAIN: iteration 3995 : loss : 0.146525, loss_ce: 0.004345, loss_dice: 0.288704
[08:21:42.286] TRAIN: iteration 3996 : loss : 0.146933, loss_ce: 0.011465, loss_dice: 0.282401
[08:21:42.494] TRAIN: iteration 3997 : loss : 0.246702, loss_ce: 0.004956, loss_dice: 0.488448
[08:21:42.701] TRAIN: iteration 3998 : loss : 0.181886, loss_ce: 0.005378, loss_dice: 0.358394
[08:21:42.908] TRAIN: iteration 3999 : loss : 0.154437, loss_ce: 0.022094, loss_dice: 0.286780
[08:21:43.117] TRAIN: iteration 4000 : loss : 0.251925, loss_ce: 0.004816, loss_dice: 0.499033
[08:21:43.350] TRAIN: iteration 4001 : loss : 0.076205, loss_ce: 0.003375, loss_dice: 0.149034
[08:21:43.558] TRAIN: iteration 4002 : loss : 0.137766, loss_ce: 0.004068, loss_dice: 0.271464
[08:21:43.765] TRAIN: iteration 4003 : loss : 0.116665, loss_ce: 0.006130, loss_dice: 0.227199
[08:21:43.973] TRAIN: iteration 4004 : loss : 0.107446, loss_ce: 0.004819, loss_dice: 0.210072
[08:21:44.183] TRAIN: iteration 4005 : loss : 0.168264, loss_ce: 0.004269, loss_dice: 0.332259
[08:21:44.390] TRAIN: iteration 4006 : loss : 0.201817, loss_ce: 0.029088, loss_dice: 0.374546
[08:21:44.600] TRAIN: iteration 4007 : loss : 0.144027, loss_ce: 0.008145, loss_dice: 0.279909
[08:21:44.808] TRAIN: iteration 4008 : loss : 0.167893, loss_ce: 0.014208, loss_dice: 0.321579
[08:21:45.016] TRAIN: iteration 4009 : loss : 0.135609, loss_ce: 0.005958, loss_dice: 0.265260
[08:21:45.224] TRAIN: iteration 4010 : loss : 0.214359, loss_ce: 0.004206, loss_dice: 0.424512
[08:21:45.432] TRAIN: iteration 4011 : loss : 0.133877, loss_ce: 0.009259, loss_dice: 0.258496
[08:21:45.645] TRAIN: iteration 4012 : loss : 0.130323, loss_ce: 0.011704, loss_dice: 0.248941
[08:21:45.857] TRAIN: iteration 4013 : loss : 0.249250, loss_ce: 0.005776, loss_dice: 0.492723
[08:21:46.064] TRAIN: iteration 4014 : loss : 0.204453, loss_ce: 0.008321, loss_dice: 0.400584
[08:21:46.271] TRAIN: iteration 4015 : loss : 0.173244, loss_ce: 0.010092, loss_dice: 0.336396
[08:21:46.478] TRAIN: iteration 4016 : loss : 0.117944, loss_ce: 0.004929, loss_dice: 0.230959
[08:21:46.685] TRAIN: iteration 4017 : loss : 0.255828, loss_ce: 0.021808, loss_dice: 0.489847
[08:21:46.892] TRAIN: iteration 4018 : loss : 0.253243, loss_ce: 0.006114, loss_dice: 0.500372
[08:21:47.100] TRAIN: iteration 4019 : loss : 0.126041, loss_ce: 0.005195, loss_dice: 0.246887
[08:21:47.309] TRAIN: iteration 4020 : loss : 0.178274, loss_ce: 0.012188, loss_dice: 0.344360
[08:21:47.570] TRAIN: iteration 4021 : loss : 0.157131, loss_ce: 0.008195, loss_dice: 0.306067
[08:21:47.777] TRAIN: iteration 4022 : loss : 0.193556, loss_ce: 0.007237, loss_dice: 0.379874
[08:21:47.985] TRAIN: iteration 4023 : loss : 0.147445, loss_ce: 0.005982, loss_dice: 0.288907
[08:21:48.197] TRAIN: iteration 4024 : loss : 0.143288, loss_ce: 0.005734, loss_dice: 0.280842
[08:21:48.406] TRAIN: iteration 4025 : loss : 0.166061, loss_ce: 0.014951, loss_dice: 0.317172
[08:21:48.617] TRAIN: iteration 4026 : loss : 0.249731, loss_ce: 0.005206, loss_dice: 0.494256
[08:21:48.827] TRAIN: iteration 4027 : loss : 0.142012, loss_ce: 0.005158, loss_dice: 0.278866
[08:21:49.035] TRAIN: iteration 4028 : loss : 0.143468, loss_ce: 0.007228, loss_dice: 0.279708
[08:21:49.247] TRAIN: iteration 4029 : loss : 0.185086, loss_ce: 0.005385, loss_dice: 0.364787
[08:21:49.468] TRAIN: iteration 4030 : loss : 0.107306, loss_ce: 0.004445, loss_dice: 0.210167
[08:21:49.678] TRAIN: iteration 4031 : loss : 0.252441, loss_ce: 0.004600, loss_dice: 0.500283
[08:21:49.892] TRAIN: iteration 4032 : loss : 0.252507, loss_ce: 0.004726, loss_dice: 0.500287
[08:21:50.099] TRAIN: iteration 4033 : loss : 0.159359, loss_ce: 0.005028, loss_dice: 0.313690
[08:21:50.308] TRAIN: iteration 4034 : loss : 0.221377, loss_ce: 0.018098, loss_dice: 0.424656
[08:21:50.515] TRAIN: iteration 4035 : loss : 0.152697, loss_ce: 0.005902, loss_dice: 0.299491
[08:21:50.722] TRAIN: iteration 4036 : loss : 0.222842, loss_ce: 0.005486, loss_dice: 0.440199
[08:21:50.929] TRAIN: iteration 4037 : loss : 0.177664, loss_ce: 0.024089, loss_dice: 0.331240
[08:21:51.137] TRAIN: iteration 4038 : loss : 0.172907, loss_ce: 0.016155, loss_dice: 0.329659
[08:21:51.346] TRAIN: iteration 4039 : loss : 0.210478, loss_ce: 0.004600, loss_dice: 0.416355
[08:21:51.553] TRAIN: iteration 4040 : loss : 0.203969, loss_ce: 0.035314, loss_dice: 0.372624
[08:21:51.787] TRAIN: iteration 4041 : loss : 0.175117, loss_ce: 0.013915, loss_dice: 0.336319
[08:21:51.993] TRAIN: iteration 4042 : loss : 0.154264, loss_ce: 0.004819, loss_dice: 0.303708
[08:21:52.200] TRAIN: iteration 4043 : loss : 0.158833, loss_ce: 0.005866, loss_dice: 0.311799
[08:21:52.407] TRAIN: iteration 4044 : loss : 0.110378, loss_ce: 0.007027, loss_dice: 0.213729
[08:21:52.615] TRAIN: iteration 4045 : loss : 0.120132, loss_ce: 0.007175, loss_dice: 0.233090
[08:21:52.822] TRAIN: iteration 4046 : loss : 0.138235, loss_ce: 0.007351, loss_dice: 0.269119
[08:21:53.030] TRAIN: iteration 4047 : loss : 0.260514, loss_ce: 0.020866, loss_dice: 0.500163
[08:21:53.238] TRAIN: iteration 4048 : loss : 0.253711, loss_ce: 0.019687, loss_dice: 0.487736
[08:21:53.447] TRAIN: iteration 4049 : loss : 0.252955, loss_ce: 0.005599, loss_dice: 0.500311
[08:21:53.655] TRAIN: iteration 4050 : loss : 0.252949, loss_ce: 0.005578, loss_dice: 0.500320
[08:21:53.865] TRAIN: iteration 4051 : loss : 0.252904, loss_ce: 0.005571, loss_dice: 0.500236
[08:21:54.076] TRAIN: iteration 4052 : loss : 0.135201, loss_ce: 0.013869, loss_dice: 0.256532
[08:21:54.290] TRAIN: iteration 4053 : loss : 0.092458, loss_ce: 0.007341, loss_dice: 0.177575
[08:21:54.497] TRAIN: iteration 4054 : loss : 0.185701, loss_ce: 0.005708, loss_dice: 0.365694
[08:21:55.009] TRAIN: iteration 4055 : loss : 0.176235, loss_ce: 0.005833, loss_dice: 0.346637
[08:21:55.215] TRAIN: iteration 4056 : loss : 0.179565, loss_ce: 0.005413, loss_dice: 0.353716
[08:21:55.421] TRAIN: iteration 4057 : loss : 0.095668, loss_ce: 0.006242, loss_dice: 0.185094
[08:21:55.628] TRAIN: iteration 4058 : loss : 0.175870, loss_ce: 0.008968, loss_dice: 0.342773
[08:21:56.062] TRAIN: iteration 4059 : loss : 0.124124, loss_ce: 0.011788, loss_dice: 0.236460
[08:21:56.277] TRAIN: iteration 4060 : loss : 0.242553, loss_ce: 0.006142, loss_dice: 0.478965
[08:21:56.522] TRAIN: iteration 4061 : loss : 0.212352, loss_ce: 0.006610, loss_dice: 0.418094
[08:21:56.728] TRAIN: iteration 4062 : loss : 0.114228, loss_ce: 0.010021, loss_dice: 0.218435
[08:21:56.935] TRAIN: iteration 4063 : loss : 0.253018, loss_ce: 0.005683, loss_dice: 0.500354
[08:21:57.298] TRAIN: iteration 4064 : loss : 0.175909, loss_ce: 0.006087, loss_dice: 0.345730
[08:21:57.507] TRAIN: iteration 4065 : loss : 0.151561, loss_ce: 0.006172, loss_dice: 0.296949
[08:21:57.713] TRAIN: iteration 4066 : loss : 0.031623, loss_ce: 0.002769, loss_dice: 0.060477
[08:21:57.922] TRAIN: iteration 4067 : loss : 0.246986, loss_ce: 0.013175, loss_dice: 0.480797
[08:21:58.130] TRAIN: iteration 4068 : loss : 0.124129, loss_ce: 0.009391, loss_dice: 0.238867
[08:21:59.118] TRAIN: iteration 4069 : loss : 0.250082, loss_ce: 0.006791, loss_dice: 0.493372
[08:21:59.326] TRAIN: iteration 4070 : loss : 0.254733, loss_ce: 0.010106, loss_dice: 0.499360
[08:21:59.533] TRAIN: iteration 4071 : loss : 0.076487, loss_ce: 0.004993, loss_dice: 0.147981
[08:21:59.747] TRAIN: iteration 4072 : loss : 0.117552, loss_ce: 0.009484, loss_dice: 0.225619
[08:21:59.962] TRAIN: iteration 4073 : loss : 0.253713, loss_ce: 0.006932, loss_dice: 0.500493
[08:22:00.169] TRAIN: iteration 4074 : loss : 0.253248, loss_ce: 0.006724, loss_dice: 0.499771
[08:22:00.389] TRAIN: iteration 4075 : loss : 0.176392, loss_ce: 0.006019, loss_dice: 0.346764
[08:22:00.599] TRAIN: iteration 4076 : loss : 0.120376, loss_ce: 0.004797, loss_dice: 0.235956
[08:22:02.376] TRAIN: iteration 4077 : loss : 0.194405, loss_ce: 0.005715, loss_dice: 0.383094
[08:22:02.583] TRAIN: iteration 4078 : loss : 0.224460, loss_ce: 0.005900, loss_dice: 0.443020
[08:22:02.792] TRAIN: iteration 4079 : loss : 0.251508, loss_ce: 0.006451, loss_dice: 0.496564
[08:22:02.999] TRAIN: iteration 4080 : loss : 0.140316, loss_ce: 0.012631, loss_dice: 0.268002
[08:22:03.233] TRAIN: iteration 4081 : loss : 0.200232, loss_ce: 0.007120, loss_dice: 0.393344
[08:22:03.441] TRAIN: iteration 4082 : loss : 0.152791, loss_ce: 0.019736, loss_dice: 0.285846
[08:22:03.650] TRAIN: iteration 4083 : loss : 0.240669, loss_ce: 0.007997, loss_dice: 0.473342
[08:22:03.860] TRAIN: iteration 4084 : loss : 0.193917, loss_ce: 0.027463, loss_dice: 0.360372
[08:22:04.067] TRAIN: iteration 4085 : loss : 0.127374, loss_ce: 0.007123, loss_dice: 0.247625
[08:22:04.274] TRAIN: iteration 4086 : loss : 0.235353, loss_ce: 0.006101, loss_dice: 0.464605
[08:22:04.481] TRAIN: iteration 4087 : loss : 0.210987, loss_ce: 0.005657, loss_dice: 0.416316
[08:22:04.691] TRAIN: iteration 4088 : loss : 0.213639, loss_ce: 0.020157, loss_dice: 0.407121
[08:22:04.906] TRAIN: iteration 4089 : loss : 0.241885, loss_ce: 0.008231, loss_dice: 0.475539
[08:22:05.117] TRAIN: iteration 4090 : loss : 0.219377, loss_ce: 0.006299, loss_dice: 0.432454
[08:22:05.328] TRAIN: iteration 4091 : loss : 0.080161, loss_ce: 0.005539, loss_dice: 0.154783
[08:22:05.538] TRAIN: iteration 4092 : loss : 0.254316, loss_ce: 0.008738, loss_dice: 0.499893
[08:22:05.975] TRAIN: iteration 4093 : loss : 0.210485, loss_ce: 0.009564, loss_dice: 0.411406
[08:22:06.185] TRAIN: iteration 4094 : loss : 0.221704, loss_ce: 0.008023, loss_dice: 0.435385
[08:22:06.399] TRAIN: iteration 4095 : loss : 0.151315, loss_ce: 0.010308, loss_dice: 0.292322
[08:22:06.606] TRAIN: iteration 4096 : loss : 0.164835, loss_ce: 0.007270, loss_dice: 0.322399
[08:22:06.813] TRAIN: iteration 4097 : loss : 0.091666, loss_ce: 0.005086, loss_dice: 0.178247
[08:22:07.024] TRAIN: iteration 4098 : loss : 0.113741, loss_ce: 0.005025, loss_dice: 0.222457
[08:22:07.240] TRAIN: iteration 4099 : loss : 0.241550, loss_ce: 0.006633, loss_dice: 0.476466
[08:22:07.447] TRAIN: iteration 4100 : loss : 0.225017, loss_ce: 0.005520, loss_dice: 0.444514
[08:22:07.688] TRAIN: iteration 4101 : loss : 0.252666, loss_ce: 0.005054, loss_dice: 0.500278
[08:22:07.895] TRAIN: iteration 4102 : loss : 0.213544, loss_ce: 0.004902, loss_dice: 0.422187
[08:22:08.109] TRAIN: iteration 4103 : loss : 0.252086, loss_ce: 0.003987, loss_dice: 0.500186
[08:22:08.317] TRAIN: iteration 4104 : loss : 0.152287, loss_ce: 0.010570, loss_dice: 0.294004
[08:22:08.523] TRAIN: iteration 4105 : loss : 0.251735, loss_ce: 0.004065, loss_dice: 0.499404
[08:22:08.736] TRAIN: iteration 4106 : loss : 0.164228, loss_ce: 0.006504, loss_dice: 0.321953
[08:22:08.946] TRAIN: iteration 4107 : loss : 0.251835, loss_ce: 0.003469, loss_dice: 0.500201
[08:22:09.553] TRAIN: iteration 4108 : loss : 0.145755, loss_ce: 0.006520, loss_dice: 0.284991
[08:22:09.760] TRAIN: iteration 4109 : loss : 0.070421, loss_ce: 0.002363, loss_dice: 0.138478
[08:22:11.139] TRAIN: iteration 4110 : loss : 0.080156, loss_ce: 0.003861, loss_dice: 0.156450
[08:22:11.347] TRAIN: iteration 4111 : loss : 0.251764, loss_ce: 0.003347, loss_dice: 0.500182
[08:22:11.554] TRAIN: iteration 4112 : loss : 0.166145, loss_ce: 0.004059, loss_dice: 0.328231
[08:22:11.762] TRAIN: iteration 4113 : loss : 0.148238, loss_ce: 0.002347, loss_dice: 0.294129
[08:22:11.972] TRAIN: iteration 4114 : loss : 0.097307, loss_ce: 0.004066, loss_dice: 0.190548
[08:22:12.181] TRAIN: iteration 4115 : loss : 0.212670, loss_ce: 0.005334, loss_dice: 0.420006
[08:22:12.388] TRAIN: iteration 4116 : loss : 0.157525, loss_ce: 0.010940, loss_dice: 0.304110
[08:22:12.598] TRAIN: iteration 4117 : loss : 0.211861, loss_ce: 0.002520, loss_dice: 0.421203
[08:22:12.806] TRAIN: iteration 4118 : loss : 0.245206, loss_ce: 0.017145, loss_dice: 0.473267
[08:22:13.019] TRAIN: iteration 4119 : loss : 0.209433, loss_ce: 0.019308, loss_dice: 0.399558
[08:22:13.226] TRAIN: iteration 4120 : loss : 0.189438, loss_ce: 0.016226, loss_dice: 0.362651
[08:22:13.461] TRAIN: iteration 4121 : loss : 0.187074, loss_ce: 0.004742, loss_dice: 0.369407
[08:22:13.670] TRAIN: iteration 4122 : loss : 0.140959, loss_ce: 0.005755, loss_dice: 0.276164
[08:22:13.883] TRAIN: iteration 4123 : loss : 0.230572, loss_ce: 0.004032, loss_dice: 0.457112
[08:22:14.548] TRAIN: iteration 4124 : loss : 0.246592, loss_ce: 0.004836, loss_dice: 0.488348
[08:22:14.757] TRAIN: iteration 4125 : loss : 0.130656, loss_ce: 0.009177, loss_dice: 0.252135
[08:22:14.966] TRAIN: iteration 4126 : loss : 0.249104, loss_ce: 0.004220, loss_dice: 0.493987
[08:22:16.457] TRAIN: iteration 4127 : loss : 0.174672, loss_ce: 0.004682, loss_dice: 0.344662
[08:22:16.666] TRAIN: iteration 4128 : loss : 0.224589, loss_ce: 0.009499, loss_dice: 0.439678
[08:22:16.873] TRAIN: iteration 4129 : loss : 0.130845, loss_ce: 0.010838, loss_dice: 0.250853
[08:22:17.081] TRAIN: iteration 4130 : loss : 0.132650, loss_ce: 0.007937, loss_dice: 0.257364
[08:22:17.289] TRAIN: iteration 4131 : loss : 0.236940, loss_ce: 0.005421, loss_dice: 0.468458
[08:22:17.498] TRAIN: iteration 4132 : loss : 0.130692, loss_ce: 0.006873, loss_dice: 0.254511
[08:22:17.742] TRAIN: iteration 4133 : loss : 0.175814, loss_ce: 0.006097, loss_dice: 0.345532
[08:22:17.953] TRAIN: iteration 4134 : loss : 0.081919, loss_ce: 0.007370, loss_dice: 0.156467
[08:22:18.160] TRAIN: iteration 4135 : loss : 0.211689, loss_ce: 0.011522, loss_dice: 0.411857
[08:22:18.368] TRAIN: iteration 4136 : loss : 0.249822, loss_ce: 0.006738, loss_dice: 0.492906
[08:22:18.577] TRAIN: iteration 4137 : loss : 0.082750, loss_ce: 0.008282, loss_dice: 0.157217
[08:22:18.789] TRAIN: iteration 4138 : loss : 0.154432, loss_ce: 0.009413, loss_dice: 0.299451
[08:22:18.996] TRAIN: iteration 4139 : loss : 0.248694, loss_ce: 0.007731, loss_dice: 0.489657
[08:22:19.210] TRAIN: iteration 4140 : loss : 0.202888, loss_ce: 0.007470, loss_dice: 0.398306
[08:22:19.449] TRAIN: iteration 4141 : loss : 0.128381, loss_ce: 0.007652, loss_dice: 0.249110
[08:22:19.700] TRAIN: iteration 4142 : loss : 0.223912, loss_ce: 0.006899, loss_dice: 0.440924
[08:22:19.915] TRAIN: iteration 4143 : loss : 0.189327, loss_ce: 0.009649, loss_dice: 0.369005
[08:22:20.127] TRAIN: iteration 4144 : loss : 0.249521, loss_ce: 0.007571, loss_dice: 0.491471
[08:22:20.335] TRAIN: iteration 4145 : loss : 0.091591, loss_ce: 0.008783, loss_dice: 0.174399
[08:22:20.581] TRAIN: iteration 4146 : loss : 0.150260, loss_ce: 0.006331, loss_dice: 0.294190
[08:22:20.788] TRAIN: iteration 4147 : loss : 0.167856, loss_ce: 0.008608, loss_dice: 0.327104
[08:22:20.996] TRAIN: iteration 4148 : loss : 0.102717, loss_ce: 0.006699, loss_dice: 0.198736
[08:22:21.203] TRAIN: iteration 4149 : loss : 0.253189, loss_ce: 0.006009, loss_dice: 0.500369
[08:22:21.414] TRAIN: iteration 4150 : loss : 0.243489, loss_ce: 0.013723, loss_dice: 0.473254
[08:22:21.622] TRAIN: iteration 4151 : loss : 0.252933, loss_ce: 0.005487, loss_dice: 0.500379
[08:22:21.831] TRAIN: iteration 4152 : loss : 0.127659, loss_ce: 0.009455, loss_dice: 0.245863
[08:22:22.039] TRAIN: iteration 4153 : loss : 0.253100, loss_ce: 0.005785, loss_dice: 0.500416
[08:22:22.246] TRAIN: iteration 4154 : loss : 0.132845, loss_ce: 0.005039, loss_dice: 0.260651
[08:22:22.453] TRAIN: iteration 4155 : loss : 0.155060, loss_ce: 0.005566, loss_dice: 0.304554
[08:22:22.660] TRAIN: iteration 4156 : loss : 0.096145, loss_ce: 0.004813, loss_dice: 0.187477
[08:22:22.867] TRAIN: iteration 4157 : loss : 0.145234, loss_ce: 0.005204, loss_dice: 0.285264
[08:22:23.074] TRAIN: iteration 4158 : loss : 0.145299, loss_ce: 0.003783, loss_dice: 0.286815
[08:22:25.193] TRAIN: iteration 4159 : loss : 0.135013, loss_ce: 0.004370, loss_dice: 0.265656
[08:22:25.400] TRAIN: iteration 4160 : loss : 0.165841, loss_ce: 0.003673, loss_dice: 0.328009
[08:22:25.643] TRAIN: iteration 4161 : loss : 0.181385, loss_ce: 0.012620, loss_dice: 0.350149
[08:22:25.854] TRAIN: iteration 4162 : loss : 0.251889, loss_ce: 0.003545, loss_dice: 0.500232
[08:22:26.064] TRAIN: iteration 4163 : loss : 0.155010, loss_ce: 0.004486, loss_dice: 0.305533
[08:22:26.271] TRAIN: iteration 4164 : loss : 0.095322, loss_ce: 0.002214, loss_dice: 0.188429
[08:22:26.481] TRAIN: iteration 4165 : loss : 0.252132, loss_ce: 0.004135, loss_dice: 0.500128
[08:22:26.691] TRAIN: iteration 4166 : loss : 0.147916, loss_ce: 0.008303, loss_dice: 0.287529
[08:22:26.898] TRAIN: iteration 4167 : loss : 0.187577, loss_ce: 0.006055, loss_dice: 0.369098
[08:22:27.113] TRAIN: iteration 4168 : loss : 0.250866, loss_ce: 0.001652, loss_dice: 0.500081
[08:22:27.328] TRAIN: iteration 4169 : loss : 0.207275, loss_ce: 0.012620, loss_dice: 0.401929
[08:22:27.536] TRAIN: iteration 4170 : loss : 0.159667, loss_ce: 0.002982, loss_dice: 0.316351
[08:22:27.743] TRAIN: iteration 4171 : loss : 0.168016, loss_ce: 0.015800, loss_dice: 0.320231
[08:22:27.949] TRAIN: iteration 4172 : loss : 0.248899, loss_ce: 0.004679, loss_dice: 0.493118
[08:22:28.164] TRAIN: iteration 4173 : loss : 0.251193, loss_ce: 0.019813, loss_dice: 0.482573
[08:22:28.371] TRAIN: iteration 4174 : loss : 0.171320, loss_ce: 0.010862, loss_dice: 0.331779
[08:22:28.581] TRAIN: iteration 4175 : loss : 0.231247, loss_ce: 0.002336, loss_dice: 0.460158
[08:22:28.789] TRAIN: iteration 4176 : loss : 0.185478, loss_ce: 0.004779, loss_dice: 0.366176
[08:22:28.996] TRAIN: iteration 4177 : loss : 0.179720, loss_ce: 0.009270, loss_dice: 0.350171
[08:22:29.203] TRAIN: iteration 4178 : loss : 0.076356, loss_ce: 0.003731, loss_dice: 0.148981
[08:22:29.410] TRAIN: iteration 4179 : loss : 0.250400, loss_ce: 0.012427, loss_dice: 0.488373
[08:22:29.616] TRAIN: iteration 4180 : loss : 0.247657, loss_ce: 0.009138, loss_dice: 0.486176
[08:22:29.850] TRAIN: iteration 4181 : loss : 0.143845, loss_ce: 0.003565, loss_dice: 0.284125
[08:22:30.057] TRAIN: iteration 4182 : loss : 0.229651, loss_ce: 0.004451, loss_dice: 0.454852
[08:22:30.273] TRAIN: iteration 4183 : loss : 0.050760, loss_ce: 0.002887, loss_dice: 0.098634
[08:22:30.480] TRAIN: iteration 4184 : loss : 0.196836, loss_ce: 0.004256, loss_dice: 0.389416
[08:22:30.688] TRAIN: iteration 4185 : loss : 0.228655, loss_ce: 0.003286, loss_dice: 0.454024
[08:22:30.921] TRAIN: iteration 4186 : loss : 0.101044, loss_ce: 0.002831, loss_dice: 0.199257
[08:22:31.131] TRAIN: iteration 4187 : loss : 0.249901, loss_ce: 0.003557, loss_dice: 0.496244
[08:22:31.341] TRAIN: iteration 4188 : loss : 0.080956, loss_ce: 0.004187, loss_dice: 0.157726
[08:22:31.553] TRAIN: iteration 4189 : loss : 0.220032, loss_ce: 0.036931, loss_dice: 0.403133
[08:22:31.768] TRAIN: iteration 4190 : loss : 0.189836, loss_ce: 0.016031, loss_dice: 0.363642
[08:22:31.974] TRAIN: iteration 4191 : loss : 0.237444, loss_ce: 0.007640, loss_dice: 0.467249
[08:22:32.701] TRAIN: iteration 4192 : loss : 0.111793, loss_ce: 0.008496, loss_dice: 0.215091
[08:22:33.194] TRAIN: iteration 4193 : loss : 0.223787, loss_ce: 0.007431, loss_dice: 0.440143
[08:22:33.405] TRAIN: iteration 4194 : loss : 0.227638, loss_ce: 0.006154, loss_dice: 0.449122
[08:22:33.614] TRAIN: iteration 4195 : loss : 0.152132, loss_ce: 0.008950, loss_dice: 0.295314
[08:22:33.823] TRAIN: iteration 4196 : loss : 0.137162, loss_ce: 0.008117, loss_dice: 0.266206
[08:22:34.039] TRAIN: iteration 4197 : loss : 0.183524, loss_ce: 0.005631, loss_dice: 0.361418
[08:22:34.251] TRAIN: iteration 4198 : loss : 0.247081, loss_ce: 0.005983, loss_dice: 0.488178
[08:22:34.462] TRAIN: iteration 4199 : loss : 0.253218, loss_ce: 0.006120, loss_dice: 0.500316
[08:22:34.670] TRAIN: iteration 4200 : loss : 0.080635, loss_ce: 0.006053, loss_dice: 0.155218
[08:22:34.905] TRAIN: iteration 4201 : loss : 0.129993, loss_ce: 0.010945, loss_dice: 0.249042
[08:22:35.113] TRAIN: iteration 4202 : loss : 0.118315, loss_ce: 0.006145, loss_dice: 0.230486
[08:22:35.327] TRAIN: iteration 4203 : loss : 0.152642, loss_ce: 0.005583, loss_dice: 0.299702
[08:22:35.535] TRAIN: iteration 4204 : loss : 0.233042, loss_ce: 0.006143, loss_dice: 0.459942
[08:22:35.748] TRAIN: iteration 4205 : loss : 0.099011, loss_ce: 0.008767, loss_dice: 0.189254
[08:22:35.957] TRAIN: iteration 4206 : loss : 0.248658, loss_ce: 0.006720, loss_dice: 0.490596
[08:22:36.164] TRAIN: iteration 4207 : loss : 0.168057, loss_ce: 0.008250, loss_dice: 0.327863
[08:22:36.375] TRAIN: iteration 4208 : loss : 0.129921, loss_ce: 0.009108, loss_dice: 0.250734
[08:22:36.584] TRAIN: iteration 4209 : loss : 0.154996, loss_ce: 0.008451, loss_dice: 0.301541
[08:22:36.791] TRAIN: iteration 4210 : loss : 0.214141, loss_ce: 0.019055, loss_dice: 0.409227
[08:22:36.999] TRAIN: iteration 4211 : loss : 0.092207, loss_ce: 0.005265, loss_dice: 0.179150
[08:22:37.206] TRAIN: iteration 4212 : loss : 0.252704, loss_ce: 0.005083, loss_dice: 0.500324
[08:22:37.416] TRAIN: iteration 4213 : loss : 0.248051, loss_ce: 0.005839, loss_dice: 0.490263
[08:22:37.624] TRAIN: iteration 4214 : loss : 0.130552, loss_ce: 0.006652, loss_dice: 0.254451
[08:22:37.833] TRAIN: iteration 4215 : loss : 0.250172, loss_ce: 0.006821, loss_dice: 0.493522
[08:22:38.041] TRAIN: iteration 4216 : loss : 0.117377, loss_ce: 0.005970, loss_dice: 0.228784
[08:22:38.250] TRAIN: iteration 4217 : loss : 0.195215, loss_ce: 0.020546, loss_dice: 0.369884
[08:22:38.464] TRAIN: iteration 4218 : loss : 0.080112, loss_ce: 0.004724, loss_dice: 0.155500
[08:22:39.217] TRAIN: iteration 4219 : loss : 0.169900, loss_ce: 0.005370, loss_dice: 0.334430
[08:22:39.424] TRAIN: iteration 4220 : loss : 0.212113, loss_ce: 0.004685, loss_dice: 0.419541
[08:22:39.668] TRAIN: iteration 4221 : loss : 0.163789, loss_ce: 0.004149, loss_dice: 0.323428
[08:22:39.877] TRAIN: iteration 4222 : loss : 0.229546, loss_ce: 0.005060, loss_dice: 0.454032
[08:22:40.086] TRAIN: iteration 4223 : loss : 0.132025, loss_ce: 0.003873, loss_dice: 0.260178
[08:22:40.311] TRAIN: iteration 4224 : loss : 0.251953, loss_ce: 0.003742, loss_dice: 0.500164
[08:22:40.666] TRAIN: iteration 4225 : loss : 0.121351, loss_ce: 0.004681, loss_dice: 0.238022
[08:22:40.876] TRAIN: iteration 4226 : loss : 0.202560, loss_ce: 0.024308, loss_dice: 0.380811
[08:22:41.084] TRAIN: iteration 4227 : loss : 0.140446, loss_ce: 0.007208, loss_dice: 0.273684
[08:22:41.290] TRAIN: iteration 4228 : loss : 0.244640, loss_ce: 0.003939, loss_dice: 0.485342
[08:22:41.516] TRAIN: iteration 4229 : loss : 0.143134, loss_ce: 0.007345, loss_dice: 0.278923
[08:22:41.724] TRAIN: iteration 4230 : loss : 0.251579, loss_ce: 0.003003, loss_dice: 0.500155
[08:22:42.009] TRAIN: iteration 4231 : loss : 0.129753, loss_ce: 0.011042, loss_dice: 0.248464
[08:22:42.217] TRAIN: iteration 4232 : loss : 0.214177, loss_ce: 0.004605, loss_dice: 0.423749
[08:22:43.119] TRAIN: iteration 4233 : loss : 0.251804, loss_ce: 0.003489, loss_dice: 0.500118
[08:22:43.328] TRAIN: iteration 4234 : loss : 0.150058, loss_ce: 0.004233, loss_dice: 0.295884
[08:22:43.535] TRAIN: iteration 4235 : loss : 0.160742, loss_ce: 0.003915, loss_dice: 0.317569
[08:22:43.742] TRAIN: iteration 4236 : loss : 0.252030, loss_ce: 0.003891, loss_dice: 0.500170
[08:22:43.950] TRAIN: iteration 4237 : loss : 0.252066, loss_ce: 0.003965, loss_dice: 0.500167
[08:22:44.158] TRAIN: iteration 4238 : loss : 0.117586, loss_ce: 0.005539, loss_dice: 0.229633
[08:22:44.369] TRAIN: iteration 4239 : loss : 0.154701, loss_ce: 0.004925, loss_dice: 0.304477
[08:22:44.579] TRAIN: iteration 4240 : loss : 0.146443, loss_ce: 0.003526, loss_dice: 0.289361
[08:22:46.369] TRAIN: iteration 4241 : loss : 0.089242, loss_ce: 0.006176, loss_dice: 0.172308
[08:22:46.576] TRAIN: iteration 4242 : loss : 0.120515, loss_ce: 0.006937, loss_dice: 0.234094
[08:22:46.790] TRAIN: iteration 4243 : loss : 0.156390, loss_ce: 0.021601, loss_dice: 0.291179
[08:22:46.998] TRAIN: iteration 4244 : loss : 0.098049, loss_ce: 0.005872, loss_dice: 0.190226
[08:22:47.206] TRAIN: iteration 4245 : loss : 0.203811, loss_ce: 0.004616, loss_dice: 0.403005
[08:22:47.413] TRAIN: iteration 4246 : loss : 0.251159, loss_ce: 0.004976, loss_dice: 0.497343
[08:22:47.621] TRAIN: iteration 4247 : loss : 0.130675, loss_ce: 0.005461, loss_dice: 0.255889
[08:22:47.827] TRAIN: iteration 4248 : loss : 0.126327, loss_ce: 0.011442, loss_dice: 0.241213
[08:22:49.354] TRAIN: iteration 4249 : loss : 0.124378, loss_ce: 0.012757, loss_dice: 0.235999
[08:22:49.564] TRAIN: iteration 4250 : loss : 0.214065, loss_ce: 0.007291, loss_dice: 0.420838
[08:22:49.775] TRAIN: iteration 4251 : loss : 0.253106, loss_ce: 0.005800, loss_dice: 0.500411
[08:22:49.983] TRAIN: iteration 4252 : loss : 0.206842, loss_ce: 0.013974, loss_dice: 0.399709
[08:22:50.193] TRAIN: iteration 4253 : loss : 0.079295, loss_ce: 0.005867, loss_dice: 0.152723
[08:22:50.408] TRAIN: iteration 4254 : loss : 0.221622, loss_ce: 0.007570, loss_dice: 0.435673
[08:22:50.619] TRAIN: iteration 4255 : loss : 0.253273, loss_ce: 0.006250, loss_dice: 0.500296
[08:22:50.831] TRAIN: iteration 4256 : loss : 0.134557, loss_ce: 0.010392, loss_dice: 0.258721
[08:22:51.323] TRAIN: iteration 4257 : loss : 0.166808, loss_ce: 0.015312, loss_dice: 0.318304
[08:22:51.529] TRAIN: iteration 4258 : loss : 0.182032, loss_ce: 0.006430, loss_dice: 0.357635
[08:22:51.736] TRAIN: iteration 4259 : loss : 0.254288, loss_ce: 0.012553, loss_dice: 0.496022
[08:22:51.945] TRAIN: iteration 4260 : loss : 0.112781, loss_ce: 0.007814, loss_dice: 0.217747
[08:22:52.182] TRAIN: iteration 4261 : loss : 0.153498, loss_ce: 0.006624, loss_dice: 0.300372
[08:22:52.391] TRAIN: iteration 4262 : loss : 0.253523, loss_ce: 0.008202, loss_dice: 0.498845
[08:22:52.598] TRAIN: iteration 4263 : loss : 0.253184, loss_ce: 0.007942, loss_dice: 0.498427
[08:22:52.807] TRAIN: iteration 4264 : loss : 0.249562, loss_ce: 0.007227, loss_dice: 0.491897
[08:22:53.014] TRAIN: iteration 4265 : loss : 0.109271, loss_ce: 0.008572, loss_dice: 0.209969
[08:22:53.229] TRAIN: iteration 4266 : loss : 0.179291, loss_ce: 0.008436, loss_dice: 0.350145
[08:22:53.438] TRAIN: iteration 4267 : loss : 0.253707, loss_ce: 0.006909, loss_dice: 0.500504
[08:22:53.645] TRAIN: iteration 4268 : loss : 0.144026, loss_ce: 0.007355, loss_dice: 0.280697
[08:22:53.855] TRAIN: iteration 4269 : loss : 0.171484, loss_ce: 0.009420, loss_dice: 0.333547
[08:22:54.066] TRAIN: iteration 4270 : loss : 0.090438, loss_ce: 0.005457, loss_dice: 0.175420
[08:22:54.276] TRAIN: iteration 4271 : loss : 0.176584, loss_ce: 0.005868, loss_dice: 0.347300
[08:22:55.259] TRAIN: iteration 4272 : loss : 0.081059, loss_ce: 0.004049, loss_dice: 0.158069
[08:22:55.470] TRAIN: iteration 4273 : loss : 0.146200, loss_ce: 0.005698, loss_dice: 0.286702
[08:22:55.679] TRAIN: iteration 4274 : loss : 0.143118, loss_ce: 0.005045, loss_dice: 0.281191
[08:22:55.888] TRAIN: iteration 4275 : loss : 0.245888, loss_ce: 0.003950, loss_dice: 0.487825
[08:22:56.098] TRAIN: iteration 4276 : loss : 0.049635, loss_ce: 0.003430, loss_dice: 0.095841
[08:22:56.309] TRAIN: iteration 4277 : loss : 0.220536, loss_ce: 0.007875, loss_dice: 0.433197
[08:22:56.516] TRAIN: iteration 4278 : loss : 0.251828, loss_ce: 0.003405, loss_dice: 0.500251
[08:22:56.724] TRAIN: iteration 4279 : loss : 0.139943, loss_ce: 0.006840, loss_dice: 0.273045
[08:22:58.381] TRAIN: iteration 4280 : loss : 0.248600, loss_ce: 0.005503, loss_dice: 0.491697
[08:22:58.618] TRAIN: iteration 4281 : loss : 0.251053, loss_ce: 0.002034, loss_dice: 0.500073
[08:22:58.826] TRAIN: iteration 4282 : loss : 0.247454, loss_ce: 0.003427, loss_dice: 0.491481
[08:22:59.035] TRAIN: iteration 4283 : loss : 0.266347, loss_ce: 0.038302, loss_dice: 0.494391
[08:22:59.247] TRAIN: iteration 4284 : loss : 0.245476, loss_ce: 0.009270, loss_dice: 0.481682
[08:22:59.454] TRAIN: iteration 4285 : loss : 0.088207, loss_ce: 0.002388, loss_dice: 0.174026
[08:22:59.662] TRAIN: iteration 4286 : loss : 0.178283, loss_ce: 0.003306, loss_dice: 0.353260
[08:22:59.877] TRAIN: iteration 4287 : loss : 0.101824, loss_ce: 0.006207, loss_dice: 0.197442
[08:23:00.088] TRAIN: iteration 4288 : loss : 0.139960, loss_ce: 0.017329, loss_dice: 0.262592
[08:23:00.295] TRAIN: iteration 4289 : loss : 0.245420, loss_ce: 0.009999, loss_dice: 0.480841
[08:23:00.503] TRAIN: iteration 4290 : loss : 0.250905, loss_ce: 0.001777, loss_dice: 0.500033
[08:23:00.711] TRAIN: iteration 4291 : loss : 0.158147, loss_ce: 0.011417, loss_dice: 0.304877
[08:23:00.924] TRAIN: iteration 4292 : loss : 0.230103, loss_ce: 0.002569, loss_dice: 0.457638
[08:23:01.138] TRAIN: iteration 4293 : loss : 0.164306, loss_ce: 0.010975, loss_dice: 0.317637
[08:23:01.345] TRAIN: iteration 4294 : loss : 0.197173, loss_ce: 0.009388, loss_dice: 0.384959
[08:23:01.552] TRAIN: iteration 4295 : loss : 0.087663, loss_ce: 0.003566, loss_dice: 0.171761
[08:23:01.760] TRAIN: iteration 4296 : loss : 0.251877, loss_ce: 0.003560, loss_dice: 0.500195
[08:23:03.341] TRAIN: iteration 4297 : loss : 0.230567, loss_ce: 0.020952, loss_dice: 0.440183
[08:23:03.548] TRAIN: iteration 4298 : loss : 0.223690, loss_ce: 0.004854, loss_dice: 0.442526
[08:23:03.762] TRAIN: iteration 4299 : loss : 0.083610, loss_ce: 0.003496, loss_dice: 0.163725
[08:23:03.969] TRAIN: iteration 4300 : loss : 0.088911, loss_ce: 0.005991, loss_dice: 0.171832
[08:23:04.212] TRAIN: iteration 4301 : loss : 0.129684, loss_ce: 0.004563, loss_dice: 0.254805
[08:23:04.419] TRAIN: iteration 4302 : loss : 0.116864, loss_ce: 0.005535, loss_dice: 0.228194
[08:23:04.629] TRAIN: iteration 4303 : loss : 0.108191, loss_ce: 0.004997, loss_dice: 0.211385
[08:23:04.843] TRAIN: iteration 4304 : loss : 0.106321, loss_ce: 0.004790, loss_dice: 0.207851
[08:23:05.052] TRAIN: iteration 4305 : loss : 0.252488, loss_ce: 0.004719, loss_dice: 0.500257
[08:23:05.265] TRAIN: iteration 4306 : loss : 0.186075, loss_ce: 0.009422, loss_dice: 0.362728
[08:23:05.472] TRAIN: iteration 4307 : loss : 0.139238, loss_ce: 0.006758, loss_dice: 0.271718
[08:23:05.682] TRAIN: iteration 4308 : loss : 0.156218, loss_ce: 0.004077, loss_dice: 0.308358
[08:23:05.889] TRAIN: iteration 4309 : loss : 0.161354, loss_ce: 0.005713, loss_dice: 0.316995
[08:23:06.106] TRAIN: iteration 4310 : loss : 0.089786, loss_ce: 0.006258, loss_dice: 0.173315
[08:23:06.315] TRAIN: iteration 4311 : loss : 0.083599, loss_ce: 0.004081, loss_dice: 0.163116
[08:23:06.529] TRAIN: iteration 4312 : loss : 0.251420, loss_ce: 0.004016, loss_dice: 0.498824
[08:23:06.736] TRAIN: iteration 4313 : loss : 0.179133, loss_ce: 0.004290, loss_dice: 0.353976
[08:23:06.945] TRAIN: iteration 4314 : loss : 0.252087, loss_ce: 0.003960, loss_dice: 0.500214
[08:23:07.177] TRAIN: iteration 4315 : loss : 0.169033, loss_ce: 0.004548, loss_dice: 0.333517
[08:23:07.387] TRAIN: iteration 4316 : loss : 0.245931, loss_ce: 0.004297, loss_dice: 0.487564
[08:23:07.597] TRAIN: iteration 4317 : loss : 0.245817, loss_ce: 0.005324, loss_dice: 0.486310
[08:23:07.808] TRAIN: iteration 4318 : loss : 0.138497, loss_ce: 0.005237, loss_dice: 0.271757
[08:23:08.687] TRAIN: iteration 4319 : loss : 0.224914, loss_ce: 0.004199, loss_dice: 0.445629
[08:23:08.895] TRAIN: iteration 4320 : loss : 0.061098, loss_ce: 0.002668, loss_dice: 0.119529
[08:23:09.136] TRAIN: iteration 4321 : loss : 0.158502, loss_ce: 0.006729, loss_dice: 0.310275
[08:23:09.344] TRAIN: iteration 4322 : loss : 0.252016, loss_ce: 0.003797, loss_dice: 0.500235
[08:23:09.553] TRAIN: iteration 4323 : loss : 0.067725, loss_ce: 0.003654, loss_dice: 0.131795
[08:23:09.762] TRAIN: iteration 4324 : loss : 0.133381, loss_ce: 0.008679, loss_dice: 0.258082
[08:23:09.969] TRAIN: iteration 4325 : loss : 0.094852, loss_ce: 0.004933, loss_dice: 0.184771
[08:23:10.179] TRAIN: iteration 4326 : loss : 0.113003, loss_ce: 0.003891, loss_dice: 0.222115
[08:23:10.386] TRAIN: iteration 4327 : loss : 0.205354, loss_ce: 0.019206, loss_dice: 0.391502
[08:23:10.594] TRAIN: iteration 4328 : loss : 0.252081, loss_ce: 0.003900, loss_dice: 0.500262
[08:23:10.801] TRAIN: iteration 4329 : loss : 0.143369, loss_ce: 0.003098, loss_dice: 0.283640
[08:23:11.008] TRAIN: iteration 4330 : loss : 0.124033, loss_ce: 0.005086, loss_dice: 0.242979
[08:23:11.218] TRAIN: iteration 4331 : loss : 0.158145, loss_ce: 0.015436, loss_dice: 0.300853
[08:23:11.425] TRAIN: iteration 4332 : loss : 0.099627, loss_ce: 0.003736, loss_dice: 0.195517
[08:23:11.633] TRAIN: iteration 4333 : loss : 0.209942, loss_ce: 0.005896, loss_dice: 0.413989
[08:23:11.841] TRAIN: iteration 4334 : loss : 0.125428, loss_ce: 0.011426, loss_dice: 0.239431
[08:23:12.058] TRAIN: iteration 4335 : loss : 0.171176, loss_ce: 0.004341, loss_dice: 0.338011
[08:23:13.503] TRAIN: iteration 4336 : loss : 0.184418, loss_ce: 0.011214, loss_dice: 0.357622
[08:23:13.711] TRAIN: iteration 4337 : loss : 0.196498, loss_ce: 0.012252, loss_dice: 0.380744
[08:23:13.919] TRAIN: iteration 4338 : loss : 0.250689, loss_ce: 0.004738, loss_dice: 0.496641
[08:23:14.128] TRAIN: iteration 4339 : loss : 0.114713, loss_ce: 0.008846, loss_dice: 0.220580
[08:23:14.534] TRAIN: iteration 4340 : loss : 0.104307, loss_ce: 0.006526, loss_dice: 0.202088
[08:23:14.777] TRAIN: iteration 4341 : loss : 0.130983, loss_ce: 0.004156, loss_dice: 0.257810
[08:23:14.988] TRAIN: iteration 4342 : loss : 0.191797, loss_ce: 0.005064, loss_dice: 0.378530
[08:23:15.197] TRAIN: iteration 4343 : loss : 0.251125, loss_ce: 0.005343, loss_dice: 0.496908
[08:23:15.403] TRAIN: iteration 4344 : loss : 0.204248, loss_ce: 0.044747, loss_dice: 0.363749
[08:23:15.610] TRAIN: iteration 4345 : loss : 0.159778, loss_ce: 0.016839, loss_dice: 0.302717
[08:23:15.820] TRAIN: iteration 4346 : loss : 0.193611, loss_ce: 0.015423, loss_dice: 0.371798
[08:23:16.026] TRAIN: iteration 4347 : loss : 0.100989, loss_ce: 0.007816, loss_dice: 0.194162
[08:23:16.235] TRAIN: iteration 4348 : loss : 0.252592, loss_ce: 0.006257, loss_dice: 0.498927
[08:23:16.995] TRAIN: iteration 4349 : loss : 0.195097, loss_ce: 0.013286, loss_dice: 0.376908
[08:23:17.204] TRAIN: iteration 4350 : loss : 0.162723, loss_ce: 0.005446, loss_dice: 0.319999
[08:23:17.412] TRAIN: iteration 4351 : loss : 0.072858, loss_ce: 0.004435, loss_dice: 0.141281
[08:23:17.620] TRAIN: iteration 4352 : loss : 0.240977, loss_ce: 0.006356, loss_dice: 0.475598
[08:23:17.830] TRAIN: iteration 4353 : loss : 0.089191, loss_ce: 0.009546, loss_dice: 0.168836
[08:23:18.038] TRAIN: iteration 4354 : loss : 0.252017, loss_ce: 0.006423, loss_dice: 0.497610
[08:23:18.255] TRAIN: iteration 4355 : loss : 0.154429, loss_ce: 0.016937, loss_dice: 0.291921
[08:23:18.462] TRAIN: iteration 4356 : loss : 0.112070, loss_ce: 0.011288, loss_dice: 0.212852
[08:23:19.753] TRAIN: iteration 4357 : loss : 0.107436, loss_ce: 0.006565, loss_dice: 0.208307
[08:23:19.960] TRAIN: iteration 4358 : loss : 0.068560, loss_ce: 0.005733, loss_dice: 0.131387
[08:23:20.175] TRAIN: iteration 4359 : loss : 0.259861, loss_ce: 0.023967, loss_dice: 0.495756
[08:23:20.386] TRAIN: iteration 4360 : loss : 0.253875, loss_ce: 0.007232, loss_dice: 0.500518
[08:23:20.387] NaN or Inf found in input tensor.
[08:23:20.605] TRAIN: iteration 4361 : loss : 0.253616, loss_ce: 0.006737, loss_dice: 0.500495
[08:23:20.813] TRAIN: iteration 4362 : loss : 0.141074, loss_ce: 0.012349, loss_dice: 0.269798
[08:23:21.023] TRAIN: iteration 4363 : loss : 0.246476, loss_ce: 0.007439, loss_dice: 0.485513
[08:23:21.234] TRAIN: iteration 4364 : loss : 0.171768, loss_ce: 0.007145, loss_dice: 0.336391
[08:23:21.442] TRAIN: iteration 4365 : loss : 0.251388, loss_ce: 0.009846, loss_dice: 0.492930
[08:23:21.652] TRAIN: iteration 4366 : loss : 0.128468, loss_ce: 0.008156, loss_dice: 0.248779
[08:23:21.862] TRAIN: iteration 4367 : loss : 0.162540, loss_ce: 0.013962, loss_dice: 0.311118
[08:23:23.043] TRAIN: iteration 4368 : loss : 0.224388, loss_ce: 0.008885, loss_dice: 0.439890
[08:23:23.250] TRAIN: iteration 4369 : loss : 0.093639, loss_ce: 0.008412, loss_dice: 0.178866
[08:23:23.457] TRAIN: iteration 4370 : loss : 0.128145, loss_ce: 0.008709, loss_dice: 0.247581
[08:23:23.663] TRAIN: iteration 4371 : loss : 0.075409, loss_ce: 0.005394, loss_dice: 0.145423
[08:23:23.870] TRAIN: iteration 4372 : loss : 0.185271, loss_ce: 0.007175, loss_dice: 0.363368
[08:23:24.077] TRAIN: iteration 4373 : loss : 0.140391, loss_ce: 0.006215, loss_dice: 0.274567
[08:23:24.284] TRAIN: iteration 4374 : loss : 0.064790, loss_ce: 0.006003, loss_dice: 0.123577
[08:23:24.491] TRAIN: iteration 4375 : loss : 0.118510, loss_ce: 0.011805, loss_dice: 0.225215
[08:23:24.698] TRAIN: iteration 4376 : loss : 0.148363, loss_ce: 0.012873, loss_dice: 0.283853
[08:23:25.229] TRAIN: iteration 4377 : loss : 0.234348, loss_ce: 0.005284, loss_dice: 0.463413
[08:23:25.437] TRAIN: iteration 4378 : loss : 0.159236, loss_ce: 0.013828, loss_dice: 0.304644
[08:23:25.645] TRAIN: iteration 4379 : loss : 0.139280, loss_ce: 0.005534, loss_dice: 0.273026
[08:23:25.853] TRAIN: iteration 4380 : loss : 0.123423, loss_ce: 0.003775, loss_dice: 0.243070
[08:23:26.088] TRAIN: iteration 4381 : loss : 0.175439, loss_ce: 0.008164, loss_dice: 0.342714
[08:23:26.296] TRAIN: iteration 4382 : loss : 0.222074, loss_ce: 0.006018, loss_dice: 0.438131
[08:23:26.503] TRAIN: iteration 4383 : loss : 0.079404, loss_ce: 0.004843, loss_dice: 0.153964
[08:23:26.710] TRAIN: iteration 4384 : loss : 0.061259, loss_ce: 0.003253, loss_dice: 0.119265
[08:23:27.436] TRAIN: iteration 4385 : loss : 0.178128, loss_ce: 0.010273, loss_dice: 0.345984
[08:23:27.644] TRAIN: iteration 4386 : loss : 0.180138, loss_ce: 0.007952, loss_dice: 0.352324
[08:23:27.987] TRAIN: iteration 4387 : loss : 0.213055, loss_ce: 0.006893, loss_dice: 0.419217
[08:23:28.195] TRAIN: iteration 4388 : loss : 0.138745, loss_ce: 0.004390, loss_dice: 0.273100
[08:23:28.402] TRAIN: iteration 4389 : loss : 0.244144, loss_ce: 0.006376, loss_dice: 0.481912
[08:23:28.609] TRAIN: iteration 4390 : loss : 0.252636, loss_ce: 0.004986, loss_dice: 0.500286
[08:23:28.817] TRAIN: iteration 4391 : loss : 0.124890, loss_ce: 0.009274, loss_dice: 0.240506
[08:23:29.025] TRAIN: iteration 4392 : loss : 0.202262, loss_ce: 0.012130, loss_dice: 0.392394
[08:23:29.487] TRAIN: iteration 4393 : loss : 0.096800, loss_ce: 0.005156, loss_dice: 0.188444
[08:23:29.696] TRAIN: iteration 4394 : loss : 0.165336, loss_ce: 0.005184, loss_dice: 0.325487
[08:23:30.769] TRAIN: iteration 4395 : loss : 0.253153, loss_ce: 0.005955, loss_dice: 0.500352
[08:23:30.977] TRAIN: iteration 4396 : loss : 0.090101, loss_ce: 0.008575, loss_dice: 0.171627
[08:23:31.186] TRAIN: iteration 4397 : loss : 0.171190, loss_ce: 0.005692, loss_dice: 0.336688
[08:23:31.395] TRAIN: iteration 4398 : loss : 0.112380, loss_ce: 0.005012, loss_dice: 0.219748
[08:23:31.604] TRAIN: iteration 4399 : loss : 0.238879, loss_ce: 0.005724, loss_dice: 0.472033
[08:23:31.813] TRAIN: iteration 4400 : loss : 0.206381, loss_ce: 0.015734, loss_dice: 0.397028
[08:23:32.051] TRAIN: iteration 4401 : loss : 0.251790, loss_ce: 0.003464, loss_dice: 0.500116
[08:23:32.259] TRAIN: iteration 4402 : loss : 0.246270, loss_ce: 0.004120, loss_dice: 0.488421
[08:23:32.852] TRAIN: iteration 4403 : loss : 0.187578, loss_ce: 0.005764, loss_dice: 0.369393
[08:23:33.061] TRAIN: iteration 4404 : loss : 0.184983, loss_ce: 0.007769, loss_dice: 0.362197
[08:23:33.269] TRAIN: iteration 4405 : loss : 0.226072, loss_ce: 0.005839, loss_dice: 0.446304
[08:23:33.479] TRAIN: iteration 4406 : loss : 0.145491, loss_ce: 0.008709, loss_dice: 0.282273
[08:23:33.692] TRAIN: iteration 4407 : loss : 0.092485, loss_ce: 0.005152, loss_dice: 0.179818
[08:23:33.901] TRAIN: iteration 4408 : loss : 0.234859, loss_ce: 0.007456, loss_dice: 0.462262
[08:23:34.107] TRAIN: iteration 4409 : loss : 0.113078, loss_ce: 0.010941, loss_dice: 0.215216
[08:23:34.323] TRAIN: iteration 4410 : loss : 0.244848, loss_ce: 0.007523, loss_dice: 0.482173
[08:23:34.620] TRAIN: iteration 4411 : loss : 0.201484, loss_ce: 0.007168, loss_dice: 0.395800
[08:23:34.828] TRAIN: iteration 4412 : loss : 0.196701, loss_ce: 0.006733, loss_dice: 0.386669
[08:23:36.076] TRAIN: iteration 4413 : loss : 0.194500, loss_ce: 0.010649, loss_dice: 0.378351
[08:23:36.286] TRAIN: iteration 4414 : loss : 0.166256, loss_ce: 0.008702, loss_dice: 0.323809
[08:23:36.495] TRAIN: iteration 4415 : loss : 0.253814, loss_ce: 0.007144, loss_dice: 0.500484
[08:23:36.709] TRAIN: iteration 4416 : loss : 0.247492, loss_ce: 0.007464, loss_dice: 0.487520
[08:23:36.975] TRAIN: iteration 4417 : loss : 0.212560, loss_ce: 0.006861, loss_dice: 0.418260
[08:23:37.183] TRAIN: iteration 4418 : loss : 0.092709, loss_ce: 0.003747, loss_dice: 0.181670
[08:23:37.392] TRAIN: iteration 4419 : loss : 0.156046, loss_ce: 0.023083, loss_dice: 0.289010
[08:23:37.602] TRAIN: iteration 4420 : loss : 0.212249, loss_ce: 0.011497, loss_dice: 0.413002
[08:23:37.853] TRAIN: iteration 4421 : loss : 0.252878, loss_ce: 0.005392, loss_dice: 0.500364
[08:23:38.061] TRAIN: iteration 4422 : loss : 0.167501, loss_ce: 0.005906, loss_dice: 0.329097
[08:23:38.269] TRAIN: iteration 4423 : loss : 0.203663, loss_ce: 0.005319, loss_dice: 0.402008
[08:23:39.632] TRAIN: iteration 4424 : loss : 0.224441, loss_ce: 0.005553, loss_dice: 0.443328
[08:23:40.557] TRAIN: iteration 4425 : loss : 0.117197, loss_ce: 0.010457, loss_dice: 0.223936
[08:23:40.764] TRAIN: iteration 4426 : loss : 0.122837, loss_ce: 0.004948, loss_dice: 0.240726
[08:23:40.971] TRAIN: iteration 4427 : loss : 0.205568, loss_ce: 0.004783, loss_dice: 0.406354
[08:23:41.178] TRAIN: iteration 4428 : loss : 0.198952, loss_ce: 0.018250, loss_dice: 0.379654
[08:23:41.385] TRAIN: iteration 4429 : loss : 0.252101, loss_ce: 0.003983, loss_dice: 0.500220
[08:23:41.598] TRAIN: iteration 4430 : loss : 0.115741, loss_ce: 0.009342, loss_dice: 0.222141
[08:23:41.807] TRAIN: iteration 4431 : loss : 0.230821, loss_ce: 0.012021, loss_dice: 0.449620
[08:23:42.606] TRAIN: iteration 4432 : loss : 0.250934, loss_ce: 0.004347, loss_dice: 0.497521
[08:23:42.812] TRAIN: iteration 4433 : loss : 0.171773, loss_ce: 0.006900, loss_dice: 0.336646
[08:23:43.020] TRAIN: iteration 4434 : loss : 0.151197, loss_ce: 0.006257, loss_dice: 0.296137
[08:23:43.230] TRAIN: iteration 4435 : loss : 0.252574, loss_ce: 0.004865, loss_dice: 0.500282
[08:23:43.437] TRAIN: iteration 4436 : loss : 0.252543, loss_ce: 0.004882, loss_dice: 0.500204
[08:23:43.645] TRAIN: iteration 4437 : loss : 0.148075, loss_ce: 0.005808, loss_dice: 0.290342
[08:23:43.855] TRAIN: iteration 4438 : loss : 0.115449, loss_ce: 0.003811, loss_dice: 0.227086
[08:23:44.066] TRAIN: iteration 4439 : loss : 0.075709, loss_ce: 0.003730, loss_dice: 0.147687
[08:23:44.306] TRAIN: iteration 4440 : loss : 0.184000, loss_ce: 0.004866, loss_dice: 0.363135
[08:23:44.546] TRAIN: iteration 4441 : loss : 0.080778, loss_ce: 0.003891, loss_dice: 0.157664
[08:23:44.755] TRAIN: iteration 4442 : loss : 0.251994, loss_ce: 0.009126, loss_dice: 0.494862
[08:23:44.963] TRAIN: iteration 4443 : loss : 0.051375, loss_ce: 0.002071, loss_dice: 0.100679
[08:23:45.170] TRAIN: iteration 4444 : loss : 0.105303, loss_ce: 0.006803, loss_dice: 0.203804
[08:23:45.378] TRAIN: iteration 4445 : loss : 0.234546, loss_ce: 0.018882, loss_dice: 0.450210
[08:23:45.588] TRAIN: iteration 4446 : loss : 0.072261, loss_ce: 0.002682, loss_dice: 0.141841
[08:23:45.795] TRAIN: iteration 4447 : loss : 0.177482, loss_ce: 0.002414, loss_dice: 0.352550
[08:23:48.077] TRAIN: iteration 4448 : loss : 0.094954, loss_ce: 0.003110, loss_dice: 0.186798
[08:23:48.284] TRAIN: iteration 4449 : loss : 0.098559, loss_ce: 0.005093, loss_dice: 0.192025
[08:23:48.492] TRAIN: iteration 4450 : loss : 0.130493, loss_ce: 0.002919, loss_dice: 0.258068
[08:23:48.700] TRAIN: iteration 4451 : loss : 0.123363, loss_ce: 0.002980, loss_dice: 0.243746
[08:23:48.909] TRAIN: iteration 4452 : loss : 0.174616, loss_ce: 0.004747, loss_dice: 0.344484
[08:23:49.117] TRAIN: iteration 4453 : loss : 0.209731, loss_ce: 0.007126, loss_dice: 0.412336
[08:23:49.325] TRAIN: iteration 4454 : loss : 0.183002, loss_ce: 0.003854, loss_dice: 0.362150
[08:23:49.533] TRAIN: iteration 4455 : loss : 0.180075, loss_ce: 0.010734, loss_dice: 0.349415
[08:23:50.428] TRAIN: iteration 4456 : loss : 0.241249, loss_ce: 0.010009, loss_dice: 0.472490
[08:23:50.636] TRAIN: iteration 4457 : loss : 0.238402, loss_ce: 0.003341, loss_dice: 0.473464
[08:23:50.843] TRAIN: iteration 4458 : loss : 0.220977, loss_ce: 0.007258, loss_dice: 0.434697
[08:23:51.052] TRAIN: iteration 4459 : loss : 0.225569, loss_ce: 0.003186, loss_dice: 0.447951
[08:23:51.267] TRAIN: iteration 4460 : loss : 0.195894, loss_ce: 0.004828, loss_dice: 0.386960
[08:23:51.505] TRAIN: iteration 4461 : loss : 0.185329, loss_ce: 0.011034, loss_dice: 0.359625
[08:23:51.715] TRAIN: iteration 4462 : loss : 0.142004, loss_ce: 0.005591, loss_dice: 0.278418
[08:23:51.925] TRAIN: iteration 4463 : loss : 0.252216, loss_ce: 0.004174, loss_dice: 0.500259
[08:23:52.198] TRAIN: iteration 4464 : loss : 0.206173, loss_ce: 0.004821, loss_dice: 0.407525
[08:23:52.405] TRAIN: iteration 4465 : loss : 0.150709, loss_ce: 0.004096, loss_dice: 0.297321
[08:23:52.612] TRAIN: iteration 4466 : loss : 0.234919, loss_ce: 0.004087, loss_dice: 0.465751
[08:23:52.819] TRAIN: iteration 4467 : loss : 0.252212, loss_ce: 0.004208, loss_dice: 0.500216
[08:23:53.027] TRAIN: iteration 4468 : loss : 0.244076, loss_ce: 0.004245, loss_dice: 0.483906
[08:23:53.236] TRAIN: iteration 4469 : loss : 0.077736, loss_ce: 0.003856, loss_dice: 0.151617
[08:23:53.446] TRAIN: iteration 4470 : loss : 0.241402, loss_ce: 0.003974, loss_dice: 0.478831
[08:23:53.653] TRAIN: iteration 4471 : loss : 0.152093, loss_ce: 0.004672, loss_dice: 0.299513
[08:23:53.861] TRAIN: iteration 4472 : loss : 0.103754, loss_ce: 0.004974, loss_dice: 0.202534
[08:23:54.069] TRAIN: iteration 4473 : loss : 0.176464, loss_ce: 0.005681, loss_dice: 0.347246
[08:23:54.275] TRAIN: iteration 4474 : loss : 0.103967, loss_ce: 0.005065, loss_dice: 0.202868
[08:23:54.484] TRAIN: iteration 4475 : loss : 0.251941, loss_ce: 0.003688, loss_dice: 0.500195
[08:23:54.711] TRAIN: iteration 4476 : loss : 0.071759, loss_ce: 0.003133, loss_dice: 0.140386
[08:23:54.922] TRAIN: iteration 4477 : loss : 0.113092, loss_ce: 0.005476, loss_dice: 0.220707
[08:23:55.134] TRAIN: iteration 4478 : loss : 0.249240, loss_ce: 0.034526, loss_dice: 0.463955
[08:23:55.341] TRAIN: iteration 4479 : loss : 0.127805, loss_ce: 0.010586, loss_dice: 0.245024
[08:23:56.702] TRAIN: iteration 4480 : loss : 0.252079, loss_ce: 0.003907, loss_dice: 0.500251
[08:23:56.947] TRAIN: iteration 4481 : loss : 0.128689, loss_ce: 0.009041, loss_dice: 0.248337
[08:23:57.155] TRAIN: iteration 4482 : loss : 0.121162, loss_ce: 0.006020, loss_dice: 0.236305
[08:23:57.362] TRAIN: iteration 4483 : loss : 0.143673, loss_ce: 0.013189, loss_dice: 0.274158
[08:23:57.743] TRAIN: iteration 4484 : loss : 0.132770, loss_ce: 0.006077, loss_dice: 0.259464
[08:23:57.951] TRAIN: iteration 4485 : loss : 0.153549, loss_ce: 0.005186, loss_dice: 0.301912
[08:23:58.158] TRAIN: iteration 4486 : loss : 0.173706, loss_ce: 0.014964, loss_dice: 0.332448
[08:23:58.367] TRAIN: iteration 4487 : loss : 0.231084, loss_ce: 0.007051, loss_dice: 0.455118
[08:23:59.232] TRAIN: iteration 4488 : loss : 0.163440, loss_ce: 0.007843, loss_dice: 0.319037
[08:23:59.438] TRAIN: iteration 4489 : loss : 0.196260, loss_ce: 0.005985, loss_dice: 0.386535
[08:23:59.644] TRAIN: iteration 4490 : loss : 0.093949, loss_ce: 0.004725, loss_dice: 0.183173
[08:23:59.851] TRAIN: iteration 4491 : loss : 0.097217, loss_ce: 0.006161, loss_dice: 0.188273
[08:24:00.770] TRAIN: iteration 4492 : loss : 0.193825, loss_ce: 0.011598, loss_dice: 0.376053
[08:24:00.977] TRAIN: iteration 4493 : loss : 0.168781, loss_ce: 0.008876, loss_dice: 0.328686
[08:24:01.188] TRAIN: iteration 4494 : loss : 0.132485, loss_ce: 0.005447, loss_dice: 0.259522
[08:24:01.396] TRAIN: iteration 4495 : loss : 0.251494, loss_ce: 0.011503, loss_dice: 0.491485
[08:24:01.744] TRAIN: iteration 4496 : loss : 0.182907, loss_ce: 0.007244, loss_dice: 0.358571
[08:24:01.966] TRAIN: iteration 4497 : loss : 0.155025, loss_ce: 0.007002, loss_dice: 0.303049
[08:24:02.181] TRAIN: iteration 4498 : loss : 0.230793, loss_ce: 0.007248, loss_dice: 0.454337
[08:24:02.390] TRAIN: iteration 4499 : loss : 0.165104, loss_ce: 0.007100, loss_dice: 0.323109
[08:24:02.609] TRAIN: iteration 4500 : loss : 0.130231, loss_ce: 0.015563, loss_dice: 0.244900
[08:24:02.846] TRAIN: iteration 4501 : loss : 0.216554, loss_ce: 0.029184, loss_dice: 0.403924
[08:24:03.054] TRAIN: iteration 4502 : loss : 0.218908, loss_ce: 0.005855, loss_dice: 0.431961
[08:24:03.263] TRAIN: iteration 4503 : loss : 0.186661, loss_ce: 0.010807, loss_dice: 0.362515
[08:24:03.570] TRAIN: iteration 4504 : loss : 0.165410, loss_ce: 0.007197, loss_dice: 0.323623
[08:24:03.779] TRAIN: iteration 4505 : loss : 0.144202, loss_ce: 0.011617, loss_dice: 0.276787
[08:24:03.994] TRAIN: iteration 4506 : loss : 0.223588, loss_ce: 0.007799, loss_dice: 0.439377
[08:24:04.203] TRAIN: iteration 4507 : loss : 0.239658, loss_ce: 0.007229, loss_dice: 0.472088
[08:24:04.410] TRAIN: iteration 4508 : loss : 0.098293, loss_ce: 0.006518, loss_dice: 0.190067
[08:24:04.625] TRAIN: iteration 4509 : loss : 0.147777, loss_ce: 0.019793, loss_dice: 0.275762
[08:24:04.833] TRAIN: iteration 4510 : loss : 0.124887, loss_ce: 0.005577, loss_dice: 0.244196
[08:24:06.413] TRAIN: iteration 4511 : loss : 0.240417, loss_ce: 0.007682, loss_dice: 0.473151
[08:24:09.198] TRAIN: iteration 4512 : loss : 0.131689, loss_ce: 0.007991, loss_dice: 0.255387
[08:24:09.405] TRAIN: iteration 4513 : loss : 0.178914, loss_ce: 0.006503, loss_dice: 0.351326
[08:24:09.611] TRAIN: iteration 4514 : loss : 0.169021, loss_ce: 0.030919, loss_dice: 0.307123
[08:24:09.818] TRAIN: iteration 4515 : loss : 0.208179, loss_ce: 0.006822, loss_dice: 0.409535
[08:24:10.025] TRAIN: iteration 4516 : loss : 0.179511, loss_ce: 0.007592, loss_dice: 0.351430
[08:24:10.232] TRAIN: iteration 4517 : loss : 0.068985, loss_ce: 0.005488, loss_dice: 0.132481
[08:24:10.438] TRAIN: iteration 4518 : loss : 0.091464, loss_ce: 0.009901, loss_dice: 0.173026
[08:24:10.647] TRAIN: iteration 4519 : loss : 0.136964, loss_ce: 0.005736, loss_dice: 0.268192
[08:24:10.853] TRAIN: iteration 4520 : loss : 0.202230, loss_ce: 0.013219, loss_dice: 0.391241
[08:24:11.093] TRAIN: iteration 4521 : loss : 0.155305, loss_ce: 0.019888, loss_dice: 0.290721
[08:24:11.300] TRAIN: iteration 4522 : loss : 0.253462, loss_ce: 0.006554, loss_dice: 0.500370
[08:24:11.510] TRAIN: iteration 4523 : loss : 0.202235, loss_ce: 0.008910, loss_dice: 0.395560
[08:24:11.716] TRAIN: iteration 4524 : loss : 0.121284, loss_ce: 0.005564, loss_dice: 0.237004
[08:24:11.929] TRAIN: iteration 4525 : loss : 0.237331, loss_ce: 0.006544, loss_dice: 0.468118
[08:24:12.136] TRAIN: iteration 4526 : loss : 0.133972, loss_ce: 0.005672, loss_dice: 0.262272
[08:24:12.345] TRAIN: iteration 4527 : loss : 0.244142, loss_ce: 0.009897, loss_dice: 0.478387
[08:24:12.617] TRAIN: iteration 4528 : loss : 0.200947, loss_ce: 0.007586, loss_dice: 0.394309
[08:24:12.824] TRAIN: iteration 4529 : loss : 0.153166, loss_ce: 0.006692, loss_dice: 0.299639
[08:24:13.031] TRAIN: iteration 4530 : loss : 0.079477, loss_ce: 0.004731, loss_dice: 0.154222
[08:24:13.399] TRAIN: iteration 4531 : loss : 0.176714, loss_ce: 0.012981, loss_dice: 0.340448
[08:24:13.726] TRAIN: iteration 4532 : loss : 0.147666, loss_ce: 0.005286, loss_dice: 0.290046
[08:24:13.934] TRAIN: iteration 4533 : loss : 0.154703, loss_ce: 0.009259, loss_dice: 0.300147
[08:24:14.142] TRAIN: iteration 4534 : loss : 0.252467, loss_ce: 0.004639, loss_dice: 0.500296
[08:24:14.353] TRAIN: iteration 4535 : loss : 0.235477, loss_ce: 0.005218, loss_dice: 0.465737
[08:24:18.013] TRAIN: iteration 4536 : loss : 0.146079, loss_ce: 0.005613, loss_dice: 0.286546
[08:24:18.221] TRAIN: iteration 4537 : loss : 0.177947, loss_ce: 0.023134, loss_dice: 0.332760
[08:24:18.514] TRAIN: iteration 4538 : loss : 0.193029, loss_ce: 0.003890, loss_dice: 0.382168
[08:24:18.726] TRAIN: iteration 4539 : loss : 0.229861, loss_ce: 0.004146, loss_dice: 0.455576
[08:24:18.933] TRAIN: iteration 4540 : loss : 0.251008, loss_ce: 0.003827, loss_dice: 0.498188
[08:24:19.170] TRAIN: iteration 4541 : loss : 0.167772, loss_ce: 0.006883, loss_dice: 0.328662
[08:24:19.384] TRAIN: iteration 4542 : loss : 0.210361, loss_ce: 0.005573, loss_dice: 0.415150
[08:24:19.591] TRAIN: iteration 4543 : loss : 0.107626, loss_ce: 0.007331, loss_dice: 0.207922
[08:24:21.284] TRAIN: iteration 4544 : loss : 0.178466, loss_ce: 0.012292, loss_dice: 0.344639
[08:24:21.491] TRAIN: iteration 4545 : loss : 0.209900, loss_ce: 0.023663, loss_dice: 0.396137
[08:24:21.699] TRAIN: iteration 4546 : loss : 0.252325, loss_ce: 0.004442, loss_dice: 0.500207
[08:24:21.906] TRAIN: iteration 4547 : loss : 0.187108, loss_ce: 0.014250, loss_dice: 0.359965
[08:24:22.113] TRAIN: iteration 4548 : loss : 0.231581, loss_ce: 0.047048, loss_dice: 0.416113
[08:24:22.326] TRAIN: iteration 4549 : loss : 0.252931, loss_ce: 0.005549, loss_dice: 0.500313
[08:24:22.537] TRAIN: iteration 4550 : loss : 0.113270, loss_ce: 0.012577, loss_dice: 0.213964
[08:24:22.743] TRAIN: iteration 4551 : loss : 0.109714, loss_ce: 0.007262, loss_dice: 0.212167
[08:24:23.150] TRAIN: iteration 4552 : loss : 0.189072, loss_ce: 0.011757, loss_dice: 0.366387
[08:24:23.356] TRAIN: iteration 4553 : loss : 0.174535, loss_ce: 0.013593, loss_dice: 0.335478
[08:24:23.564] TRAIN: iteration 4554 : loss : 0.238386, loss_ce: 0.009330, loss_dice: 0.467442
[08:24:23.771] TRAIN: iteration 4555 : loss : 0.194562, loss_ce: 0.007604, loss_dice: 0.381520
[08:24:23.979] TRAIN: iteration 4556 : loss : 0.254740, loss_ce: 0.008871, loss_dice: 0.500609
[08:24:24.188] TRAIN: iteration 4557 : loss : 0.208474, loss_ce: 0.012593, loss_dice: 0.404355
[08:24:24.395] TRAIN: iteration 4558 : loss : 0.123626, loss_ce: 0.015916, loss_dice: 0.231337
[08:24:24.603] TRAIN: iteration 4559 : loss : 0.203665, loss_ce: 0.013625, loss_dice: 0.393706
[08:24:24.971] TRAIN: iteration 4560 : loss : 0.108524, loss_ce: 0.011800, loss_dice: 0.205249
[08:24:25.206] TRAIN: iteration 4561 : loss : 0.176903, loss_ce: 0.008544, loss_dice: 0.345262
[08:24:25.413] TRAIN: iteration 4562 : loss : 0.222600, loss_ce: 0.009392, loss_dice: 0.435808
[08:24:25.619] TRAIN: iteration 4563 : loss : 0.233244, loss_ce: 0.009625, loss_dice: 0.456864
[08:24:25.831] TRAIN: iteration 4564 : loss : 0.153522, loss_ce: 0.008551, loss_dice: 0.298493
[08:24:26.039] TRAIN: iteration 4565 : loss : 0.136508, loss_ce: 0.009778, loss_dice: 0.263238
[08:24:26.252] TRAIN: iteration 4566 : loss : 0.254462, loss_ce: 0.008875, loss_dice: 0.500050
[08:24:26.462] TRAIN: iteration 4567 : loss : 0.212300, loss_ce: 0.012863, loss_dice: 0.411737
[08:24:28.187] TRAIN: iteration 4568 : loss : 0.233054, loss_ce: 0.012105, loss_dice: 0.454003
[08:24:28.400] TRAIN: iteration 4569 : loss : 0.183570, loss_ce: 0.015009, loss_dice: 0.352131
[08:24:28.608] TRAIN: iteration 4570 : loss : 0.230812, loss_ce: 0.009819, loss_dice: 0.451805
[08:24:28.818] TRAIN: iteration 4571 : loss : 0.113129, loss_ce: 0.005372, loss_dice: 0.220886
[08:24:29.026] TRAIN: iteration 4572 : loss : 0.127097, loss_ce: 0.006206, loss_dice: 0.247987
[08:24:29.236] TRAIN: iteration 4573 : loss : 0.152764, loss_ce: 0.005483, loss_dice: 0.300045
[08:24:29.446] TRAIN: iteration 4574 : loss : 0.128192, loss_ce: 0.005877, loss_dice: 0.250507
[08:24:31.694] TRAIN: iteration 4575 : loss : 0.156032, loss_ce: 0.008929, loss_dice: 0.303135
[08:24:31.903] TRAIN: iteration 4576 : loss : 0.252461, loss_ce: 0.004663, loss_dice: 0.500259
[08:24:32.110] TRAIN: iteration 4577 : loss : 0.210409, loss_ce: 0.004159, loss_dice: 0.416658
[08:24:32.318] TRAIN: iteration 4578 : loss : 0.252012, loss_ce: 0.003831, loss_dice: 0.500194
[08:24:32.532] TRAIN: iteration 4579 : loss : 0.139605, loss_ce: 0.003962, loss_dice: 0.275248
[08:24:32.740] TRAIN: iteration 4580 : loss : 0.110413, loss_ce: 0.004419, loss_dice: 0.216406
[08:24:32.741] NaN or Inf found in input tensor.
[08:24:32.958] TRAIN: iteration 4581 : loss : 0.242716, loss_ce: 0.003787, loss_dice: 0.481646
[08:24:33.166] TRAIN: iteration 4582 : loss : 0.251457, loss_ce: 0.002768, loss_dice: 0.500146
[08:24:35.402] TRAIN: iteration 4583 : loss : 0.198627, loss_ce: 0.020815, loss_dice: 0.376440
[08:24:35.609] TRAIN: iteration 4584 : loss : 0.198627, loss_ce: 0.002902, loss_dice: 0.394352
[08:24:35.817] TRAIN: iteration 4585 : loss : 0.094454, loss_ce: 0.001817, loss_dice: 0.187092
[08:24:36.621] TRAIN: iteration 4586 : loss : 0.181541, loss_ce: 0.002060, loss_dice: 0.361022
[08:24:36.829] TRAIN: iteration 4587 : loss : 0.234218, loss_ce: 0.002493, loss_dice: 0.465943
[08:24:37.044] TRAIN: iteration 4588 : loss : 0.234147, loss_ce: 0.013870, loss_dice: 0.454424
[08:24:37.251] TRAIN: iteration 4589 : loss : 0.243579, loss_ce: 0.011535, loss_dice: 0.475623
[08:24:37.464] TRAIN: iteration 4590 : loss : 0.222982, loss_ce: 0.016296, loss_dice: 0.429668
[08:24:37.673] TRAIN: iteration 4591 : loss : 0.239679, loss_ce: 0.001894, loss_dice: 0.477465
[08:24:37.880] TRAIN: iteration 4592 : loss : 0.228125, loss_ce: 0.004991, loss_dice: 0.451260
[08:24:38.092] TRAIN: iteration 4593 : loss : 0.184578, loss_ce: 0.004397, loss_dice: 0.364759
[08:24:38.777] TRAIN: iteration 4594 : loss : 0.089347, loss_ce: 0.003177, loss_dice: 0.175518
[08:24:38.992] TRAIN: iteration 4595 : loss : 0.255482, loss_ce: 0.011037, loss_dice: 0.499927
[08:24:39.200] TRAIN: iteration 4596 : loss : 0.140392, loss_ce: 0.005745, loss_dice: 0.275039
[08:24:39.410] TRAIN: iteration 4597 : loss : 0.178459, loss_ce: 0.005317, loss_dice: 0.351601
[08:24:39.619] TRAIN: iteration 4598 : loss : 0.137369, loss_ce: 0.008044, loss_dice: 0.266694
[08:24:39.829] TRAIN: iteration 4599 : loss : 0.239176, loss_ce: 0.006415, loss_dice: 0.471938
[08:24:40.039] TRAIN: iteration 4600 : loss : 0.165518, loss_ce: 0.011751, loss_dice: 0.319285
[08:24:40.282] TRAIN: iteration 4601 : loss : 0.206679, loss_ce: 0.006000, loss_dice: 0.407357
[08:24:40.604] TRAIN: iteration 4602 : loss : 0.195532, loss_ce: 0.005071, loss_dice: 0.385993
[08:24:40.811] TRAIN: iteration 4603 : loss : 0.140606, loss_ce: 0.004910, loss_dice: 0.276302
[08:24:41.029] TRAIN: iteration 4604 : loss : 0.207352, loss_ce: 0.004996, loss_dice: 0.409707
[08:24:41.244] TRAIN: iteration 4605 : loss : 0.223296, loss_ce: 0.007193, loss_dice: 0.439398
[08:24:41.457] TRAIN: iteration 4606 : loss : 0.106765, loss_ce: 0.004431, loss_dice: 0.209099
[08:24:42.010] TRAIN: iteration 4607 : loss : 0.180211, loss_ce: 0.009852, loss_dice: 0.350569
[08:24:42.217] TRAIN: iteration 4608 : loss : 0.202466, loss_ce: 0.008457, loss_dice: 0.396476
[08:24:42.424] TRAIN: iteration 4609 : loss : 0.219466, loss_ce: 0.005479, loss_dice: 0.433454
[08:24:42.630] TRAIN: iteration 4610 : loss : 0.252834, loss_ce: 0.005346, loss_dice: 0.500322
[08:24:42.838] TRAIN: iteration 4611 : loss : 0.122596, loss_ce: 0.007151, loss_dice: 0.238042
[08:24:43.045] TRAIN: iteration 4612 : loss : 0.236451, loss_ce: 0.005528, loss_dice: 0.467374
[08:24:43.258] TRAIN: iteration 4613 : loss : 0.162786, loss_ce: 0.005406, loss_dice: 0.320166
[08:24:43.466] TRAIN: iteration 4614 : loss : 0.133582, loss_ce: 0.004092, loss_dice: 0.263072
[08:24:43.765] TRAIN: iteration 4615 : loss : 0.143525, loss_ce: 0.007234, loss_dice: 0.279817
[08:24:44.976] TRAIN: iteration 4616 : loss : 0.252612, loss_ce: 0.004939, loss_dice: 0.500284
[08:24:45.187] TRAIN: iteration 4617 : loss : 0.166384, loss_ce: 0.004929, loss_dice: 0.327839
[08:24:45.398] TRAIN: iteration 4618 : loss : 0.252265, loss_ce: 0.004290, loss_dice: 0.500240
[08:24:45.606] TRAIN: iteration 4619 : loss : 0.239168, loss_ce: 0.004379, loss_dice: 0.473956
[08:24:45.814] TRAIN: iteration 4620 : loss : 0.102184, loss_ce: 0.005135, loss_dice: 0.199232
[08:24:46.053] TRAIN: iteration 4621 : loss : 0.187637, loss_ce: 0.006989, loss_dice: 0.368284
[08:24:46.261] TRAIN: iteration 4622 : loss : 0.139382, loss_ce: 0.004901, loss_dice: 0.273863
[08:24:46.469] TRAIN: iteration 4623 : loss : 0.252013, loss_ce: 0.003799, loss_dice: 0.500228
[08:24:47.625] TRAIN: iteration 4624 : loss : 0.219017, loss_ce: 0.003792, loss_dice: 0.434242
[08:24:47.831] TRAIN: iteration 4625 : loss : 0.130685, loss_ce: 0.008836, loss_dice: 0.252535
[08:24:48.037] TRAIN: iteration 4626 : loss : 0.251872, loss_ce: 0.003530, loss_dice: 0.500215
[08:24:48.245] TRAIN: iteration 4627 : loss : 0.213759, loss_ce: 0.006835, loss_dice: 0.420682
[08:24:48.450] TRAIN: iteration 4628 : loss : 0.104698, loss_ce: 0.006864, loss_dice: 0.202532
[08:24:48.657] TRAIN: iteration 4629 : loss : 0.236765, loss_ce: 0.004164, loss_dice: 0.469366
[08:24:48.865] TRAIN: iteration 4630 : loss : 0.132349, loss_ce: 0.004753, loss_dice: 0.259944
[08:24:49.082] TRAIN: iteration 4631 : loss : 0.225576, loss_ce: 0.004959, loss_dice: 0.446192
[08:24:49.747] TRAIN: iteration 4632 : loss : 0.217601, loss_ce: 0.004613, loss_dice: 0.430589
[08:24:51.403] TRAIN: iteration 4633 : loss : 0.187635, loss_ce: 0.011480, loss_dice: 0.363789
[08:24:51.610] TRAIN: iteration 4634 : loss : 0.082564, loss_ce: 0.004902, loss_dice: 0.160225
[08:24:51.822] TRAIN: iteration 4635 : loss : 0.115343, loss_ce: 0.004243, loss_dice: 0.226443
[08:24:52.033] TRAIN: iteration 4636 : loss : 0.180535, loss_ce: 0.005717, loss_dice: 0.355352
[08:24:52.243] TRAIN: iteration 4637 : loss : 0.084167, loss_ce: 0.005053, loss_dice: 0.163281
[08:24:52.452] TRAIN: iteration 4638 : loss : 0.252839, loss_ce: 0.005296, loss_dice: 0.500381
[08:24:52.661] TRAIN: iteration 4639 : loss : 0.173157, loss_ce: 0.005041, loss_dice: 0.341273
[08:24:52.869] TRAIN: iteration 4640 : loss : 0.219565, loss_ce: 0.006167, loss_dice: 0.432962
[08:24:53.594] TRAIN: iteration 4641 : loss : 0.137401, loss_ce: 0.006792, loss_dice: 0.268010
[08:24:53.808] TRAIN: iteration 4642 : loss : 0.151471, loss_ce: 0.008046, loss_dice: 0.294896
[08:24:54.015] TRAIN: iteration 4643 : loss : 0.066588, loss_ce: 0.003790, loss_dice: 0.129387
[08:24:54.223] TRAIN: iteration 4644 : loss : 0.209153, loss_ce: 0.005289, loss_dice: 0.413016
[08:24:54.871] TRAIN: iteration 4645 : loss : 0.162325, loss_ce: 0.019334, loss_dice: 0.305317
[08:24:55.081] TRAIN: iteration 4646 : loss : 0.184537, loss_ce: 0.015149, loss_dice: 0.353925
[08:24:55.294] TRAIN: iteration 4647 : loss : 0.251435, loss_ce: 0.015792, loss_dice: 0.487078
[08:24:55.501] TRAIN: iteration 4648 : loss : 0.161241, loss_ce: 0.006519, loss_dice: 0.315963
[08:24:56.815] TRAIN: iteration 4649 : loss : 0.138485, loss_ce: 0.008539, loss_dice: 0.268431
[08:24:57.025] TRAIN: iteration 4650 : loss : 0.127618, loss_ce: 0.009394, loss_dice: 0.245843
[08:24:57.239] TRAIN: iteration 4651 : loss : 0.253087, loss_ce: 0.005766, loss_dice: 0.500409
[08:24:57.446] TRAIN: iteration 4652 : loss : 0.125586, loss_ce: 0.005231, loss_dice: 0.245940
[08:24:57.653] TRAIN: iteration 4653 : loss : 0.174013, loss_ce: 0.010651, loss_dice: 0.337375
[08:24:57.861] TRAIN: iteration 4654 : loss : 0.097194, loss_ce: 0.008067, loss_dice: 0.186321
[08:24:58.068] TRAIN: iteration 4655 : loss : 0.087722, loss_ce: 0.006101, loss_dice: 0.169343
[08:25:00.386] TRAIN: iteration 4656 : loss : 0.114983, loss_ce: 0.012323, loss_dice: 0.217644
[08:25:00.599] TRAIN: iteration 4657 : loss : 0.216000, loss_ce: 0.005846, loss_dice: 0.426155
[08:25:00.819] TRAIN: iteration 4658 : loss : 0.150407, loss_ce: 0.018938, loss_dice: 0.281877
[08:25:01.026] TRAIN: iteration 4659 : loss : 0.098648, loss_ce: 0.005514, loss_dice: 0.191781
[08:25:01.234] TRAIN: iteration 4660 : loss : 0.179105, loss_ce: 0.005777, loss_dice: 0.352432
[08:25:01.470] TRAIN: iteration 4661 : loss : 0.160330, loss_ce: 0.005749, loss_dice: 0.314911
[08:25:01.691] TRAIN: iteration 4662 : loss : 0.204803, loss_ce: 0.005426, loss_dice: 0.404181
[08:25:01.900] TRAIN: iteration 4663 : loss : 0.209291, loss_ce: 0.005721, loss_dice: 0.412860
[08:25:02.159] TRAIN: iteration 4664 : loss : 0.214196, loss_ce: 0.005520, loss_dice: 0.422872
[08:25:02.368] TRAIN: iteration 4665 : loss : 0.059595, loss_ce: 0.002787, loss_dice: 0.116403
[08:25:02.577] TRAIN: iteration 4666 : loss : 0.250287, loss_ce: 0.009082, loss_dice: 0.491492
[08:25:02.791] TRAIN: iteration 4667 : loss : 0.164865, loss_ce: 0.004851, loss_dice: 0.324880
[08:25:02.999] TRAIN: iteration 4668 : loss : 0.173716, loss_ce: 0.006397, loss_dice: 0.341035
[08:25:03.342] TRAIN: iteration 4669 : loss : 0.179917, loss_ce: 0.005997, loss_dice: 0.353837
[08:25:03.550] TRAIN: iteration 4670 : loss : 0.254190, loss_ce: 0.008883, loss_dice: 0.499498
[08:25:03.758] TRAIN: iteration 4671 : loss : 0.251053, loss_ce: 0.003698, loss_dice: 0.498408
[08:25:05.531] TRAIN: iteration 4672 : loss : 0.101072, loss_ce: 0.003601, loss_dice: 0.198544
[08:25:05.739] TRAIN: iteration 4673 : loss : 0.174336, loss_ce: 0.025664, loss_dice: 0.323007
[08:25:05.950] TRAIN: iteration 4674 : loss : 0.111269, loss_ce: 0.004144, loss_dice: 0.218394
[08:25:06.157] TRAIN: iteration 4675 : loss : 0.143274, loss_ce: 0.011219, loss_dice: 0.275329
[08:25:06.374] TRAIN: iteration 4676 : loss : 0.066312, loss_ce: 0.002680, loss_dice: 0.129945
[08:25:06.595] TRAIN: iteration 4677 : loss : 0.242714, loss_ce: 0.004843, loss_dice: 0.480586
[08:25:06.801] TRAIN: iteration 4678 : loss : 0.246949, loss_ce: 0.004410, loss_dice: 0.489487
[08:25:07.008] TRAIN: iteration 4679 : loss : 0.244908, loss_ce: 0.006632, loss_dice: 0.483184
[08:25:09.052] TRAIN: iteration 4680 : loss : 0.088001, loss_ce: 0.005644, loss_dice: 0.170358
[08:25:09.296] TRAIN: iteration 4681 : loss : 0.160542, loss_ce: 0.010813, loss_dice: 0.310271
[08:25:09.503] TRAIN: iteration 4682 : loss : 0.250084, loss_ce: 0.005494, loss_dice: 0.494673
[08:25:09.712] TRAIN: iteration 4683 : loss : 0.150619, loss_ce: 0.006423, loss_dice: 0.294814
[08:25:09.925] TRAIN: iteration 4684 : loss : 0.105171, loss_ce: 0.004189, loss_dice: 0.206154
[08:25:10.132] TRAIN: iteration 4685 : loss : 0.249254, loss_ce: 0.004967, loss_dice: 0.493541
[08:25:10.342] TRAIN: iteration 4686 : loss : 0.189841, loss_ce: 0.010169, loss_dice: 0.369513
[08:25:10.551] TRAIN: iteration 4687 : loss : 0.198622, loss_ce: 0.005507, loss_dice: 0.391736
[08:25:12.329] TRAIN: iteration 4688 : loss : 0.088871, loss_ce: 0.004382, loss_dice: 0.173359
[08:25:12.535] TRAIN: iteration 4689 : loss : 0.240253, loss_ce: 0.005178, loss_dice: 0.475328
[08:25:12.744] TRAIN: iteration 4690 : loss : 0.252534, loss_ce: 0.004798, loss_dice: 0.500269
[08:25:12.953] TRAIN: iteration 4691 : loss : 0.228657, loss_ce: 0.004221, loss_dice: 0.453094
[08:25:13.162] TRAIN: iteration 4692 : loss : 0.239024, loss_ce: 0.004713, loss_dice: 0.473334
[08:25:13.370] TRAIN: iteration 4693 : loss : 0.091541, loss_ce: 0.003744, loss_dice: 0.179338
[08:25:13.582] TRAIN: iteration 4694 : loss : 0.213310, loss_ce: 0.007388, loss_dice: 0.419233
[08:25:13.790] TRAIN: iteration 4695 : loss : 0.237413, loss_ce: 0.010109, loss_dice: 0.464716
[08:25:15.519] TRAIN: iteration 4696 : loss : 0.128802, loss_ce: 0.007184, loss_dice: 0.250419
[08:25:15.728] TRAIN: iteration 4697 : loss : 0.177702, loss_ce: 0.010186, loss_dice: 0.345218
[08:25:15.936] TRAIN: iteration 4698 : loss : 0.097117, loss_ce: 0.008065, loss_dice: 0.186168
[08:25:16.142] TRAIN: iteration 4699 : loss : 0.245006, loss_ce: 0.007560, loss_dice: 0.482453
[08:25:16.350] TRAIN: iteration 4700 : loss : 0.187101, loss_ce: 0.014175, loss_dice: 0.360028
[08:25:16.592] TRAIN: iteration 4701 : loss : 0.107515, loss_ce: 0.005861, loss_dice: 0.209169
[08:25:16.800] TRAIN: iteration 4702 : loss : 0.217704, loss_ce: 0.004184, loss_dice: 0.431224
[08:25:17.014] TRAIN: iteration 4703 : loss : 0.070479, loss_ce: 0.003872, loss_dice: 0.137085
[08:25:20.589] TRAIN: iteration 4704 : loss : 0.090279, loss_ce: 0.004545, loss_dice: 0.176013
[08:25:20.805] TRAIN: iteration 4705 : loss : 0.156327, loss_ce: 0.009865, loss_dice: 0.302790
[08:25:21.017] TRAIN: iteration 4706 : loss : 0.103718, loss_ce: 0.006464, loss_dice: 0.200972
[08:25:21.226] TRAIN: iteration 4707 : loss : 0.219007, loss_ce: 0.005937, loss_dice: 0.432077
[08:25:21.432] TRAIN: iteration 4708 : loss : 0.252342, loss_ce: 0.005858, loss_dice: 0.498827
[08:25:21.641] TRAIN: iteration 4709 : loss : 0.155078, loss_ce: 0.018200, loss_dice: 0.291956
[08:25:21.848] TRAIN: iteration 4710 : loss : 0.199461, loss_ce: 0.005315, loss_dice: 0.393606
[08:25:22.055] TRAIN: iteration 4711 : loss : 0.224628, loss_ce: 0.006560, loss_dice: 0.442696
[08:25:23.394] TRAIN: iteration 4712 : loss : 0.149267, loss_ce: 0.021566, loss_dice: 0.276969
[08:25:23.601] TRAIN: iteration 4713 : loss : 0.185284, loss_ce: 0.021185, loss_dice: 0.349383
[08:25:23.815] TRAIN: iteration 4714 : loss : 0.243495, loss_ce: 0.005630, loss_dice: 0.481360
[08:25:24.037] TRAIN: iteration 4715 : loss : 0.249108, loss_ce: 0.009003, loss_dice: 0.489214
[08:25:24.243] TRAIN: iteration 4716 : loss : 0.071579, loss_ce: 0.004497, loss_dice: 0.138661
[08:25:24.449] TRAIN: iteration 4717 : loss : 0.192743, loss_ce: 0.011967, loss_dice: 0.373519
[08:25:24.658] TRAIN: iteration 4718 : loss : 0.200230, loss_ce: 0.006362, loss_dice: 0.394098
[08:25:24.865] TRAIN: iteration 4719 : loss : 0.093044, loss_ce: 0.009046, loss_dice: 0.177043
[08:25:27.672] TRAIN: iteration 4720 : loss : 0.201019, loss_ce: 0.005555, loss_dice: 0.396484
[08:25:27.911] TRAIN: iteration 4721 : loss : 0.253053, loss_ce: 0.005759, loss_dice: 0.500347
[08:25:28.127] TRAIN: iteration 4722 : loss : 0.152772, loss_ce: 0.005551, loss_dice: 0.299993
[08:25:28.340] TRAIN: iteration 4723 : loss : 0.194081, loss_ce: 0.009353, loss_dice: 0.378810
[08:25:28.547] TRAIN: iteration 4724 : loss : 0.227741, loss_ce: 0.005736, loss_dice: 0.449745
[08:25:28.759] TRAIN: iteration 4725 : loss : 0.252949, loss_ce: 0.005559, loss_dice: 0.500339
[08:25:28.970] TRAIN: iteration 4726 : loss : 0.156326, loss_ce: 0.005037, loss_dice: 0.307615
[08:25:29.179] TRAIN: iteration 4727 : loss : 0.167864, loss_ce: 0.027406, loss_dice: 0.308322
[08:25:30.154] TRAIN: iteration 4728 : loss : 0.158183, loss_ce: 0.005556, loss_dice: 0.310809
[08:25:30.361] TRAIN: iteration 4729 : loss : 0.252748, loss_ce: 0.005195, loss_dice: 0.500301
[08:25:30.568] TRAIN: iteration 4730 : loss : 0.128097, loss_ce: 0.010017, loss_dice: 0.246176
[08:25:30.795] TRAIN: iteration 4731 : loss : 0.225441, loss_ce: 0.004270, loss_dice: 0.446611
[08:25:31.004] TRAIN: iteration 4732 : loss : 0.133632, loss_ce: 0.004475, loss_dice: 0.262789
[08:25:31.212] TRAIN: iteration 4733 : loss : 0.193502, loss_ce: 0.005996, loss_dice: 0.381008
[08:25:31.422] TRAIN: iteration 4734 : loss : 0.144563, loss_ce: 0.004920, loss_dice: 0.284205
[08:25:31.631] TRAIN: iteration 4735 : loss : 0.192153, loss_ce: 0.005068, loss_dice: 0.379239
[08:25:33.292] TRAIN: iteration 4736 : loss : 0.179330, loss_ce: 0.009822, loss_dice: 0.348838
[08:25:33.499] TRAIN: iteration 4737 : loss : 0.252245, loss_ce: 0.004197, loss_dice: 0.500293
[08:25:33.710] TRAIN: iteration 4738 : loss : 0.165176, loss_ce: 0.003841, loss_dice: 0.326510
[08:25:33.919] TRAIN: iteration 4739 : loss : 0.172177, loss_ce: 0.006052, loss_dice: 0.338302
[08:25:34.132] TRAIN: iteration 4740 : loss : 0.115241, loss_ce: 0.009042, loss_dice: 0.221439
[08:25:34.375] TRAIN: iteration 4741 : loss : 0.179665, loss_ce: 0.005212, loss_dice: 0.354118
[08:25:34.585] TRAIN: iteration 4742 : loss : 0.239689, loss_ce: 0.005828, loss_dice: 0.473550
[08:25:34.792] TRAIN: iteration 4743 : loss : 0.114293, loss_ce: 0.005760, loss_dice: 0.222825
[08:25:38.459] TRAIN: iteration 4744 : loss : 0.143986, loss_ce: 0.008124, loss_dice: 0.279847
[08:25:38.666] TRAIN: iteration 4745 : loss : 0.150765, loss_ce: 0.016970, loss_dice: 0.284561
[08:25:38.873] TRAIN: iteration 4746 : loss : 0.159607, loss_ce: 0.022343, loss_dice: 0.296872
[08:25:39.091] TRAIN: iteration 4747 : loss : 0.252443, loss_ce: 0.004611, loss_dice: 0.500274
[08:25:39.301] TRAIN: iteration 4748 : loss : 0.252338, loss_ce: 0.005455, loss_dice: 0.499220
[08:25:39.508] TRAIN: iteration 4749 : loss : 0.252859, loss_ce: 0.005629, loss_dice: 0.500089
[08:25:39.716] TRAIN: iteration 4750 : loss : 0.112415, loss_ce: 0.015143, loss_dice: 0.209687
[08:25:39.925] TRAIN: iteration 4751 : loss : 0.188790, loss_ce: 0.011121, loss_dice: 0.366458
[08:25:42.801] TRAIN: iteration 4752 : loss : 0.249461, loss_ce: 0.006677, loss_dice: 0.492245
[08:25:43.008] TRAIN: iteration 4753 : loss : 0.253753, loss_ce: 0.007049, loss_dice: 0.500457
[08:25:43.218] TRAIN: iteration 4754 : loss : 0.179850, loss_ce: 0.011414, loss_dice: 0.348287
[08:25:43.430] TRAIN: iteration 4755 : loss : 0.246296, loss_ce: 0.008586, loss_dice: 0.484007
[08:25:43.636] TRAIN: iteration 4756 : loss : 0.132007, loss_ce: 0.006734, loss_dice: 0.257281
[08:25:43.842] TRAIN: iteration 4757 : loss : 0.181538, loss_ce: 0.010082, loss_dice: 0.352995
[08:25:44.050] TRAIN: iteration 4758 : loss : 0.203200, loss_ce: 0.008510, loss_dice: 0.397889
[08:25:44.257] TRAIN: iteration 4759 : loss : 0.078893, loss_ce: 0.006363, loss_dice: 0.151423
[08:25:44.465] TRAIN: iteration 4760 : loss : 0.118008, loss_ce: 0.007161, loss_dice: 0.228854
[08:25:44.709] TRAIN: iteration 4761 : loss : 0.147362, loss_ce: 0.009187, loss_dice: 0.285536
[08:25:44.916] TRAIN: iteration 4762 : loss : 0.138484, loss_ce: 0.007651, loss_dice: 0.269317
[08:25:45.123] TRAIN: iteration 4763 : loss : 0.198330, loss_ce: 0.013290, loss_dice: 0.383370
[08:25:45.335] TRAIN: iteration 4764 : loss : 0.246948, loss_ce: 0.008096, loss_dice: 0.485799
[08:25:45.541] TRAIN: iteration 4765 : loss : 0.146481, loss_ce: 0.010178, loss_dice: 0.282784
[08:25:45.749] TRAIN: iteration 4766 : loss : 0.254277, loss_ce: 0.007969, loss_dice: 0.500584
[08:25:45.962] TRAIN: iteration 4767 : loss : 0.111057, loss_ce: 0.006388, loss_dice: 0.215725
[08:25:47.124] TRAIN: iteration 4768 : loss : 0.251547, loss_ce: 0.006507, loss_dice: 0.496587
[08:25:47.458] TRAIN: iteration 4769 : loss : 0.086300, loss_ce: 0.007657, loss_dice: 0.164943
[08:25:47.665] TRAIN: iteration 4770 : loss : 0.159165, loss_ce: 0.008766, loss_dice: 0.309564
[08:25:48.231] TRAIN: iteration 4771 : loss : 0.252752, loss_ce: 0.006302, loss_dice: 0.499202
[08:25:48.437] TRAIN: iteration 4772 : loss : 0.247855, loss_ce: 0.006478, loss_dice: 0.489232
[08:25:48.644] TRAIN: iteration 4773 : loss : 0.233010, loss_ce: 0.005840, loss_dice: 0.460181
[08:25:48.851] TRAIN: iteration 4774 : loss : 0.046425, loss_ce: 0.004549, loss_dice: 0.088301
[08:25:49.059] TRAIN: iteration 4775 : loss : 0.153659, loss_ce: 0.006329, loss_dice: 0.300988
[08:25:49.830] TRAIN: iteration 4776 : loss : 0.253165, loss_ce: 0.005939, loss_dice: 0.500392
[08:25:51.380] TRAIN: iteration 4777 : loss : 0.239393, loss_ce: 0.005766, loss_dice: 0.473019
[08:25:51.589] TRAIN: iteration 4778 : loss : 0.139609, loss_ce: 0.005609, loss_dice: 0.273610
[08:25:52.018] TRAIN: iteration 4779 : loss : 0.208873, loss_ce: 0.026901, loss_dice: 0.390844
[08:25:53.971] TRAIN: iteration 4780 : loss : 0.253016, loss_ce: 0.005645, loss_dice: 0.500387
[08:25:54.208] TRAIN: iteration 4781 : loss : 0.131970, loss_ce: 0.010080, loss_dice: 0.253861
[08:25:54.416] TRAIN: iteration 4782 : loss : 0.234016, loss_ce: 0.044194, loss_dice: 0.423837
[08:25:54.623] TRAIN: iteration 4783 : loss : 0.105044, loss_ce: 0.009419, loss_dice: 0.200669
[08:25:54.830] TRAIN: iteration 4784 : loss : 0.103538, loss_ce: 0.006529, loss_dice: 0.200548
[08:25:56.429] TRAIN: iteration 4785 : loss : 0.197102, loss_ce: 0.017054, loss_dice: 0.377150
[08:25:56.643] TRAIN: iteration 4786 : loss : 0.211541, loss_ce: 0.006939, loss_dice: 0.416144
[08:25:56.850] TRAIN: iteration 4787 : loss : 0.130364, loss_ce: 0.007886, loss_dice: 0.252841
[08:25:57.265] TRAIN: iteration 4788 : loss : 0.092258, loss_ce: 0.009327, loss_dice: 0.175188
[08:25:57.472] TRAIN: iteration 4789 : loss : 0.252117, loss_ce: 0.007121, loss_dice: 0.497112
[08:25:57.679] TRAIN: iteration 4790 : loss : 0.207253, loss_ce: 0.016897, loss_dice: 0.397610
[08:25:57.886] TRAIN: iteration 4791 : loss : 0.142108, loss_ce: 0.015224, loss_dice: 0.268993
[08:26:00.461] TRAIN: iteration 4792 : loss : 0.240631, loss_ce: 0.009130, loss_dice: 0.472131
[08:26:01.415] TRAIN: iteration 4793 : loss : 0.176813, loss_ce: 0.010384, loss_dice: 0.343242
[08:26:01.622] TRAIN: iteration 4794 : loss : 0.080691, loss_ce: 0.007186, loss_dice: 0.154197
[08:26:01.829] TRAIN: iteration 4795 : loss : 0.155815, loss_ce: 0.007763, loss_dice: 0.303867
[08:26:02.792] TRAIN: iteration 4796 : loss : 0.167728, loss_ce: 0.010680, loss_dice: 0.324776
[08:26:03.000] TRAIN: iteration 4797 : loss : 0.076731, loss_ce: 0.006651, loss_dice: 0.146811
[08:26:03.207] TRAIN: iteration 4798 : loss : 0.254288, loss_ce: 0.008014, loss_dice: 0.500563
[08:26:03.414] TRAIN: iteration 4799 : loss : 0.125955, loss_ce: 0.006215, loss_dice: 0.245694
[08:26:03.622] TRAIN: iteration 4800 : loss : 0.063889, loss_ce: 0.005507, loss_dice: 0.122271
[08:26:05.589] TRAIN: iteration 4801 : loss : 0.137201, loss_ce: 0.009971, loss_dice: 0.264431
[08:26:05.798] TRAIN: iteration 4802 : loss : 0.253883, loss_ce: 0.007273, loss_dice: 0.500492
[08:26:06.011] TRAIN: iteration 4803 : loss : 0.233482, loss_ce: 0.007085, loss_dice: 0.459879
[08:26:06.265] TRAIN: iteration 4804 : loss : 0.221883, loss_ce: 0.007047, loss_dice: 0.436719
[08:26:06.474] TRAIN: iteration 4805 : loss : 0.205503, loss_ce: 0.028350, loss_dice: 0.382656
[08:26:06.680] TRAIN: iteration 4806 : loss : 0.095987, loss_ce: 0.008986, loss_dice: 0.182989
[08:26:06.889] TRAIN: iteration 4807 : loss : 0.251596, loss_ce: 0.009434, loss_dice: 0.493758
[08:26:07.096] TRAIN: iteration 4808 : loss : 0.241481, loss_ce: 0.007073, loss_dice: 0.475890
[08:26:10.988] TRAIN: iteration 4809 : loss : 0.151010, loss_ce: 0.006004, loss_dice: 0.296016
[08:26:11.196] TRAIN: iteration 4810 : loss : 0.116731, loss_ce: 0.005995, loss_dice: 0.227466
[08:26:11.405] TRAIN: iteration 4811 : loss : 0.191752, loss_ce: 0.008499, loss_dice: 0.375004
[08:26:11.611] TRAIN: iteration 4812 : loss : 0.239016, loss_ce: 0.009477, loss_dice: 0.468555
[08:26:11.818] TRAIN: iteration 4813 : loss : 0.180604, loss_ce: 0.028797, loss_dice: 0.332412
[08:26:12.025] TRAIN: iteration 4814 : loss : 0.208639, loss_ce: 0.007140, loss_dice: 0.410139
[08:26:12.234] TRAIN: iteration 4815 : loss : 0.253018, loss_ce: 0.005771, loss_dice: 0.500266
[08:26:12.442] TRAIN: iteration 4816 : loss : 0.209441, loss_ce: 0.005669, loss_dice: 0.413213
[08:26:14.663] TRAIN: iteration 4817 : loss : 0.133881, loss_ce: 0.014963, loss_dice: 0.252799
[08:26:14.870] TRAIN: iteration 4818 : loss : 0.193909, loss_ce: 0.005429, loss_dice: 0.382389
[08:26:15.078] TRAIN: iteration 4819 : loss : 0.239272, loss_ce: 0.006200, loss_dice: 0.472343
[08:26:15.290] TRAIN: iteration 4820 : loss : 0.121526, loss_ce: 0.011837, loss_dice: 0.231214
[08:26:15.530] TRAIN: iteration 4821 : loss : 0.252823, loss_ce: 0.005368, loss_dice: 0.500277
[08:26:15.738] TRAIN: iteration 4822 : loss : 0.169936, loss_ce: 0.006276, loss_dice: 0.333595
[08:26:15.947] TRAIN: iteration 4823 : loss : 0.102156, loss_ce: 0.005597, loss_dice: 0.198715
[08:26:16.156] TRAIN: iteration 4824 : loss : 0.080154, loss_ce: 0.004614, loss_dice: 0.155694
[08:26:16.366] TRAIN: iteration 4825 : loss : 0.236919, loss_ce: 0.005139, loss_dice: 0.468700
[08:26:16.574] TRAIN: iteration 4826 : loss : 0.083684, loss_ce: 0.004623, loss_dice: 0.162745
[08:26:16.782] TRAIN: iteration 4827 : loss : 0.252539, loss_ce: 0.004832, loss_dice: 0.500245
[08:26:18.060] TRAIN: iteration 4828 : loss : 0.089875, loss_ce: 0.008676, loss_dice: 0.171074
[08:26:18.271] TRAIN: iteration 4829 : loss : 0.133203, loss_ce: 0.005055, loss_dice: 0.261352
[08:26:18.478] TRAIN: iteration 4830 : loss : 0.252501, loss_ce: 0.004739, loss_dice: 0.500263
[08:26:18.687] TRAIN: iteration 4831 : loss : 0.252498, loss_ce: 0.004691, loss_dice: 0.500306
[08:26:18.900] TRAIN: iteration 4832 : loss : 0.156702, loss_ce: 0.008533, loss_dice: 0.304871
[08:26:19.110] TRAIN: iteration 4833 : loss : 0.249101, loss_ce: 0.004606, loss_dice: 0.493597
[08:26:19.316] TRAIN: iteration 4834 : loss : 0.070151, loss_ce: 0.005380, loss_dice: 0.134921
[08:26:19.816] TRAIN: iteration 4835 : loss : 0.124756, loss_ce: 0.016898, loss_dice: 0.232614
[08:26:21.732] TRAIN: iteration 4836 : loss : 0.110796, loss_ce: 0.003266, loss_dice: 0.218327
[08:26:22.033] TRAIN: iteration 4837 : loss : 0.079317, loss_ce: 0.004216, loss_dice: 0.154419
[08:26:22.240] TRAIN: iteration 4838 : loss : 0.132414, loss_ce: 0.008066, loss_dice: 0.256762
[08:26:22.447] TRAIN: iteration 4839 : loss : 0.248137, loss_ce: 0.007226, loss_dice: 0.489049
[08:26:23.894] TRAIN: iteration 4840 : loss : 0.153369, loss_ce: 0.008801, loss_dice: 0.297937
[08:26:24.141] TRAIN: iteration 4841 : loss : 0.207494, loss_ce: 0.004874, loss_dice: 0.410113
[08:26:24.348] TRAIN: iteration 4842 : loss : 0.252578, loss_ce: 0.004795, loss_dice: 0.500360
[08:26:24.814] TRAIN: iteration 4843 : loss : 0.158101, loss_ce: 0.027360, loss_dice: 0.288841
[08:26:25.597] TRAIN: iteration 4844 : loss : 0.094096, loss_ce: 0.010894, loss_dice: 0.177299
[08:26:25.804] TRAIN: iteration 4845 : loss : 0.222458, loss_ce: 0.004787, loss_dice: 0.440128
[08:26:26.011] TRAIN: iteration 4846 : loss : 0.253386, loss_ce: 0.008313, loss_dice: 0.498458
[08:26:26.218] TRAIN: iteration 4847 : loss : 0.145297, loss_ce: 0.008070, loss_dice: 0.282523
[08:26:28.453] TRAIN: iteration 4848 : loss : 0.222725, loss_ce: 0.004805, loss_dice: 0.440645
[08:26:28.665] TRAIN: iteration 4849 : loss : 0.108512, loss_ce: 0.004579, loss_dice: 0.212445
[08:26:28.877] TRAIN: iteration 4850 : loss : 0.076936, loss_ce: 0.005729, loss_dice: 0.148144
[08:26:29.088] TRAIN: iteration 4851 : loss : 0.102594, loss_ce: 0.010778, loss_dice: 0.194410
[08:26:30.186] TRAIN: iteration 4852 : loss : 0.138177, loss_ce: 0.010341, loss_dice: 0.266013
[08:26:31.821] TRAIN: iteration 4853 : loss : 0.154437, loss_ce: 0.016556, loss_dice: 0.292318
[08:26:32.028] TRAIN: iteration 4854 : loss : 0.052245, loss_ce: 0.004734, loss_dice: 0.099755
[08:26:32.235] TRAIN: iteration 4855 : loss : 0.211255, loss_ce: 0.005742, loss_dice: 0.416767
[08:26:32.443] TRAIN: iteration 4856 : loss : 0.249812, loss_ce: 0.008403, loss_dice: 0.491221
[08:26:32.650] TRAIN: iteration 4857 : loss : 0.109516, loss_ce: 0.005017, loss_dice: 0.214015
[08:26:32.858] TRAIN: iteration 4858 : loss : 0.180046, loss_ce: 0.008251, loss_dice: 0.351840
[08:26:33.066] TRAIN: iteration 4859 : loss : 0.116738, loss_ce: 0.005700, loss_dice: 0.227776
[08:26:33.881] TRAIN: iteration 4860 : loss : 0.200798, loss_ce: 0.018908, loss_dice: 0.382688
[08:26:37.030] TRAIN: iteration 4861 : loss : 0.123525, loss_ce: 0.011594, loss_dice: 0.235455
[08:26:37.242] TRAIN: iteration 4862 : loss : 0.162714, loss_ce: 0.006862, loss_dice: 0.318567
[08:26:37.449] TRAIN: iteration 4863 : loss : 0.142405, loss_ce: 0.007698, loss_dice: 0.277112
[08:26:37.656] TRAIN: iteration 4864 : loss : 0.123403, loss_ce: 0.012075, loss_dice: 0.234732
[08:26:37.863] TRAIN: iteration 4865 : loss : 0.092876, loss_ce: 0.006571, loss_dice: 0.179182
[08:26:38.078] TRAIN: iteration 4866 : loss : 0.142695, loss_ce: 0.006747, loss_dice: 0.278643
[08:26:38.289] TRAIN: iteration 4867 : loss : 0.253462, loss_ce: 0.006682, loss_dice: 0.500242
[08:26:38.496] TRAIN: iteration 4868 : loss : 0.142042, loss_ce: 0.005989, loss_dice: 0.278096
[08:26:41.341] TRAIN: iteration 4869 : loss : 0.104963, loss_ce: 0.007016, loss_dice: 0.202910
[08:26:41.556] TRAIN: iteration 4870 : loss : 0.152278, loss_ce: 0.019794, loss_dice: 0.284763
[08:26:41.767] TRAIN: iteration 4871 : loss : 0.151924, loss_ce: 0.005605, loss_dice: 0.298242
[08:26:41.976] TRAIN: iteration 4872 : loss : 0.202981, loss_ce: 0.006191, loss_dice: 0.399772
[08:26:42.185] TRAIN: iteration 4873 : loss : 0.252514, loss_ce: 0.004805, loss_dice: 0.500223
[08:26:42.392] TRAIN: iteration 4874 : loss : 0.249082, loss_ce: 0.006837, loss_dice: 0.491327
[08:26:44.097] TRAIN: iteration 4875 : loss : 0.120100, loss_ce: 0.005153, loss_dice: 0.235047
[08:26:44.303] TRAIN: iteration 4876 : loss : 0.252822, loss_ce: 0.005306, loss_dice: 0.500338
[08:26:44.511] TRAIN: iteration 4877 : loss : 0.116201, loss_ce: 0.009129, loss_dice: 0.223274
[08:26:44.718] TRAIN: iteration 4878 : loss : 0.251932, loss_ce: 0.003730, loss_dice: 0.500133
[08:26:44.924] TRAIN: iteration 4879 : loss : 0.135929, loss_ce: 0.019018, loss_dice: 0.252839
[08:26:45.131] TRAIN: iteration 4880 : loss : 0.099174, loss_ce: 0.008773, loss_dice: 0.189576
[08:26:45.348] TRAIN: iteration 4881 : loss : 0.173443, loss_ce: 0.012695, loss_dice: 0.334192
[08:26:45.555] TRAIN: iteration 4882 : loss : 0.101792, loss_ce: 0.005547, loss_dice: 0.198037
[08:26:48.282] TRAIN: iteration 4883 : loss : 0.103909, loss_ce: 0.005859, loss_dice: 0.201959
[08:26:48.488] TRAIN: iteration 4884 : loss : 0.054761, loss_ce: 0.003391, loss_dice: 0.106130
[08:26:48.696] TRAIN: iteration 4885 : loss : 0.252532, loss_ce: 0.004757, loss_dice: 0.500307
[08:26:48.903] TRAIN: iteration 4886 : loss : 0.163966, loss_ce: 0.004932, loss_dice: 0.323001
[08:26:49.110] TRAIN: iteration 4887 : loss : 0.247408, loss_ce: 0.008443, loss_dice: 0.486372
[08:26:50.399] TRAIN: iteration 4888 : loss : 0.251765, loss_ce: 0.003434, loss_dice: 0.500097
[08:26:50.606] TRAIN: iteration 4889 : loss : 0.083409, loss_ce: 0.003704, loss_dice: 0.163113
[08:26:50.813] TRAIN: iteration 4890 : loss : 0.247020, loss_ce: 0.006128, loss_dice: 0.487912
[08:26:52.052] TRAIN: iteration 4891 : loss : 0.220087, loss_ce: 0.004727, loss_dice: 0.435447
[08:26:52.261] TRAIN: iteration 4892 : loss : 0.245176, loss_ce: 0.005155, loss_dice: 0.485198
[08:26:52.467] TRAIN: iteration 4893 : loss : 0.220416, loss_ce: 0.009079, loss_dice: 0.431753
[08:26:52.674] TRAIN: iteration 4894 : loss : 0.084595, loss_ce: 0.004185, loss_dice: 0.165005
[08:26:52.883] TRAIN: iteration 4895 : loss : 0.098352, loss_ce: 0.005848, loss_dice: 0.190857
[08:26:55.828] TRAIN: iteration 4896 : loss : 0.101283, loss_ce: 0.006392, loss_dice: 0.196173
[08:26:56.038] TRAIN: iteration 4897 : loss : 0.114153, loss_ce: 0.010532, loss_dice: 0.217774
[08:26:56.244] TRAIN: iteration 4898 : loss : 0.099878, loss_ce: 0.006494, loss_dice: 0.193262
[08:26:56.650] TRAIN: iteration 4899 : loss : 0.246839, loss_ce: 0.004668, loss_dice: 0.489010
[08:26:56.858] TRAIN: iteration 4900 : loss : 0.252301, loss_ce: 0.004324, loss_dice: 0.500278
[08:26:57.094] TRAIN: iteration 4901 : loss : 0.132196, loss_ce: 0.005358, loss_dice: 0.259034
[08:26:57.302] TRAIN: iteration 4902 : loss : 0.063821, loss_ce: 0.004502, loss_dice: 0.123140
[08:27:00.091] TRAIN: iteration 4903 : loss : 0.127352, loss_ce: 0.006385, loss_dice: 0.248319
[08:27:00.298] TRAIN: iteration 4904 : loss : 0.176167, loss_ce: 0.007268, loss_dice: 0.345066
[08:27:00.504] TRAIN: iteration 4905 : loss : 0.147370, loss_ce: 0.012179, loss_dice: 0.282562
[08:27:00.712] TRAIN: iteration 4906 : loss : 0.154181, loss_ce: 0.031883, loss_dice: 0.276479
[08:27:03.615] TRAIN: iteration 4907 : loss : 0.110004, loss_ce: 0.004993, loss_dice: 0.215015
[08:27:03.821] TRAIN: iteration 4908 : loss : 0.089494, loss_ce: 0.004131, loss_dice: 0.174856
[08:27:04.028] TRAIN: iteration 4909 : loss : 0.242759, loss_ce: 0.007040, loss_dice: 0.478478
[08:27:04.235] TRAIN: iteration 4910 : loss : 0.253298, loss_ce: 0.006896, loss_dice: 0.499700
[08:27:06.211] TRAIN: iteration 4911 : loss : 0.125537, loss_ce: 0.009232, loss_dice: 0.241841
[08:27:06.418] TRAIN: iteration 4912 : loss : 0.067987, loss_ce: 0.006043, loss_dice: 0.129932
[08:27:06.626] TRAIN: iteration 4913 : loss : 0.060976, loss_ce: 0.006284, loss_dice: 0.115668
[08:27:06.834] TRAIN: iteration 4914 : loss : 0.253498, loss_ce: 0.006612, loss_dice: 0.500384
[08:27:07.758] TRAIN: iteration 4915 : loss : 0.245156, loss_ce: 0.035874, loss_dice: 0.454437
[08:27:07.966] TRAIN: iteration 4916 : loss : 0.217092, loss_ce: 0.008445, loss_dice: 0.425740
[08:27:08.172] TRAIN: iteration 4917 : loss : 0.253832, loss_ce: 0.007124, loss_dice: 0.500540
[08:27:08.379] TRAIN: iteration 4918 : loss : 0.093823, loss_ce: 0.005984, loss_dice: 0.181662
[08:27:10.725] TRAIN: iteration 4919 : loss : 0.096219, loss_ce: 0.006640, loss_dice: 0.185798
[08:27:10.937] TRAIN: iteration 4920 : loss : 0.223890, loss_ce: 0.006652, loss_dice: 0.441129
[08:27:11.173] TRAIN: iteration 4921 : loss : 0.169400, loss_ce: 0.006633, loss_dice: 0.332166
[08:27:11.380] TRAIN: iteration 4922 : loss : 0.166617, loss_ce: 0.027635, loss_dice: 0.305600
[08:27:11.902] TRAIN: iteration 4923 : loss : 0.131256, loss_ce: 0.011775, loss_dice: 0.250737
[08:27:12.109] TRAIN: iteration 4924 : loss : 0.188667, loss_ce: 0.007558, loss_dice: 0.369776
[08:27:12.323] TRAIN: iteration 4925 : loss : 0.152633, loss_ce: 0.007316, loss_dice: 0.297949
[08:27:12.533] TRAIN: iteration 4926 : loss : 0.254062, loss_ce: 0.007582, loss_dice: 0.500542
[08:27:16.220] TRAIN: iteration 4927 : loss : 0.207634, loss_ce: 0.007189, loss_dice: 0.408079
[08:27:16.427] TRAIN: iteration 4928 : loss : 0.253240, loss_ce: 0.006127, loss_dice: 0.500353
[08:27:16.637] TRAIN: iteration 4929 : loss : 0.144470, loss_ce: 0.005688, loss_dice: 0.283251
[08:27:16.846] TRAIN: iteration 4930 : loss : 0.184479, loss_ce: 0.006362, loss_dice: 0.362596
[08:27:17.055] TRAIN: iteration 4931 : loss : 0.237910, loss_ce: 0.006100, loss_dice: 0.469720
[08:27:17.262] TRAIN: iteration 4932 : loss : 0.168650, loss_ce: 0.006989, loss_dice: 0.330311
[08:27:17.469] TRAIN: iteration 4933 : loss : 0.142561, loss_ce: 0.003923, loss_dice: 0.281200
[08:27:17.676] TRAIN: iteration 4934 : loss : 0.206781, loss_ce: 0.005376, loss_dice: 0.408187
[08:27:19.201] TRAIN: iteration 4935 : loss : 0.147738, loss_ce: 0.004651, loss_dice: 0.290824
[08:27:19.408] TRAIN: iteration 4936 : loss : 0.206107, loss_ce: 0.007938, loss_dice: 0.404277
[08:27:19.615] TRAIN: iteration 4937 : loss : 0.121569, loss_ce: 0.004268, loss_dice: 0.238870
[08:27:19.821] TRAIN: iteration 4938 : loss : 0.192509, loss_ce: 0.031211, loss_dice: 0.353807
[08:27:22.201] TRAIN: iteration 4939 : loss : 0.066670, loss_ce: 0.001746, loss_dice: 0.131594
[08:27:22.410] TRAIN: iteration 4940 : loss : 0.246620, loss_ce: 0.003586, loss_dice: 0.489653
[08:27:22.643] TRAIN: iteration 4941 : loss : 0.092903, loss_ce: 0.003893, loss_dice: 0.181912
[08:27:22.849] TRAIN: iteration 4942 : loss : 0.112835, loss_ce: 0.003613, loss_dice: 0.222058
[08:27:23.817] TRAIN: iteration 4943 : loss : 0.247624, loss_ce: 0.017663, loss_dice: 0.477585
[08:27:24.025] TRAIN: iteration 4944 : loss : 0.251149, loss_ce: 0.002192, loss_dice: 0.500105
[08:27:24.232] TRAIN: iteration 4945 : loss : 0.199881, loss_ce: 0.019066, loss_dice: 0.380697
[08:27:25.253] TRAIN: iteration 4946 : loss : 0.189399, loss_ce: 0.009750, loss_dice: 0.369047
[08:27:26.373] TRAIN: iteration 4947 : loss : 0.157078, loss_ce: 0.009929, loss_dice: 0.304228
[08:27:26.767] TRAIN: iteration 4948 : loss : 0.205678, loss_ce: 0.003705, loss_dice: 0.407650
[08:27:26.974] TRAIN: iteration 4949 : loss : 0.210154, loss_ce: 0.012378, loss_dice: 0.407930
[08:27:27.350] TRAIN: iteration 4950 : loss : 0.191613, loss_ce: 0.004231, loss_dice: 0.378996
[08:27:29.750] TRAIN: iteration 4951 : loss : 0.125749, loss_ce: 0.003120, loss_dice: 0.248378
[08:27:29.957] TRAIN: iteration 4952 : loss : 0.121418, loss_ce: 0.003036, loss_dice: 0.239800
[08:27:30.170] TRAIN: iteration 4953 : loss : 0.251308, loss_ce: 0.002882, loss_dice: 0.499734
[08:27:32.954] TRAIN: iteration 4954 : loss : 0.181690, loss_ce: 0.003159, loss_dice: 0.360222
[08:27:33.161] TRAIN: iteration 4955 : loss : 0.198258, loss_ce: 0.011063, loss_dice: 0.385453
[08:27:33.368] TRAIN: iteration 4956 : loss : 0.217084, loss_ce: 0.005985, loss_dice: 0.428182
[08:27:33.575] TRAIN: iteration 4957 : loss : 0.251368, loss_ce: 0.002659, loss_dice: 0.500077
[08:27:33.784] TRAIN: iteration 4958 : loss : 0.155448, loss_ce: 0.003456, loss_dice: 0.307440
[08:27:35.758] TRAIN: iteration 4959 : loss : 0.214100, loss_ce: 0.006640, loss_dice: 0.421560
[08:27:36.316] TRAIN: iteration 4960 : loss : 0.231719, loss_ce: 0.003746, loss_dice: 0.459692
[08:27:36.554] TRAIN: iteration 4961 : loss : 0.251486, loss_ce: 0.002848, loss_dice: 0.500124
[08:27:37.082] TRAIN: iteration 4962 : loss : 0.241166, loss_ce: 0.007941, loss_dice: 0.474391
[08:27:37.290] TRAIN: iteration 4963 : loss : 0.140460, loss_ce: 0.003118, loss_dice: 0.277802
[08:27:38.187] TRAIN: iteration 4964 : loss : 0.207026, loss_ce: 0.005985, loss_dice: 0.408067
[08:27:38.394] TRAIN: iteration 4965 : loss : 0.251522, loss_ce: 0.002922, loss_dice: 0.500122
[08:27:38.601] TRAIN: iteration 4966 : loss : 0.158623, loss_ce: 0.006667, loss_dice: 0.310579
[08:27:40.383] TRAIN: iteration 4967 : loss : 0.178283, loss_ce: 0.015141, loss_dice: 0.341424
[08:27:44.084] TRAIN: iteration 4968 : loss : 0.157510, loss_ce: 0.007125, loss_dice: 0.307896
[08:27:44.293] TRAIN: iteration 4969 : loss : 0.152405, loss_ce: 0.009467, loss_dice: 0.295344
[08:27:44.500] TRAIN: iteration 4970 : loss : 0.133423, loss_ce: 0.009674, loss_dice: 0.257172
[08:27:44.706] TRAIN: iteration 4971 : loss : 0.142969, loss_ce: 0.005490, loss_dice: 0.280448
[08:27:46.410] TRAIN: iteration 4972 : loss : 0.190269, loss_ce: 0.004918, loss_dice: 0.375620
[08:27:46.616] TRAIN: iteration 4973 : loss : 0.123858, loss_ce: 0.004386, loss_dice: 0.243330
[08:27:46.823] TRAIN: iteration 4974 : loss : 0.205677, loss_ce: 0.004647, loss_dice: 0.406707
[08:27:47.030] TRAIN: iteration 4975 : loss : 0.252187, loss_ce: 0.004094, loss_dice: 0.500281
[08:27:50.732] TRAIN: iteration 4976 : loss : 0.135364, loss_ce: 0.004646, loss_dice: 0.266082
[08:27:50.942] TRAIN: iteration 4977 : loss : 0.177781, loss_ce: 0.004818, loss_dice: 0.350745
[08:27:51.150] TRAIN: iteration 4978 : loss : 0.159379, loss_ce: 0.005684, loss_dice: 0.313073
[08:27:51.357] TRAIN: iteration 4979 : loss : 0.241962, loss_ce: 0.012318, loss_dice: 0.471607
[08:27:54.650] TRAIN: iteration 4980 : loss : 0.153700, loss_ce: 0.009211, loss_dice: 0.298189
[08:27:54.890] TRAIN: iteration 4981 : loss : 0.200882, loss_ce: 0.010101, loss_dice: 0.391663
[08:27:55.105] TRAIN: iteration 4982 : loss : 0.168611, loss_ce: 0.005445, loss_dice: 0.331777
[08:27:55.312] TRAIN: iteration 4983 : loss : 0.115793, loss_ce: 0.003967, loss_dice: 0.227619
[08:28:00.695] TRAIN: iteration 4984 : loss : 0.232543, loss_ce: 0.003633, loss_dice: 0.461453
[08:28:00.902] TRAIN: iteration 4985 : loss : 0.121256, loss_ce: 0.004397, loss_dice: 0.238115
[08:28:01.110] TRAIN: iteration 4986 : loss : 0.134026, loss_ce: 0.005452, loss_dice: 0.262599
[08:28:01.322] TRAIN: iteration 4987 : loss : 0.208909, loss_ce: 0.010510, loss_dice: 0.407308
[08:28:01.528] TRAIN: iteration 4988 : loss : 0.168575, loss_ce: 0.013502, loss_dice: 0.323647
[08:28:01.735] TRAIN: iteration 4989 : loss : 0.226631, loss_ce: 0.005964, loss_dice: 0.447299
[08:28:01.942] TRAIN: iteration 4990 : loss : 0.207985, loss_ce: 0.013646, loss_dice: 0.402324
[08:28:02.148] TRAIN: iteration 4991 : loss : 0.186755, loss_ce: 0.004786, loss_dice: 0.368724
[08:28:06.702] TRAIN: iteration 4992 : loss : 0.130726, loss_ce: 0.008608, loss_dice: 0.252844
[08:28:06.912] TRAIN: iteration 4993 : loss : 0.055785, loss_ce: 0.003272, loss_dice: 0.108297
[08:28:07.123] TRAIN: iteration 4994 : loss : 0.240915, loss_ce: 0.005005, loss_dice: 0.476825
[08:28:07.330] TRAIN: iteration 4995 : loss : 0.098006, loss_ce: 0.003951, loss_dice: 0.192060
[08:28:07.536] TRAIN: iteration 4996 : loss : 0.224485, loss_ce: 0.005832, loss_dice: 0.443137
[08:28:07.745] TRAIN: iteration 4997 : loss : 0.253017, loss_ce: 0.005658, loss_dice: 0.500377
[08:28:07.952] TRAIN: iteration 4998 : loss : 0.086666, loss_ce: 0.004066, loss_dice: 0.169266
[08:28:08.159] TRAIN: iteration 4999 : loss : 0.141515, loss_ce: 0.008926, loss_dice: 0.274104
[08:28:09.734] TRAIN: iteration 5000 : loss : 0.147333, loss_ce: 0.006337, loss_dice: 0.288330
[08:28:09.969] TRAIN: iteration 5001 : loss : 0.077257, loss_ce: 0.008325, loss_dice: 0.146190
[08:28:10.175] TRAIN: iteration 5002 : loss : 0.097650, loss_ce: 0.007009, loss_dice: 0.188292
[08:28:10.382] TRAIN: iteration 5003 : loss : 0.123660, loss_ce: 0.009563, loss_dice: 0.237756
[08:28:10.590] TRAIN: iteration 5004 : loss : 0.243043, loss_ce: 0.006668, loss_dice: 0.479418
[08:28:10.796] TRAIN: iteration 5005 : loss : 0.153862, loss_ce: 0.019720, loss_dice: 0.288004
[08:28:11.006] TRAIN: iteration 5006 : loss : 0.183892, loss_ce: 0.028044, loss_dice: 0.339740
[08:28:11.212] TRAIN: iteration 5007 : loss : 0.145808, loss_ce: 0.012778, loss_dice: 0.278838
[08:28:15.830] TRAIN: iteration 5008 : loss : 0.116556, loss_ce: 0.010153, loss_dice: 0.222960
[08:28:16.037] TRAIN: iteration 5009 : loss : 0.139280, loss_ce: 0.008658, loss_dice: 0.269902
[08:28:16.245] TRAIN: iteration 5010 : loss : 0.049568, loss_ce: 0.005943, loss_dice: 0.093194
[08:28:16.601] TRAIN: iteration 5011 : loss : 0.254952, loss_ce: 0.009296, loss_dice: 0.500607
[08:28:16.808] TRAIN: iteration 5012 : loss : 0.164850, loss_ce: 0.011367, loss_dice: 0.318334
[08:28:17.016] TRAIN: iteration 5013 : loss : 0.254648, loss_ce: 0.008675, loss_dice: 0.500620
[08:28:17.225] TRAIN: iteration 5014 : loss : 0.232849, loss_ce: 0.011248, loss_dice: 0.454450
[08:28:17.433] TRAIN: iteration 5015 : loss : 0.191964, loss_ce: 0.011235, loss_dice: 0.372694
[08:28:23.355] TRAIN: iteration 5016 : loss : 0.256459, loss_ce: 0.011988, loss_dice: 0.500931
[08:28:23.568] TRAIN: iteration 5017 : loss : 0.248395, loss_ce: 0.012646, loss_dice: 0.484144
[08:28:23.776] TRAIN: iteration 5018 : loss : 0.153653, loss_ce: 0.010515, loss_dice: 0.296792
[08:28:23.983] TRAIN: iteration 5019 : loss : 0.187176, loss_ce: 0.013181, loss_dice: 0.361171
[08:28:24.190] TRAIN: iteration 5020 : loss : 0.056346, loss_ce: 0.006183, loss_dice: 0.106509
[08:28:24.191] NaN or Inf found in input tensor.
[08:28:24.405] TRAIN: iteration 5021 : loss : 0.159401, loss_ce: 0.009061, loss_dice: 0.309742
[08:28:24.612] TRAIN: iteration 5022 : loss : 0.184077, loss_ce: 0.010620, loss_dice: 0.357535
[08:28:25.592] TRAIN: iteration 5023 : loss : 0.106728, loss_ce: 0.017249, loss_dice: 0.196207
[08:28:28.048] TRAIN: iteration 5024 : loss : 0.245733, loss_ce: 0.009362, loss_dice: 0.482103
[08:28:28.256] TRAIN: iteration 5025 : loss : 0.154539, loss_ce: 0.008166, loss_dice: 0.300913
[08:28:28.463] TRAIN: iteration 5026 : loss : 0.244075, loss_ce: 0.007418, loss_dice: 0.480733
[08:28:30.669] TRAIN: iteration 5027 : loss : 0.118800, loss_ce: 0.007164, loss_dice: 0.230435
[08:28:30.876] TRAIN: iteration 5028 : loss : 0.128109, loss_ce: 0.009864, loss_dice: 0.246355
[08:28:31.085] TRAIN: iteration 5029 : loss : 0.113679, loss_ce: 0.006099, loss_dice: 0.221259
[08:28:31.298] TRAIN: iteration 5030 : loss : 0.135492, loss_ce: 0.014852, loss_dice: 0.256133
[08:28:32.087] TRAIN: iteration 5031 : loss : 0.251405, loss_ce: 0.006632, loss_dice: 0.496179
[08:28:34.187] TRAIN: iteration 5032 : loss : 0.204833, loss_ce: 0.009970, loss_dice: 0.399696
[08:28:34.397] TRAIN: iteration 5033 : loss : 0.119690, loss_ce: 0.007856, loss_dice: 0.231523
[08:28:34.605] TRAIN: iteration 5034 : loss : 0.109006, loss_ce: 0.005800, loss_dice: 0.212211
[08:28:36.511] TRAIN: iteration 5035 : loss : 0.176614, loss_ce: 0.030532, loss_dice: 0.322697
[08:28:36.720] TRAIN: iteration 5036 : loss : 0.135656, loss_ce: 0.020622, loss_dice: 0.250690
[08:28:36.927] TRAIN: iteration 5037 : loss : 0.253371, loss_ce: 0.006275, loss_dice: 0.500466
[08:28:37.135] TRAIN: iteration 5038 : loss : 0.254378, loss_ce: 0.008245, loss_dice: 0.500510
[08:28:40.280] TRAIN: iteration 5039 : loss : 0.208345, loss_ce: 0.010891, loss_dice: 0.405799
[08:28:42.952] TRAIN: iteration 5040 : loss : 0.138006, loss_ce: 0.006911, loss_dice: 0.269102
[08:28:43.191] TRAIN: iteration 5041 : loss : 0.104852, loss_ce: 0.005205, loss_dice: 0.204500
[08:28:43.398] TRAIN: iteration 5042 : loss : 0.147591, loss_ce: 0.013247, loss_dice: 0.281934
[08:28:43.606] TRAIN: iteration 5043 : loss : 0.180862, loss_ce: 0.014576, loss_dice: 0.347147
[08:28:43.816] TRAIN: iteration 5044 : loss : 0.142132, loss_ce: 0.008329, loss_dice: 0.275935
[08:28:44.024] TRAIN: iteration 5045 : loss : 0.110749, loss_ce: 0.006826, loss_dice: 0.214671
[08:28:44.230] TRAIN: iteration 5046 : loss : 0.127831, loss_ce: 0.005695, loss_dice: 0.249967
[08:28:45.639] TRAIN: iteration 5047 : loss : 0.185625, loss_ce: 0.007892, loss_dice: 0.363358
[08:28:54.261] TRAIN: iteration 5048 : loss : 0.253581, loss_ce: 0.006706, loss_dice: 0.500455
[08:28:54.467] TRAIN: iteration 5049 : loss : 0.206382, loss_ce: 0.009318, loss_dice: 0.403447
[08:28:54.677] TRAIN: iteration 5050 : loss : 0.252544, loss_ce: 0.004772, loss_dice: 0.500315
[08:28:54.885] TRAIN: iteration 5051 : loss : 0.160623, loss_ce: 0.005531, loss_dice: 0.315715
[08:28:55.095] TRAIN: iteration 5052 : loss : 0.158936, loss_ce: 0.005710, loss_dice: 0.312161
[08:28:55.306] TRAIN: iteration 5053 : loss : 0.158002, loss_ce: 0.012836, loss_dice: 0.303169
[08:28:55.515] TRAIN: iteration 5054 : loss : 0.155171, loss_ce: 0.007779, loss_dice: 0.302563
[08:28:55.721] TRAIN: iteration 5055 : loss : 0.148059, loss_ce: 0.004570, loss_dice: 0.291549
[08:29:01.623] TRAIN: iteration 5056 : loss : 0.189763, loss_ce: 0.004131, loss_dice: 0.375395
[08:29:01.830] TRAIN: iteration 5057 : loss : 0.252216, loss_ce: 0.004217, loss_dice: 0.500215
[08:29:02.036] TRAIN: iteration 5058 : loss : 0.081836, loss_ce: 0.003152, loss_dice: 0.160520
[08:29:02.243] TRAIN: iteration 5059 : loss : 0.237232, loss_ce: 0.006740, loss_dice: 0.467724
[08:29:02.450] TRAIN: iteration 5060 : loss : 0.075825, loss_ce: 0.004091, loss_dice: 0.147559
[08:29:02.686] TRAIN: iteration 5061 : loss : 0.161472, loss_ce: 0.003807, loss_dice: 0.319136
[08:29:02.892] TRAIN: iteration 5062 : loss : 0.251890, loss_ce: 0.003608, loss_dice: 0.500173
[08:29:03.100] TRAIN: iteration 5063 : loss : 0.158521, loss_ce: 0.012591, loss_dice: 0.304450
[08:29:10.507] TRAIN: iteration 5064 : loss : 0.213342, loss_ce: 0.004450, loss_dice: 0.422234
[08:29:10.714] TRAIN: iteration 5065 : loss : 0.126320, loss_ce: 0.012281, loss_dice: 0.240358
[08:29:10.920] TRAIN: iteration 5066 : loss : 0.117470, loss_ce: 0.006700, loss_dice: 0.228240
[08:29:11.128] TRAIN: iteration 5067 : loss : 0.151900, loss_ce: 0.007046, loss_dice: 0.296755
[08:29:11.335] TRAIN: iteration 5068 : loss : 0.251283, loss_ce: 0.002480, loss_dice: 0.500085
[08:29:11.542] TRAIN: iteration 5069 : loss : 0.252217, loss_ce: 0.004271, loss_dice: 0.500164
[08:29:11.749] TRAIN: iteration 5070 : loss : 0.131937, loss_ce: 0.003455, loss_dice: 0.260419
[08:29:11.957] TRAIN: iteration 5071 : loss : 0.096639, loss_ce: 0.009792, loss_dice: 0.183486
[08:29:16.964] TRAIN: iteration 5072 : loss : 0.199966, loss_ce: 0.007032, loss_dice: 0.392899
[08:29:17.171] TRAIN: iteration 5073 : loss : 0.245760, loss_ce: 0.006234, loss_dice: 0.485287
[08:29:17.377] TRAIN: iteration 5074 : loss : 0.246697, loss_ce: 0.004356, loss_dice: 0.489038
[08:29:17.584] TRAIN: iteration 5075 : loss : 0.245327, loss_ce: 0.007191, loss_dice: 0.483462
[08:29:17.791] TRAIN: iteration 5076 : loss : 0.205951, loss_ce: 0.006103, loss_dice: 0.405800
[08:29:18.001] TRAIN: iteration 5077 : loss : 0.191547, loss_ce: 0.004613, loss_dice: 0.378481
[08:29:18.210] TRAIN: iteration 5078 : loss : 0.097926, loss_ce: 0.007223, loss_dice: 0.188630
[08:29:18.417] TRAIN: iteration 5079 : loss : 0.198202, loss_ce: 0.005832, loss_dice: 0.390572
[08:29:23.858] TRAIN: iteration 5080 : loss : 0.145642, loss_ce: 0.006281, loss_dice: 0.285004
[08:29:24.097] TRAIN: iteration 5081 : loss : 0.153155, loss_ce: 0.008214, loss_dice: 0.298097
[08:29:24.309] TRAIN: iteration 5082 : loss : 0.250531, loss_ce: 0.006306, loss_dice: 0.494755
[08:29:24.516] TRAIN: iteration 5083 : loss : 0.220722, loss_ce: 0.044313, loss_dice: 0.397130
[08:29:24.723] TRAIN: iteration 5084 : loss : 0.112125, loss_ce: 0.006667, loss_dice: 0.217583
[08:29:26.983] TRAIN: iteration 5085 : loss : 0.225473, loss_ce: 0.006265, loss_dice: 0.444682
[08:29:27.194] TRAIN: iteration 5086 : loss : 0.150607, loss_ce: 0.014663, loss_dice: 0.286550
[08:29:27.400] TRAIN: iteration 5087 : loss : 0.223178, loss_ce: 0.006692, loss_dice: 0.439664
[08:29:32.932] TRAIN: iteration 5088 : loss : 0.133840, loss_ce: 0.006696, loss_dice: 0.260984
[08:29:33.145] TRAIN: iteration 5089 : loss : 0.108439, loss_ce: 0.006874, loss_dice: 0.210004
[08:29:33.355] TRAIN: iteration 5090 : loss : 0.251356, loss_ce: 0.007704, loss_dice: 0.495009
[08:29:33.564] TRAIN: iteration 5091 : loss : 0.187788, loss_ce: 0.010800, loss_dice: 0.364777
[08:29:33.770] TRAIN: iteration 5092 : loss : 0.146977, loss_ce: 0.007819, loss_dice: 0.286134
[08:29:36.352] TRAIN: iteration 5093 : loss : 0.107951, loss_ce: 0.007868, loss_dice: 0.208034
[08:29:36.564] TRAIN: iteration 5094 : loss : 0.137642, loss_ce: 0.010933, loss_dice: 0.264351
[08:29:36.771] TRAIN: iteration 5095 : loss : 0.153537, loss_ce: 0.008085, loss_dice: 0.298989
[08:29:43.492] TRAIN: iteration 5096 : loss : 0.253619, loss_ce: 0.006815, loss_dice: 0.500424
[08:29:43.700] TRAIN: iteration 5097 : loss : 0.063705, loss_ce: 0.005663, loss_dice: 0.121748
[08:29:43.907] TRAIN: iteration 5098 : loss : 0.253829, loss_ce: 0.007165, loss_dice: 0.500492
[08:29:44.114] TRAIN: iteration 5099 : loss : 0.187633, loss_ce: 0.006623, loss_dice: 0.368644
[08:29:44.321] TRAIN: iteration 5100 : loss : 0.203127, loss_ce: 0.008596, loss_dice: 0.397659
[08:29:45.286] TRAIN: iteration 5101 : loss : 0.253613, loss_ce: 0.006809, loss_dice: 0.500417
[08:29:45.494] TRAIN: iteration 5102 : loss : 0.254481, loss_ce: 0.008963, loss_dice: 0.500000
[08:29:45.706] TRAIN: iteration 5103 : loss : 0.139817, loss_ce: 0.006587, loss_dice: 0.273046
[08:29:51.233] TRAIN: iteration 5104 : loss : 0.140427, loss_ce: 0.006740, loss_dice: 0.274114
[08:29:51.441] TRAIN: iteration 5105 : loss : 0.112287, loss_ce: 0.009771, loss_dice: 0.214804
[08:29:51.649] TRAIN: iteration 5106 : loss : 0.253266, loss_ce: 0.006314, loss_dice: 0.500217
[08:29:51.856] TRAIN: iteration 5107 : loss : 0.096688, loss_ce: 0.006640, loss_dice: 0.186736
[08:29:52.062] TRAIN: iteration 5108 : loss : 0.111188, loss_ce: 0.005717, loss_dice: 0.216658
[08:29:52.272] TRAIN: iteration 5109 : loss : 0.242272, loss_ce: 0.005715, loss_dice: 0.478830
[08:29:52.479] TRAIN: iteration 5110 : loss : 0.252932, loss_ce: 0.005576, loss_dice: 0.500289
[08:29:52.687] TRAIN: iteration 5111 : loss : 0.081192, loss_ce: 0.003743, loss_dice: 0.158640
[08:29:58.564] TRAIN: iteration 5112 : loss : 0.111563, loss_ce: 0.003981, loss_dice: 0.219145
[08:29:58.773] TRAIN: iteration 5113 : loss : 0.163189, loss_ce: 0.005083, loss_dice: 0.321295
[08:29:58.982] TRAIN: iteration 5114 : loss : 0.119484, loss_ce: 0.003291, loss_dice: 0.235677
[08:29:59.188] TRAIN: iteration 5115 : loss : 0.116285, loss_ce: 0.008505, loss_dice: 0.224064
[08:29:59.399] TRAIN: iteration 5116 : loss : 0.211720, loss_ce: 0.010265, loss_dice: 0.413174
[08:30:00.754] TRAIN: iteration 5117 : loss : 0.193104, loss_ce: 0.004255, loss_dice: 0.381952
[08:30:00.963] TRAIN: iteration 5118 : loss : 0.205930, loss_ce: 0.003635, loss_dice: 0.408225
[08:30:01.171] TRAIN: iteration 5119 : loss : 0.229090, loss_ce: 0.004359, loss_dice: 0.453822
[08:30:06.680] TRAIN: iteration 5120 : loss : 0.158273, loss_ce: 0.007942, loss_dice: 0.308603
[08:30:06.916] TRAIN: iteration 5121 : loss : 0.239098, loss_ce: 0.003246, loss_dice: 0.474950
[08:30:07.125] TRAIN: iteration 5122 : loss : 0.247820, loss_ce: 0.005123, loss_dice: 0.490516
[08:30:07.333] TRAIN: iteration 5123 : loss : 0.217969, loss_ce: 0.003747, loss_dice: 0.432190
[08:30:07.540] TRAIN: iteration 5124 : loss : 0.171577, loss_ce: 0.005520, loss_dice: 0.337633
[08:30:09.157] TRAIN: iteration 5125 : loss : 0.235885, loss_ce: 0.003563, loss_dice: 0.468208
[08:30:09.363] TRAIN: iteration 5126 : loss : 0.118610, loss_ce: 0.005268, loss_dice: 0.231952
[08:30:09.570] TRAIN: iteration 5127 : loss : 0.251889, loss_ce: 0.003603, loss_dice: 0.500175
[08:30:16.895] TRAIN: iteration 5128 : loss : 0.190383, loss_ce: 0.005020, loss_dice: 0.375746
[08:30:17.103] TRAIN: iteration 5129 : loss : 0.130704, loss_ce: 0.008027, loss_dice: 0.253381
[08:30:17.310] TRAIN: iteration 5130 : loss : 0.135025, loss_ce: 0.004910, loss_dice: 0.265140
[08:30:17.518] TRAIN: iteration 5131 : loss : 0.236461, loss_ce: 0.004518, loss_dice: 0.468404
[08:30:17.727] TRAIN: iteration 5132 : loss : 0.246917, loss_ce: 0.006572, loss_dice: 0.487261
[08:30:17.935] TRAIN: iteration 5133 : loss : 0.154965, loss_ce: 0.006064, loss_dice: 0.303867
[08:30:18.142] TRAIN: iteration 5134 : loss : 0.252309, loss_ce: 0.004363, loss_dice: 0.500254
[08:30:18.350] TRAIN: iteration 5135 : loss : 0.092823, loss_ce: 0.005095, loss_dice: 0.180552
[08:30:27.562] TRAIN: iteration 5136 : loss : 0.229728, loss_ce: 0.005064, loss_dice: 0.454391
[08:30:27.769] TRAIN: iteration 5137 : loss : 0.116630, loss_ce: 0.004633, loss_dice: 0.228628
[08:30:27.976] TRAIN: iteration 5138 : loss : 0.101787, loss_ce: 0.005523, loss_dice: 0.198051
[08:30:28.184] TRAIN: iteration 5139 : loss : 0.098300, loss_ce: 0.005098, loss_dice: 0.191503
[08:30:28.391] TRAIN: iteration 5140 : loss : 0.148688, loss_ce: 0.004526, loss_dice: 0.292851
[08:30:28.392] NaN or Inf found in input tensor.
[08:30:28.608] TRAIN: iteration 5141 : loss : 0.239749, loss_ce: 0.009782, loss_dice: 0.469715
[08:30:28.815] TRAIN: iteration 5142 : loss : 0.088938, loss_ce: 0.005097, loss_dice: 0.172779
[08:30:29.022] TRAIN: iteration 5143 : loss : 0.149828, loss_ce: 0.005170, loss_dice: 0.294485
[08:30:36.117] TRAIN: iteration 5144 : loss : 0.142963, loss_ce: 0.008759, loss_dice: 0.277167
[08:30:36.327] TRAIN: iteration 5145 : loss : 0.049182, loss_ce: 0.002819, loss_dice: 0.095546
[08:30:36.541] TRAIN: iteration 5146 : loss : 0.232934, loss_ce: 0.029333, loss_dice: 0.436534
[08:30:36.748] TRAIN: iteration 5147 : loss : 0.182537, loss_ce: 0.009664, loss_dice: 0.355411
[08:30:36.954] TRAIN: iteration 5148 : loss : 0.128370, loss_ce: 0.004682, loss_dice: 0.252058
[08:30:37.163] TRAIN: iteration 5149 : loss : 0.115127, loss_ce: 0.003986, loss_dice: 0.226268
[08:30:37.370] TRAIN: iteration 5150 : loss : 0.156089, loss_ce: 0.011035, loss_dice: 0.301143
[08:30:37.578] TRAIN: iteration 5151 : loss : 0.182593, loss_ce: 0.006936, loss_dice: 0.358250
[08:30:45.825] TRAIN: iteration 5152 : loss : 0.120030, loss_ce: 0.008063, loss_dice: 0.231998
[08:30:46.033] TRAIN: iteration 5153 : loss : 0.158247, loss_ce: 0.004219, loss_dice: 0.312274
[08:30:46.239] TRAIN: iteration 5154 : loss : 0.133623, loss_ce: 0.007337, loss_dice: 0.259910
[08:30:46.454] TRAIN: iteration 5155 : loss : 0.079079, loss_ce: 0.005024, loss_dice: 0.153134
[08:30:46.668] TRAIN: iteration 5156 : loss : 0.194908, loss_ce: 0.005762, loss_dice: 0.384053
[08:30:46.875] TRAIN: iteration 5157 : loss : 0.144464, loss_ce: 0.011346, loss_dice: 0.277581
[08:30:47.082] TRAIN: iteration 5158 : loss : 0.250272, loss_ce: 0.006989, loss_dice: 0.493555
[08:30:47.289] TRAIN: iteration 5159 : loss : 0.252744, loss_ce: 0.005254, loss_dice: 0.500235
[08:30:57.924] TRAIN: iteration 5160 : loss : 0.101043, loss_ce: 0.009273, loss_dice: 0.192813
[08:30:58.151] TRAIN: iteration 5161 : loss : 0.252882, loss_ce: 0.005403, loss_dice: 0.500361
[08:30:58.357] TRAIN: iteration 5162 : loss : 0.148964, loss_ce: 0.008587, loss_dice: 0.289341
[08:30:58.563] TRAIN: iteration 5163 : loss : 0.211600, loss_ce: 0.006496, loss_dice: 0.416704
[08:30:58.769] TRAIN: iteration 5164 : loss : 0.207995, loss_ce: 0.008842, loss_dice: 0.407148
[08:30:58.976] TRAIN: iteration 5165 : loss : 0.113094, loss_ce: 0.006508, loss_dice: 0.219681
[08:30:59.182] TRAIN: iteration 5166 : loss : 0.162330, loss_ce: 0.007926, loss_dice: 0.316734
[08:30:59.387] TRAIN: iteration 5167 : loss : 0.166738, loss_ce: 0.005284, loss_dice: 0.328192
[08:31:07.168] TRAIN: iteration 5168 : loss : 0.094138, loss_ce: 0.009670, loss_dice: 0.178605
[08:31:07.376] TRAIN: iteration 5169 : loss : 0.075943, loss_ce: 0.006007, loss_dice: 0.145879
[08:31:07.584] TRAIN: iteration 5170 : loss : 0.107597, loss_ce: 0.007680, loss_dice: 0.207514
[08:31:07.847] TRAIN: iteration 5171 : loss : 0.116393, loss_ce: 0.005298, loss_dice: 0.227489
[08:31:08.054] TRAIN: iteration 5172 : loss : 0.262982, loss_ce: 0.028443, loss_dice: 0.497521
[08:31:08.260] TRAIN: iteration 5173 : loss : 0.090432, loss_ce: 0.006549, loss_dice: 0.174314
[08:31:08.467] TRAIN: iteration 5174 : loss : 0.168973, loss_ce: 0.006263, loss_dice: 0.331684
[08:31:08.674] TRAIN: iteration 5175 : loss : 0.248863, loss_ce: 0.004943, loss_dice: 0.492782
[08:31:16.067] TRAIN: iteration 5176 : loss : 0.252357, loss_ce: 0.004471, loss_dice: 0.500244
[08:31:16.273] TRAIN: iteration 5177 : loss : 0.138341, loss_ce: 0.007438, loss_dice: 0.269245
[08:31:16.481] TRAIN: iteration 5178 : loss : 0.088328, loss_ce: 0.005149, loss_dice: 0.171507
[08:31:16.688] TRAIN: iteration 5179 : loss : 0.204557, loss_ce: 0.019304, loss_dice: 0.389810
[08:31:16.894] TRAIN: iteration 5180 : loss : 0.113673, loss_ce: 0.002952, loss_dice: 0.224394
[08:31:17.129] TRAIN: iteration 5181 : loss : 0.251911, loss_ce: 0.003647, loss_dice: 0.500175
[08:31:17.337] TRAIN: iteration 5182 : loss : 0.251806, loss_ce: 0.003465, loss_dice: 0.500146
[08:31:17.543] TRAIN: iteration 5183 : loss : 0.230192, loss_ce: 0.004627, loss_dice: 0.455757
[08:31:26.212] TRAIN: iteration 5184 : loss : 0.181469, loss_ce: 0.006044, loss_dice: 0.356894
[08:31:26.423] TRAIN: iteration 5185 : loss : 0.117563, loss_ce: 0.003838, loss_dice: 0.231288
[08:31:26.630] TRAIN: iteration 5186 : loss : 0.246838, loss_ce: 0.002966, loss_dice: 0.490710
[08:31:26.836] TRAIN: iteration 5187 : loss : 0.179867, loss_ce: 0.005539, loss_dice: 0.354196
[08:31:27.043] TRAIN: iteration 5188 : loss : 0.137109, loss_ce: 0.003315, loss_dice: 0.270903
[08:31:27.251] TRAIN: iteration 5189 : loss : 0.110271, loss_ce: 0.006556, loss_dice: 0.213986
[08:31:27.459] TRAIN: iteration 5190 : loss : 0.150730, loss_ce: 0.004525, loss_dice: 0.296936
[08:31:27.668] TRAIN: iteration 5191 : loss : 0.155142, loss_ce: 0.008192, loss_dice: 0.302092
[08:31:38.284] TRAIN: iteration 5192 : loss : 0.118640, loss_ce: 0.013498, loss_dice: 0.223782
[08:31:38.491] TRAIN: iteration 5193 : loss : 0.136022, loss_ce: 0.011942, loss_dice: 0.260103
[08:31:38.697] TRAIN: iteration 5194 : loss : 0.221445, loss_ce: 0.006040, loss_dice: 0.436849
[08:31:38.904] TRAIN: iteration 5195 : loss : 0.192837, loss_ce: 0.004523, loss_dice: 0.381152
[08:31:39.110] TRAIN: iteration 5196 : loss : 0.129921, loss_ce: 0.006024, loss_dice: 0.253819
[08:31:39.317] TRAIN: iteration 5197 : loss : 0.252910, loss_ce: 0.006733, loss_dice: 0.499088
[08:31:39.526] TRAIN: iteration 5198 : loss : 0.233576, loss_ce: 0.005303, loss_dice: 0.461848
[08:31:39.733] TRAIN: iteration 5199 : loss : 0.117971, loss_ce: 0.004885, loss_dice: 0.231058
[08:31:48.921] TRAIN: iteration 5200 : loss : 0.152325, loss_ce: 0.025628, loss_dice: 0.279021
[08:31:49.159] TRAIN: iteration 5201 : loss : 0.176983, loss_ce: 0.012257, loss_dice: 0.341708
[08:31:49.366] TRAIN: iteration 5202 : loss : 0.252913, loss_ce: 0.005438, loss_dice: 0.500388
[08:31:49.573] TRAIN: iteration 5203 : loss : 0.253396, loss_ce: 0.008839, loss_dice: 0.497953
[08:31:49.779] TRAIN: iteration 5204 : loss : 0.120322, loss_ce: 0.008850, loss_dice: 0.231794
[08:31:49.873] TRAIN: iteration 5205 : loss : 0.254498, loss_ce: 0.008377, loss_dice: 0.500618
[08:37:48.837] VALIDATION: iteration 2 : loss : 0.167908, loss_ce: 0.008961, loss_dice: 0.326856
[08:37:49.612] TRAIN: iteration 5206 : loss : 0.083678, loss_ce: 0.006891, loss_dice: 0.160465
[08:37:49.822] TRAIN: iteration 5207 : loss : 0.096443, loss_ce: 0.006861, loss_dice: 0.186025
[08:37:50.031] TRAIN: iteration 5208 : loss : 0.252278, loss_ce: 0.008278, loss_dice: 0.496279
[08:37:50.239] TRAIN: iteration 5209 : loss : 0.254344, loss_ce: 0.008103, loss_dice: 0.500585
[08:37:50.453] TRAIN: iteration 5210 : loss : 0.252773, loss_ce: 0.006871, loss_dice: 0.498674
[08:37:50.661] TRAIN: iteration 5211 : loss : 0.136624, loss_ce: 0.006799, loss_dice: 0.266448
[08:37:50.871] TRAIN: iteration 5212 : loss : 0.254214, loss_ce: 0.007832, loss_dice: 0.500596
[08:37:51.087] TRAIN: iteration 5213 : loss : 0.090992, loss_ce: 0.006891, loss_dice: 0.175093
[08:37:51.297] TRAIN: iteration 5214 : loss : 0.250319, loss_ce: 0.014496, loss_dice: 0.486142
[08:37:51.507] TRAIN: iteration 5215 : loss : 0.161470, loss_ce: 0.009789, loss_dice: 0.313151
[08:37:51.717] TRAIN: iteration 5216 : loss : 0.108741, loss_ce: 0.006250, loss_dice: 0.211231
[08:37:51.928] TRAIN: iteration 5217 : loss : 0.097398, loss_ce: 0.009979, loss_dice: 0.184818
[08:37:52.144] TRAIN: iteration 5218 : loss : 0.079767, loss_ce: 0.008274, loss_dice: 0.151261
[08:37:52.352] TRAIN: iteration 5219 : loss : 0.253751, loss_ce: 0.006999, loss_dice: 0.500503
[08:37:52.560] TRAIN: iteration 5220 : loss : 0.253404, loss_ce: 0.006360, loss_dice: 0.500447
[08:37:52.799] TRAIN: iteration 5221 : loss : 0.182670, loss_ce: 0.009448, loss_dice: 0.355893
[08:37:53.006] TRAIN: iteration 5222 : loss : 0.049928, loss_ce: 0.004713, loss_dice: 0.095143
[08:37:53.215] TRAIN: iteration 5223 : loss : 0.253176, loss_ce: 0.006212, loss_dice: 0.500140
[08:37:53.430] TRAIN: iteration 5224 : loss : 0.181968, loss_ce: 0.005189, loss_dice: 0.358748
[08:37:53.638] TRAIN: iteration 5225 : loss : 0.253582, loss_ce: 0.006704, loss_dice: 0.500459
[08:37:53.846] TRAIN: iteration 5226 : loss : 0.116697, loss_ce: 0.006767, loss_dice: 0.226628
[08:37:54.059] TRAIN: iteration 5227 : loss : 0.142555, loss_ce: 0.009803, loss_dice: 0.275308
[08:37:54.267] TRAIN: iteration 5228 : loss : 0.252618, loss_ce: 0.004935, loss_dice: 0.500302
[08:37:54.475] TRAIN: iteration 5229 : loss : 0.234881, loss_ce: 0.007432, loss_dice: 0.462330
[08:37:54.682] TRAIN: iteration 5230 : loss : 0.254823, loss_ce: 0.009328, loss_dice: 0.500317
[08:37:54.889] TRAIN: iteration 5231 : loss : 0.106666, loss_ce: 0.006187, loss_dice: 0.207146
[08:37:55.098] TRAIN: iteration 5232 : loss : 0.252838, loss_ce: 0.005310, loss_dice: 0.500367
[08:37:55.305] TRAIN: iteration 5233 : loss : 0.074985, loss_ce: 0.004221, loss_dice: 0.145749
[08:37:55.512] TRAIN: iteration 5234 : loss : 0.203126, loss_ce: 0.008846, loss_dice: 0.397407
[08:37:55.719] TRAIN: iteration 5235 : loss : 0.100630, loss_ce: 0.007635, loss_dice: 0.193626
[08:37:55.926] TRAIN: iteration 5236 : loss : 0.062474, loss_ce: 0.004743, loss_dice: 0.120205
[08:37:56.135] TRAIN: iteration 5237 : loss : 0.153884, loss_ce: 0.006199, loss_dice: 0.301570
[08:37:56.342] TRAIN: iteration 5238 : loss : 0.252237, loss_ce: 0.005594, loss_dice: 0.498881
[08:37:56.554] TRAIN: iteration 5239 : loss : 0.253546, loss_ce: 0.009841, loss_dice: 0.497251
[08:37:56.763] TRAIN: iteration 5240 : loss : 0.136257, loss_ce: 0.003796, loss_dice: 0.268717
[08:37:57.000] TRAIN: iteration 5241 : loss : 0.095614, loss_ce: 0.006845, loss_dice: 0.184384
[08:37:57.206] TRAIN: iteration 5242 : loss : 0.080627, loss_ce: 0.003345, loss_dice: 0.157909
[08:37:57.502] TRAIN: iteration 5243 : loss : 0.118918, loss_ce: 0.005031, loss_dice: 0.232805
[08:37:57.710] TRAIN: iteration 5244 : loss : 0.252004, loss_ce: 0.005148, loss_dice: 0.498859
[08:37:57.920] TRAIN: iteration 5245 : loss : 0.117606, loss_ce: 0.009030, loss_dice: 0.226182
[08:37:58.130] TRAIN: iteration 5246 : loss : 0.095231, loss_ce: 0.004755, loss_dice: 0.185707
[08:37:58.337] TRAIN: iteration 5247 : loss : 0.153921, loss_ce: 0.012396, loss_dice: 0.295446
[08:37:58.547] TRAIN: iteration 5248 : loss : 0.225030, loss_ce: 0.005946, loss_dice: 0.444113
[08:37:58.755] TRAIN: iteration 5249 : loss : 0.174811, loss_ce: 0.013366, loss_dice: 0.336257
[08:37:58.966] TRAIN: iteration 5250 : loss : 0.105158, loss_ce: 0.004334, loss_dice: 0.205983
[08:37:59.180] TRAIN: iteration 5251 : loss : 0.129822, loss_ce: 0.008451, loss_dice: 0.251194
[08:37:59.391] TRAIN: iteration 5252 : loss : 0.205503, loss_ce: 0.009309, loss_dice: 0.401696
[08:37:59.607] TRAIN: iteration 5253 : loss : 0.197168, loss_ce: 0.004459, loss_dice: 0.389877
[08:37:59.814] TRAIN: iteration 5254 : loss : 0.057685, loss_ce: 0.003519, loss_dice: 0.111852
[08:38:00.024] TRAIN: iteration 5255 : loss : 0.164701, loss_ce: 0.006820, loss_dice: 0.322583
[08:38:00.234] TRAIN: iteration 5256 : loss : 0.252713, loss_ce: 0.005716, loss_dice: 0.499709
[08:38:00.448] TRAIN: iteration 5257 : loss : 0.230156, loss_ce: 0.008151, loss_dice: 0.452161
[08:38:00.660] TRAIN: iteration 5258 : loss : 0.114647, loss_ce: 0.004959, loss_dice: 0.224334
[08:38:00.869] TRAIN: iteration 5259 : loss : 0.239025, loss_ce: 0.005123, loss_dice: 0.472926
[08:38:01.078] TRAIN: iteration 5260 : loss : 0.148087, loss_ce: 0.010328, loss_dice: 0.285846
[08:38:01.316] TRAIN: iteration 5261 : loss : 0.116359, loss_ce: 0.004397, loss_dice: 0.228322
[08:38:01.531] TRAIN: iteration 5262 : loss : 0.042972, loss_ce: 0.002645, loss_dice: 0.083299
[08:38:01.746] TRAIN: iteration 5263 : loss : 0.247437, loss_ce: 0.004963, loss_dice: 0.489910
[08:38:01.955] TRAIN: iteration 5264 : loss : 0.081598, loss_ce: 0.005341, loss_dice: 0.157854
[08:38:02.163] TRAIN: iteration 5265 : loss : 0.236606, loss_ce: 0.006150, loss_dice: 0.467061
[08:38:02.373] TRAIN: iteration 5266 : loss : 0.131739, loss_ce: 0.006357, loss_dice: 0.257122
[08:38:02.582] TRAIN: iteration 5267 : loss : 0.116600, loss_ce: 0.009711, loss_dice: 0.223488
[08:38:02.794] TRAIN: iteration 5268 : loss : 0.159843, loss_ce: 0.004598, loss_dice: 0.315088
[08:38:03.008] TRAIN: iteration 5269 : loss : 0.097411, loss_ce: 0.003936, loss_dice: 0.190885
[08:38:03.217] TRAIN: iteration 5270 : loss : 0.247196, loss_ce: 0.037200, loss_dice: 0.457192
[08:38:03.425] TRAIN: iteration 5271 : loss : 0.203755, loss_ce: 0.004155, loss_dice: 0.403354
[08:38:03.633] TRAIN: iteration 5272 : loss : 0.212123, loss_ce: 0.007896, loss_dice: 0.416350
[08:38:03.840] TRAIN: iteration 5273 : loss : 0.223780, loss_ce: 0.012222, loss_dice: 0.435338
[08:38:04.051] TRAIN: iteration 5274 : loss : 0.217440, loss_ce: 0.004650, loss_dice: 0.430229
[08:38:04.259] TRAIN: iteration 5275 : loss : 0.238431, loss_ce: 0.007955, loss_dice: 0.468907
[08:38:04.467] TRAIN: iteration 5276 : loss : 0.114696, loss_ce: 0.009092, loss_dice: 0.220301
[08:38:04.677] TRAIN: iteration 5277 : loss : 0.137331, loss_ce: 0.005918, loss_dice: 0.268743
[08:38:04.884] TRAIN: iteration 5278 : loss : 0.059504, loss_ce: 0.003490, loss_dice: 0.115517
[08:38:05.093] TRAIN: iteration 5279 : loss : 0.156850, loss_ce: 0.005185, loss_dice: 0.308515
[08:38:05.301] TRAIN: iteration 5280 : loss : 0.120006, loss_ce: 0.005878, loss_dice: 0.234134
[08:38:05.539] TRAIN: iteration 5281 : loss : 0.252168, loss_ce: 0.005202, loss_dice: 0.499134
[08:38:05.747] TRAIN: iteration 5282 : loss : 0.110586, loss_ce: 0.007038, loss_dice: 0.214134
[08:38:05.955] TRAIN: iteration 5283 : loss : 0.142892, loss_ce: 0.004653, loss_dice: 0.281132
[08:38:06.167] TRAIN: iteration 5284 : loss : 0.084834, loss_ce: 0.009125, loss_dice: 0.160543
[08:38:06.374] TRAIN: iteration 5285 : loss : 0.252243, loss_ce: 0.005026, loss_dice: 0.499460
[08:38:06.590] TRAIN: iteration 5286 : loss : 0.221185, loss_ce: 0.029694, loss_dice: 0.412675
[08:38:06.804] TRAIN: iteration 5287 : loss : 0.252231, loss_ce: 0.009823, loss_dice: 0.494638
[08:38:07.011] TRAIN: iteration 5288 : loss : 0.092928, loss_ce: 0.004041, loss_dice: 0.181814
[08:38:07.220] TRAIN: iteration 5289 : loss : 0.083621, loss_ce: 0.005318, loss_dice: 0.161924
[08:38:07.429] TRAIN: iteration 5290 : loss : 0.164362, loss_ce: 0.004535, loss_dice: 0.324190
[08:38:07.643] TRAIN: iteration 5291 : loss : 0.217481, loss_ce: 0.005621, loss_dice: 0.429341
[08:38:07.851] TRAIN: iteration 5292 : loss : 0.212299, loss_ce: 0.008251, loss_dice: 0.416348
[08:38:08.060] TRAIN: iteration 5293 : loss : 0.201667, loss_ce: 0.008034, loss_dice: 0.395299
[08:38:08.273] TRAIN: iteration 5294 : loss : 0.231212, loss_ce: 0.004280, loss_dice: 0.458143
[08:38:08.481] TRAIN: iteration 5295 : loss : 0.116672, loss_ce: 0.004139, loss_dice: 0.229205
[08:38:08.689] TRAIN: iteration 5296 : loss : 0.094852, loss_ce: 0.010176, loss_dice: 0.179527
[08:38:08.896] TRAIN: iteration 5297 : loss : 0.252950, loss_ce: 0.005501, loss_dice: 0.500398
[08:38:09.104] TRAIN: iteration 5298 : loss : 0.137258, loss_ce: 0.005961, loss_dice: 0.268555
[08:38:09.340] TRAIN: iteration 5299 : loss : 0.182782, loss_ce: 0.006583, loss_dice: 0.358980
[08:38:09.547] TRAIN: iteration 5300 : loss : 0.079298, loss_ce: 0.009043, loss_dice: 0.149553
[08:38:09.784] TRAIN: iteration 5301 : loss : 0.107744, loss_ce: 0.006630, loss_dice: 0.208858
[08:38:09.993] TRAIN: iteration 5302 : loss : 0.088966, loss_ce: 0.007628, loss_dice: 0.170305
[08:38:10.200] TRAIN: iteration 5303 : loss : 0.244802, loss_ce: 0.006515, loss_dice: 0.483090
[08:38:10.408] TRAIN: iteration 5304 : loss : 0.228131, loss_ce: 0.031971, loss_dice: 0.424291
[08:38:10.617] TRAIN: iteration 5305 : loss : 0.134553, loss_ce: 0.005650, loss_dice: 0.263456
[08:38:10.823] TRAIN: iteration 5306 : loss : 0.182076, loss_ce: 0.006321, loss_dice: 0.357831
[08:38:11.039] TRAIN: iteration 5307 : loss : 0.204668, loss_ce: 0.006640, loss_dice: 0.402696
[08:38:11.249] TRAIN: iteration 5308 : loss : 0.090227, loss_ce: 0.008010, loss_dice: 0.172444
[08:38:11.460] TRAIN: iteration 5309 : loss : 0.057950, loss_ce: 0.003865, loss_dice: 0.112034
[08:38:11.667] TRAIN: iteration 5310 : loss : 0.206333, loss_ce: 0.008355, loss_dice: 0.404311
[08:38:11.880] TRAIN: iteration 5311 : loss : 0.227154, loss_ce: 0.007812, loss_dice: 0.446495
[08:38:12.092] TRAIN: iteration 5312 : loss : 0.249906, loss_ce: 0.007349, loss_dice: 0.492463
[08:38:12.305] TRAIN: iteration 5313 : loss : 0.172514, loss_ce: 0.012961, loss_dice: 0.332066
[08:38:12.515] TRAIN: iteration 5314 : loss : 0.121384, loss_ce: 0.004619, loss_dice: 0.238149
[08:38:12.723] TRAIN: iteration 5315 : loss : 0.119209, loss_ce: 0.004276, loss_dice: 0.234143
[08:38:12.930] TRAIN: iteration 5316 : loss : 0.193546, loss_ce: 0.004978, loss_dice: 0.382114
[08:38:13.140] TRAIN: iteration 5317 : loss : 0.252011, loss_ce: 0.003791, loss_dice: 0.500231
[08:38:13.349] TRAIN: iteration 5318 : loss : 0.211401, loss_ce: 0.004871, loss_dice: 0.417931
[08:38:13.563] TRAIN: iteration 5319 : loss : 0.252216, loss_ce: 0.004154, loss_dice: 0.500279
[08:38:13.772] TRAIN: iteration 5320 : loss : 0.091154, loss_ce: 0.005912, loss_dice: 0.176397
[08:38:14.010] TRAIN: iteration 5321 : loss : 0.073826, loss_ce: 0.002376, loss_dice: 0.145276
[08:38:14.220] TRAIN: iteration 5322 : loss : 0.204918, loss_ce: 0.003534, loss_dice: 0.406302
[08:38:14.437] TRAIN: iteration 5323 : loss : 0.107890, loss_ce: 0.006141, loss_dice: 0.209639
[08:38:14.646] TRAIN: iteration 5324 : loss : 0.251553, loss_ce: 0.002931, loss_dice: 0.500174
[08:38:14.854] TRAIN: iteration 5325 : loss : 0.236870, loss_ce: 0.003572, loss_dice: 0.470168
[08:38:15.285] TRAIN: iteration 5326 : loss : 0.159269, loss_ce: 0.011549, loss_dice: 0.306989
[08:38:15.493] TRAIN: iteration 5327 : loss : 0.112987, loss_ce: 0.004511, loss_dice: 0.221463
[08:38:15.704] TRAIN: iteration 5328 : loss : 0.145338, loss_ce: 0.003277, loss_dice: 0.287398
[08:38:15.916] TRAIN: iteration 5329 : loss : 0.239657, loss_ce: 0.003103, loss_dice: 0.476211
[08:38:16.125] TRAIN: iteration 5330 : loss : 0.098543, loss_ce: 0.005279, loss_dice: 0.191806
[08:38:16.332] TRAIN: iteration 5331 : loss : 0.134393, loss_ce: 0.006295, loss_dice: 0.262491
[08:38:16.540] TRAIN: iteration 5332 : loss : 0.167821, loss_ce: 0.023717, loss_dice: 0.311925
[08:38:16.755] TRAIN: iteration 5333 : loss : 0.076906, loss_ce: 0.003630, loss_dice: 0.150181
[08:38:16.964] TRAIN: iteration 5334 : loss : 0.156297, loss_ce: 0.008062, loss_dice: 0.304531
[08:38:17.178] TRAIN: iteration 5335 : loss : 0.133499, loss_ce: 0.004497, loss_dice: 0.262502
[08:38:17.388] TRAIN: iteration 5336 : loss : 0.247392, loss_ce: 0.004202, loss_dice: 0.490581
[08:38:17.598] TRAIN: iteration 5337 : loss : 0.098654, loss_ce: 0.006114, loss_dice: 0.191195
[08:38:17.808] TRAIN: iteration 5338 : loss : 0.252818, loss_ce: 0.005674, loss_dice: 0.499962
[08:38:18.018] TRAIN: iteration 5339 : loss : 0.247956, loss_ce: 0.004877, loss_dice: 0.491035
[08:38:18.227] TRAIN: iteration 5340 : loss : 0.250485, loss_ce: 0.004752, loss_dice: 0.496217
[08:38:18.480] TRAIN: iteration 5341 : loss : 0.142105, loss_ce: 0.004921, loss_dice: 0.279289
[08:38:18.688] TRAIN: iteration 5342 : loss : 0.227677, loss_ce: 0.006207, loss_dice: 0.449147
[08:38:18.897] TRAIN: iteration 5343 : loss : 0.156536, loss_ce: 0.013742, loss_dice: 0.299330
[08:38:19.111] TRAIN: iteration 5344 : loss : 0.108152, loss_ce: 0.005648, loss_dice: 0.210655
[08:38:19.320] TRAIN: iteration 5345 : loss : 0.179630, loss_ce: 0.015627, loss_dice: 0.343632
[08:38:19.528] TRAIN: iteration 5346 : loss : 0.256832, loss_ce: 0.013715, loss_dice: 0.499949
[08:38:19.742] TRAIN: iteration 5347 : loss : 0.252467, loss_ce: 0.008881, loss_dice: 0.496054
[08:38:19.958] TRAIN: iteration 5348 : loss : 0.184634, loss_ce: 0.006282, loss_dice: 0.362986
[08:38:20.166] TRAIN: iteration 5349 : loss : 0.234836, loss_ce: 0.006070, loss_dice: 0.463602
[08:38:20.374] TRAIN: iteration 5350 : loss : 0.253505, loss_ce: 0.007017, loss_dice: 0.499994
[08:38:20.582] TRAIN: iteration 5351 : loss : 0.141971, loss_ce: 0.007785, loss_dice: 0.276157
[08:38:20.792] TRAIN: iteration 5352 : loss : 0.201175, loss_ce: 0.007853, loss_dice: 0.394498
[08:38:21.001] TRAIN: iteration 5353 : loss : 0.142037, loss_ce: 0.006615, loss_dice: 0.277459
[08:38:21.211] TRAIN: iteration 5354 : loss : 0.149019, loss_ce: 0.010162, loss_dice: 0.287876
[08:38:21.420] TRAIN: iteration 5355 : loss : 0.221438, loss_ce: 0.005646, loss_dice: 0.437229
[08:38:21.629] TRAIN: iteration 5356 : loss : 0.094364, loss_ce: 0.005665, loss_dice: 0.183062
[08:38:21.836] TRAIN: iteration 5357 : loss : 0.153386, loss_ce: 0.006970, loss_dice: 0.299802
[08:38:22.045] TRAIN: iteration 5358 : loss : 0.076960, loss_ce: 0.003986, loss_dice: 0.149933
[08:38:22.254] TRAIN: iteration 5359 : loss : 0.160093, loss_ce: 0.005522, loss_dice: 0.314664
[08:38:22.488] TRAIN: iteration 5360 : loss : 0.199267, loss_ce: 0.005526, loss_dice: 0.393009
[08:38:22.724] TRAIN: iteration 5361 : loss : 0.086157, loss_ce: 0.005352, loss_dice: 0.166961
[08:38:22.933] TRAIN: iteration 5362 : loss : 0.163354, loss_ce: 0.006597, loss_dice: 0.320111
[08:38:23.152] TRAIN: iteration 5363 : loss : 0.208816, loss_ce: 0.008170, loss_dice: 0.409462
[08:38:23.362] TRAIN: iteration 5364 : loss : 0.146838, loss_ce: 0.005254, loss_dice: 0.288421
[08:38:23.571] TRAIN: iteration 5365 : loss : 0.124778, loss_ce: 0.002982, loss_dice: 0.246573
[08:38:23.782] TRAIN: iteration 5366 : loss : 0.100016, loss_ce: 0.003058, loss_dice: 0.196975
[08:38:23.990] TRAIN: iteration 5367 : loss : 0.072233, loss_ce: 0.004215, loss_dice: 0.140252
[08:38:24.201] TRAIN: iteration 5368 : loss : 0.089683, loss_ce: 0.003975, loss_dice: 0.175391
[08:38:24.409] TRAIN: iteration 5369 : loss : 0.240805, loss_ce: 0.002954, loss_dice: 0.478656
[08:38:24.618] TRAIN: iteration 5370 : loss : 0.229078, loss_ce: 0.011837, loss_dice: 0.446318
[08:38:24.828] TRAIN: iteration 5371 : loss : 0.214456, loss_ce: 0.003682, loss_dice: 0.425231
[08:38:25.039] TRAIN: iteration 5372 : loss : 0.183838, loss_ce: 0.003826, loss_dice: 0.363849
[08:38:25.246] TRAIN: iteration 5373 : loss : 0.138632, loss_ce: 0.008890, loss_dice: 0.268374
[08:38:25.463] TRAIN: iteration 5374 : loss : 0.078852, loss_ce: 0.003209, loss_dice: 0.154494
[08:38:25.674] TRAIN: iteration 5375 : loss : 0.137770, loss_ce: 0.006020, loss_dice: 0.269519
[08:38:25.883] TRAIN: iteration 5376 : loss : 0.251650, loss_ce: 0.003110, loss_dice: 0.500190
[08:38:26.093] TRAIN: iteration 5377 : loss : 0.082323, loss_ce: 0.003531, loss_dice: 0.161116
[08:38:26.300] TRAIN: iteration 5378 : loss : 0.105166, loss_ce: 0.004976, loss_dice: 0.205357
[08:38:26.512] TRAIN: iteration 5379 : loss : 0.162962, loss_ce: 0.008585, loss_dice: 0.317338
[08:38:26.720] TRAIN: iteration 5380 : loss : 0.212762, loss_ce: 0.004775, loss_dice: 0.420749
[08:38:26.960] TRAIN: iteration 5381 : loss : 0.142914, loss_ce: 0.009725, loss_dice: 0.276104
[08:38:27.168] TRAIN: iteration 5382 : loss : 0.204120, loss_ce: 0.015677, loss_dice: 0.392563
[08:38:27.376] TRAIN: iteration 5383 : loss : 0.226253, loss_ce: 0.005618, loss_dice: 0.446887
[08:38:27.585] TRAIN: iteration 5384 : loss : 0.108804, loss_ce: 0.004869, loss_dice: 0.212739
[08:38:27.794] TRAIN: iteration 5385 : loss : 0.219735, loss_ce: 0.006104, loss_dice: 0.433366
[08:38:28.002] TRAIN: iteration 5386 : loss : 0.165015, loss_ce: 0.006112, loss_dice: 0.323918
[08:38:28.211] TRAIN: iteration 5387 : loss : 0.083309, loss_ce: 0.005085, loss_dice: 0.161532
[08:38:28.422] TRAIN: iteration 5388 : loss : 0.125368, loss_ce: 0.022708, loss_dice: 0.228027
[08:38:28.637] TRAIN: iteration 5389 : loss : 0.253422, loss_ce: 0.006363, loss_dice: 0.500481
[08:38:28.846] TRAIN: iteration 5390 : loss : 0.175234, loss_ce: 0.007542, loss_dice: 0.342927
[08:38:29.055] TRAIN: iteration 5391 : loss : 0.214985, loss_ce: 0.009721, loss_dice: 0.420249
[08:38:29.264] TRAIN: iteration 5392 : loss : 0.120855, loss_ce: 0.019957, loss_dice: 0.221754
[08:38:29.474] TRAIN: iteration 5393 : loss : 0.179097, loss_ce: 0.007927, loss_dice: 0.350267
[08:38:29.685] TRAIN: iteration 5394 : loss : 0.172131, loss_ce: 0.005489, loss_dice: 0.338773
[08:38:29.894] TRAIN: iteration 5395 : loss : 0.192535, loss_ce: 0.007829, loss_dice: 0.377241
[08:38:30.102] TRAIN: iteration 5396 : loss : 0.254326, loss_ce: 0.008037, loss_dice: 0.500615
[08:38:30.315] TRAIN: iteration 5397 : loss : 0.113821, loss_ce: 0.005074, loss_dice: 0.222568
[08:38:30.526] TRAIN: iteration 5398 : loss : 0.219806, loss_ce: 0.007152, loss_dice: 0.432460
[08:38:30.734] TRAIN: iteration 5399 : loss : 0.227548, loss_ce: 0.007576, loss_dice: 0.447519
[08:38:30.947] TRAIN: iteration 5400 : loss : 0.079189, loss_ce: 0.007941, loss_dice: 0.150438
[08:38:31.196] TRAIN: iteration 5401 : loss : 0.246096, loss_ce: 0.006668, loss_dice: 0.485525
[08:38:31.407] TRAIN: iteration 5402 : loss : 0.244075, loss_ce: 0.006974, loss_dice: 0.481175
[08:38:31.617] TRAIN: iteration 5403 : loss : 0.191594, loss_ce: 0.008740, loss_dice: 0.374448
[08:38:31.832] TRAIN: iteration 5404 : loss : 0.143514, loss_ce: 0.011403, loss_dice: 0.275626
[08:38:32.155] TRAIN: iteration 5405 : loss : 0.253208, loss_ce: 0.006030, loss_dice: 0.500385
[08:38:32.370] TRAIN: iteration 5406 : loss : 0.245573, loss_ce: 0.006769, loss_dice: 0.484377
[08:38:32.578] TRAIN: iteration 5407 : loss : 0.156006, loss_ce: 0.006563, loss_dice: 0.305448
[08:38:32.787] TRAIN: iteration 5408 : loss : 0.184821, loss_ce: 0.007197, loss_dice: 0.362444
[08:38:32.995] TRAIN: iteration 5409 : loss : 0.229787, loss_ce: 0.006143, loss_dice: 0.453432
[08:38:33.203] TRAIN: iteration 5410 : loss : 0.144246, loss_ce: 0.012815, loss_dice: 0.275677
[08:38:33.413] TRAIN: iteration 5411 : loss : 0.152340, loss_ce: 0.007852, loss_dice: 0.296829
[08:38:33.629] TRAIN: iteration 5412 : loss : 0.165311, loss_ce: 0.006552, loss_dice: 0.324070
[08:38:33.866] TRAIN: iteration 5413 : loss : 0.114364, loss_ce: 0.013073, loss_dice: 0.215655
[08:38:34.078] TRAIN: iteration 5414 : loss : 0.166582, loss_ce: 0.007564, loss_dice: 0.325600
[08:38:34.292] TRAIN: iteration 5415 : loss : 0.250915, loss_ce: 0.007785, loss_dice: 0.494045
[08:38:34.503] TRAIN: iteration 5416 : loss : 0.253074, loss_ce: 0.005775, loss_dice: 0.500374
[08:38:34.710] TRAIN: iteration 5417 : loss : 0.097502, loss_ce: 0.011108, loss_dice: 0.183895
[08:38:34.925] TRAIN: iteration 5418 : loss : 0.239241, loss_ce: 0.006154, loss_dice: 0.472327
[08:38:35.133] TRAIN: iteration 5419 : loss : 0.177475, loss_ce: 0.004951, loss_dice: 0.349999
[08:38:35.343] TRAIN: iteration 5420 : loss : 0.179999, loss_ce: 0.008134, loss_dice: 0.351864
[08:38:35.594] TRAIN: iteration 5421 : loss : 0.181964, loss_ce: 0.006952, loss_dice: 0.356976
[08:38:35.802] TRAIN: iteration 5422 : loss : 0.253090, loss_ce: 0.005831, loss_dice: 0.500350
[08:38:36.015] TRAIN: iteration 5423 : loss : 0.226725, loss_ce: 0.007139, loss_dice: 0.446311
[08:38:36.229] TRAIN: iteration 5424 : loss : 0.076449, loss_ce: 0.004872, loss_dice: 0.148026
[08:38:36.437] TRAIN: iteration 5425 : loss : 0.252800, loss_ce: 0.005293, loss_dice: 0.500306
[08:38:36.644] TRAIN: iteration 5426 : loss : 0.173850, loss_ce: 0.006550, loss_dice: 0.341150
[08:38:36.852] TRAIN: iteration 5427 : loss : 0.135864, loss_ce: 0.013037, loss_dice: 0.258691
[08:38:37.061] TRAIN: iteration 5428 : loss : 0.138925, loss_ce: 0.018828, loss_dice: 0.259021
[08:38:37.271] TRAIN: iteration 5429 : loss : 0.253006, loss_ce: 0.005653, loss_dice: 0.500360
[08:38:37.487] TRAIN: iteration 5430 : loss : 0.253130, loss_ce: 0.005902, loss_dice: 0.500359
[08:38:37.696] TRAIN: iteration 5431 : loss : 0.150449, loss_ce: 0.019664, loss_dice: 0.281233
[08:38:37.908] TRAIN: iteration 5432 : loss : 0.131299, loss_ce: 0.006037, loss_dice: 0.256561
[08:38:38.115] TRAIN: iteration 5433 : loss : 0.101046, loss_ce: 0.005775, loss_dice: 0.196318
[08:38:38.324] TRAIN: iteration 5434 : loss : 0.114487, loss_ce: 0.005314, loss_dice: 0.223660
[08:38:38.531] TRAIN: iteration 5435 : loss : 0.245048, loss_ce: 0.006023, loss_dice: 0.484074
[08:38:38.738] TRAIN: iteration 5436 : loss : 0.073479, loss_ce: 0.004022, loss_dice: 0.142935
[08:38:38.954] TRAIN: iteration 5437 : loss : 0.148904, loss_ce: 0.006050, loss_dice: 0.291757
[08:38:39.170] TRAIN: iteration 5438 : loss : 0.193288, loss_ce: 0.005337, loss_dice: 0.381238
[08:38:39.377] TRAIN: iteration 5439 : loss : 0.214448, loss_ce: 0.006393, loss_dice: 0.422502
[08:38:39.585] TRAIN: iteration 5440 : loss : 0.075016, loss_ce: 0.003665, loss_dice: 0.146367
[08:38:39.586] NaN or Inf found in input tensor.
[08:38:39.800] TRAIN: iteration 5441 : loss : 0.127683, loss_ce: 0.010645, loss_dice: 0.244721
[08:38:40.007] TRAIN: iteration 5442 : loss : 0.207362, loss_ce: 0.006251, loss_dice: 0.408473
[08:38:40.215] TRAIN: iteration 5443 : loss : 0.156569, loss_ce: 0.005003, loss_dice: 0.308135
[08:38:40.422] TRAIN: iteration 5444 : loss : 0.198175, loss_ce: 0.004414, loss_dice: 0.391937
[08:38:40.631] TRAIN: iteration 5445 : loss : 0.078251, loss_ce: 0.004281, loss_dice: 0.152221
[08:38:40.838] TRAIN: iteration 5446 : loss : 0.102515, loss_ce: 0.010791, loss_dice: 0.194239
[08:38:41.046] TRAIN: iteration 5447 : loss : 0.109551, loss_ce: 0.009625, loss_dice: 0.209477
[08:38:41.256] TRAIN: iteration 5448 : loss : 0.087908, loss_ce: 0.008151, loss_dice: 0.167664
[08:38:41.463] TRAIN: iteration 5449 : loss : 0.251721, loss_ce: 0.005153, loss_dice: 0.498288
[08:38:41.671] TRAIN: iteration 5450 : loss : 0.096326, loss_ce: 0.008224, loss_dice: 0.184427
[08:38:41.884] TRAIN: iteration 5451 : loss : 0.062750, loss_ce: 0.003549, loss_dice: 0.121952
[08:38:42.092] TRAIN: iteration 5452 : loss : 0.194680, loss_ce: 0.005339, loss_dice: 0.384022
[08:38:42.307] TRAIN: iteration 5453 : loss : 0.154007, loss_ce: 0.005577, loss_dice: 0.302438
[08:38:42.522] TRAIN: iteration 5454 : loss : 0.082202, loss_ce: 0.005546, loss_dice: 0.158858
[08:38:42.733] TRAIN: iteration 5455 : loss : 0.142026, loss_ce: 0.004645, loss_dice: 0.279408
[08:38:42.949] TRAIN: iteration 5456 : loss : 0.054987, loss_ce: 0.002686, loss_dice: 0.107289
[08:38:43.873] TRAIN: iteration 5457 : loss : 0.160512, loss_ce: 0.003874, loss_dice: 0.317149
[08:38:44.085] TRAIN: iteration 5458 : loss : 0.190330, loss_ce: 0.014339, loss_dice: 0.366320
[08:38:44.301] TRAIN: iteration 5459 : loss : 0.227719, loss_ce: 0.006993, loss_dice: 0.448445
[08:38:44.509] TRAIN: iteration 5460 : loss : 0.065522, loss_ce: 0.003900, loss_dice: 0.127143
[08:38:44.749] TRAIN: iteration 5461 : loss : 0.168735, loss_ce: 0.005945, loss_dice: 0.331525
[08:38:44.962] TRAIN: iteration 5462 : loss : 0.199504, loss_ce: 0.004143, loss_dice: 0.394865
[08:38:45.172] TRAIN: iteration 5463 : loss : 0.251466, loss_ce: 0.002761, loss_dice: 0.500171
[08:38:45.386] TRAIN: iteration 5464 : loss : 0.247830, loss_ce: 0.004962, loss_dice: 0.490697
[08:38:45.595] TRAIN: iteration 5465 : loss : 0.251027, loss_ce: 0.001956, loss_dice: 0.500098
[08:38:45.803] TRAIN: iteration 5466 : loss : 0.120452, loss_ce: 0.004511, loss_dice: 0.236394
[08:38:46.018] TRAIN: iteration 5467 : loss : 0.250728, loss_ce: 0.002386, loss_dice: 0.499070
[08:38:46.233] TRAIN: iteration 5468 : loss : 0.148565, loss_ce: 0.008186, loss_dice: 0.288944
[08:38:46.440] TRAIN: iteration 5469 : loss : 0.173183, loss_ce: 0.009998, loss_dice: 0.336368
[08:38:46.651] TRAIN: iteration 5470 : loss : 0.232378, loss_ce: 0.007407, loss_dice: 0.457349
[08:38:46.890] TRAIN: iteration 5471 : loss : 0.164537, loss_ce: 0.005477, loss_dice: 0.323598
[08:38:47.099] TRAIN: iteration 5472 : loss : 0.192892, loss_ce: 0.025541, loss_dice: 0.360244
[08:38:47.307] TRAIN: iteration 5473 : loss : 0.236614, loss_ce: 0.007035, loss_dice: 0.466193
[08:38:47.514] TRAIN: iteration 5474 : loss : 0.112656, loss_ce: 0.012327, loss_dice: 0.212984
[08:38:47.730] TRAIN: iteration 5475 : loss : 0.168971, loss_ce: 0.013297, loss_dice: 0.324644
[08:38:47.990] TRAIN: iteration 5476 : loss : 0.173273, loss_ce: 0.006102, loss_dice: 0.340444
[08:38:48.198] TRAIN: iteration 5477 : loss : 0.189183, loss_ce: 0.006452, loss_dice: 0.371913
[08:38:48.408] TRAIN: iteration 5478 : loss : 0.146292, loss_ce: 0.009075, loss_dice: 0.283509
[08:38:49.211] TRAIN: iteration 5479 : loss : 0.162674, loss_ce: 0.005783, loss_dice: 0.319565
[08:38:49.421] TRAIN: iteration 5480 : loss : 0.241011, loss_ce: 0.007644, loss_dice: 0.474377
[08:38:49.659] TRAIN: iteration 5481 : loss : 0.138506, loss_ce: 0.007282, loss_dice: 0.269731
[08:38:49.868] TRAIN: iteration 5482 : loss : 0.145699, loss_ce: 0.007287, loss_dice: 0.284110
[08:38:50.085] TRAIN: iteration 5483 : loss : 0.148947, loss_ce: 0.015316, loss_dice: 0.282578
[08:38:50.792] TRAIN: iteration 5484 : loss : 0.178232, loss_ce: 0.034744, loss_dice: 0.321720
[08:38:51.001] TRAIN: iteration 5485 : loss : 0.172614, loss_ce: 0.017251, loss_dice: 0.327978
[08:38:51.209] TRAIN: iteration 5486 : loss : 0.063697, loss_ce: 0.006788, loss_dice: 0.120607
[08:38:51.416] TRAIN: iteration 5487 : loss : 0.186931, loss_ce: 0.008063, loss_dice: 0.365800
[08:38:51.624] TRAIN: iteration 5488 : loss : 0.189296, loss_ce: 0.009355, loss_dice: 0.369238
[08:38:51.831] TRAIN: iteration 5489 : loss : 0.236558, loss_ce: 0.008021, loss_dice: 0.465095
[08:38:52.039] TRAIN: iteration 5490 : loss : 0.249772, loss_ce: 0.008260, loss_dice: 0.491283
[08:38:52.246] TRAIN: iteration 5491 : loss : 0.154480, loss_ce: 0.006717, loss_dice: 0.302243
[08:38:52.454] TRAIN: iteration 5492 : loss : 0.235429, loss_ce: 0.007313, loss_dice: 0.463544
[08:38:52.660] TRAIN: iteration 5493 : loss : 0.253780, loss_ce: 0.007141, loss_dice: 0.500418
[08:38:52.868] TRAIN: iteration 5494 : loss : 0.171673, loss_ce: 0.008404, loss_dice: 0.334942
[08:38:53.076] TRAIN: iteration 5495 : loss : 0.180709, loss_ce: 0.014885, loss_dice: 0.346534
[08:38:53.285] TRAIN: iteration 5496 : loss : 0.243100, loss_ce: 0.006278, loss_dice: 0.479922
[08:38:53.498] TRAIN: iteration 5497 : loss : 0.188393, loss_ce: 0.008692, loss_dice: 0.368094
[08:38:53.707] TRAIN: iteration 5498 : loss : 0.120742, loss_ce: 0.006525, loss_dice: 0.234959
[08:38:53.935] TRAIN: iteration 5499 : loss : 0.253109, loss_ce: 0.005866, loss_dice: 0.500352
[08:38:54.147] TRAIN: iteration 5500 : loss : 0.216851, loss_ce: 0.005438, loss_dice: 0.428264
[08:38:54.383] TRAIN: iteration 5501 : loss : 0.116629, loss_ce: 0.008907, loss_dice: 0.224351
[08:38:54.591] TRAIN: iteration 5502 : loss : 0.238192, loss_ce: 0.005211, loss_dice: 0.471172
[08:38:54.806] TRAIN: iteration 5503 : loss : 0.170059, loss_ce: 0.004415, loss_dice: 0.335703
[08:38:55.019] TRAIN: iteration 5504 : loss : 0.236526, loss_ce: 0.010428, loss_dice: 0.462624
[08:38:55.234] TRAIN: iteration 5505 : loss : 0.145831, loss_ce: 0.006194, loss_dice: 0.285468
[08:38:55.441] TRAIN: iteration 5506 : loss : 0.117735, loss_ce: 0.006570, loss_dice: 0.228901
[08:38:55.650] TRAIN: iteration 5507 : loss : 0.075078, loss_ce: 0.004706, loss_dice: 0.145451
[08:38:55.857] TRAIN: iteration 5508 : loss : 0.211655, loss_ce: 0.015944, loss_dice: 0.407366
[08:38:56.066] TRAIN: iteration 5509 : loss : 0.133483, loss_ce: 0.015169, loss_dice: 0.251797
[08:38:56.272] TRAIN: iteration 5510 : loss : 0.069730, loss_ce: 0.003342, loss_dice: 0.136118
[08:38:56.480] TRAIN: iteration 5511 : loss : 0.149626, loss_ce: 0.005536, loss_dice: 0.293717
[08:38:56.687] TRAIN: iteration 5512 : loss : 0.179437, loss_ce: 0.015051, loss_dice: 0.343823
[08:38:56.897] TRAIN: iteration 5513 : loss : 0.207364, loss_ce: 0.006693, loss_dice: 0.408035
[08:38:57.105] TRAIN: iteration 5514 : loss : 0.130503, loss_ce: 0.005202, loss_dice: 0.255804
[08:38:57.312] TRAIN: iteration 5515 : loss : 0.171325, loss_ce: 0.014455, loss_dice: 0.328195
[08:38:57.524] TRAIN: iteration 5516 : loss : 0.134240, loss_ce: 0.005692, loss_dice: 0.262788
[08:38:58.631] TRAIN: iteration 5517 : loss : 0.235515, loss_ce: 0.005890, loss_dice: 0.465140
[08:38:58.838] TRAIN: iteration 5518 : loss : 0.227524, loss_ce: 0.006251, loss_dice: 0.448798
[08:38:59.046] TRAIN: iteration 5519 : loss : 0.189871, loss_ce: 0.005293, loss_dice: 0.374449
[08:38:59.258] TRAIN: iteration 5520 : loss : 0.252713, loss_ce: 0.005164, loss_dice: 0.500262
[08:38:59.497] TRAIN: iteration 5521 : loss : 0.251835, loss_ce: 0.006372, loss_dice: 0.497299
[08:38:59.704] TRAIN: iteration 5522 : loss : 0.168752, loss_ce: 0.017628, loss_dice: 0.319875
[08:38:59.911] TRAIN: iteration 5523 : loss : 0.141682, loss_ce: 0.005160, loss_dice: 0.278204
[08:39:00.120] TRAIN: iteration 5524 : loss : 0.220733, loss_ce: 0.008752, loss_dice: 0.432714
[08:39:01.006] TRAIN: iteration 5525 : loss : 0.145229, loss_ce: 0.004961, loss_dice: 0.285497
[08:39:01.226] TRAIN: iteration 5526 : loss : 0.220458, loss_ce: 0.005547, loss_dice: 0.435369
[08:39:01.438] TRAIN: iteration 5527 : loss : 0.061475, loss_ce: 0.004110, loss_dice: 0.118840
[08:39:01.649] TRAIN: iteration 5528 : loss : 0.252780, loss_ce: 0.005239, loss_dice: 0.500320
[08:39:01.856] TRAIN: iteration 5529 : loss : 0.211377, loss_ce: 0.005210, loss_dice: 0.417544
[08:39:02.064] TRAIN: iteration 5530 : loss : 0.199251, loss_ce: 0.007754, loss_dice: 0.390747
[08:39:02.272] TRAIN: iteration 5531 : loss : 0.235847, loss_ce: 0.004909, loss_dice: 0.466786
[08:39:02.480] TRAIN: iteration 5532 : loss : 0.215635, loss_ce: 0.004005, loss_dice: 0.427265
[08:39:02.686] TRAIN: iteration 5533 : loss : 0.173184, loss_ce: 0.004590, loss_dice: 0.341778
[08:39:02.895] TRAIN: iteration 5534 : loss : 0.148901, loss_ce: 0.007693, loss_dice: 0.290108
[08:39:03.102] TRAIN: iteration 5535 : loss : 0.202069, loss_ce: 0.005460, loss_dice: 0.398677
[08:39:03.310] TRAIN: iteration 5536 : loss : 0.235422, loss_ce: 0.004671, loss_dice: 0.466172
[08:39:03.516] TRAIN: iteration 5537 : loss : 0.168843, loss_ce: 0.004838, loss_dice: 0.332848
[08:39:03.728] TRAIN: iteration 5538 : loss : 0.130731, loss_ce: 0.006620, loss_dice: 0.254841
[08:39:03.943] TRAIN: iteration 5539 : loss : 0.162103, loss_ce: 0.015142, loss_dice: 0.309064
[08:39:04.154] TRAIN: iteration 5540 : loss : 0.090474, loss_ce: 0.005706, loss_dice: 0.175242
[08:39:04.392] TRAIN: iteration 5541 : loss : 0.077650, loss_ce: 0.003453, loss_dice: 0.151848
[08:39:04.598] TRAIN: iteration 5542 : loss : 0.147567, loss_ce: 0.004559, loss_dice: 0.290575
[08:39:04.805] TRAIN: iteration 5543 : loss : 0.203626, loss_ce: 0.011187, loss_dice: 0.396066
[08:39:05.012] TRAIN: iteration 5544 : loss : 0.252654, loss_ce: 0.005001, loss_dice: 0.500308
[08:39:05.219] TRAIN: iteration 5545 : loss : 0.198058, loss_ce: 0.007147, loss_dice: 0.388970
[08:39:05.428] TRAIN: iteration 5546 : loss : 0.252849, loss_ce: 0.005384, loss_dice: 0.500314
[08:39:05.648] TRAIN: iteration 5547 : loss : 0.137053, loss_ce: 0.006638, loss_dice: 0.267468
[08:39:05.855] TRAIN: iteration 5548 : loss : 0.201593, loss_ce: 0.043806, loss_dice: 0.359380
[08:39:06.063] TRAIN: iteration 5549 : loss : 0.253704, loss_ce: 0.006881, loss_dice: 0.500527
[08:39:06.272] TRAIN: iteration 5550 : loss : 0.197083, loss_ce: 0.006388, loss_dice: 0.387778
[08:39:06.483] TRAIN: iteration 5551 : loss : 0.105809, loss_ce: 0.010471, loss_dice: 0.201147
[08:39:06.691] TRAIN: iteration 5552 : loss : 0.126288, loss_ce: 0.005435, loss_dice: 0.247140
[08:39:06.917] TRAIN: iteration 5553 : loss : 0.159561, loss_ce: 0.006326, loss_dice: 0.312796
[08:39:07.125] TRAIN: iteration 5554 : loss : 0.198040, loss_ce: 0.006031, loss_dice: 0.390049
[08:39:07.332] TRAIN: iteration 5555 : loss : 0.109161, loss_ce: 0.006591, loss_dice: 0.211731
[08:39:07.541] TRAIN: iteration 5556 : loss : 0.101643, loss_ce: 0.004611, loss_dice: 0.198676
[08:39:07.752] TRAIN: iteration 5557 : loss : 0.143210, loss_ce: 0.005728, loss_dice: 0.280693
[08:39:07.958] TRAIN: iteration 5558 : loss : 0.127162, loss_ce: 0.006947, loss_dice: 0.247378
[08:39:08.167] TRAIN: iteration 5559 : loss : 0.112021, loss_ce: 0.004386, loss_dice: 0.219656
[08:39:08.400] TRAIN: iteration 5560 : loss : 0.161732, loss_ce: 0.030080, loss_dice: 0.293384
[08:39:08.640] TRAIN: iteration 5561 : loss : 0.251466, loss_ce: 0.006239, loss_dice: 0.496694
[08:39:08.848] TRAIN: iteration 5562 : loss : 0.133543, loss_ce: 0.013822, loss_dice: 0.253264
[08:39:09.057] TRAIN: iteration 5563 : loss : 0.094977, loss_ce: 0.004885, loss_dice: 0.185069
[08:39:09.268] TRAIN: iteration 5564 : loss : 0.117612, loss_ce: 0.005131, loss_dice: 0.230094
[08:39:09.479] TRAIN: iteration 5565 : loss : 0.126619, loss_ce: 0.010166, loss_dice: 0.243071
[08:39:09.688] TRAIN: iteration 5566 : loss : 0.090992, loss_ce: 0.003074, loss_dice: 0.178909
[08:39:09.895] TRAIN: iteration 5567 : loss : 0.169068, loss_ce: 0.025914, loss_dice: 0.312223
[08:39:10.102] TRAIN: iteration 5568 : loss : 0.170189, loss_ce: 0.005123, loss_dice: 0.335255
[08:39:10.309] TRAIN: iteration 5569 : loss : 0.136308, loss_ce: 0.021155, loss_dice: 0.251461
[08:39:10.531] TRAIN: iteration 5570 : loss : 0.252178, loss_ce: 0.004124, loss_dice: 0.500231
[08:39:10.738] TRAIN: iteration 5571 : loss : 0.108031, loss_ce: 0.010166, loss_dice: 0.205897
[08:39:10.950] TRAIN: iteration 5572 : loss : 0.209187, loss_ce: 0.005034, loss_dice: 0.413340
[08:39:11.157] TRAIN: iteration 5573 : loss : 0.182612, loss_ce: 0.006952, loss_dice: 0.358271
[08:39:11.365] TRAIN: iteration 5574 : loss : 0.182602, loss_ce: 0.011368, loss_dice: 0.353836
[08:39:11.573] TRAIN: iteration 5575 : loss : 0.137689, loss_ce: 0.006629, loss_dice: 0.268749
[08:39:11.780] TRAIN: iteration 5576 : loss : 0.081072, loss_ce: 0.008460, loss_dice: 0.153684
[08:39:11.987] TRAIN: iteration 5577 : loss : 0.239188, loss_ce: 0.008386, loss_dice: 0.469989
[08:39:12.195] TRAIN: iteration 5578 : loss : 0.191345, loss_ce: 0.006244, loss_dice: 0.376446
[08:39:12.403] TRAIN: iteration 5579 : loss : 0.246425, loss_ce: 0.008572, loss_dice: 0.484278
[08:39:12.618] TRAIN: iteration 5580 : loss : 0.206867, loss_ce: 0.007573, loss_dice: 0.406162
[08:39:12.854] TRAIN: iteration 5581 : loss : 0.222435, loss_ce: 0.009654, loss_dice: 0.435216
[08:39:13.062] TRAIN: iteration 5582 : loss : 0.253737, loss_ce: 0.008427, loss_dice: 0.499046
[08:39:13.270] TRAIN: iteration 5583 : loss : 0.208247, loss_ce: 0.008970, loss_dice: 0.407525
[08:39:13.478] TRAIN: iteration 5584 : loss : 0.254479, loss_ce: 0.008315, loss_dice: 0.500642
[08:39:13.693] TRAIN: iteration 5585 : loss : 0.218824, loss_ce: 0.008354, loss_dice: 0.429294
[08:39:13.911] TRAIN: iteration 5586 : loss : 0.201927, loss_ce: 0.008322, loss_dice: 0.395532
[08:39:14.122] TRAIN: iteration 5587 : loss : 0.214312, loss_ce: 0.007807, loss_dice: 0.420818
[08:39:14.339] TRAIN: iteration 5588 : loss : 0.229808, loss_ce: 0.007483, loss_dice: 0.452133
[08:39:14.555] TRAIN: iteration 5589 : loss : 0.089842, loss_ce: 0.009949, loss_dice: 0.169735
[08:39:14.764] TRAIN: iteration 5590 : loss : 0.198116, loss_ce: 0.006705, loss_dice: 0.389527
[08:39:14.972] TRAIN: iteration 5591 : loss : 0.192246, loss_ce: 0.010289, loss_dice: 0.374202
[08:39:15.179] TRAIN: iteration 5592 : loss : 0.165711, loss_ce: 0.006139, loss_dice: 0.325284
[08:39:15.390] TRAIN: iteration 5593 : loss : 0.165501, loss_ce: 0.005979, loss_dice: 0.325024
[08:39:15.857] TRAIN: iteration 5594 : loss : 0.170210, loss_ce: 0.005460, loss_dice: 0.334961
[08:39:16.065] TRAIN: iteration 5595 : loss : 0.179164, loss_ce: 0.036508, loss_dice: 0.321820
[08:39:16.274] TRAIN: iteration 5596 : loss : 0.112868, loss_ce: 0.012306, loss_dice: 0.213430
[08:39:16.485] TRAIN: iteration 5597 : loss : 0.152061, loss_ce: 0.006879, loss_dice: 0.297243
[08:39:16.917] TRAIN: iteration 5598 : loss : 0.203294, loss_ce: 0.007953, loss_dice: 0.398634
[08:39:17.127] TRAIN: iteration 5599 : loss : 0.251556, loss_ce: 0.005695, loss_dice: 0.497416
[08:39:17.339] TRAIN: iteration 5600 : loss : 0.121741, loss_ce: 0.004955, loss_dice: 0.238527
[08:39:17.340] NaN or Inf found in input tensor.
[08:39:17.560] TRAIN: iteration 5601 : loss : 0.191624, loss_ce: 0.005732, loss_dice: 0.377516
[08:39:17.768] TRAIN: iteration 5602 : loss : 0.252634, loss_ce: 0.004971, loss_dice: 0.500296
[08:39:17.976] TRAIN: iteration 5603 : loss : 0.145808, loss_ce: 0.006946, loss_dice: 0.284671
[08:39:18.183] TRAIN: iteration 5604 : loss : 0.186273, loss_ce: 0.006744, loss_dice: 0.365802
[08:39:18.390] TRAIN: iteration 5605 : loss : 0.196358, loss_ce: 0.011685, loss_dice: 0.381031
[08:39:18.599] TRAIN: iteration 5606 : loss : 0.143905, loss_ce: 0.012396, loss_dice: 0.275413
[08:39:18.823] TRAIN: iteration 5607 : loss : 0.152714, loss_ce: 0.005663, loss_dice: 0.299765
[08:39:19.033] TRAIN: iteration 5608 : loss : 0.252634, loss_ce: 0.004992, loss_dice: 0.500277
[08:39:19.240] TRAIN: iteration 5609 : loss : 0.085702, loss_ce: 0.006422, loss_dice: 0.164983
[08:39:19.452] TRAIN: iteration 5610 : loss : 0.165279, loss_ce: 0.017220, loss_dice: 0.313338
[08:39:19.660] TRAIN: iteration 5611 : loss : 0.252548, loss_ce: 0.004840, loss_dice: 0.500257
[08:39:19.872] TRAIN: iteration 5612 : loss : 0.207332, loss_ce: 0.006148, loss_dice: 0.408516
[08:39:20.082] TRAIN: iteration 5613 : loss : 0.062522, loss_ce: 0.003343, loss_dice: 0.121701
[08:39:20.294] TRAIN: iteration 5614 : loss : 0.227191, loss_ce: 0.005164, loss_dice: 0.449218
[08:39:20.505] TRAIN: iteration 5615 : loss : 0.155940, loss_ce: 0.006891, loss_dice: 0.304990
[08:39:20.714] TRAIN: iteration 5616 : loss : 0.138333, loss_ce: 0.006823, loss_dice: 0.269842
[08:39:20.923] TRAIN: iteration 5617 : loss : 0.150193, loss_ce: 0.008695, loss_dice: 0.291692
[08:39:21.132] TRAIN: iteration 5618 : loss : 0.143362, loss_ce: 0.008151, loss_dice: 0.278574
[08:39:21.339] TRAIN: iteration 5619 : loss : 0.252454, loss_ce: 0.004597, loss_dice: 0.500310
[08:39:21.545] TRAIN: iteration 5620 : loss : 0.218515, loss_ce: 0.004742, loss_dice: 0.432288
[08:39:21.783] TRAIN: iteration 5621 : loss : 0.252559, loss_ce: 0.004829, loss_dice: 0.500288
[08:39:21.993] TRAIN: iteration 5622 : loss : 0.123372, loss_ce: 0.006669, loss_dice: 0.240074
[08:39:22.201] TRAIN: iteration 5623 : loss : 0.103635, loss_ce: 0.007733, loss_dice: 0.199537
[08:39:22.409] TRAIN: iteration 5624 : loss : 0.156586, loss_ce: 0.004989, loss_dice: 0.308183
[08:39:22.617] TRAIN: iteration 5625 : loss : 0.163766, loss_ce: 0.004632, loss_dice: 0.322900
[08:39:22.827] TRAIN: iteration 5626 : loss : 0.252161, loss_ce: 0.004092, loss_dice: 0.500231
[08:39:23.037] TRAIN: iteration 5627 : loss : 0.115754, loss_ce: 0.005124, loss_dice: 0.226385
[08:39:23.248] TRAIN: iteration 5628 : loss : 0.247562, loss_ce: 0.004364, loss_dice: 0.490761
[08:39:23.459] TRAIN: iteration 5629 : loss : 0.244298, loss_ce: 0.004504, loss_dice: 0.484093
[08:39:23.669] TRAIN: iteration 5630 : loss : 0.166133, loss_ce: 0.010452, loss_dice: 0.321814
[08:39:23.881] TRAIN: iteration 5631 : loss : 0.176906, loss_ce: 0.004996, loss_dice: 0.348815
[08:39:24.092] TRAIN: iteration 5632 : loss : 0.207619, loss_ce: 0.007281, loss_dice: 0.407957
[08:39:24.300] TRAIN: iteration 5633 : loss : 0.098845, loss_ce: 0.003779, loss_dice: 0.193910
[08:39:24.509] TRAIN: iteration 5634 : loss : 0.209181, loss_ce: 0.040194, loss_dice: 0.378167
[08:39:24.718] TRAIN: iteration 5635 : loss : 0.229093, loss_ce: 0.004346, loss_dice: 0.453840
[08:39:24.928] TRAIN: iteration 5636 : loss : 0.059031, loss_ce: 0.004527, loss_dice: 0.113536
[08:39:25.138] TRAIN: iteration 5637 : loss : 0.171454, loss_ce: 0.005525, loss_dice: 0.337383
[08:39:25.349] TRAIN: iteration 5638 : loss : 0.068472, loss_ce: 0.002802, loss_dice: 0.134143
[08:39:25.564] TRAIN: iteration 5639 : loss : 0.156479, loss_ce: 0.004677, loss_dice: 0.308282
[08:39:25.772] TRAIN: iteration 5640 : loss : 0.149515, loss_ce: 0.005029, loss_dice: 0.294001
[08:39:26.017] TRAIN: iteration 5641 : loss : 0.183423, loss_ce: 0.005759, loss_dice: 0.361086
[08:39:26.226] TRAIN: iteration 5642 : loss : 0.230625, loss_ce: 0.008344, loss_dice: 0.452907
[08:39:26.446] TRAIN: iteration 5643 : loss : 0.163710, loss_ce: 0.004709, loss_dice: 0.322710
[08:39:26.655] TRAIN: iteration 5644 : loss : 0.248360, loss_ce: 0.003766, loss_dice: 0.492953
[08:39:28.335] TRAIN: iteration 5645 : loss : 0.071072, loss_ce: 0.004941, loss_dice: 0.137202
[08:39:28.543] TRAIN: iteration 5646 : loss : 0.190715, loss_ce: 0.004943, loss_dice: 0.376487
[08:39:28.752] TRAIN: iteration 5647 : loss : 0.080075, loss_ce: 0.004142, loss_dice: 0.156009
[08:39:28.962] TRAIN: iteration 5648 : loss : 0.246776, loss_ce: 0.004418, loss_dice: 0.489135
[08:39:29.173] TRAIN: iteration 5649 : loss : 0.061837, loss_ce: 0.003575, loss_dice: 0.120100
[08:39:29.381] TRAIN: iteration 5650 : loss : 0.158632, loss_ce: 0.008113, loss_dice: 0.309150
[08:39:29.588] TRAIN: iteration 5651 : loss : 0.107931, loss_ce: 0.010704, loss_dice: 0.205159
[08:39:29.800] TRAIN: iteration 5652 : loss : 0.236267, loss_ce: 0.004714, loss_dice: 0.467820
[08:39:31.094] TRAIN: iteration 5653 : loss : 0.103596, loss_ce: 0.006323, loss_dice: 0.200869
[08:39:31.301] TRAIN: iteration 5654 : loss : 0.201831, loss_ce: 0.012875, loss_dice: 0.390787
[08:39:31.510] TRAIN: iteration 5655 : loss : 0.115470, loss_ce: 0.005939, loss_dice: 0.225002
[08:39:31.717] TRAIN: iteration 5656 : loss : 0.100566, loss_ce: 0.007662, loss_dice: 0.193471
[08:39:31.928] TRAIN: iteration 5657 : loss : 0.162087, loss_ce: 0.008607, loss_dice: 0.315567
[08:39:32.140] TRAIN: iteration 5658 : loss : 0.176902, loss_ce: 0.008880, loss_dice: 0.344925
[08:39:32.349] TRAIN: iteration 5659 : loss : 0.081321, loss_ce: 0.007441, loss_dice: 0.155202
[08:39:32.557] TRAIN: iteration 5660 : loss : 0.212538, loss_ce: 0.006030, loss_dice: 0.419045
[08:39:32.791] TRAIN: iteration 5661 : loss : 0.204201, loss_ce: 0.006906, loss_dice: 0.401496
[08:39:32.998] TRAIN: iteration 5662 : loss : 0.213091, loss_ce: 0.016234, loss_dice: 0.409949
[08:39:33.206] TRAIN: iteration 5663 : loss : 0.150746, loss_ce: 0.007191, loss_dice: 0.294301
[08:39:33.420] TRAIN: iteration 5664 : loss : 0.252627, loss_ce: 0.004933, loss_dice: 0.500322
[08:39:33.630] TRAIN: iteration 5665 : loss : 0.204681, loss_ce: 0.011401, loss_dice: 0.397961
[08:39:33.837] TRAIN: iteration 5666 : loss : 0.176246, loss_ce: 0.009015, loss_dice: 0.343476
[08:39:34.044] TRAIN: iteration 5667 : loss : 0.044793, loss_ce: 0.003368, loss_dice: 0.086218
[08:39:34.252] TRAIN: iteration 5668 : loss : 0.124182, loss_ce: 0.008820, loss_dice: 0.239543
[08:39:34.490] TRAIN: iteration 5669 : loss : 0.108967, loss_ce: 0.005062, loss_dice: 0.212872
[08:39:34.698] TRAIN: iteration 5670 : loss : 0.244065, loss_ce: 0.006357, loss_dice: 0.481773
[08:39:34.904] TRAIN: iteration 5671 : loss : 0.124673, loss_ce: 0.004949, loss_dice: 0.244397
[08:39:35.114] TRAIN: iteration 5672 : loss : 0.164760, loss_ce: 0.009262, loss_dice: 0.320259
[08:39:35.327] TRAIN: iteration 5673 : loss : 0.112975, loss_ce: 0.005123, loss_dice: 0.220827
[08:39:35.542] TRAIN: iteration 5674 : loss : 0.194678, loss_ce: 0.017500, loss_dice: 0.371856
[08:39:35.751] TRAIN: iteration 5675 : loss : 0.172022, loss_ce: 0.005389, loss_dice: 0.338656
[08:39:35.960] TRAIN: iteration 5676 : loss : 0.252842, loss_ce: 0.005360, loss_dice: 0.500325
[08:39:36.173] TRAIN: iteration 5677 : loss : 0.243174, loss_ce: 0.005517, loss_dice: 0.480832
[08:39:36.381] TRAIN: iteration 5678 : loss : 0.174682, loss_ce: 0.005139, loss_dice: 0.344225
[08:39:36.590] TRAIN: iteration 5679 : loss : 0.117805, loss_ce: 0.009336, loss_dice: 0.226275
[08:39:36.801] TRAIN: iteration 5680 : loss : 0.140486, loss_ce: 0.005286, loss_dice: 0.275686
[08:39:37.048] TRAIN: iteration 5681 : loss : 0.231183, loss_ce: 0.005287, loss_dice: 0.457078
[08:39:37.258] TRAIN: iteration 5682 : loss : 0.213539, loss_ce: 0.004128, loss_dice: 0.422949
[08:39:37.466] TRAIN: iteration 5683 : loss : 0.121449, loss_ce: 0.003517, loss_dice: 0.239381
[08:39:37.673] TRAIN: iteration 5684 : loss : 0.246358, loss_ce: 0.020100, loss_dice: 0.472617
[08:39:37.883] TRAIN: iteration 5685 : loss : 0.175517, loss_ce: 0.003758, loss_dice: 0.347277
[08:39:38.090] TRAIN: iteration 5686 : loss : 0.107968, loss_ce: 0.003704, loss_dice: 0.212232
[08:39:38.297] TRAIN: iteration 5687 : loss : 0.112111, loss_ce: 0.009226, loss_dice: 0.214996
[08:39:38.505] TRAIN: iteration 5688 : loss : 0.120598, loss_ce: 0.005701, loss_dice: 0.235495
[08:39:39.322] TRAIN: iteration 5689 : loss : 0.250807, loss_ce: 0.004386, loss_dice: 0.497228
[08:39:39.529] TRAIN: iteration 5690 : loss : 0.167318, loss_ce: 0.014234, loss_dice: 0.320403
[08:39:39.736] TRAIN: iteration 5691 : loss : 0.098304, loss_ce: 0.002491, loss_dice: 0.194118
[08:39:39.944] TRAIN: iteration 5692 : loss : 0.146283, loss_ce: 0.018126, loss_dice: 0.274441
[08:39:40.152] TRAIN: iteration 5693 : loss : 0.152177, loss_ce: 0.008277, loss_dice: 0.296078
[08:39:40.359] TRAIN: iteration 5694 : loss : 0.150604, loss_ce: 0.002599, loss_dice: 0.298609
[08:39:40.566] TRAIN: iteration 5695 : loss : 0.202973, loss_ce: 0.043071, loss_dice: 0.362875
[08:39:40.782] TRAIN: iteration 5696 : loss : 0.088687, loss_ce: 0.004714, loss_dice: 0.172660
[08:39:40.994] TRAIN: iteration 5697 : loss : 0.172412, loss_ce: 0.003165, loss_dice: 0.341660
[08:39:41.204] TRAIN: iteration 5698 : loss : 0.121697, loss_ce: 0.007400, loss_dice: 0.235994
[08:39:41.412] TRAIN: iteration 5699 : loss : 0.068697, loss_ce: 0.003266, loss_dice: 0.134128
[08:39:41.620] TRAIN: iteration 5700 : loss : 0.250981, loss_ce: 0.003917, loss_dice: 0.498045
[08:39:41.871] TRAIN: iteration 5701 : loss : 0.175226, loss_ce: 0.004151, loss_dice: 0.346302
[08:39:42.078] TRAIN: iteration 5702 : loss : 0.155264, loss_ce: 0.010625, loss_dice: 0.299903
[08:39:42.286] TRAIN: iteration 5703 : loss : 0.247568, loss_ce: 0.002940, loss_dice: 0.492196
[08:39:42.493] TRAIN: iteration 5704 : loss : 0.134770, loss_ce: 0.004154, loss_dice: 0.265387
[08:39:42.702] TRAIN: iteration 5705 : loss : 0.251634, loss_ce: 0.003144, loss_dice: 0.500124
[08:39:42.909] TRAIN: iteration 5706 : loss : 0.136994, loss_ce: 0.005815, loss_dice: 0.268173
[08:39:43.117] TRAIN: iteration 5707 : loss : 0.236863, loss_ce: 0.004848, loss_dice: 0.468878
[08:39:43.326] TRAIN: iteration 5708 : loss : 0.233434, loss_ce: 0.008897, loss_dice: 0.457972
[08:39:43.539] TRAIN: iteration 5709 : loss : 0.078405, loss_ce: 0.003706, loss_dice: 0.153105
[08:39:43.746] TRAIN: iteration 5710 : loss : 0.223539, loss_ce: 0.003872, loss_dice: 0.443207
[08:39:43.952] TRAIN: iteration 5711 : loss : 0.166759, loss_ce: 0.005503, loss_dice: 0.328015
[08:39:44.159] TRAIN: iteration 5712 : loss : 0.156005, loss_ce: 0.009239, loss_dice: 0.302771
[08:39:44.367] TRAIN: iteration 5713 : loss : 0.110037, loss_ce: 0.004781, loss_dice: 0.215294
[08:39:44.574] TRAIN: iteration 5714 : loss : 0.226565, loss_ce: 0.004663, loss_dice: 0.448467
[08:39:44.781] TRAIN: iteration 5715 : loss : 0.096835, loss_ce: 0.004236, loss_dice: 0.189434
[08:39:44.989] TRAIN: iteration 5716 : loss : 0.146712, loss_ce: 0.009074, loss_dice: 0.284349
[08:39:45.197] TRAIN: iteration 5717 : loss : 0.252599, loss_ce: 0.004907, loss_dice: 0.500290
[08:39:45.404] TRAIN: iteration 5718 : loss : 0.252635, loss_ce: 0.004983, loss_dice: 0.500287
[08:39:45.619] TRAIN: iteration 5719 : loss : 0.132243, loss_ce: 0.005900, loss_dice: 0.258586
[08:39:45.828] TRAIN: iteration 5720 : loss : 0.193909, loss_ce: 0.003615, loss_dice: 0.384203
[08:39:46.068] TRAIN: iteration 5721 : loss : 0.228685, loss_ce: 0.006214, loss_dice: 0.451155
[08:39:46.278] TRAIN: iteration 5722 : loss : 0.214523, loss_ce: 0.006516, loss_dice: 0.422530
[08:39:46.495] TRAIN: iteration 5723 : loss : 0.238819, loss_ce: 0.004166, loss_dice: 0.473473
[08:39:46.708] TRAIN: iteration 5724 : loss : 0.106158, loss_ce: 0.005993, loss_dice: 0.206323
[08:39:46.916] TRAIN: iteration 5725 : loss : 0.247145, loss_ce: 0.006643, loss_dice: 0.487646
[08:39:47.130] TRAIN: iteration 5726 : loss : 0.100490, loss_ce: 0.004370, loss_dice: 0.196609
[08:39:47.337] TRAIN: iteration 5727 : loss : 0.174391, loss_ce: 0.004870, loss_dice: 0.343913
[08:39:47.544] TRAIN: iteration 5728 : loss : 0.183168, loss_ce: 0.008935, loss_dice: 0.357401
[08:39:47.751] TRAIN: iteration 5729 : loss : 0.119456, loss_ce: 0.004923, loss_dice: 0.233988
[08:39:47.959] TRAIN: iteration 5730 : loss : 0.121806, loss_ce: 0.007710, loss_dice: 0.235903
[08:39:48.173] TRAIN: iteration 5731 : loss : 0.129098, loss_ce: 0.005632, loss_dice: 0.252564
[08:39:48.381] TRAIN: iteration 5732 : loss : 0.149232, loss_ce: 0.020499, loss_dice: 0.277966
[08:39:48.591] TRAIN: iteration 5733 : loss : 0.097312, loss_ce: 0.007214, loss_dice: 0.187409
[08:39:48.805] TRAIN: iteration 5734 : loss : 0.161482, loss_ce: 0.004773, loss_dice: 0.318191
[08:39:49.014] TRAIN: iteration 5735 : loss : 0.074056, loss_ce: 0.004979, loss_dice: 0.143132
[08:39:49.223] TRAIN: iteration 5736 : loss : 0.252405, loss_ce: 0.004537, loss_dice: 0.500273
[08:39:49.430] TRAIN: iteration 5737 : loss : 0.085283, loss_ce: 0.003618, loss_dice: 0.166949
[08:39:49.638] TRAIN: iteration 5738 : loss : 0.248270, loss_ce: 0.006197, loss_dice: 0.490343
[08:39:49.854] TRAIN: iteration 5739 : loss : 0.253491, loss_ce: 0.006456, loss_dice: 0.500525
[08:39:50.061] TRAIN: iteration 5740 : loss : 0.085502, loss_ce: 0.004740, loss_dice: 0.166265
[08:39:50.299] TRAIN: iteration 5741 : loss : 0.253933, loss_ce: 0.009720, loss_dice: 0.498145
[08:39:50.507] TRAIN: iteration 5742 : loss : 0.072549, loss_ce: 0.008081, loss_dice: 0.137018
[08:39:50.717] TRAIN: iteration 5743 : loss : 0.088261, loss_ce: 0.003767, loss_dice: 0.172755
[08:39:50.927] TRAIN: iteration 5744 : loss : 0.170615, loss_ce: 0.003992, loss_dice: 0.337238
[08:39:51.274] TRAIN: iteration 5745 : loss : 0.170295, loss_ce: 0.014686, loss_dice: 0.325905
[08:39:51.482] TRAIN: iteration 5746 : loss : 0.103426, loss_ce: 0.003322, loss_dice: 0.203531
[08:39:51.689] TRAIN: iteration 5747 : loss : 0.252627, loss_ce: 0.004854, loss_dice: 0.500401
[08:39:51.896] TRAIN: iteration 5748 : loss : 0.241986, loss_ce: 0.004720, loss_dice: 0.479253
[08:39:52.110] TRAIN: iteration 5749 : loss : 0.179961, loss_ce: 0.005196, loss_dice: 0.354726
[08:39:52.495] TRAIN: iteration 5750 : loss : 0.136852, loss_ce: 0.012948, loss_dice: 0.260756
[08:39:52.703] TRAIN: iteration 5751 : loss : 0.155827, loss_ce: 0.011738, loss_dice: 0.299915
[08:39:52.913] TRAIN: iteration 5752 : loss : 0.211354, loss_ce: 0.006363, loss_dice: 0.416345
[08:39:53.737] TRAIN: iteration 5753 : loss : 0.232923, loss_ce: 0.004096, loss_dice: 0.461750
[08:39:53.945] TRAIN: iteration 5754 : loss : 0.157993, loss_ce: 0.004756, loss_dice: 0.311231
[08:39:54.159] TRAIN: iteration 5755 : loss : 0.110593, loss_ce: 0.006300, loss_dice: 0.214885
[08:39:54.369] TRAIN: iteration 5756 : loss : 0.149557, loss_ce: 0.007920, loss_dice: 0.291193
[08:39:54.578] TRAIN: iteration 5757 : loss : 0.243278, loss_ce: 0.004325, loss_dice: 0.482232
[08:39:54.787] TRAIN: iteration 5758 : loss : 0.251998, loss_ce: 0.003708, loss_dice: 0.500288
[08:39:54.998] TRAIN: iteration 5759 : loss : 0.252424, loss_ce: 0.016114, loss_dice: 0.488734
[08:39:55.206] TRAIN: iteration 5760 : loss : 0.082978, loss_ce: 0.006577, loss_dice: 0.159379
[08:39:55.582] TRAIN: iteration 5761 : loss : 0.244245, loss_ce: 0.012355, loss_dice: 0.476134
[08:39:55.790] TRAIN: iteration 5762 : loss : 0.172607, loss_ce: 0.002943, loss_dice: 0.342271
[08:39:56.000] TRAIN: iteration 5763 : loss : 0.244857, loss_ce: 0.002638, loss_dice: 0.487077
[08:39:56.209] TRAIN: iteration 5764 : loss : 0.068209, loss_ce: 0.005066, loss_dice: 0.131351
[08:39:56.416] TRAIN: iteration 5765 : loss : 0.240679, loss_ce: 0.009170, loss_dice: 0.472188
[08:39:56.623] TRAIN: iteration 5766 : loss : 0.251776, loss_ce: 0.003321, loss_dice: 0.500231
[08:39:56.830] TRAIN: iteration 5767 : loss : 0.248297, loss_ce: 0.047789, loss_dice: 0.448804
[08:39:57.046] TRAIN: iteration 5768 : loss : 0.186597, loss_ce: 0.003050, loss_dice: 0.370144
[08:39:57.292] TRAIN: iteration 5769 : loss : 0.081970, loss_ce: 0.006293, loss_dice: 0.157646
[08:39:57.509] TRAIN: iteration 5770 : loss : 0.085234, loss_ce: 0.006608, loss_dice: 0.163860
[08:39:57.716] TRAIN: iteration 5771 : loss : 0.227250, loss_ce: 0.017415, loss_dice: 0.437084
[08:39:57.924] TRAIN: iteration 5772 : loss : 0.115614, loss_ce: 0.011024, loss_dice: 0.220204
[08:39:58.132] TRAIN: iteration 5773 : loss : 0.137841, loss_ce: 0.016795, loss_dice: 0.258888
[08:39:58.347] TRAIN: iteration 5774 : loss : 0.151787, loss_ce: 0.006021, loss_dice: 0.297554
[08:39:58.556] TRAIN: iteration 5775 : loss : 0.192670, loss_ce: 0.008251, loss_dice: 0.377089
[08:39:58.764] TRAIN: iteration 5776 : loss : 0.116625, loss_ce: 0.008273, loss_dice: 0.224978
[08:39:58.974] TRAIN: iteration 5777 : loss : 0.174159, loss_ce: 0.007013, loss_dice: 0.341305
[08:39:59.184] TRAIN: iteration 5778 : loss : 0.073484, loss_ce: 0.006750, loss_dice: 0.140218
[08:39:59.391] TRAIN: iteration 5779 : loss : 0.121099, loss_ce: 0.007255, loss_dice: 0.234942
[08:39:59.599] TRAIN: iteration 5780 : loss : 0.112048, loss_ce: 0.006793, loss_dice: 0.217304
[08:39:59.848] TRAIN: iteration 5781 : loss : 0.253618, loss_ce: 0.006841, loss_dice: 0.500395
[08:40:00.055] TRAIN: iteration 5782 : loss : 0.194306, loss_ce: 0.013268, loss_dice: 0.375343
[08:40:00.264] TRAIN: iteration 5783 : loss : 0.072630, loss_ce: 0.006799, loss_dice: 0.138461
[08:40:00.471] TRAIN: iteration 5784 : loss : 0.167140, loss_ce: 0.020905, loss_dice: 0.313376
[08:40:00.678] TRAIN: iteration 5785 : loss : 0.068953, loss_ce: 0.005241, loss_dice: 0.132665
[08:40:00.887] TRAIN: iteration 5786 : loss : 0.119678, loss_ce: 0.007634, loss_dice: 0.231722
[08:40:01.096] TRAIN: iteration 5787 : loss : 0.220171, loss_ce: 0.005806, loss_dice: 0.434537
[08:40:01.304] TRAIN: iteration 5788 : loss : 0.107313, loss_ce: 0.007302, loss_dice: 0.207323
[08:40:01.970] TRAIN: iteration 5789 : loss : 0.192955, loss_ce: 0.006054, loss_dice: 0.379855
[08:40:02.177] TRAIN: iteration 5790 : loss : 0.252758, loss_ce: 0.005232, loss_dice: 0.500284
[08:40:02.384] TRAIN: iteration 5791 : loss : 0.078047, loss_ce: 0.004642, loss_dice: 0.151453
[08:40:02.593] TRAIN: iteration 5792 : loss : 0.251618, loss_ce: 0.006914, loss_dice: 0.496322
[08:40:02.813] TRAIN: iteration 5793 : loss : 0.122973, loss_ce: 0.004241, loss_dice: 0.241706
[08:40:03.023] TRAIN: iteration 5794 : loss : 0.188207, loss_ce: 0.005762, loss_dice: 0.370652
[08:40:03.231] TRAIN: iteration 5795 : loss : 0.198402, loss_ce: 0.008433, loss_dice: 0.388370
[08:40:03.441] TRAIN: iteration 5796 : loss : 0.092850, loss_ce: 0.006279, loss_dice: 0.179421
[08:40:03.652] TRAIN: iteration 5797 : loss : 0.128981, loss_ce: 0.004410, loss_dice: 0.253552
[08:40:03.859] TRAIN: iteration 5798 : loss : 0.183207, loss_ce: 0.013205, loss_dice: 0.353209
[08:40:04.066] TRAIN: iteration 5799 : loss : 0.146218, loss_ce: 0.010357, loss_dice: 0.282080
[08:40:04.274] TRAIN: iteration 5800 : loss : 0.154990, loss_ce: 0.004680, loss_dice: 0.305300
[08:40:04.275] NaN or Inf found in input tensor.
[08:40:04.489] TRAIN: iteration 5801 : loss : 0.127856, loss_ce: 0.005000, loss_dice: 0.250712
[08:40:04.751] TRAIN: iteration 5802 : loss : 0.208266, loss_ce: 0.005690, loss_dice: 0.410841
[08:40:04.958] TRAIN: iteration 5803 : loss : 0.198641, loss_ce: 0.008590, loss_dice: 0.388692
[08:40:05.168] TRAIN: iteration 5804 : loss : 0.252640, loss_ce: 0.004927, loss_dice: 0.500352
[08:40:05.378] TRAIN: iteration 5805 : loss : 0.150426, loss_ce: 0.003730, loss_dice: 0.297122
[08:40:05.589] TRAIN: iteration 5806 : loss : 0.187385, loss_ce: 0.008179, loss_dice: 0.366590
[08:40:05.798] TRAIN: iteration 5807 : loss : 0.091410, loss_ce: 0.004323, loss_dice: 0.178496
[08:40:06.007] TRAIN: iteration 5808 : loss : 0.250233, loss_ce: 0.005383, loss_dice: 0.495083
[08:40:06.216] TRAIN: iteration 5809 : loss : 0.124275, loss_ce: 0.005552, loss_dice: 0.242998
[08:40:06.427] TRAIN: iteration 5810 : loss : 0.211208, loss_ce: 0.005042, loss_dice: 0.417375
[08:40:06.635] TRAIN: iteration 5811 : loss : 0.241799, loss_ce: 0.004235, loss_dice: 0.479363
[08:40:06.847] TRAIN: iteration 5812 : loss : 0.252124, loss_ce: 0.004031, loss_dice: 0.500217
[08:40:07.056] TRAIN: iteration 5813 : loss : 0.172451, loss_ce: 0.004453, loss_dice: 0.340448
[08:40:07.265] TRAIN: iteration 5814 : loss : 0.134637, loss_ce: 0.004273, loss_dice: 0.265001
[08:40:07.474] TRAIN: iteration 5815 : loss : 0.231603, loss_ce: 0.010785, loss_dice: 0.452422
[08:40:07.681] TRAIN: iteration 5816 : loss : 0.083925, loss_ce: 0.004872, loss_dice: 0.162977
[08:40:07.893] TRAIN: iteration 5817 : loss : 0.128536, loss_ce: 0.003552, loss_dice: 0.253520
[08:40:08.102] TRAIN: iteration 5818 : loss : 0.171546, loss_ce: 0.007157, loss_dice: 0.335935
[08:40:08.309] TRAIN: iteration 5819 : loss : 0.248146, loss_ce: 0.009434, loss_dice: 0.486858
[08:40:08.516] TRAIN: iteration 5820 : loss : 0.111550, loss_ce: 0.006008, loss_dice: 0.217092
[08:40:08.782] TRAIN: iteration 5821 : loss : 0.084469, loss_ce: 0.003037, loss_dice: 0.165901
[08:40:08.990] TRAIN: iteration 5822 : loss : 0.128412, loss_ce: 0.008471, loss_dice: 0.248353
[08:40:09.199] TRAIN: iteration 5823 : loss : 0.218650, loss_ce: 0.033091, loss_dice: 0.404210
[08:40:09.409] TRAIN: iteration 5824 : loss : 0.217652, loss_ce: 0.004207, loss_dice: 0.431097
[08:40:09.620] TRAIN: iteration 5825 : loss : 0.252223, loss_ce: 0.004150, loss_dice: 0.500296
[08:40:09.828] TRAIN: iteration 5826 : loss : 0.141755, loss_ce: 0.014287, loss_dice: 0.269223
[08:40:10.036] TRAIN: iteration 5827 : loss : 0.158557, loss_ce: 0.013599, loss_dice: 0.303516
[08:40:10.246] TRAIN: iteration 5828 : loss : 0.155302, loss_ce: 0.013778, loss_dice: 0.296826
[08:40:10.543] TRAIN: iteration 5829 : loss : 0.090929, loss_ce: 0.003965, loss_dice: 0.177892
[08:40:10.755] TRAIN: iteration 5830 : loss : 0.121362, loss_ce: 0.005227, loss_dice: 0.237497
[08:40:10.963] TRAIN: iteration 5831 : loss : 0.251611, loss_ce: 0.006339, loss_dice: 0.496883
[08:40:11.175] TRAIN: iteration 5832 : loss : 0.198749, loss_ce: 0.007113, loss_dice: 0.390385
[08:40:11.383] TRAIN: iteration 5833 : loss : 0.162398, loss_ce: 0.007873, loss_dice: 0.316922
[08:40:11.590] TRAIN: iteration 5834 : loss : 0.086355, loss_ce: 0.007969, loss_dice: 0.164741
[08:40:11.797] TRAIN: iteration 5835 : loss : 0.137286, loss_ce: 0.006324, loss_dice: 0.268249
[08:40:12.005] TRAIN: iteration 5836 : loss : 0.189075, loss_ce: 0.005528, loss_dice: 0.372621
[08:40:13.206] TRAIN: iteration 5837 : loss : 0.145561, loss_ce: 0.005867, loss_dice: 0.285255
[08:40:13.414] TRAIN: iteration 5838 : loss : 0.223969, loss_ce: 0.004703, loss_dice: 0.443235
[08:40:13.631] TRAIN: iteration 5839 : loss : 0.252640, loss_ce: 0.004919, loss_dice: 0.500362
[08:40:13.845] TRAIN: iteration 5840 : loss : 0.247245, loss_ce: 0.005284, loss_dice: 0.489205
[08:40:14.089] TRAIN: iteration 5841 : loss : 0.252577, loss_ce: 0.004799, loss_dice: 0.500355
[08:40:14.296] TRAIN: iteration 5842 : loss : 0.083860, loss_ce: 0.005422, loss_dice: 0.162299
[08:40:14.509] TRAIN: iteration 5843 : loss : 0.252394, loss_ce: 0.004474, loss_dice: 0.500314
[08:40:14.721] TRAIN: iteration 5844 : loss : 0.201221, loss_ce: 0.012657, loss_dice: 0.389784
[08:40:14.948] TRAIN: iteration 5845 : loss : 0.088505, loss_ce: 0.006077, loss_dice: 0.170932
[08:40:15.157] TRAIN: iteration 5846 : loss : 0.249420, loss_ce: 0.019464, loss_dice: 0.479377
[08:40:15.368] TRAIN: iteration 5847 : loss : 0.247111, loss_ce: 0.004769, loss_dice: 0.489454
[08:40:15.575] TRAIN: iteration 5848 : loss : 0.188233, loss_ce: 0.005291, loss_dice: 0.371175
[08:40:15.783] TRAIN: iteration 5849 : loss : 0.206208, loss_ce: 0.009007, loss_dice: 0.403408
[08:40:15.991] TRAIN: iteration 5850 : loss : 0.082838, loss_ce: 0.005194, loss_dice: 0.160483
[08:40:16.198] TRAIN: iteration 5851 : loss : 0.252569, loss_ce: 0.004820, loss_dice: 0.500318
[08:40:16.406] TRAIN: iteration 5852 : loss : 0.251828, loss_ce: 0.004397, loss_dice: 0.499259
[08:40:16.614] TRAIN: iteration 5853 : loss : 0.156095, loss_ce: 0.005362, loss_dice: 0.306828
[08:40:16.827] TRAIN: iteration 5854 : loss : 0.086188, loss_ce: 0.006451, loss_dice: 0.165924
[08:40:17.035] TRAIN: iteration 5855 : loss : 0.201477, loss_ce: 0.004941, loss_dice: 0.398012
[08:40:17.246] TRAIN: iteration 5856 : loss : 0.096361, loss_ce: 0.004781, loss_dice: 0.187941
[08:40:17.454] TRAIN: iteration 5857 : loss : 0.124866, loss_ce: 0.005816, loss_dice: 0.243916
[08:40:17.665] TRAIN: iteration 5858 : loss : 0.082158, loss_ce: 0.003880, loss_dice: 0.160437
[08:40:17.876] TRAIN: iteration 5859 : loss : 0.125387, loss_ce: 0.008808, loss_dice: 0.241967
[08:40:18.083] TRAIN: iteration 5860 : loss : 0.096475, loss_ce: 0.004121, loss_dice: 0.188830
[08:40:18.328] TRAIN: iteration 5861 : loss : 0.098942, loss_ce: 0.004922, loss_dice: 0.192963
[08:40:18.538] TRAIN: iteration 5862 : loss : 0.071770, loss_ce: 0.004739, loss_dice: 0.138800
[08:40:18.746] TRAIN: iteration 5863 : loss : 0.150441, loss_ce: 0.010442, loss_dice: 0.290439
[08:40:18.954] TRAIN: iteration 5864 : loss : 0.252199, loss_ce: 0.004118, loss_dice: 0.500280
[08:40:19.167] TRAIN: iteration 5865 : loss : 0.132606, loss_ce: 0.006855, loss_dice: 0.258356
[08:40:19.379] TRAIN: iteration 5866 : loss : 0.138986, loss_ce: 0.004305, loss_dice: 0.273667
[08:40:19.590] TRAIN: iteration 5867 : loss : 0.155176, loss_ce: 0.003540, loss_dice: 0.306813
[08:40:19.798] TRAIN: iteration 5868 : loss : 0.159252, loss_ce: 0.013377, loss_dice: 0.305128
[08:40:20.007] TRAIN: iteration 5869 : loss : 0.078917, loss_ce: 0.005096, loss_dice: 0.152737
[08:40:20.217] TRAIN: iteration 5870 : loss : 0.181805, loss_ce: 0.009311, loss_dice: 0.354298
[08:40:20.488] TRAIN: iteration 5871 : loss : 0.176069, loss_ce: 0.003193, loss_dice: 0.348945
[08:40:20.695] TRAIN: iteration 5872 : loss : 0.108407, loss_ce: 0.008861, loss_dice: 0.207953
[08:40:20.905] TRAIN: iteration 5873 : loss : 0.164545, loss_ce: 0.024877, loss_dice: 0.304213
[08:40:21.114] TRAIN: iteration 5874 : loss : 0.251806, loss_ce: 0.003419, loss_dice: 0.500193
[08:40:21.322] TRAIN: iteration 5875 : loss : 0.165265, loss_ce: 0.003062, loss_dice: 0.327468
[08:40:21.790] TRAIN: iteration 5876 : loss : 0.190792, loss_ce: 0.007416, loss_dice: 0.374168
[08:40:22.006] TRAIN: iteration 5877 : loss : 0.106982, loss_ce: 0.010913, loss_dice: 0.203051
[08:40:22.218] TRAIN: iteration 5878 : loss : 0.167483, loss_ce: 0.022922, loss_dice: 0.312045
[08:40:22.425] TRAIN: iteration 5879 : loss : 0.208752, loss_ce: 0.007387, loss_dice: 0.410116
[08:40:22.721] TRAIN: iteration 5880 : loss : 0.089917, loss_ce: 0.005590, loss_dice: 0.174244
[08:40:22.958] TRAIN: iteration 5881 : loss : 0.114760, loss_ce: 0.006013, loss_dice: 0.223508
[08:40:25.137] TRAIN: iteration 5882 : loss : 0.207589, loss_ce: 0.006059, loss_dice: 0.409118
[08:40:25.352] TRAIN: iteration 5883 : loss : 0.080519, loss_ce: 0.003585, loss_dice: 0.157452
[08:40:25.560] TRAIN: iteration 5884 : loss : 0.153790, loss_ce: 0.009147, loss_dice: 0.298432
[08:40:25.770] TRAIN: iteration 5885 : loss : 0.252333, loss_ce: 0.004369, loss_dice: 0.500296
[08:40:25.978] TRAIN: iteration 5886 : loss : 0.207614, loss_ce: 0.007332, loss_dice: 0.407895
[08:40:26.193] TRAIN: iteration 5887 : loss : 0.199808, loss_ce: 0.004895, loss_dice: 0.394720
[08:40:26.404] TRAIN: iteration 5888 : loss : 0.168928, loss_ce: 0.017356, loss_dice: 0.320500
[08:40:26.620] TRAIN: iteration 5889 : loss : 0.251565, loss_ce: 0.003255, loss_dice: 0.499874
[08:40:26.830] TRAIN: iteration 5890 : loss : 0.067840, loss_ce: 0.004167, loss_dice: 0.131512
[08:40:27.041] TRAIN: iteration 5891 : loss : 0.079414, loss_ce: 0.005178, loss_dice: 0.153651
[08:40:27.255] TRAIN: iteration 5892 : loss : 0.114270, loss_ce: 0.006751, loss_dice: 0.221790
[08:40:27.470] TRAIN: iteration 5893 : loss : 0.146755, loss_ce: 0.004376, loss_dice: 0.289134
[08:40:27.679] TRAIN: iteration 5894 : loss : 0.140803, loss_ce: 0.007332, loss_dice: 0.274274
[08:40:27.888] TRAIN: iteration 5895 : loss : 0.106288, loss_ce: 0.003540, loss_dice: 0.209036
[08:40:28.098] TRAIN: iteration 5896 : loss : 0.251626, loss_ce: 0.004278, loss_dice: 0.498974
[08:40:28.306] TRAIN: iteration 5897 : loss : 0.251780, loss_ce: 0.003412, loss_dice: 0.500148
[08:40:28.515] TRAIN: iteration 5898 : loss : 0.249547, loss_ce: 0.007529, loss_dice: 0.491565
[08:40:28.722] TRAIN: iteration 5899 : loss : 0.136831, loss_ce: 0.003704, loss_dice: 0.269957
[08:40:28.935] TRAIN: iteration 5900 : loss : 0.243757, loss_ce: 0.034252, loss_dice: 0.453262
[08:40:29.180] TRAIN: iteration 5901 : loss : 0.085437, loss_ce: 0.005586, loss_dice: 0.165288
[08:40:29.392] TRAIN: iteration 5902 : loss : 0.157026, loss_ce: 0.009831, loss_dice: 0.304221
[08:40:29.605] TRAIN: iteration 5903 : loss : 0.235640, loss_ce: 0.009301, loss_dice: 0.461979
[08:40:29.821] TRAIN: iteration 5904 : loss : 0.251814, loss_ce: 0.003487, loss_dice: 0.500141
[08:40:30.035] TRAIN: iteration 5905 : loss : 0.249085, loss_ce: 0.006677, loss_dice: 0.491493
[08:40:30.242] TRAIN: iteration 5906 : loss : 0.215626, loss_ce: 0.004656, loss_dice: 0.426597
[08:40:30.457] TRAIN: iteration 5907 : loss : 0.124657, loss_ce: 0.007689, loss_dice: 0.241624
[08:40:30.665] TRAIN: iteration 5908 : loss : 0.238156, loss_ce: 0.005435, loss_dice: 0.470877
[08:40:30.874] TRAIN: iteration 5909 : loss : 0.153698, loss_ce: 0.004958, loss_dice: 0.302438
[08:40:31.087] TRAIN: iteration 5910 : loss : 0.179406, loss_ce: 0.004678, loss_dice: 0.354133
[08:40:31.295] TRAIN: iteration 5911 : loss : 0.161153, loss_ce: 0.007384, loss_dice: 0.314921
[08:40:31.508] TRAIN: iteration 5912 : loss : 0.121549, loss_ce: 0.005817, loss_dice: 0.237281
[08:40:31.887] TRAIN: iteration 5913 : loss : 0.233841, loss_ce: 0.011752, loss_dice: 0.455931
[08:40:32.100] TRAIN: iteration 5914 : loss : 0.106454, loss_ce: 0.004821, loss_dice: 0.208087
[08:40:32.310] TRAIN: iteration 5915 : loss : 0.144342, loss_ce: 0.018569, loss_dice: 0.270115
[08:40:32.520] TRAIN: iteration 5916 : loss : 0.116412, loss_ce: 0.005335, loss_dice: 0.227488
[08:40:32.729] TRAIN: iteration 5917 : loss : 0.204419, loss_ce: 0.008203, loss_dice: 0.400635
[08:40:32.938] TRAIN: iteration 5918 : loss : 0.080311, loss_ce: 0.003623, loss_dice: 0.156999
[08:40:33.146] TRAIN: iteration 5919 : loss : 0.179637, loss_ce: 0.007564, loss_dice: 0.351710
[08:40:33.353] TRAIN: iteration 5920 : loss : 0.130159, loss_ce: 0.006844, loss_dice: 0.253475
[08:40:33.589] TRAIN: iteration 5921 : loss : 0.198119, loss_ce: 0.011824, loss_dice: 0.384414
[08:40:33.796] TRAIN: iteration 5922 : loss : 0.136404, loss_ce: 0.005890, loss_dice: 0.266918
[08:40:34.003] TRAIN: iteration 5923 : loss : 0.196349, loss_ce: 0.006847, loss_dice: 0.385850
[08:40:34.212] TRAIN: iteration 5924 : loss : 0.253268, loss_ce: 0.006141, loss_dice: 0.500394
[08:40:34.456] TRAIN: iteration 5925 : loss : 0.137091, loss_ce: 0.005657, loss_dice: 0.268526
[08:40:34.663] TRAIN: iteration 5926 : loss : 0.168832, loss_ce: 0.027120, loss_dice: 0.310544
[08:40:34.870] TRAIN: iteration 5927 : loss : 0.112600, loss_ce: 0.007856, loss_dice: 0.217345
[08:40:35.823] TRAIN: iteration 5928 : loss : 0.128131, loss_ce: 0.005734, loss_dice: 0.250529
[08:40:36.031] TRAIN: iteration 5929 : loss : 0.150629, loss_ce: 0.006288, loss_dice: 0.294970
[08:40:36.243] TRAIN: iteration 5930 : loss : 0.221414, loss_ce: 0.006083, loss_dice: 0.436745
[08:40:36.450] TRAIN: iteration 5931 : loss : 0.131435, loss_ce: 0.008462, loss_dice: 0.254408
[08:40:36.658] TRAIN: iteration 5932 : loss : 0.200097, loss_ce: 0.006066, loss_dice: 0.394128
[08:40:36.872] TRAIN: iteration 5933 : loss : 0.252665, loss_ce: 0.005028, loss_dice: 0.500301
[08:40:37.113] TRAIN: iteration 5934 : loss : 0.079876, loss_ce: 0.004368, loss_dice: 0.155384
[08:40:37.321] TRAIN: iteration 5935 : loss : 0.181455, loss_ce: 0.004745, loss_dice: 0.358165
[08:40:37.528] TRAIN: iteration 5936 : loss : 0.147581, loss_ce: 0.006513, loss_dice: 0.288649
[08:40:37.741] TRAIN: iteration 5937 : loss : 0.171570, loss_ce: 0.008187, loss_dice: 0.334954
[08:40:38.001] TRAIN: iteration 5938 : loss : 0.252265, loss_ce: 0.004289, loss_dice: 0.500242
[08:40:38.209] TRAIN: iteration 5939 : loss : 0.232994, loss_ce: 0.005105, loss_dice: 0.460884
[08:40:38.848] TRAIN: iteration 5940 : loss : 0.134692, loss_ce: 0.010535, loss_dice: 0.258848
[08:40:39.099] TRAIN: iteration 5941 : loss : 0.159240, loss_ce: 0.006061, loss_dice: 0.312419
[08:40:39.308] TRAIN: iteration 5942 : loss : 0.139965, loss_ce: 0.003152, loss_dice: 0.276778
[08:40:39.516] TRAIN: iteration 5943 : loss : 0.092574, loss_ce: 0.005803, loss_dice: 0.179346
[08:40:39.723] TRAIN: iteration 5944 : loss : 0.095871, loss_ce: 0.011057, loss_dice: 0.180684
[08:40:39.932] TRAIN: iteration 5945 : loss : 0.165762, loss_ce: 0.008257, loss_dice: 0.323267
[08:40:40.139] TRAIN: iteration 5946 : loss : 0.210917, loss_ce: 0.011803, loss_dice: 0.410030
[08:40:40.350] TRAIN: iteration 5947 : loss : 0.174245, loss_ce: 0.012338, loss_dice: 0.336152
[08:40:40.558] TRAIN: iteration 5948 : loss : 0.132663, loss_ce: 0.004848, loss_dice: 0.260478
[08:40:40.765] TRAIN: iteration 5949 : loss : 0.251392, loss_ce: 0.005206, loss_dice: 0.497577
[08:40:40.977] TRAIN: iteration 5950 : loss : 0.252450, loss_ce: 0.004775, loss_dice: 0.500125
[08:40:41.186] TRAIN: iteration 5951 : loss : 0.178693, loss_ce: 0.006066, loss_dice: 0.351319
[08:40:41.393] TRAIN: iteration 5952 : loss : 0.186423, loss_ce: 0.005343, loss_dice: 0.367503
[08:40:41.602] TRAIN: iteration 5953 : loss : 0.197996, loss_ce: 0.008186, loss_dice: 0.387807
[08:40:42.339] TRAIN: iteration 5954 : loss : 0.251761, loss_ce: 0.003410, loss_dice: 0.500111
[08:40:42.547] TRAIN: iteration 5955 : loss : 0.214824, loss_ce: 0.005307, loss_dice: 0.424341
[08:40:42.761] TRAIN: iteration 5956 : loss : 0.210355, loss_ce: 0.005629, loss_dice: 0.415082
[08:40:42.971] TRAIN: iteration 5957 : loss : 0.248265, loss_ce: 0.005323, loss_dice: 0.491206
[08:40:43.180] TRAIN: iteration 5958 : loss : 0.108531, loss_ce: 0.004556, loss_dice: 0.212507
[08:40:43.388] TRAIN: iteration 5959 : loss : 0.154717, loss_ce: 0.015029, loss_dice: 0.294405
[08:40:43.598] TRAIN: iteration 5960 : loss : 0.252213, loss_ce: 0.005464, loss_dice: 0.498962
[08:40:43.838] TRAIN: iteration 5961 : loss : 0.075002, loss_ce: 0.004624, loss_dice: 0.145381
[08:40:44.046] TRAIN: iteration 5962 : loss : 0.244277, loss_ce: 0.031871, loss_dice: 0.456684
[08:40:44.254] TRAIN: iteration 5963 : loss : 0.166913, loss_ce: 0.008374, loss_dice: 0.325452
[08:40:44.461] TRAIN: iteration 5964 : loss : 0.119777, loss_ce: 0.005217, loss_dice: 0.234337
[08:40:44.668] TRAIN: iteration 5965 : loss : 0.094717, loss_ce: 0.004848, loss_dice: 0.184586
[08:40:44.882] TRAIN: iteration 5966 : loss : 0.079951, loss_ce: 0.005853, loss_dice: 0.154049
[08:40:45.089] TRAIN: iteration 5967 : loss : 0.173572, loss_ce: 0.005359, loss_dice: 0.341784
[08:40:45.297] TRAIN: iteration 5968 : loss : 0.210556, loss_ce: 0.005534, loss_dice: 0.415578
[08:40:45.506] TRAIN: iteration 5969 : loss : 0.200200, loss_ce: 0.025781, loss_dice: 0.374619
[08:40:45.714] TRAIN: iteration 5970 : loss : 0.206295, loss_ce: 0.007162, loss_dice: 0.405428
[08:40:45.921] TRAIN: iteration 5971 : loss : 0.200614, loss_ce: 0.004680, loss_dice: 0.396548
[08:40:46.135] TRAIN: iteration 5972 : loss : 0.098940, loss_ce: 0.006326, loss_dice: 0.191554
[08:40:46.437] TRAIN: iteration 5973 : loss : 0.032391, loss_ce: 0.002363, loss_dice: 0.062418
[08:40:46.855] TRAIN: iteration 5974 : loss : 0.073607, loss_ce: 0.003641, loss_dice: 0.143572
[08:40:47.062] TRAIN: iteration 5975 : loss : 0.251795, loss_ce: 0.003459, loss_dice: 0.500131
[08:40:47.273] TRAIN: iteration 5976 : loss : 0.243136, loss_ce: 0.005769, loss_dice: 0.480502
[08:40:47.480] TRAIN: iteration 5977 : loss : 0.232496, loss_ce: 0.014496, loss_dice: 0.450495
[08:40:47.688] TRAIN: iteration 5978 : loss : 0.188548, loss_ce: 0.006513, loss_dice: 0.370582
[08:40:47.895] TRAIN: iteration 5979 : loss : 0.233161, loss_ce: 0.004385, loss_dice: 0.461937
[08:40:48.103] TRAIN: iteration 5980 : loss : 0.251900, loss_ce: 0.003618, loss_dice: 0.500182
[08:40:48.340] TRAIN: iteration 5981 : loss : 0.178697, loss_ce: 0.008762, loss_dice: 0.348632
[08:40:49.103] TRAIN: iteration 5982 : loss : 0.189953, loss_ce: 0.006789, loss_dice: 0.373118
[08:40:49.311] TRAIN: iteration 5983 : loss : 0.143429, loss_ce: 0.011876, loss_dice: 0.274983
[08:40:49.519] TRAIN: iteration 5984 : loss : 0.200867, loss_ce: 0.006378, loss_dice: 0.395356
[08:40:49.728] TRAIN: iteration 5985 : loss : 0.113407, loss_ce: 0.008451, loss_dice: 0.218363
[08:40:49.938] TRAIN: iteration 5986 : loss : 0.173658, loss_ce: 0.005876, loss_dice: 0.341441
[08:40:50.149] TRAIN: iteration 5987 : loss : 0.081514, loss_ce: 0.005709, loss_dice: 0.157320
[08:40:50.358] TRAIN: iteration 5988 : loss : 0.230960, loss_ce: 0.006512, loss_dice: 0.455409
[08:40:50.570] TRAIN: iteration 5989 : loss : 0.139842, loss_ce: 0.015747, loss_dice: 0.263938
[08:40:50.845] TRAIN: iteration 5990 : loss : 0.210118, loss_ce: 0.007533, loss_dice: 0.412703
[08:40:51.061] TRAIN: iteration 5991 : loss : 0.158083, loss_ce: 0.007828, loss_dice: 0.308338
[08:40:51.273] TRAIN: iteration 5992 : loss : 0.068672, loss_ce: 0.006044, loss_dice: 0.131299
[08:40:51.486] TRAIN: iteration 5993 : loss : 0.182579, loss_ce: 0.008084, loss_dice: 0.357073
[08:40:51.695] TRAIN: iteration 5994 : loss : 0.253939, loss_ce: 0.007386, loss_dice: 0.500493
[08:40:51.913] TRAIN: iteration 5995 : loss : 0.232131, loss_ce: 0.008135, loss_dice: 0.456127
[08:40:52.124] TRAIN: iteration 5996 : loss : 0.166168, loss_ce: 0.007606, loss_dice: 0.324731
[08:40:52.333] TRAIN: iteration 5997 : loss : 0.237226, loss_ce: 0.007986, loss_dice: 0.466466
[08:40:52.541] TRAIN: iteration 5998 : loss : 0.091512, loss_ce: 0.007361, loss_dice: 0.175664
[08:40:52.748] TRAIN: iteration 5999 : loss : 0.210742, loss_ce: 0.008363, loss_dice: 0.413120
[08:40:52.958] TRAIN: iteration 6000 : loss : 0.254070, loss_ce: 0.007589, loss_dice: 0.500551
[08:40:53.200] TRAIN: iteration 6001 : loss : 0.243667, loss_ce: 0.007043, loss_dice: 0.480291
[08:40:53.406] TRAIN: iteration 6002 : loss : 0.143400, loss_ce: 0.004751, loss_dice: 0.282050
[08:40:53.620] TRAIN: iteration 6003 : loss : 0.200344, loss_ce: 0.004843, loss_dice: 0.395846
[08:40:53.827] TRAIN: iteration 6004 : loss : 0.145594, loss_ce: 0.007692, loss_dice: 0.283497
[08:40:54.034] TRAIN: iteration 6005 : loss : 0.199236, loss_ce: 0.004970, loss_dice: 0.393503
[08:40:54.242] TRAIN: iteration 6006 : loss : 0.239633, loss_ce: 0.005862, loss_dice: 0.473403
[08:40:54.449] TRAIN: iteration 6007 : loss : 0.103669, loss_ce: 0.003460, loss_dice: 0.203877
[08:40:54.658] TRAIN: iteration 6008 : loss : 0.206993, loss_ce: 0.007232, loss_dice: 0.406755
[08:40:54.927] TRAIN: iteration 6009 : loss : 0.141865, loss_ce: 0.007120, loss_dice: 0.276611
[08:40:55.135] TRAIN: iteration 6010 : loss : 0.175226, loss_ce: 0.018032, loss_dice: 0.332420
[08:40:55.344] TRAIN: iteration 6011 : loss : 0.120244, loss_ce: 0.010027, loss_dice: 0.230462
[08:40:55.551] TRAIN: iteration 6012 : loss : 0.239104, loss_ce: 0.005622, loss_dice: 0.472585
[08:40:55.758] TRAIN: iteration 6013 : loss : 0.072464, loss_ce: 0.004527, loss_dice: 0.140400
[08:40:55.966] TRAIN: iteration 6014 : loss : 0.139187, loss_ce: 0.006638, loss_dice: 0.271735
[08:40:56.249] TRAIN: iteration 6015 : loss : 0.157939, loss_ce: 0.003728, loss_dice: 0.312150
[08:40:56.457] TRAIN: iteration 6016 : loss : 0.250603, loss_ce: 0.004933, loss_dice: 0.496273
[08:40:57.379] TRAIN: iteration 6017 : loss : 0.252282, loss_ce: 0.004260, loss_dice: 0.500304
[08:40:57.586] TRAIN: iteration 6018 : loss : 0.252153, loss_ce: 0.004041, loss_dice: 0.500266
[08:40:57.793] TRAIN: iteration 6019 : loss : 0.235763, loss_ce: 0.004294, loss_dice: 0.467233
[08:40:58.001] TRAIN: iteration 6020 : loss : 0.167276, loss_ce: 0.007201, loss_dice: 0.327351
[08:40:58.242] TRAIN: iteration 6021 : loss : 0.114958, loss_ce: 0.012376, loss_dice: 0.217540
[08:40:58.449] TRAIN: iteration 6022 : loss : 0.206434, loss_ce: 0.004276, loss_dice: 0.408593
[08:40:58.656] TRAIN: iteration 6023 : loss : 0.234734, loss_ce: 0.004555, loss_dice: 0.464914
[08:40:58.864] TRAIN: iteration 6024 : loss : 0.182076, loss_ce: 0.008882, loss_dice: 0.355269
[08:41:00.476] TRAIN: iteration 6025 : loss : 0.241975, loss_ce: 0.004027, loss_dice: 0.479922
[08:41:00.685] TRAIN: iteration 6026 : loss : 0.248605, loss_ce: 0.006989, loss_dice: 0.490220
[08:41:00.894] TRAIN: iteration 6027 : loss : 0.110813, loss_ce: 0.006187, loss_dice: 0.215439
[08:41:01.102] TRAIN: iteration 6028 : loss : 0.126193, loss_ce: 0.006384, loss_dice: 0.246002
[08:41:01.310] TRAIN: iteration 6029 : loss : 0.097312, loss_ce: 0.005305, loss_dice: 0.189318
[08:41:01.520] TRAIN: iteration 6030 : loss : 0.236891, loss_ce: 0.005125, loss_dice: 0.468657
[08:41:01.729] TRAIN: iteration 6031 : loss : 0.196187, loss_ce: 0.006525, loss_dice: 0.385849
[08:41:01.938] TRAIN: iteration 6032 : loss : 0.231263, loss_ce: 0.012534, loss_dice: 0.449992
[08:41:02.293] TRAIN: iteration 6033 : loss : 0.096628, loss_ce: 0.004274, loss_dice: 0.188982
[08:41:02.501] TRAIN: iteration 6034 : loss : 0.253199, loss_ce: 0.006561, loss_dice: 0.499837
[08:41:02.710] TRAIN: iteration 6035 : loss : 0.178426, loss_ce: 0.024693, loss_dice: 0.332160
[08:41:02.919] TRAIN: iteration 6036 : loss : 0.151954, loss_ce: 0.004983, loss_dice: 0.298924
[08:41:03.127] TRAIN: iteration 6037 : loss : 0.077587, loss_ce: 0.004582, loss_dice: 0.150592
[08:41:03.336] TRAIN: iteration 6038 : loss : 0.223766, loss_ce: 0.006622, loss_dice: 0.440911
[08:41:03.550] TRAIN: iteration 6039 : loss : 0.177020, loss_ce: 0.006573, loss_dice: 0.347468
[08:41:03.760] TRAIN: iteration 6040 : loss : 0.252794, loss_ce: 0.005234, loss_dice: 0.500355
[08:41:03.999] TRAIN: iteration 6041 : loss : 0.218357, loss_ce: 0.007181, loss_dice: 0.429534
[08:41:04.207] TRAIN: iteration 6042 : loss : 0.173236, loss_ce: 0.004200, loss_dice: 0.342273
[08:41:04.417] TRAIN: iteration 6043 : loss : 0.175888, loss_ce: 0.005179, loss_dice: 0.346597
[08:41:04.625] TRAIN: iteration 6044 : loss : 0.199895, loss_ce: 0.012287, loss_dice: 0.387502
[08:41:04.840] TRAIN: iteration 6045 : loss : 0.228682, loss_ce: 0.004353, loss_dice: 0.453011
[08:41:05.050] TRAIN: iteration 6046 : loss : 0.120121, loss_ce: 0.005046, loss_dice: 0.235195
[08:41:05.291] TRAIN: iteration 6047 : loss : 0.208656, loss_ce: 0.005870, loss_dice: 0.411441
[08:41:05.498] TRAIN: iteration 6048 : loss : 0.113452, loss_ce: 0.007131, loss_dice: 0.219773
[08:41:05.706] TRAIN: iteration 6049 : loss : 0.251770, loss_ce: 0.003361, loss_dice: 0.500179
[08:41:05.913] TRAIN: iteration 6050 : loss : 0.139023, loss_ce: 0.008364, loss_dice: 0.269682
[08:41:06.122] TRAIN: iteration 6051 : loss : 0.225197, loss_ce: 0.003375, loss_dice: 0.447020
[08:41:07.117] TRAIN: iteration 6052 : loss : 0.251738, loss_ce: 0.003292, loss_dice: 0.500184
[08:41:07.325] TRAIN: iteration 6053 : loss : 0.137226, loss_ce: 0.004275, loss_dice: 0.270177
[08:41:07.533] TRAIN: iteration 6054 : loss : 0.096549, loss_ce: 0.004089, loss_dice: 0.189009
[08:41:07.758] TRAIN: iteration 6055 : loss : 0.251566, loss_ce: 0.003017, loss_dice: 0.500115
[08:41:07.965] TRAIN: iteration 6056 : loss : 0.139089, loss_ce: 0.003104, loss_dice: 0.275075
[08:41:08.174] TRAIN: iteration 6057 : loss : 0.133872, loss_ce: 0.003402, loss_dice: 0.264341
[08:41:08.382] TRAIN: iteration 6058 : loss : 0.179394, loss_ce: 0.003071, loss_dice: 0.355716
[08:41:08.592] TRAIN: iteration 6059 : loss : 0.138770, loss_ce: 0.003019, loss_dice: 0.274521
[08:41:08.799] TRAIN: iteration 6060 : loss : 0.206399, loss_ce: 0.004528, loss_dice: 0.408270
[08:41:09.032] TRAIN: iteration 6061 : loss : 0.177672, loss_ce: 0.010582, loss_dice: 0.344762
[08:41:09.241] TRAIN: iteration 6062 : loss : 0.185432, loss_ce: 0.015671, loss_dice: 0.355193
[08:41:09.448] TRAIN: iteration 6063 : loss : 0.203260, loss_ce: 0.004153, loss_dice: 0.402368
[08:41:09.656] TRAIN: iteration 6064 : loss : 0.141313, loss_ce: 0.009330, loss_dice: 0.273296
[08:41:09.864] TRAIN: iteration 6065 : loss : 0.061295, loss_ce: 0.002437, loss_dice: 0.120152
[08:41:10.071] TRAIN: iteration 6066 : loss : 0.160504, loss_ce: 0.017682, loss_dice: 0.303326
[08:41:10.279] TRAIN: iteration 6067 : loss : 0.251430, loss_ce: 0.002717, loss_dice: 0.500143
[08:41:11.200] TRAIN: iteration 6068 : loss : 0.100054, loss_ce: 0.004536, loss_dice: 0.195572
[08:41:11.408] TRAIN: iteration 6069 : loss : 0.251507, loss_ce: 0.002878, loss_dice: 0.500137
[08:41:11.616] TRAIN: iteration 6070 : loss : 0.078736, loss_ce: 0.002557, loss_dice: 0.154915
[08:41:11.823] TRAIN: iteration 6071 : loss : 0.251459, loss_ce: 0.002818, loss_dice: 0.500101
[08:41:12.030] TRAIN: iteration 6072 : loss : 0.251532, loss_ce: 0.003590, loss_dice: 0.499473
[08:41:12.237] TRAIN: iteration 6073 : loss : 0.103512, loss_ce: 0.008556, loss_dice: 0.198469
[08:41:12.444] TRAIN: iteration 6074 : loss : 0.136479, loss_ce: 0.006369, loss_dice: 0.266589
[08:41:12.659] TRAIN: iteration 6075 : loss : 0.211156, loss_ce: 0.004315, loss_dice: 0.417996
[08:41:12.868] TRAIN: iteration 6076 : loss : 0.126524, loss_ce: 0.002926, loss_dice: 0.250122
[08:41:13.076] TRAIN: iteration 6077 : loss : 0.132545, loss_ce: 0.003565, loss_dice: 0.261526
[08:41:13.283] TRAIN: iteration 6078 : loss : 0.128984, loss_ce: 0.004735, loss_dice: 0.253233
[08:41:13.491] TRAIN: iteration 6079 : loss : 0.218749, loss_ce: 0.003661, loss_dice: 0.433837
[08:41:13.699] TRAIN: iteration 6080 : loss : 0.215569, loss_ce: 0.003128, loss_dice: 0.428010
[08:41:13.942] TRAIN: iteration 6081 : loss : 0.151036, loss_ce: 0.003429, loss_dice: 0.298644
[08:41:14.151] TRAIN: iteration 6082 : loss : 0.121509, loss_ce: 0.002594, loss_dice: 0.240423
[08:41:14.365] TRAIN: iteration 6083 : loss : 0.103607, loss_ce: 0.009747, loss_dice: 0.197467
[08:41:14.573] TRAIN: iteration 6084 : loss : 0.083304, loss_ce: 0.002818, loss_dice: 0.163791
[08:41:14.788] TRAIN: iteration 6085 : loss : 0.106106, loss_ce: 0.006544, loss_dice: 0.205667
[08:41:14.995] TRAIN: iteration 6086 : loss : 0.114391, loss_ce: 0.005296, loss_dice: 0.223485
[08:41:15.204] TRAIN: iteration 6087 : loss : 0.126867, loss_ce: 0.008743, loss_dice: 0.244991
[08:41:16.514] TRAIN: iteration 6088 : loss : 0.167570, loss_ce: 0.005484, loss_dice: 0.329656
[08:41:16.722] TRAIN: iteration 6089 : loss : 0.122916, loss_ce: 0.012194, loss_dice: 0.233638
[08:41:16.931] TRAIN: iteration 6090 : loss : 0.163780, loss_ce: 0.005305, loss_dice: 0.322255
[08:41:17.146] TRAIN: iteration 6091 : loss : 0.171232, loss_ce: 0.003558, loss_dice: 0.338907
[08:41:17.353] TRAIN: iteration 6092 : loss : 0.251949, loss_ce: 0.003693, loss_dice: 0.500205
[08:41:17.562] TRAIN: iteration 6093 : loss : 0.139159, loss_ce: 0.004265, loss_dice: 0.274053
[08:41:17.770] TRAIN: iteration 6094 : loss : 0.156018, loss_ce: 0.005341, loss_dice: 0.306695
[08:41:17.978] TRAIN: iteration 6095 : loss : 0.252371, loss_ce: 0.004465, loss_dice: 0.500278
[08:41:19.794] TRAIN: iteration 6096 : loss : 0.196260, loss_ce: 0.008021, loss_dice: 0.384500
[08:41:20.005] TRAIN: iteration 6097 : loss : 0.142546, loss_ce: 0.008214, loss_dice: 0.276877
[08:41:20.216] TRAIN: iteration 6098 : loss : 0.252207, loss_ce: 0.004096, loss_dice: 0.500317
[08:41:20.424] TRAIN: iteration 6099 : loss : 0.147051, loss_ce: 0.005710, loss_dice: 0.288391
[08:41:20.639] TRAIN: iteration 6100 : loss : 0.176306, loss_ce: 0.006016, loss_dice: 0.346597
[08:41:20.877] TRAIN: iteration 6101 : loss : 0.163916, loss_ce: 0.008098, loss_dice: 0.319735
[08:41:21.086] TRAIN: iteration 6102 : loss : 0.192295, loss_ce: 0.007183, loss_dice: 0.377408
[08:41:21.296] TRAIN: iteration 6103 : loss : 0.192008, loss_ce: 0.006279, loss_dice: 0.377738
[08:41:21.506] TRAIN: iteration 6104 : loss : 0.152888, loss_ce: 0.007247, loss_dice: 0.298529
[08:41:21.835] TRAIN: iteration 6105 : loss : 0.233397, loss_ce: 0.008858, loss_dice: 0.457935
[08:41:22.046] TRAIN: iteration 6106 : loss : 0.090508, loss_ce: 0.005617, loss_dice: 0.175399
[08:41:22.255] TRAIN: iteration 6107 : loss : 0.093043, loss_ce: 0.004869, loss_dice: 0.181218
[08:41:22.463] TRAIN: iteration 6108 : loss : 0.070734, loss_ce: 0.004229, loss_dice: 0.137239
[08:41:23.038] TRAIN: iteration 6109 : loss : 0.125878, loss_ce: 0.005803, loss_dice: 0.245953
[08:41:23.247] TRAIN: iteration 6110 : loss : 0.111843, loss_ce: 0.006310, loss_dice: 0.217376
[08:41:23.459] TRAIN: iteration 6111 : loss : 0.252574, loss_ce: 0.004891, loss_dice: 0.500257
[08:41:23.669] TRAIN: iteration 6112 : loss : 0.083321, loss_ce: 0.008785, loss_dice: 0.157857
[08:41:23.877] TRAIN: iteration 6113 : loss : 0.239501, loss_ce: 0.014695, loss_dice: 0.464306
[08:41:24.086] TRAIN: iteration 6114 : loss : 0.109972, loss_ce: 0.005590, loss_dice: 0.214354
[08:41:24.294] TRAIN: iteration 6115 : loss : 0.119425, loss_ce: 0.003603, loss_dice: 0.235246
[08:41:24.503] TRAIN: iteration 6116 : loss : 0.234444, loss_ce: 0.007413, loss_dice: 0.461476
[08:41:24.713] TRAIN: iteration 6117 : loss : 0.250885, loss_ce: 0.005272, loss_dice: 0.496498
[08:41:24.923] TRAIN: iteration 6118 : loss : 0.211224, loss_ce: 0.008927, loss_dice: 0.413522
[08:41:25.130] TRAIN: iteration 6119 : loss : 0.227400, loss_ce: 0.005756, loss_dice: 0.449045
[08:41:26.999] TRAIN: iteration 6120 : loss : 0.247488, loss_ce: 0.005580, loss_dice: 0.489396
[08:41:27.245] TRAIN: iteration 6121 : loss : 0.124664, loss_ce: 0.014259, loss_dice: 0.235069
[08:41:27.452] TRAIN: iteration 6122 : loss : 0.245834, loss_ce: 0.005149, loss_dice: 0.486519
[08:41:27.664] TRAIN: iteration 6123 : loss : 0.206377, loss_ce: 0.012676, loss_dice: 0.400078
[08:41:27.873] TRAIN: iteration 6124 : loss : 0.260620, loss_ce: 0.022643, loss_dice: 0.498598
[08:41:28.081] TRAIN: iteration 6125 : loss : 0.200043, loss_ce: 0.007825, loss_dice: 0.392262
[08:41:28.291] TRAIN: iteration 6126 : loss : 0.115317, loss_ce: 0.005484, loss_dice: 0.225150
[08:41:28.507] TRAIN: iteration 6127 : loss : 0.240772, loss_ce: 0.005979, loss_dice: 0.475565
[08:41:28.715] TRAIN: iteration 6128 : loss : 0.147784, loss_ce: 0.004523, loss_dice: 0.291045
[08:41:28.923] TRAIN: iteration 6129 : loss : 0.083945, loss_ce: 0.004754, loss_dice: 0.163135
[08:41:29.132] TRAIN: iteration 6130 : loss : 0.247055, loss_ce: 0.004431, loss_dice: 0.489678
[08:41:29.965] TRAIN: iteration 6131 : loss : 0.242431, loss_ce: 0.023486, loss_dice: 0.461376
[08:41:30.172] TRAIN: iteration 6132 : loss : 0.216484, loss_ce: 0.030754, loss_dice: 0.402214
[08:41:30.380] TRAIN: iteration 6133 : loss : 0.098142, loss_ce: 0.005958, loss_dice: 0.190326
[08:41:30.588] TRAIN: iteration 6134 : loss : 0.190812, loss_ce: 0.005194, loss_dice: 0.376430
[08:41:30.797] TRAIN: iteration 6135 : loss : 0.231316, loss_ce: 0.005650, loss_dice: 0.456982
[08:41:31.004] TRAIN: iteration 6136 : loss : 0.154430, loss_ce: 0.012421, loss_dice: 0.296438
[08:41:31.212] TRAIN: iteration 6137 : loss : 0.252526, loss_ce: 0.004824, loss_dice: 0.500227
[08:41:31.419] TRAIN: iteration 6138 : loss : 0.121046, loss_ce: 0.004610, loss_dice: 0.237481
[08:41:32.750] TRAIN: iteration 6139 : loss : 0.091215, loss_ce: 0.007833, loss_dice: 0.174598
[08:41:32.957] TRAIN: iteration 6140 : loss : 0.129336, loss_ce: 0.005915, loss_dice: 0.252758
[08:41:33.193] TRAIN: iteration 6141 : loss : 0.199750, loss_ce: 0.005430, loss_dice: 0.394070
[08:41:33.402] TRAIN: iteration 6142 : loss : 0.252042, loss_ce: 0.005129, loss_dice: 0.498954
[08:41:33.609] TRAIN: iteration 6143 : loss : 0.157181, loss_ce: 0.006417, loss_dice: 0.307946
[08:41:33.816] TRAIN: iteration 6144 : loss : 0.252711, loss_ce: 0.005122, loss_dice: 0.500301
[08:41:34.022] TRAIN: iteration 6145 : loss : 0.184206, loss_ce: 0.006616, loss_dice: 0.361797
[08:41:34.229] TRAIN: iteration 6146 : loss : 0.252750, loss_ce: 0.005194, loss_dice: 0.500305
[08:41:35.125] TRAIN: iteration 6147 : loss : 0.079851, loss_ce: 0.004208, loss_dice: 0.155493
[08:41:35.332] TRAIN: iteration 6148 : loss : 0.253299, loss_ce: 0.006143, loss_dice: 0.500455
[08:41:35.539] TRAIN: iteration 6149 : loss : 0.134058, loss_ce: 0.005283, loss_dice: 0.262833
[08:41:35.746] TRAIN: iteration 6150 : loss : 0.252566, loss_ce: 0.004911, loss_dice: 0.500221
[08:41:35.952] TRAIN: iteration 6151 : loss : 0.248512, loss_ce: 0.004424, loss_dice: 0.492601
[08:41:36.161] TRAIN: iteration 6152 : loss : 0.121059, loss_ce: 0.009553, loss_dice: 0.232566
[08:41:36.369] TRAIN: iteration 6153 : loss : 0.240318, loss_ce: 0.013367, loss_dice: 0.467270
[08:41:36.576] TRAIN: iteration 6154 : loss : 0.050423, loss_ce: 0.002378, loss_dice: 0.098467
[08:41:36.783] TRAIN: iteration 6155 : loss : 0.171506, loss_ce: 0.033211, loss_dice: 0.309802
[08:41:36.993] TRAIN: iteration 6156 : loss : 0.238276, loss_ce: 0.004628, loss_dice: 0.471923
[08:41:37.200] TRAIN: iteration 6157 : loss : 0.252546, loss_ce: 0.004738, loss_dice: 0.500354
[08:41:37.407] TRAIN: iteration 6158 : loss : 0.089063, loss_ce: 0.011570, loss_dice: 0.166556
[08:41:38.378] TRAIN: iteration 6159 : loss : 0.239514, loss_ce: 0.005040, loss_dice: 0.473988
[08:41:38.585] TRAIN: iteration 6160 : loss : 0.122609, loss_ce: 0.008011, loss_dice: 0.237206
[08:41:38.821] TRAIN: iteration 6161 : loss : 0.101411, loss_ce: 0.004726, loss_dice: 0.198096
[08:41:39.035] TRAIN: iteration 6162 : loss : 0.092742, loss_ce: 0.010097, loss_dice: 0.175388
[08:41:39.243] TRAIN: iteration 6163 : loss : 0.253894, loss_ce: 0.007724, loss_dice: 0.500064
[08:41:39.452] TRAIN: iteration 6164 : loss : 0.109455, loss_ce: 0.006504, loss_dice: 0.212407
[08:41:39.667] TRAIN: iteration 6165 : loss : 0.110231, loss_ce: 0.003708, loss_dice: 0.216755
[08:41:39.880] TRAIN: iteration 6166 : loss : 0.227274, loss_ce: 0.005268, loss_dice: 0.449281
[08:41:40.873] TRAIN: iteration 6167 : loss : 0.082843, loss_ce: 0.005729, loss_dice: 0.159958
[08:41:41.081] TRAIN: iteration 6168 : loss : 0.149975, loss_ce: 0.008350, loss_dice: 0.291599
[08:41:41.292] TRAIN: iteration 6169 : loss : 0.095165, loss_ce: 0.003360, loss_dice: 0.186970
[08:41:41.499] TRAIN: iteration 6170 : loss : 0.069951, loss_ce: 0.004343, loss_dice: 0.135558
[08:41:41.711] TRAIN: iteration 6171 : loss : 0.067068, loss_ce: 0.005048, loss_dice: 0.129088
[08:41:41.923] TRAIN: iteration 6172 : loss : 0.252473, loss_ce: 0.004667, loss_dice: 0.500278
[08:41:42.139] TRAIN: iteration 6173 : loss : 0.213639, loss_ce: 0.006282, loss_dice: 0.420996
[08:41:42.346] TRAIN: iteration 6174 : loss : 0.137459, loss_ce: 0.012606, loss_dice: 0.262311
[08:41:44.710] TRAIN: iteration 6175 : loss : 0.102327, loss_ce: 0.005784, loss_dice: 0.198870
[08:41:44.919] TRAIN: iteration 6176 : loss : 0.247835, loss_ce: 0.005066, loss_dice: 0.490603
[08:41:45.133] TRAIN: iteration 6177 : loss : 0.252353, loss_ce: 0.004412, loss_dice: 0.500294
[08:41:45.340] TRAIN: iteration 6178 : loss : 0.253491, loss_ce: 0.006553, loss_dice: 0.500429
[08:41:45.549] TRAIN: iteration 6179 : loss : 0.115551, loss_ce: 0.004081, loss_dice: 0.227021
[08:41:45.757] TRAIN: iteration 6180 : loss : 0.207901, loss_ce: 0.004916, loss_dice: 0.410885
[08:41:45.998] TRAIN: iteration 6181 : loss : 0.187952, loss_ce: 0.005376, loss_dice: 0.370527
[08:41:46.206] TRAIN: iteration 6182 : loss : 0.174804, loss_ce: 0.012292, loss_dice: 0.337316
[08:41:46.413] TRAIN: iteration 6183 : loss : 0.142120, loss_ce: 0.005487, loss_dice: 0.278754
[08:41:46.622] TRAIN: iteration 6184 : loss : 0.075029, loss_ce: 0.006716, loss_dice: 0.143343
[08:41:46.829] TRAIN: iteration 6185 : loss : 0.178664, loss_ce: 0.012155, loss_dice: 0.345173
[08:41:47.039] TRAIN: iteration 6186 : loss : 0.238072, loss_ce: 0.009588, loss_dice: 0.466557
[08:41:47.249] TRAIN: iteration 6187 : loss : 0.093855, loss_ce: 0.009412, loss_dice: 0.178299
[08:41:47.457] TRAIN: iteration 6188 : loss : 0.139501, loss_ce: 0.006635, loss_dice: 0.272367
[08:41:47.666] TRAIN: iteration 6189 : loss : 0.167674, loss_ce: 0.007093, loss_dice: 0.328255
[08:41:47.874] TRAIN: iteration 6190 : loss : 0.251920, loss_ce: 0.005436, loss_dice: 0.498405
[08:41:48.088] TRAIN: iteration 6191 : loss : 0.125641, loss_ce: 0.009434, loss_dice: 0.241849
[08:41:48.294] TRAIN: iteration 6192 : loss : 0.082768, loss_ce: 0.004219, loss_dice: 0.161316
[08:41:48.501] TRAIN: iteration 6193 : loss : 0.148748, loss_ce: 0.007447, loss_dice: 0.290049
[08:41:48.709] TRAIN: iteration 6194 : loss : 0.240358, loss_ce: 0.005299, loss_dice: 0.475418
[08:41:48.916] TRAIN: iteration 6195 : loss : 0.131351, loss_ce: 0.005229, loss_dice: 0.257473
[08:41:49.362] TRAIN: iteration 6196 : loss : 0.078780, loss_ce: 0.004083, loss_dice: 0.153476
[08:41:49.569] TRAIN: iteration 6197 : loss : 0.096373, loss_ce: 0.005354, loss_dice: 0.187392
[08:41:49.776] TRAIN: iteration 6198 : loss : 0.231166, loss_ce: 0.004649, loss_dice: 0.457683
[08:41:49.985] TRAIN: iteration 6199 : loss : 0.147349, loss_ce: 0.009650, loss_dice: 0.285047
[08:41:50.193] TRAIN: iteration 6200 : loss : 0.140206, loss_ce: 0.019447, loss_dice: 0.260965
[08:41:50.435] TRAIN: iteration 6201 : loss : 0.250225, loss_ce: 0.004790, loss_dice: 0.495660
[08:41:50.653] TRAIN: iteration 6202 : loss : 0.121904, loss_ce: 0.006219, loss_dice: 0.237589
[08:41:50.861] TRAIN: iteration 6203 : loss : 0.139373, loss_ce: 0.014860, loss_dice: 0.263887
[08:41:51.071] TRAIN: iteration 6204 : loss : 0.110778, loss_ce: 0.004657, loss_dice: 0.216900
[08:41:51.279] TRAIN: iteration 6205 : loss : 0.231805, loss_ce: 0.006334, loss_dice: 0.457276
[08:41:51.487] TRAIN: iteration 6206 : loss : 0.121437, loss_ce: 0.006088, loss_dice: 0.236785
[08:41:52.334] TRAIN: iteration 6207 : loss : 0.105487, loss_ce: 0.005274, loss_dice: 0.205700
[08:41:52.745] TRAIN: iteration 6208 : loss : 0.169567, loss_ce: 0.005330, loss_dice: 0.333803
[08:41:53.750] TRAIN: iteration 6209 : loss : 0.142470, loss_ce: 0.006465, loss_dice: 0.278476
[08:41:53.957] TRAIN: iteration 6210 : loss : 0.114858, loss_ce: 0.005579, loss_dice: 0.224137
[08:41:54.165] TRAIN: iteration 6211 : loss : 0.084706, loss_ce: 0.003934, loss_dice: 0.165478
[08:41:54.373] TRAIN: iteration 6212 : loss : 0.066336, loss_ce: 0.005366, loss_dice: 0.127306
[08:41:54.584] TRAIN: iteration 6213 : loss : 0.146172, loss_ce: 0.009665, loss_dice: 0.282679
[08:41:54.799] TRAIN: iteration 6214 : loss : 0.252051, loss_ce: 0.003855, loss_dice: 0.500247
[08:41:55.006] TRAIN: iteration 6215 : loss : 0.089571, loss_ce: 0.006446, loss_dice: 0.172697
[08:41:55.212] TRAIN: iteration 6216 : loss : 0.256779, loss_ce: 0.016649, loss_dice: 0.496910
[08:41:56.166] TRAIN: iteration 6217 : loss : 0.146587, loss_ce: 0.013098, loss_dice: 0.280076
[08:41:56.373] TRAIN: iteration 6218 : loss : 0.234995, loss_ce: 0.002517, loss_dice: 0.467473
[08:41:56.579] TRAIN: iteration 6219 : loss : 0.137239, loss_ce: 0.006383, loss_dice: 0.268095
[08:41:56.786] TRAIN: iteration 6220 : loss : 0.090101, loss_ce: 0.007071, loss_dice: 0.173131
[08:41:57.025] TRAIN: iteration 6221 : loss : 0.093607, loss_ce: 0.003025, loss_dice: 0.184188
[08:41:57.233] TRAIN: iteration 6222 : loss : 0.089463, loss_ce: 0.003669, loss_dice: 0.175256
[08:41:57.447] TRAIN: iteration 6223 : loss : 0.172595, loss_ce: 0.015105, loss_dice: 0.330085
[08:41:57.653] TRAIN: iteration 6224 : loss : 0.153488, loss_ce: 0.003248, loss_dice: 0.303729
[08:41:57.863] TRAIN: iteration 6225 : loss : 0.140087, loss_ce: 0.007355, loss_dice: 0.272819
[08:41:58.070] TRAIN: iteration 6226 : loss : 0.161958, loss_ce: 0.010392, loss_dice: 0.313525
[08:41:58.279] TRAIN: iteration 6227 : loss : 0.189392, loss_ce: 0.005399, loss_dice: 0.373386
[08:41:58.491] TRAIN: iteration 6228 : loss : 0.251610, loss_ce: 0.003046, loss_dice: 0.500173
[08:41:58.699] TRAIN: iteration 6229 : loss : 0.080848, loss_ce: 0.005988, loss_dice: 0.155708
[08:41:58.907] TRAIN: iteration 6230 : loss : 0.244280, loss_ce: 0.005299, loss_dice: 0.483261
[08:42:01.339] TRAIN: iteration 6231 : loss : 0.136060, loss_ce: 0.008651, loss_dice: 0.263469
[08:42:01.547] TRAIN: iteration 6232 : loss : 0.240982, loss_ce: 0.005537, loss_dice: 0.476426
[08:42:01.754] TRAIN: iteration 6233 : loss : 0.252805, loss_ce: 0.005229, loss_dice: 0.500380
[08:42:01.961] TRAIN: iteration 6234 : loss : 0.197556, loss_ce: 0.007687, loss_dice: 0.387425
[08:42:02.169] TRAIN: iteration 6235 : loss : 0.156153, loss_ce: 0.005735, loss_dice: 0.306570
[08:42:02.376] TRAIN: iteration 6236 : loss : 0.252713, loss_ce: 0.005422, loss_dice: 0.500004
[08:42:02.592] TRAIN: iteration 6237 : loss : 0.151038, loss_ce: 0.006680, loss_dice: 0.295396
[08:42:02.807] TRAIN: iteration 6238 : loss : 0.252524, loss_ce: 0.004735, loss_dice: 0.500312
[08:42:04.065] TRAIN: iteration 6239 : loss : 0.084120, loss_ce: 0.003835, loss_dice: 0.164405
[08:42:04.271] TRAIN: iteration 6240 : loss : 0.042435, loss_ce: 0.004652, loss_dice: 0.080218
[08:42:04.512] TRAIN: iteration 6241 : loss : 0.107075, loss_ce: 0.006099, loss_dice: 0.208051
[08:42:04.719] TRAIN: iteration 6242 : loss : 0.131056, loss_ce: 0.012368, loss_dice: 0.249743
[08:42:04.928] TRAIN: iteration 6243 : loss : 0.252891, loss_ce: 0.005447, loss_dice: 0.500336
[08:42:05.136] TRAIN: iteration 6244 : loss : 0.150468, loss_ce: 0.004814, loss_dice: 0.296121
[08:42:05.343] TRAIN: iteration 6245 : loss : 0.065718, loss_ce: 0.006911, loss_dice: 0.124525
[08:42:05.552] TRAIN: iteration 6246 : loss : 0.069826, loss_ce: 0.005701, loss_dice: 0.133951
[08:42:05.759] TRAIN: iteration 6247 : loss : 0.252830, loss_ce: 0.005306, loss_dice: 0.500353
[08:42:05.966] TRAIN: iteration 6248 : loss : 0.170872, loss_ce: 0.005476, loss_dice: 0.336268
[08:42:06.460] TRAIN: iteration 6249 : loss : 0.212744, loss_ce: 0.006296, loss_dice: 0.419193
[08:42:06.676] TRAIN: iteration 6250 : loss : 0.247787, loss_ce: 0.006701, loss_dice: 0.488874
[08:42:06.885] TRAIN: iteration 6251 : loss : 0.252332, loss_ce: 0.007975, loss_dice: 0.496689
[08:42:07.096] TRAIN: iteration 6252 : loss : 0.079402, loss_ce: 0.007478, loss_dice: 0.151326
[08:42:07.302] TRAIN: iteration 6253 : loss : 0.262259, loss_ce: 0.024520, loss_dice: 0.499998
[08:42:07.509] TRAIN: iteration 6254 : loss : 0.250675, loss_ce: 0.006036, loss_dice: 0.495314
[08:42:07.716] TRAIN: iteration 6255 : loss : 0.138051, loss_ce: 0.013696, loss_dice: 0.262406
[08:42:07.923] TRAIN: iteration 6256 : loss : 0.056410, loss_ce: 0.007218, loss_dice: 0.105602
[08:42:08.556] TRAIN: iteration 6257 : loss : 0.094426, loss_ce: 0.012556, loss_dice: 0.176297
[08:42:08.762] TRAIN: iteration 6258 : loss : 0.057493, loss_ce: 0.005490, loss_dice: 0.109496
[08:42:08.970] TRAIN: iteration 6259 : loss : 0.247311, loss_ce: 0.006115, loss_dice: 0.488507
[08:42:09.179] TRAIN: iteration 6260 : loss : 0.250792, loss_ce: 0.008205, loss_dice: 0.493380
[08:42:09.414] TRAIN: iteration 6261 : loss : 0.104219, loss_ce: 0.007202, loss_dice: 0.201237
[08:42:09.864] TRAIN: iteration 6262 : loss : 0.080542, loss_ce: 0.005957, loss_dice: 0.155128
[08:42:10.073] TRAIN: iteration 6263 : loss : 0.222648, loss_ce: 0.005816, loss_dice: 0.439479
[08:42:10.281] TRAIN: iteration 6264 : loss : 0.253507, loss_ce: 0.006565, loss_dice: 0.500449
[08:42:10.531] TRAIN: iteration 6265 : loss : 0.036049, loss_ce: 0.003789, loss_dice: 0.068309
[08:42:10.739] TRAIN: iteration 6266 : loss : 0.153856, loss_ce: 0.004649, loss_dice: 0.303062
[08:42:10.946] TRAIN: iteration 6267 : loss : 0.159183, loss_ce: 0.006634, loss_dice: 0.311732
[08:42:11.160] TRAIN: iteration 6268 : loss : 0.184917, loss_ce: 0.006940, loss_dice: 0.362894
[08:42:13.442] TRAIN: iteration 6269 : loss : 0.234050, loss_ce: 0.007858, loss_dice: 0.460243
[08:42:13.657] TRAIN: iteration 6270 : loss : 0.248843, loss_ce: 0.005317, loss_dice: 0.492369
[08:42:13.872] TRAIN: iteration 6271 : loss : 0.083519, loss_ce: 0.004521, loss_dice: 0.162517
[08:42:14.080] TRAIN: iteration 6272 : loss : 0.087004, loss_ce: 0.003969, loss_dice: 0.170038
[08:42:14.288] TRAIN: iteration 6273 : loss : 0.079579, loss_ce: 0.005053, loss_dice: 0.154105
[08:42:14.584] TRAIN: iteration 6274 : loss : 0.064855, loss_ce: 0.004009, loss_dice: 0.125702
[08:42:14.792] TRAIN: iteration 6275 : loss : 0.214634, loss_ce: 0.008765, loss_dice: 0.420502
[08:42:15.002] TRAIN: iteration 6276 : loss : 0.140775, loss_ce: 0.003536, loss_dice: 0.278014
[08:42:15.211] TRAIN: iteration 6277 : loss : 0.178393, loss_ce: 0.004464, loss_dice: 0.352322
[08:42:16.714] TRAIN: iteration 6278 : loss : 0.164099, loss_ce: 0.016196, loss_dice: 0.312001
[08:42:16.925] TRAIN: iteration 6279 : loss : 0.215505, loss_ce: 0.003616, loss_dice: 0.427393
[08:42:17.138] TRAIN: iteration 6280 : loss : 0.164654, loss_ce: 0.010523, loss_dice: 0.318785
[08:42:17.374] TRAIN: iteration 6281 : loss : 0.117695, loss_ce: 0.005267, loss_dice: 0.230123
[08:42:17.583] TRAIN: iteration 6282 : loss : 0.150002, loss_ce: 0.021848, loss_dice: 0.278157
[08:42:17.790] TRAIN: iteration 6283 : loss : 0.090335, loss_ce: 0.003307, loss_dice: 0.177363
[08:42:17.999] TRAIN: iteration 6284 : loss : 0.239563, loss_ce: 0.019306, loss_dice: 0.459820
[08:42:18.207] TRAIN: iteration 6285 : loss : 0.166944, loss_ce: 0.003832, loss_dice: 0.330056
[08:42:18.934] TRAIN: iteration 6286 : loss : 0.132908, loss_ce: 0.005101, loss_dice: 0.260714
[08:42:19.143] TRAIN: iteration 6287 : loss : 0.184776, loss_ce: 0.007061, loss_dice: 0.362490
[08:42:19.352] TRAIN: iteration 6288 : loss : 0.128782, loss_ce: 0.010804, loss_dice: 0.246759
[08:42:19.561] TRAIN: iteration 6289 : loss : 0.136199, loss_ce: 0.006168, loss_dice: 0.266231
[08:42:19.772] TRAIN: iteration 6290 : loss : 0.250988, loss_ce: 0.011216, loss_dice: 0.490760
[08:42:19.982] TRAIN: iteration 6291 : loss : 0.189761, loss_ce: 0.007359, loss_dice: 0.372163
[08:42:20.197] TRAIN: iteration 6292 : loss : 0.198984, loss_ce: 0.009268, loss_dice: 0.388699
[08:42:20.404] TRAIN: iteration 6293 : loss : 0.236651, loss_ce: 0.013616, loss_dice: 0.459685
[08:42:21.611] TRAIN: iteration 6294 : loss : 0.243622, loss_ce: 0.014462, loss_dice: 0.472782
[08:42:21.818] TRAIN: iteration 6295 : loss : 0.190520, loss_ce: 0.012543, loss_dice: 0.368496
[08:42:22.025] TRAIN: iteration 6296 : loss : 0.171408, loss_ce: 0.006058, loss_dice: 0.336758
[08:42:22.233] TRAIN: iteration 6297 : loss : 0.253690, loss_ce: 0.006963, loss_dice: 0.500417
[08:42:22.440] TRAIN: iteration 6298 : loss : 0.064926, loss_ce: 0.006576, loss_dice: 0.123277
[08:42:22.650] TRAIN: iteration 6299 : loss : 0.123487, loss_ce: 0.007821, loss_dice: 0.239153
[08:42:22.859] TRAIN: iteration 6300 : loss : 0.214199, loss_ce: 0.009096, loss_dice: 0.419302
[08:42:23.108] TRAIN: iteration 6301 : loss : 0.241908, loss_ce: 0.007425, loss_dice: 0.476391
[08:42:24.657] TRAIN: iteration 6302 : loss : 0.253781, loss_ce: 0.007156, loss_dice: 0.500405
[08:42:24.865] TRAIN: iteration 6303 : loss : 0.253460, loss_ce: 0.006600, loss_dice: 0.500320
[08:42:25.073] TRAIN: iteration 6304 : loss : 0.202336, loss_ce: 0.006687, loss_dice: 0.397984
[08:42:25.281] TRAIN: iteration 6305 : loss : 0.084706, loss_ce: 0.006709, loss_dice: 0.162704
[08:42:25.488] TRAIN: iteration 6306 : loss : 0.134521, loss_ce: 0.007442, loss_dice: 0.261601
[08:42:25.695] TRAIN: iteration 6307 : loss : 0.253460, loss_ce: 0.006535, loss_dice: 0.500384
[08:42:25.902] TRAIN: iteration 6308 : loss : 0.054209, loss_ce: 0.004000, loss_dice: 0.104417
[08:42:26.108] TRAIN: iteration 6309 : loss : 0.236030, loss_ce: 0.008378, loss_dice: 0.463682
[08:42:27.750] TRAIN: iteration 6310 : loss : 0.255742, loss_ce: 0.011336, loss_dice: 0.500149
[08:42:27.957] TRAIN: iteration 6311 : loss : 0.227833, loss_ce: 0.005275, loss_dice: 0.450391
[08:42:28.171] TRAIN: iteration 6312 : loss : 0.251331, loss_ce: 0.006401, loss_dice: 0.496261
[08:42:28.385] TRAIN: iteration 6313 : loss : 0.079910, loss_ce: 0.005094, loss_dice: 0.154726
[08:42:28.670] TRAIN: iteration 6314 : loss : 0.146024, loss_ce: 0.005327, loss_dice: 0.286721
[08:42:28.879] TRAIN: iteration 6315 : loss : 0.027277, loss_ce: 0.002529, loss_dice: 0.052026
[08:42:29.086] TRAIN: iteration 6316 : loss : 0.122128, loss_ce: 0.003782, loss_dice: 0.240474
[08:42:29.297] TRAIN: iteration 6317 : loss : 0.144768, loss_ce: 0.003573, loss_dice: 0.285963
[08:42:30.428] TRAIN: iteration 6318 : loss : 0.145219, loss_ce: 0.002899, loss_dice: 0.287539
[08:42:30.635] TRAIN: iteration 6319 : loss : 0.155683, loss_ce: 0.007900, loss_dice: 0.303466
[08:42:30.848] TRAIN: iteration 6320 : loss : 0.213539, loss_ce: 0.025344, loss_dice: 0.401735
[08:42:31.085] TRAIN: iteration 6321 : loss : 0.250856, loss_ce: 0.001688, loss_dice: 0.500024
[08:42:31.297] TRAIN: iteration 6322 : loss : 0.247171, loss_ce: 0.001866, loss_dice: 0.492476
[08:42:31.506] TRAIN: iteration 6323 : loss : 0.107141, loss_ce: 0.004049, loss_dice: 0.210234
[08:42:31.716] TRAIN: iteration 6324 : loss : 0.063219, loss_ce: 0.002517, loss_dice: 0.123921
[08:42:31.923] TRAIN: iteration 6325 : loss : 0.172864, loss_ce: 0.003057, loss_dice: 0.342671
[08:42:35.771] TRAIN: iteration 6326 : loss : 0.210501, loss_ce: 0.006024, loss_dice: 0.414979
[08:42:35.991] TRAIN: iteration 6327 : loss : 0.206030, loss_ce: 0.005343, loss_dice: 0.406716
[08:42:36.199] TRAIN: iteration 6328 : loss : 0.251245, loss_ce: 0.002385, loss_dice: 0.500105
[08:42:36.407] TRAIN: iteration 6329 : loss : 0.119028, loss_ce: 0.005859, loss_dice: 0.232197
[08:42:36.645] TRAIN: iteration 6330 : loss : 0.110398, loss_ce: 0.006424, loss_dice: 0.214372
[08:42:36.852] TRAIN: iteration 6331 : loss : 0.071063, loss_ce: 0.005454, loss_dice: 0.136672
[08:42:37.060] TRAIN: iteration 6332 : loss : 0.251887, loss_ce: 0.003546, loss_dice: 0.500228
[08:42:37.275] TRAIN: iteration 6333 : loss : 0.260178, loss_ce: 0.021067, loss_dice: 0.499289
[08:42:38.976] TRAIN: iteration 6334 : loss : 0.128434, loss_ce: 0.008506, loss_dice: 0.248361
[08:42:39.335] TRAIN: iteration 6335 : loss : 0.253054, loss_ce: 0.005652, loss_dice: 0.500456
[08:42:39.542] TRAIN: iteration 6336 : loss : 0.167550, loss_ce: 0.008924, loss_dice: 0.326176
[08:42:39.749] TRAIN: iteration 6337 : loss : 0.167826, loss_ce: 0.011625, loss_dice: 0.324028
[08:42:39.962] TRAIN: iteration 6338 : loss : 0.197325, loss_ce: 0.045788, loss_dice: 0.348861
[08:42:40.169] TRAIN: iteration 6339 : loss : 0.237933, loss_ce: 0.006348, loss_dice: 0.469518
[08:42:40.376] TRAIN: iteration 6340 : loss : 0.213063, loss_ce: 0.015826, loss_dice: 0.410300
[08:42:40.616] TRAIN: iteration 6341 : loss : 0.212531, loss_ce: 0.007343, loss_dice: 0.417720
[08:42:43.042] TRAIN: iteration 6342 : loss : 0.205798, loss_ce: 0.009914, loss_dice: 0.401681
[08:42:43.256] TRAIN: iteration 6343 : loss : 0.197263, loss_ce: 0.010197, loss_dice: 0.384329
[08:42:43.462] TRAIN: iteration 6344 : loss : 0.238120, loss_ce: 0.009142, loss_dice: 0.467098
[08:42:43.669] TRAIN: iteration 6345 : loss : 0.075271, loss_ce: 0.005884, loss_dice: 0.144657
[08:42:43.876] TRAIN: iteration 6346 : loss : 0.149807, loss_ce: 0.006696, loss_dice: 0.292918
[08:42:44.082] TRAIN: iteration 6347 : loss : 0.140595, loss_ce: 0.006391, loss_dice: 0.274800
[08:42:44.289] TRAIN: iteration 6348 : loss : 0.180757, loss_ce: 0.009591, loss_dice: 0.351923
[08:42:44.498] TRAIN: iteration 6349 : loss : 0.217291, loss_ce: 0.012746, loss_dice: 0.421837
[08:42:47.408] TRAIN: iteration 6350 : loss : 0.152045, loss_ce: 0.005820, loss_dice: 0.298270
[08:42:47.617] TRAIN: iteration 6351 : loss : 0.118094, loss_ce: 0.006524, loss_dice: 0.229665
[08:42:47.825] TRAIN: iteration 6352 : loss : 0.252759, loss_ce: 0.006288, loss_dice: 0.499231
[08:42:48.036] TRAIN: iteration 6353 : loss : 0.131903, loss_ce: 0.005685, loss_dice: 0.258121
[08:42:48.249] TRAIN: iteration 6354 : loss : 0.098215, loss_ce: 0.004521, loss_dice: 0.191909
[08:42:48.457] TRAIN: iteration 6355 : loss : 0.173192, loss_ce: 0.005748, loss_dice: 0.340635
[08:42:48.664] TRAIN: iteration 6356 : loss : 0.162659, loss_ce: 0.016399, loss_dice: 0.308919
[08:42:48.871] TRAIN: iteration 6357 : loss : 0.251922, loss_ce: 0.003618, loss_dice: 0.500226
[08:42:49.716] TRAIN: iteration 6358 : loss : 0.251672, loss_ce: 0.003336, loss_dice: 0.500008
[08:42:50.018] TRAIN: iteration 6359 : loss : 0.192410, loss_ce: 0.007599, loss_dice: 0.377221
[08:42:50.226] TRAIN: iteration 6360 : loss : 0.191283, loss_ce: 0.004065, loss_dice: 0.378502
[08:42:50.463] TRAIN: iteration 6361 : loss : 0.165819, loss_ce: 0.023395, loss_dice: 0.308243
[08:42:50.672] TRAIN: iteration 6362 : loss : 0.138240, loss_ce: 0.003981, loss_dice: 0.272499
[08:42:50.880] TRAIN: iteration 6363 : loss : 0.224980, loss_ce: 0.007957, loss_dice: 0.442003
[08:42:51.089] TRAIN: iteration 6364 : loss : 0.230443, loss_ce: 0.010349, loss_dice: 0.450537
[08:42:51.297] TRAIN: iteration 6365 : loss : 0.141048, loss_ce: 0.009584, loss_dice: 0.272511
[08:42:54.001] TRAIN: iteration 6366 : loss : 0.249842, loss_ce: 0.006347, loss_dice: 0.493336
[08:42:54.208] TRAIN: iteration 6367 : loss : 0.065205, loss_ce: 0.004069, loss_dice: 0.126340
[08:42:54.415] TRAIN: iteration 6368 : loss : 0.102154, loss_ce: 0.004928, loss_dice: 0.199379
[08:42:54.623] TRAIN: iteration 6369 : loss : 0.153874, loss_ce: 0.004844, loss_dice: 0.302905
[08:42:54.829] TRAIN: iteration 6370 : loss : 0.178021, loss_ce: 0.014854, loss_dice: 0.341187
[08:42:55.039] TRAIN: iteration 6371 : loss : 0.228062, loss_ce: 0.006897, loss_dice: 0.449227
[08:42:55.246] TRAIN: iteration 6372 : loss : 0.198198, loss_ce: 0.008500, loss_dice: 0.387895
[08:42:55.453] TRAIN: iteration 6373 : loss : 0.255917, loss_ce: 0.021524, loss_dice: 0.490310
[08:42:57.237] TRAIN: iteration 6374 : loss : 0.232014, loss_ce: 0.005711, loss_dice: 0.458317
[08:42:57.444] TRAIN: iteration 6375 : loss : 0.227192, loss_ce: 0.013487, loss_dice: 0.440898
[08:42:57.652] TRAIN: iteration 6376 : loss : 0.109725, loss_ce: 0.014741, loss_dice: 0.204709
[08:42:57.858] TRAIN: iteration 6377 : loss : 0.188064, loss_ce: 0.009733, loss_dice: 0.366395
[08:42:58.065] TRAIN: iteration 6378 : loss : 0.117620, loss_ce: 0.006200, loss_dice: 0.229040
[08:42:58.271] TRAIN: iteration 6379 : loss : 0.082107, loss_ce: 0.007039, loss_dice: 0.157175
[08:42:58.479] TRAIN: iteration 6380 : loss : 0.233202, loss_ce: 0.007828, loss_dice: 0.458576
[08:42:58.716] TRAIN: iteration 6381 : loss : 0.078885, loss_ce: 0.005267, loss_dice: 0.152502
[08:42:59.361] TRAIN: iteration 6382 : loss : 0.180836, loss_ce: 0.008130, loss_dice: 0.353541
[08:42:59.573] TRAIN: iteration 6383 : loss : 0.222032, loss_ce: 0.007513, loss_dice: 0.436551
[08:42:59.780] TRAIN: iteration 6384 : loss : 0.211002, loss_ce: 0.018744, loss_dice: 0.403260
[08:42:59.988] TRAIN: iteration 6385 : loss : 0.230184, loss_ce: 0.012409, loss_dice: 0.447959
[08:43:00.199] TRAIN: iteration 6386 : loss : 0.253780, loss_ce: 0.007092, loss_dice: 0.500468
[08:43:00.410] TRAIN: iteration 6387 : loss : 0.086652, loss_ce: 0.006184, loss_dice: 0.167121
[08:43:00.618] TRAIN: iteration 6388 : loss : 0.140488, loss_ce: 0.006332, loss_dice: 0.274644
[08:43:00.824] TRAIN: iteration 6389 : loss : 0.243417, loss_ce: 0.006848, loss_dice: 0.479987
[08:43:03.323] TRAIN: iteration 6390 : loss : 0.200305, loss_ce: 0.006012, loss_dice: 0.394597
[08:43:03.533] TRAIN: iteration 6391 : loss : 0.203924, loss_ce: 0.005930, loss_dice: 0.401918
[08:43:03.747] TRAIN: iteration 6392 : loss : 0.204502, loss_ce: 0.005600, loss_dice: 0.403405
[08:43:03.953] TRAIN: iteration 6393 : loss : 0.245938, loss_ce: 0.005613, loss_dice: 0.486262
[08:43:04.160] TRAIN: iteration 6394 : loss : 0.155039, loss_ce: 0.005095, loss_dice: 0.304983
[08:43:04.368] TRAIN: iteration 6395 : loss : 0.252152, loss_ce: 0.004081, loss_dice: 0.500222
[08:43:04.575] TRAIN: iteration 6396 : loss : 0.128925, loss_ce: 0.005820, loss_dice: 0.252029
[08:43:04.923] TRAIN: iteration 6397 : loss : 0.187897, loss_ce: 0.003305, loss_dice: 0.372489
[08:43:05.999] TRAIN: iteration 6398 : loss : 0.190009, loss_ce: 0.005089, loss_dice: 0.374929
[08:43:06.207] TRAIN: iteration 6399 : loss : 0.204919, loss_ce: 0.011824, loss_dice: 0.398013
[08:43:06.414] TRAIN: iteration 6400 : loss : 0.128667, loss_ce: 0.010139, loss_dice: 0.247194
[08:43:06.415] NaN or Inf found in input tensor.
[08:43:06.824] TRAIN: iteration 6401 : loss : 0.145759, loss_ce: 0.005211, loss_dice: 0.286306
[08:43:07.031] TRAIN: iteration 6402 : loss : 0.239540, loss_ce: 0.005947, loss_dice: 0.473133
[08:43:07.239] TRAIN: iteration 6403 : loss : 0.093813, loss_ce: 0.002811, loss_dice: 0.184814
[08:43:07.447] TRAIN: iteration 6404 : loss : 0.251772, loss_ce: 0.003327, loss_dice: 0.500217
[08:43:08.664] TRAIN: iteration 6405 : loss : 0.199680, loss_ce: 0.003563, loss_dice: 0.395796
[08:43:09.087] TRAIN: iteration 6406 : loss : 0.210060, loss_ce: 0.010621, loss_dice: 0.409500
[08:43:09.296] TRAIN: iteration 6407 : loss : 0.184987, loss_ce: 0.005975, loss_dice: 0.363998
[08:43:09.506] TRAIN: iteration 6408 : loss : 0.099678, loss_ce: 0.005911, loss_dice: 0.193446
[08:43:10.710] TRAIN: iteration 6409 : loss : 0.133003, loss_ce: 0.007025, loss_dice: 0.258981
[08:43:10.919] TRAIN: iteration 6410 : loss : 0.239676, loss_ce: 0.038818, loss_dice: 0.440534
[08:43:11.126] TRAIN: iteration 6411 : loss : 0.254644, loss_ce: 0.009592, loss_dice: 0.499696
[08:43:11.341] TRAIN: iteration 6412 : loss : 0.223107, loss_ce: 0.006471, loss_dice: 0.439743
[08:43:13.292] TRAIN: iteration 6413 : loss : 0.092327, loss_ce: 0.003718, loss_dice: 0.180937
[08:43:13.500] TRAIN: iteration 6414 : loss : 0.083320, loss_ce: 0.006876, loss_dice: 0.159764
[08:43:13.707] TRAIN: iteration 6415 : loss : 0.212992, loss_ce: 0.013500, loss_dice: 0.412484
[08:43:13.916] TRAIN: iteration 6416 : loss : 0.225635, loss_ce: 0.006177, loss_dice: 0.445094
[08:43:14.123] TRAIN: iteration 6417 : loss : 0.243697, loss_ce: 0.006785, loss_dice: 0.480610
[08:43:14.330] TRAIN: iteration 6418 : loss : 0.100993, loss_ce: 0.009213, loss_dice: 0.192773
[08:43:14.538] TRAIN: iteration 6419 : loss : 0.251769, loss_ce: 0.008614, loss_dice: 0.494925
[08:43:15.969] TRAIN: iteration 6420 : loss : 0.094101, loss_ce: 0.004146, loss_dice: 0.184056
[08:43:16.542] TRAIN: iteration 6421 : loss : 0.205972, loss_ce: 0.008750, loss_dice: 0.403194
[08:43:16.751] TRAIN: iteration 6422 : loss : 0.209083, loss_ce: 0.008067, loss_dice: 0.410099
[08:43:16.959] TRAIN: iteration 6423 : loss : 0.031469, loss_ce: 0.002964, loss_dice: 0.059974
[08:43:17.168] TRAIN: iteration 6424 : loss : 0.252634, loss_ce: 0.005970, loss_dice: 0.499298
[08:43:17.380] TRAIN: iteration 6425 : loss : 0.252947, loss_ce: 0.005516, loss_dice: 0.500377
[08:43:17.588] TRAIN: iteration 6426 : loss : 0.065091, loss_ce: 0.006032, loss_dice: 0.124151
[08:43:17.796] TRAIN: iteration 6427 : loss : 0.112993, loss_ce: 0.011613, loss_dice: 0.214374
[08:43:20.950] TRAIN: iteration 6428 : loss : 0.242663, loss_ce: 0.005740, loss_dice: 0.479585
[08:43:21.158] TRAIN: iteration 6429 : loss : 0.228437, loss_ce: 0.009281, loss_dice: 0.447594
[08:43:21.366] TRAIN: iteration 6430 : loss : 0.253429, loss_ce: 0.006396, loss_dice: 0.500461
[08:43:21.574] TRAIN: iteration 6431 : loss : 0.131253, loss_ce: 0.005197, loss_dice: 0.257308
[08:43:21.782] TRAIN: iteration 6432 : loss : 0.249925, loss_ce: 0.009842, loss_dice: 0.490008
[08:43:21.990] TRAIN: iteration 6433 : loss : 0.106326, loss_ce: 0.010928, loss_dice: 0.201724
[08:43:22.198] TRAIN: iteration 6434 : loss : 0.253044, loss_ce: 0.005721, loss_dice: 0.500368
[08:43:22.409] TRAIN: iteration 6435 : loss : 0.144805, loss_ce: 0.003711, loss_dice: 0.285900
[08:43:23.398] TRAIN: iteration 6436 : loss : 0.080160, loss_ce: 0.003918, loss_dice: 0.156402
[08:43:23.606] TRAIN: iteration 6437 : loss : 0.136777, loss_ce: 0.023912, loss_dice: 0.249641
[08:43:23.818] TRAIN: iteration 6438 : loss : 0.091157, loss_ce: 0.008396, loss_dice: 0.173918
[08:43:24.026] TRAIN: iteration 6439 : loss : 0.184830, loss_ce: 0.005921, loss_dice: 0.363738
[08:43:24.237] TRAIN: iteration 6440 : loss : 0.209634, loss_ce: 0.006410, loss_dice: 0.412859
[08:43:24.475] TRAIN: iteration 6441 : loss : 0.222302, loss_ce: 0.044452, loss_dice: 0.400152
[08:43:24.684] TRAIN: iteration 6442 : loss : 0.058252, loss_ce: 0.004129, loss_dice: 0.112376
[08:43:24.893] TRAIN: iteration 6443 : loss : 0.073797, loss_ce: 0.005248, loss_dice: 0.142345
[08:43:26.945] TRAIN: iteration 6444 : loss : 0.252794, loss_ce: 0.005286, loss_dice: 0.500302
[08:43:27.154] TRAIN: iteration 6445 : loss : 0.095591, loss_ce: 0.004876, loss_dice: 0.186306
[08:43:27.362] TRAIN: iteration 6446 : loss : 0.154616, loss_ce: 0.006073, loss_dice: 0.303158
[08:43:27.571] TRAIN: iteration 6447 : loss : 0.130811, loss_ce: 0.005990, loss_dice: 0.255633
[08:43:29.049] TRAIN: iteration 6448 : loss : 0.169778, loss_ce: 0.005875, loss_dice: 0.333681
[08:43:29.257] TRAIN: iteration 6449 : loss : 0.122115, loss_ce: 0.003883, loss_dice: 0.240348
[08:43:29.466] TRAIN: iteration 6450 : loss : 0.146343, loss_ce: 0.004166, loss_dice: 0.288520
[08:43:29.673] TRAIN: iteration 6451 : loss : 0.244778, loss_ce: 0.005300, loss_dice: 0.484257
[08:43:30.233] TRAIN: iteration 6452 : loss : 0.120979, loss_ce: 0.003070, loss_dice: 0.238888
[08:43:30.441] TRAIN: iteration 6453 : loss : 0.245899, loss_ce: 0.006079, loss_dice: 0.485720
[08:43:30.650] TRAIN: iteration 6454 : loss : 0.128937, loss_ce: 0.011445, loss_dice: 0.246430
[08:43:30.860] TRAIN: iteration 6455 : loss : 0.231723, loss_ce: 0.004043, loss_dice: 0.459402
[08:43:34.605] TRAIN: iteration 6456 : loss : 0.244225, loss_ce: 0.007052, loss_dice: 0.481398
[08:43:34.821] TRAIN: iteration 6457 : loss : 0.083890, loss_ce: 0.002626, loss_dice: 0.165154
[08:43:35.119] TRAIN: iteration 6458 : loss : 0.176662, loss_ce: 0.006494, loss_dice: 0.346829
[08:43:35.327] TRAIN: iteration 6459 : loss : 0.127249, loss_ce: 0.002417, loss_dice: 0.252080
[08:43:35.535] TRAIN: iteration 6460 : loss : 0.251485, loss_ce: 0.002825, loss_dice: 0.500145
[08:43:35.772] TRAIN: iteration 6461 : loss : 0.158071, loss_ce: 0.006486, loss_dice: 0.309655
[08:43:35.980] TRAIN: iteration 6462 : loss : 0.251483, loss_ce: 0.002843, loss_dice: 0.500122
[08:43:36.189] TRAIN: iteration 6463 : loss : 0.179951, loss_ce: 0.005666, loss_dice: 0.354237
[08:43:36.610] TRAIN: iteration 6464 : loss : 0.167283, loss_ce: 0.019010, loss_dice: 0.315556
[08:43:36.818] TRAIN: iteration 6465 : loss : 0.152884, loss_ce: 0.017303, loss_dice: 0.288464
[08:43:37.025] TRAIN: iteration 6466 : loss : 0.108930, loss_ce: 0.003235, loss_dice: 0.214625
[08:43:37.232] TRAIN: iteration 6467 : loss : 0.126434, loss_ce: 0.006853, loss_dice: 0.246015
[08:43:37.439] TRAIN: iteration 6468 : loss : 0.060608, loss_ce: 0.002849, loss_dice: 0.118368
[08:43:37.647] TRAIN: iteration 6469 : loss : 0.125595, loss_ce: 0.003517, loss_dice: 0.247672
[08:43:37.854] TRAIN: iteration 6470 : loss : 0.041901, loss_ce: 0.002715, loss_dice: 0.081086
[08:43:38.062] TRAIN: iteration 6471 : loss : 0.228588, loss_ce: 0.005008, loss_dice: 0.452168
[08:43:38.269] TRAIN: iteration 6472 : loss : 0.063825, loss_ce: 0.003727, loss_dice: 0.123922
[08:43:38.476] TRAIN: iteration 6473 : loss : 0.123865, loss_ce: 0.005018, loss_dice: 0.242713
[08:43:38.685] TRAIN: iteration 6474 : loss : 0.207658, loss_ce: 0.004664, loss_dice: 0.410652
[08:43:38.893] TRAIN: iteration 6475 : loss : 0.069058, loss_ce: 0.004761, loss_dice: 0.133354
[08:43:40.160] TRAIN: iteration 6476 : loss : 0.121462, loss_ce: 0.003143, loss_dice: 0.239781
[08:43:40.368] TRAIN: iteration 6477 : loss : 0.252091, loss_ce: 0.003911, loss_dice: 0.500271
[08:43:41.169] TRAIN: iteration 6478 : loss : 0.086943, loss_ce: 0.003099, loss_dice: 0.170788
[08:43:41.377] TRAIN: iteration 6479 : loss : 0.101183, loss_ce: 0.004087, loss_dice: 0.198279
[08:43:41.587] TRAIN: iteration 6480 : loss : 0.249592, loss_ce: 0.005867, loss_dice: 0.493317
[08:43:41.830] TRAIN: iteration 6481 : loss : 0.166600, loss_ce: 0.007969, loss_dice: 0.325231
[08:43:42.041] TRAIN: iteration 6482 : loss : 0.250494, loss_ce: 0.003196, loss_dice: 0.497793
[08:43:42.249] TRAIN: iteration 6483 : loss : 0.122825, loss_ce: 0.003896, loss_dice: 0.241753
[08:43:45.343] TRAIN: iteration 6484 : loss : 0.092359, loss_ce: 0.005308, loss_dice: 0.179409
[08:43:45.550] TRAIN: iteration 6485 : loss : 0.124224, loss_ce: 0.009030, loss_dice: 0.239418
[08:43:45.757] TRAIN: iteration 6486 : loss : 0.251959, loss_ce: 0.003668, loss_dice: 0.500251
[08:43:45.965] TRAIN: iteration 6487 : loss : 0.048265, loss_ce: 0.002973, loss_dice: 0.093557
[08:43:46.174] TRAIN: iteration 6488 : loss : 0.159594, loss_ce: 0.007571, loss_dice: 0.311618
[08:43:46.381] TRAIN: iteration 6489 : loss : 0.148613, loss_ce: 0.025237, loss_dice: 0.271990
[08:43:46.587] TRAIN: iteration 6490 : loss : 0.249935, loss_ce: 0.005163, loss_dice: 0.494708
[08:43:46.795] TRAIN: iteration 6491 : loss : 0.224013, loss_ce: 0.004828, loss_dice: 0.443198
[08:43:50.770] TRAIN: iteration 6492 : loss : 0.058309, loss_ce: 0.003943, loss_dice: 0.112675
[08:43:50.979] TRAIN: iteration 6493 : loss : 0.064282, loss_ce: 0.005052, loss_dice: 0.123512
[08:43:51.189] TRAIN: iteration 6494 : loss : 0.134509, loss_ce: 0.005762, loss_dice: 0.263256
[08:43:51.397] TRAIN: iteration 6495 : loss : 0.244186, loss_ce: 0.010641, loss_dice: 0.477730
[08:43:51.606] TRAIN: iteration 6496 : loss : 0.121671, loss_ce: 0.004334, loss_dice: 0.239008
[08:43:51.814] TRAIN: iteration 6497 : loss : 0.121792, loss_ce: 0.008817, loss_dice: 0.234767
[08:43:52.026] TRAIN: iteration 6498 : loss : 0.162387, loss_ce: 0.007381, loss_dice: 0.317394
[08:43:52.235] TRAIN: iteration 6499 : loss : 0.111316, loss_ce: 0.009728, loss_dice: 0.212904
[08:43:53.349] TRAIN: iteration 6500 : loss : 0.211853, loss_ce: 0.004952, loss_dice: 0.418753
[08:43:53.580] TRAIN: iteration 6501 : loss : 0.135057, loss_ce: 0.028051, loss_dice: 0.242063
[08:43:53.788] TRAIN: iteration 6502 : loss : 0.253139, loss_ce: 0.005986, loss_dice: 0.500292
[08:43:53.998] TRAIN: iteration 6503 : loss : 0.204448, loss_ce: 0.005694, loss_dice: 0.403201
[08:43:54.207] TRAIN: iteration 6504 : loss : 0.145716, loss_ce: 0.006227, loss_dice: 0.285206
[08:43:54.417] TRAIN: iteration 6505 : loss : 0.120267, loss_ce: 0.004879, loss_dice: 0.235654
[08:43:54.628] TRAIN: iteration 6506 : loss : 0.257853, loss_ce: 0.019191, loss_dice: 0.496516
[08:43:54.840] TRAIN: iteration 6507 : loss : 0.252692, loss_ce: 0.005015, loss_dice: 0.500370
[08:43:56.671] TRAIN: iteration 6508 : loss : 0.158082, loss_ce: 0.004506, loss_dice: 0.311659
[08:43:56.878] TRAIN: iteration 6509 : loss : 0.236575, loss_ce: 0.015303, loss_dice: 0.457847
[08:43:57.086] TRAIN: iteration 6510 : loss : 0.238584, loss_ce: 0.003996, loss_dice: 0.473171
[08:43:57.295] TRAIN: iteration 6511 : loss : 0.125478, loss_ce: 0.015107, loss_dice: 0.235849
[08:43:57.502] TRAIN: iteration 6512 : loss : 0.165423, loss_ce: 0.006446, loss_dice: 0.324400
[08:43:57.709] TRAIN: iteration 6513 : loss : 0.252375, loss_ce: 0.004420, loss_dice: 0.500330
[08:43:57.920] TRAIN: iteration 6514 : loss : 0.251887, loss_ce: 0.003608, loss_dice: 0.500166
[08:43:58.130] TRAIN: iteration 6515 : loss : 0.123666, loss_ce: 0.005530, loss_dice: 0.241803
[08:44:00.588] TRAIN: iteration 6516 : loss : 0.207481, loss_ce: 0.005733, loss_dice: 0.409229
[08:44:00.796] TRAIN: iteration 6517 : loss : 0.249141, loss_ce: 0.006157, loss_dice: 0.492125
[08:44:01.005] TRAIN: iteration 6518 : loss : 0.144242, loss_ce: 0.004487, loss_dice: 0.283998
[08:44:03.780] TRAIN: iteration 6519 : loss : 0.205116, loss_ce: 0.004279, loss_dice: 0.405952
[08:44:03.994] TRAIN: iteration 6520 : loss : 0.100734, loss_ce: 0.003510, loss_dice: 0.197958
[08:44:04.232] TRAIN: iteration 6521 : loss : 0.251443, loss_ce: 0.002760, loss_dice: 0.500126
[08:44:04.440] TRAIN: iteration 6522 : loss : 0.252052, loss_ce: 0.003848, loss_dice: 0.500256
[08:44:04.647] TRAIN: iteration 6523 : loss : 0.141129, loss_ce: 0.006498, loss_dice: 0.275760
[08:44:04.856] TRAIN: iteration 6524 : loss : 0.174409, loss_ce: 0.006851, loss_dice: 0.341968
[08:44:05.064] TRAIN: iteration 6525 : loss : 0.114533, loss_ce: 0.003861, loss_dice: 0.225205
[08:44:05.272] TRAIN: iteration 6526 : loss : 0.021740, loss_ce: 0.001891, loss_dice: 0.041589
[08:44:07.358] TRAIN: iteration 6527 : loss : 0.077670, loss_ce: 0.003661, loss_dice: 0.151679
[08:44:07.572] TRAIN: iteration 6528 : loss : 0.150249, loss_ce: 0.008024, loss_dice: 0.292473
[08:44:07.810] TRAIN: iteration 6529 : loss : 0.185279, loss_ce: 0.003602, loss_dice: 0.366956
[08:44:08.020] TRAIN: iteration 6530 : loss : 0.251721, loss_ce: 0.003255, loss_dice: 0.500186
[08:44:08.233] TRAIN: iteration 6531 : loss : 0.129822, loss_ce: 0.006081, loss_dice: 0.253564
[08:44:08.443] TRAIN: iteration 6532 : loss : 0.145464, loss_ce: 0.007568, loss_dice: 0.283359
[08:44:08.656] TRAIN: iteration 6533 : loss : 0.237949, loss_ce: 0.004985, loss_dice: 0.470913
[08:44:08.863] TRAIN: iteration 6534 : loss : 0.251458, loss_ce: 0.002799, loss_dice: 0.500117
[08:44:10.560] TRAIN: iteration 6535 : loss : 0.050648, loss_ce: 0.002315, loss_dice: 0.098980
[08:44:10.767] TRAIN: iteration 6536 : loss : 0.089847, loss_ce: 0.004027, loss_dice: 0.175666
[08:44:10.977] TRAIN: iteration 6537 : loss : 0.127334, loss_ce: 0.004152, loss_dice: 0.250516
[08:44:11.189] TRAIN: iteration 6538 : loss : 0.237292, loss_ce: 0.002506, loss_dice: 0.472078
[08:44:11.398] TRAIN: iteration 6539 : loss : 0.202042, loss_ce: 0.007551, loss_dice: 0.396532
[08:44:11.609] TRAIN: iteration 6540 : loss : 0.104826, loss_ce: 0.002881, loss_dice: 0.206771
[08:44:11.845] TRAIN: iteration 6541 : loss : 0.251482, loss_ce: 0.002837, loss_dice: 0.500128
[08:44:12.052] TRAIN: iteration 6542 : loss : 0.190135, loss_ce: 0.002546, loss_dice: 0.377725
[08:44:16.574] TRAIN: iteration 6543 : loss : 0.206620, loss_ce: 0.006859, loss_dice: 0.406382
[08:44:16.786] TRAIN: iteration 6544 : loss : 0.251474, loss_ce: 0.002822, loss_dice: 0.500126
[08:44:16.994] TRAIN: iteration 6545 : loss : 0.210872, loss_ce: 0.007692, loss_dice: 0.414053
[08:44:17.201] TRAIN: iteration 6546 : loss : 0.183679, loss_ce: 0.003599, loss_dice: 0.363759
[08:44:17.411] TRAIN: iteration 6547 : loss : 0.226021, loss_ce: 0.002495, loss_dice: 0.449547
[08:44:17.619] TRAIN: iteration 6548 : loss : 0.221436, loss_ce: 0.028456, loss_dice: 0.414415
[08:44:17.838] TRAIN: iteration 6549 : loss : 0.081786, loss_ce: 0.005044, loss_dice: 0.158528
[08:44:18.046] TRAIN: iteration 6550 : loss : 0.243776, loss_ce: 0.002593, loss_dice: 0.484959
[08:44:21.698] TRAIN: iteration 6551 : loss : 0.150054, loss_ce: 0.009083, loss_dice: 0.291025
[08:44:21.905] TRAIN: iteration 6552 : loss : 0.101944, loss_ce: 0.003395, loss_dice: 0.200493
[08:44:22.114] TRAIN: iteration 6553 : loss : 0.253152, loss_ce: 0.009010, loss_dice: 0.497293
[08:44:22.321] TRAIN: iteration 6554 : loss : 0.243030, loss_ce: 0.004414, loss_dice: 0.481646
[08:44:22.527] TRAIN: iteration 6555 : loss : 0.215467, loss_ce: 0.005457, loss_dice: 0.425476
[08:44:22.735] TRAIN: iteration 6556 : loss : 0.128762, loss_ce: 0.003893, loss_dice: 0.253631
[08:44:22.942] TRAIN: iteration 6557 : loss : 0.245430, loss_ce: 0.005852, loss_dice: 0.485009
[08:44:23.151] TRAIN: iteration 6558 : loss : 0.156768, loss_ce: 0.004800, loss_dice: 0.308737
[08:44:24.569] TRAIN: iteration 6559 : loss : 0.250184, loss_ce: 0.005016, loss_dice: 0.495352
[08:44:24.777] TRAIN: iteration 6560 : loss : 0.048340, loss_ce: 0.002706, loss_dice: 0.093974
[08:44:25.017] TRAIN: iteration 6561 : loss : 0.085202, loss_ce: 0.004603, loss_dice: 0.165802
[08:44:26.340] TRAIN: iteration 6562 : loss : 0.078199, loss_ce: 0.005416, loss_dice: 0.150982
[08:44:26.546] TRAIN: iteration 6563 : loss : 0.108005, loss_ce: 0.004949, loss_dice: 0.211061
[08:44:26.752] TRAIN: iteration 6564 : loss : 0.226363, loss_ce: 0.024811, loss_dice: 0.427915
[08:44:26.959] TRAIN: iteration 6565 : loss : 0.251379, loss_ce: 0.005465, loss_dice: 0.497293
[08:44:27.167] TRAIN: iteration 6566 : loss : 0.129393, loss_ce: 0.004362, loss_dice: 0.254424
[08:44:29.558] TRAIN: iteration 6567 : loss : 0.096551, loss_ce: 0.007596, loss_dice: 0.185505
[08:44:29.766] TRAIN: iteration 6568 : loss : 0.087313, loss_ce: 0.005730, loss_dice: 0.168897
[08:44:29.973] TRAIN: iteration 6569 : loss : 0.252435, loss_ce: 0.004621, loss_dice: 0.500249
[08:44:30.467] TRAIN: iteration 6570 : loss : 0.138478, loss_ce: 0.018899, loss_dice: 0.258056
[08:44:30.680] TRAIN: iteration 6571 : loss : 0.124197, loss_ce: 0.005649, loss_dice: 0.242745
[08:44:30.896] TRAIN: iteration 6572 : loss : 0.204489, loss_ce: 0.005960, loss_dice: 0.403019
[08:44:31.104] TRAIN: iteration 6573 : loss : 0.170221, loss_ce: 0.004701, loss_dice: 0.335741
[08:44:31.320] TRAIN: iteration 6574 : loss : 0.192444, loss_ce: 0.007958, loss_dice: 0.376930
[08:44:31.528] TRAIN: iteration 6575 : loss : 0.238868, loss_ce: 0.005555, loss_dice: 0.472181
[08:44:31.735] TRAIN: iteration 6576 : loss : 0.079076, loss_ce: 0.005183, loss_dice: 0.152969
[08:44:31.944] TRAIN: iteration 6577 : loss : 0.113563, loss_ce: 0.014846, loss_dice: 0.212279
[08:44:34.854] TRAIN: iteration 6578 : loss : 0.251513, loss_ce: 0.010121, loss_dice: 0.492904
[08:44:35.061] TRAIN: iteration 6579 : loss : 0.065907, loss_ce: 0.003302, loss_dice: 0.128513
[08:44:35.269] TRAIN: iteration 6580 : loss : 0.119538, loss_ce: 0.004908, loss_dice: 0.234168
[08:44:35.519] TRAIN: iteration 6581 : loss : 0.253171, loss_ce: 0.005905, loss_dice: 0.500438
[08:44:35.728] TRAIN: iteration 6582 : loss : 0.253148, loss_ce: 0.005889, loss_dice: 0.500407
[08:44:36.001] TRAIN: iteration 6583 : loss : 0.086016, loss_ce: 0.004751, loss_dice: 0.167282
[08:44:36.212] TRAIN: iteration 6584 : loss : 0.100167, loss_ce: 0.008193, loss_dice: 0.192142
[08:44:36.420] TRAIN: iteration 6585 : loss : 0.133433, loss_ce: 0.005150, loss_dice: 0.261715
[08:44:37.020] TRAIN: iteration 6586 : loss : 0.201488, loss_ce: 0.004799, loss_dice: 0.398178
[08:44:37.639] TRAIN: iteration 6587 : loss : 0.244557, loss_ce: 0.006230, loss_dice: 0.482884
[08:44:37.846] TRAIN: iteration 6588 : loss : 0.142311, loss_ce: 0.006404, loss_dice: 0.278217
[08:44:38.055] TRAIN: iteration 6589 : loss : 0.165107, loss_ce: 0.004604, loss_dice: 0.325610
[08:44:38.262] TRAIN: iteration 6590 : loss : 0.194586, loss_ce: 0.006990, loss_dice: 0.382182
[08:44:39.431] TRAIN: iteration 6591 : loss : 0.216479, loss_ce: 0.006808, loss_dice: 0.426150
[08:44:39.638] TRAIN: iteration 6592 : loss : 0.190629, loss_ce: 0.010473, loss_dice: 0.370786
[08:44:39.845] TRAIN: iteration 6593 : loss : 0.096502, loss_ce: 0.003255, loss_dice: 0.189749
[08:44:42.432] TRAIN: iteration 6594 : loss : 0.130736, loss_ce: 0.004781, loss_dice: 0.256692
[08:44:42.639] TRAIN: iteration 6595 : loss : 0.168911, loss_ce: 0.004046, loss_dice: 0.333777
[08:44:42.857] TRAIN: iteration 6596 : loss : 0.062496, loss_ce: 0.002607, loss_dice: 0.122386
[08:44:43.064] TRAIN: iteration 6597 : loss : 0.251478, loss_ce: 0.002820, loss_dice: 0.500137
[08:44:43.272] TRAIN: iteration 6598 : loss : 0.104907, loss_ce: 0.003437, loss_dice: 0.206377
[08:44:44.552] TRAIN: iteration 6599 : loss : 0.170798, loss_ce: 0.002440, loss_dice: 0.339156
[08:44:44.759] TRAIN: iteration 6600 : loss : 0.253790, loss_ce: 0.007992, loss_dice: 0.499589
[08:44:44.997] TRAIN: iteration 6601 : loss : 0.148246, loss_ce: 0.010090, loss_dice: 0.286402
[08:44:47.618] TRAIN: iteration 6602 : loss : 0.245273, loss_ce: 0.002064, loss_dice: 0.488482
[08:44:47.831] TRAIN: iteration 6603 : loss : 0.250976, loss_ce: 0.001875, loss_dice: 0.500077
[08:44:48.038] TRAIN: iteration 6604 : loss : 0.250555, loss_ce: 0.006027, loss_dice: 0.495082
[08:44:48.247] TRAIN: iteration 6605 : loss : 0.150434, loss_ce: 0.006172, loss_dice: 0.294695
[08:44:48.453] TRAIN: iteration 6606 : loss : 0.204095, loss_ce: 0.003988, loss_dice: 0.404202
[08:44:48.663] TRAIN: iteration 6607 : loss : 0.251380, loss_ce: 0.002615, loss_dice: 0.500144
[08:44:48.874] TRAIN: iteration 6608 : loss : 0.154224, loss_ce: 0.005164, loss_dice: 0.303284
[08:44:49.082] TRAIN: iteration 6609 : loss : 0.161008, loss_ce: 0.010287, loss_dice: 0.311729
[08:44:52.160] TRAIN: iteration 6610 : loss : 0.253537, loss_ce: 0.007111, loss_dice: 0.499964
[08:44:52.374] TRAIN: iteration 6611 : loss : 0.155585, loss_ce: 0.004821, loss_dice: 0.306350
[08:44:52.582] TRAIN: iteration 6612 : loss : 0.120515, loss_ce: 0.007478, loss_dice: 0.233552
[08:44:52.790] TRAIN: iteration 6613 : loss : 0.251753, loss_ce: 0.003293, loss_dice: 0.500213
[08:44:53.002] TRAIN: iteration 6614 : loss : 0.158340, loss_ce: 0.005814, loss_dice: 0.310866
[08:44:53.210] TRAIN: iteration 6615 : loss : 0.129715, loss_ce: 0.004111, loss_dice: 0.255320
[08:44:53.417] TRAIN: iteration 6616 : loss : 0.252567, loss_ce: 0.004800, loss_dice: 0.500334
[08:44:54.433] TRAIN: iteration 6617 : loss : 0.084282, loss_ce: 0.002932, loss_dice: 0.165632
[08:44:55.155] TRAIN: iteration 6618 : loss : 0.111184, loss_ce: 0.007388, loss_dice: 0.214980
[08:44:55.363] TRAIN: iteration 6619 : loss : 0.252646, loss_ce: 0.004959, loss_dice: 0.500332
[08:44:55.571] TRAIN: iteration 6620 : loss : 0.125308, loss_ce: 0.007575, loss_dice: 0.243041
[08:44:55.818] TRAIN: iteration 6621 : loss : 0.247225, loss_ce: 0.005779, loss_dice: 0.488672
[08:44:56.024] TRAIN: iteration 6622 : loss : 0.193975, loss_ce: 0.005391, loss_dice: 0.382558
[08:44:56.735] TRAIN: iteration 6623 : loss : 0.162191, loss_ce: 0.004727, loss_dice: 0.319656
[08:44:56.942] TRAIN: iteration 6624 : loss : 0.173676, loss_ce: 0.025122, loss_dice: 0.322230
[08:44:57.149] TRAIN: iteration 6625 : loss : 0.135393, loss_ce: 0.004886, loss_dice: 0.265901
[08:45:00.240] TRAIN: iteration 6626 : loss : 0.115719, loss_ce: 0.008386, loss_dice: 0.223052
[08:45:00.447] TRAIN: iteration 6627 : loss : 0.217245, loss_ce: 0.007450, loss_dice: 0.427040
[08:45:00.656] TRAIN: iteration 6628 : loss : 0.111218, loss_ce: 0.016074, loss_dice: 0.206361
[08:45:00.863] TRAIN: iteration 6629 : loss : 0.255894, loss_ce: 0.018828, loss_dice: 0.492960
[08:45:01.078] TRAIN: iteration 6630 : loss : 0.106773, loss_ce: 0.006559, loss_dice: 0.206986
[08:45:01.285] TRAIN: iteration 6631 : loss : 0.123604, loss_ce: 0.004331, loss_dice: 0.242877
[08:45:01.494] TRAIN: iteration 6632 : loss : 0.252473, loss_ce: 0.004662, loss_dice: 0.500284
[08:45:01.702] TRAIN: iteration 6633 : loss : 0.252077, loss_ce: 0.003897, loss_dice: 0.500257
[08:45:02.273] TRAIN: iteration 6634 : loss : 0.215458, loss_ce: 0.006336, loss_dice: 0.424580
[08:45:02.479] TRAIN: iteration 6635 : loss : 0.126622, loss_ce: 0.009394, loss_dice: 0.243849
[08:45:02.690] TRAIN: iteration 6636 : loss : 0.079750, loss_ce: 0.004463, loss_dice: 0.155037
[08:45:03.270] TRAIN: iteration 6637 : loss : 0.128774, loss_ce: 0.004707, loss_dice: 0.252841
[08:45:03.478] TRAIN: iteration 6638 : loss : 0.155360, loss_ce: 0.006423, loss_dice: 0.304297
[08:45:03.685] TRAIN: iteration 6639 : loss : 0.245334, loss_ce: 0.004998, loss_dice: 0.485670
[08:45:05.564] TRAIN: iteration 6640 : loss : 0.076762, loss_ce: 0.005418, loss_dice: 0.148106
[08:45:05.565] NaN or Inf found in input tensor.
[08:45:06.577] TRAIN: iteration 6641 : loss : 0.091686, loss_ce: 0.009039, loss_dice: 0.174333
[08:45:08.605] TRAIN: iteration 6642 : loss : 0.129760, loss_ce: 0.004016, loss_dice: 0.255503
[08:45:08.814] TRAIN: iteration 6643 : loss : 0.072036, loss_ce: 0.007641, loss_dice: 0.136431
[08:45:09.021] TRAIN: iteration 6644 : loss : 0.165185, loss_ce: 0.005172, loss_dice: 0.325199
[08:45:10.146] TRAIN: iteration 6645 : loss : 0.100054, loss_ce: 0.003973, loss_dice: 0.196134
[08:45:10.353] TRAIN: iteration 6646 : loss : 0.252009, loss_ce: 0.003774, loss_dice: 0.500243
[08:45:10.560] TRAIN: iteration 6647 : loss : 0.228620, loss_ce: 0.004774, loss_dice: 0.452466
[08:45:11.416] TRAIN: iteration 6648 : loss : 0.229294, loss_ce: 0.005140, loss_dice: 0.453448
[08:45:11.645] TRAIN: iteration 6649 : loss : 0.131235, loss_ce: 0.003586, loss_dice: 0.258883
[08:45:13.642] TRAIN: iteration 6650 : loss : 0.061378, loss_ce: 0.003602, loss_dice: 0.119154
[08:45:13.849] TRAIN: iteration 6651 : loss : 0.133959, loss_ce: 0.004263, loss_dice: 0.263655
[08:45:14.063] TRAIN: iteration 6652 : loss : 0.152617, loss_ce: 0.005095, loss_dice: 0.300139
[08:45:16.061] TRAIN: iteration 6653 : loss : 0.250713, loss_ce: 0.004561, loss_dice: 0.496866
[08:45:16.268] TRAIN: iteration 6654 : loss : 0.251739, loss_ce: 0.003284, loss_dice: 0.500194
[08:45:16.476] TRAIN: iteration 6655 : loss : 0.230411, loss_ce: 0.003264, loss_dice: 0.457559
[08:45:17.859] TRAIN: iteration 6656 : loss : 0.243062, loss_ce: 0.003351, loss_dice: 0.482772
[08:45:18.986] TRAIN: iteration 6657 : loss : 0.120923, loss_ce: 0.005575, loss_dice: 0.236270
[08:45:20.598] TRAIN: iteration 6658 : loss : 0.175189, loss_ce: 0.003908, loss_dice: 0.346470
[08:45:20.806] TRAIN: iteration 6659 : loss : 0.122291, loss_ce: 0.004857, loss_dice: 0.239725
[08:45:21.014] TRAIN: iteration 6660 : loss : 0.179426, loss_ce: 0.005538, loss_dice: 0.353315
[08:45:21.263] TRAIN: iteration 6661 : loss : 0.163452, loss_ce: 0.005500, loss_dice: 0.321403
[08:45:21.472] TRAIN: iteration 6662 : loss : 0.227893, loss_ce: 0.019950, loss_dice: 0.435836
[08:45:21.682] TRAIN: iteration 6663 : loss : 0.120462, loss_ce: 0.006200, loss_dice: 0.234723
[08:45:21.889] TRAIN: iteration 6664 : loss : 0.091278, loss_ce: 0.008013, loss_dice: 0.174544
[08:45:23.475] TRAIN: iteration 6665 : loss : 0.133445, loss_ce: 0.009061, loss_dice: 0.257828
[08:45:23.871] TRAIN: iteration 6666 : loss : 0.236676, loss_ce: 0.006894, loss_dice: 0.466458
[08:45:24.080] TRAIN: iteration 6667 : loss : 0.135485, loss_ce: 0.013254, loss_dice: 0.257716
[08:45:24.614] TRAIN: iteration 6668 : loss : 0.199034, loss_ce: 0.006246, loss_dice: 0.391822
[08:45:29.253] TRAIN: iteration 6669 : loss : 0.244153, loss_ce: 0.007159, loss_dice: 0.481147
[08:45:29.470] TRAIN: iteration 6670 : loss : 0.240768, loss_ce: 0.010706, loss_dice: 0.470829
[08:45:29.684] TRAIN: iteration 6671 : loss : 0.254303, loss_ce: 0.008029, loss_dice: 0.500577
[08:45:29.895] TRAIN: iteration 6672 : loss : 0.215530, loss_ce: 0.011530, loss_dice: 0.419530
[08:45:30.102] TRAIN: iteration 6673 : loss : 0.075989, loss_ce: 0.005399, loss_dice: 0.146579
[08:45:30.320] TRAIN: iteration 6674 : loss : 0.105720, loss_ce: 0.012018, loss_dice: 0.199423
[08:45:30.529] TRAIN: iteration 6675 : loss : 0.145247, loss_ce: 0.006308, loss_dice: 0.284187
[08:45:30.736] TRAIN: iteration 6676 : loss : 0.216974, loss_ce: 0.010332, loss_dice: 0.423616
[08:45:37.494] TRAIN: iteration 6677 : loss : 0.075979, loss_ce: 0.007131, loss_dice: 0.144827
[08:45:37.703] TRAIN: iteration 6678 : loss : 0.092976, loss_ce: 0.006095, loss_dice: 0.179857
[08:45:37.910] TRAIN: iteration 6679 : loss : 0.090154, loss_ce: 0.007302, loss_dice: 0.173006
[08:45:38.118] TRAIN: iteration 6680 : loss : 0.251863, loss_ce: 0.008339, loss_dice: 0.495387
[08:45:38.358] TRAIN: iteration 6681 : loss : 0.159689, loss_ce: 0.008295, loss_dice: 0.311084
[08:45:38.566] TRAIN: iteration 6682 : loss : 0.198925, loss_ce: 0.008196, loss_dice: 0.389654
[08:45:38.775] TRAIN: iteration 6683 : loss : 0.094441, loss_ce: 0.006112, loss_dice: 0.182770
[08:45:38.983] TRAIN: iteration 6684 : loss : 0.185443, loss_ce: 0.008137, loss_dice: 0.362749
[08:45:43.441] TRAIN: iteration 6685 : loss : 0.095947, loss_ce: 0.005008, loss_dice: 0.186886
[08:45:43.649] TRAIN: iteration 6686 : loss : 0.084609, loss_ce: 0.004699, loss_dice: 0.164518
[08:45:43.861] TRAIN: iteration 6687 : loss : 0.078353, loss_ce: 0.003799, loss_dice: 0.152908
[08:45:44.074] TRAIN: iteration 6688 : loss : 0.252368, loss_ce: 0.004510, loss_dice: 0.500225
[08:45:44.282] TRAIN: iteration 6689 : loss : 0.199841, loss_ce: 0.006651, loss_dice: 0.393031
[08:45:44.489] TRAIN: iteration 6690 : loss : 0.212150, loss_ce: 0.005034, loss_dice: 0.419265
[08:45:44.703] TRAIN: iteration 6691 : loss : 0.063725, loss_ce: 0.003679, loss_dice: 0.123771
[08:45:44.910] TRAIN: iteration 6692 : loss : 0.085396, loss_ce: 0.002947, loss_dice: 0.167845
[08:45:50.927] TRAIN: iteration 6693 : loss : 0.203493, loss_ce: 0.005889, loss_dice: 0.401097
[08:45:51.134] TRAIN: iteration 6694 : loss : 0.128488, loss_ce: 0.008655, loss_dice: 0.248322
[08:45:51.340] TRAIN: iteration 6695 : loss : 0.135797, loss_ce: 0.005164, loss_dice: 0.266429
[08:45:51.546] TRAIN: iteration 6696 : loss : 0.112857, loss_ce: 0.009099, loss_dice: 0.216616
[08:45:51.758] TRAIN: iteration 6697 : loss : 0.064242, loss_ce: 0.002194, loss_dice: 0.126291
[08:45:51.964] TRAIN: iteration 6698 : loss : 0.135529, loss_ce: 0.005339, loss_dice: 0.265720
[08:45:52.177] TRAIN: iteration 6699 : loss : 0.248711, loss_ce: 0.004359, loss_dice: 0.493063
[08:45:52.384] TRAIN: iteration 6700 : loss : 0.233117, loss_ce: 0.003402, loss_dice: 0.462833
[08:45:58.151] TRAIN: iteration 6701 : loss : 0.233468, loss_ce: 0.004934, loss_dice: 0.462002
[08:45:58.359] TRAIN: iteration 6702 : loss : 0.251827, loss_ce: 0.003432, loss_dice: 0.500221
[08:45:58.565] TRAIN: iteration 6703 : loss : 0.198894, loss_ce: 0.008664, loss_dice: 0.389123
[08:45:58.773] TRAIN: iteration 6704 : loss : 0.252491, loss_ce: 0.004646, loss_dice: 0.500336
[08:45:58.981] TRAIN: iteration 6705 : loss : 0.069461, loss_ce: 0.003102, loss_dice: 0.135820
[08:45:59.190] TRAIN: iteration 6706 : loss : 0.110327, loss_ce: 0.008681, loss_dice: 0.211972
[08:45:59.396] TRAIN: iteration 6707 : loss : 0.152324, loss_ce: 0.005401, loss_dice: 0.299247
[08:45:59.604] TRAIN: iteration 6708 : loss : 0.131314, loss_ce: 0.019483, loss_dice: 0.243144
[08:46:05.124] TRAIN: iteration 6709 : loss : 0.204382, loss_ce: 0.006586, loss_dice: 0.402177
[08:46:05.330] TRAIN: iteration 6710 : loss : 0.139477, loss_ce: 0.005250, loss_dice: 0.273704
[08:46:05.539] TRAIN: iteration 6711 : loss : 0.110620, loss_ce: 0.004969, loss_dice: 0.216271
[08:46:05.746] TRAIN: iteration 6712 : loss : 0.130509, loss_ce: 0.011963, loss_dice: 0.249055
[08:46:05.954] TRAIN: iteration 6713 : loss : 0.182825, loss_ce: 0.005508, loss_dice: 0.360142
[08:46:06.160] TRAIN: iteration 6714 : loss : 0.252870, loss_ce: 0.005369, loss_dice: 0.500370
[08:46:06.367] TRAIN: iteration 6715 : loss : 0.085366, loss_ce: 0.009142, loss_dice: 0.161590
[08:46:06.575] TRAIN: iteration 6716 : loss : 0.175141, loss_ce: 0.007948, loss_dice: 0.342333
[08:46:09.725] TRAIN: iteration 6717 : loss : 0.142876, loss_ce: 0.005095, loss_dice: 0.280656
[08:46:09.934] TRAIN: iteration 6718 : loss : 0.251808, loss_ce: 0.007323, loss_dice: 0.496292
[08:46:10.141] TRAIN: iteration 6719 : loss : 0.192449, loss_ce: 0.005930, loss_dice: 0.378968
[08:46:10.351] TRAIN: iteration 6720 : loss : 0.203229, loss_ce: 0.008830, loss_dice: 0.397629
[08:46:10.594] TRAIN: iteration 6721 : loss : 0.083862, loss_ce: 0.004701, loss_dice: 0.163022
[08:46:10.801] TRAIN: iteration 6722 : loss : 0.179559, loss_ce: 0.021474, loss_dice: 0.337644
[08:46:11.008] TRAIN: iteration 6723 : loss : 0.174122, loss_ce: 0.004839, loss_dice: 0.343406
[08:46:11.215] TRAIN: iteration 6724 : loss : 0.116523, loss_ce: 0.011510, loss_dice: 0.221537
[08:46:13.683] TRAIN: iteration 6725 : loss : 0.122917, loss_ce: 0.009718, loss_dice: 0.236116
[08:46:13.890] TRAIN: iteration 6726 : loss : 0.252648, loss_ce: 0.005034, loss_dice: 0.500262
[08:46:14.097] TRAIN: iteration 6727 : loss : 0.251565, loss_ce: 0.005022, loss_dice: 0.498107
[08:46:14.984] TRAIN: iteration 6728 : loss : 0.128344, loss_ce: 0.008185, loss_dice: 0.248504
[08:46:15.191] TRAIN: iteration 6729 : loss : 0.140429, loss_ce: 0.021283, loss_dice: 0.259574
[08:46:15.402] TRAIN: iteration 6730 : loss : 0.196564, loss_ce: 0.005708, loss_dice: 0.387420
[08:46:15.608] TRAIN: iteration 6731 : loss : 0.100736, loss_ce: 0.007624, loss_dice: 0.193848
[08:46:17.343] TRAIN: iteration 6732 : loss : 0.137582, loss_ce: 0.006345, loss_dice: 0.268819
[08:46:19.813] TRAIN: iteration 6733 : loss : 0.093640, loss_ce: 0.007468, loss_dice: 0.179812
[08:46:20.023] TRAIN: iteration 6734 : loss : 0.217963, loss_ce: 0.005892, loss_dice: 0.430034
[08:46:20.494] TRAIN: iteration 6735 : loss : 0.244741, loss_ce: 0.006883, loss_dice: 0.482599
[08:46:21.095] TRAIN: iteration 6736 : loss : 0.167577, loss_ce: 0.006578, loss_dice: 0.328577
[08:46:21.302] TRAIN: iteration 6737 : loss : 0.153577, loss_ce: 0.008126, loss_dice: 0.299027
[08:46:21.512] TRAIN: iteration 6738 : loss : 0.253158, loss_ce: 0.005945, loss_dice: 0.500371
[08:46:21.720] TRAIN: iteration 6739 : loss : 0.253155, loss_ce: 0.005941, loss_dice: 0.500368
[08:46:25.319] TRAIN: iteration 6740 : loss : 0.182283, loss_ce: 0.006707, loss_dice: 0.357859
[08:46:25.556] TRAIN: iteration 6741 : loss : 0.138067, loss_ce: 0.004596, loss_dice: 0.271539
[08:46:25.763] TRAIN: iteration 6742 : loss : 0.152151, loss_ce: 0.008374, loss_dice: 0.295928
[08:46:27.898] TRAIN: iteration 6743 : loss : 0.254338, loss_ce: 0.010094, loss_dice: 0.498582
[08:46:28.105] TRAIN: iteration 6744 : loss : 0.172689, loss_ce: 0.004767, loss_dice: 0.340612
[08:46:28.314] TRAIN: iteration 6745 : loss : 0.136820, loss_ce: 0.006067, loss_dice: 0.267573
[08:46:28.521] TRAIN: iteration 6746 : loss : 0.199409, loss_ce: 0.007362, loss_dice: 0.391456
[08:46:28.729] TRAIN: iteration 6747 : loss : 0.095173, loss_ce: 0.003996, loss_dice: 0.186349
[08:46:32.306] TRAIN: iteration 6748 : loss : 0.094649, loss_ce: 0.003964, loss_dice: 0.185335
[08:46:36.597] TRAIN: iteration 6749 : loss : 0.094875, loss_ce: 0.003004, loss_dice: 0.186746
[08:46:36.804] TRAIN: iteration 6750 : loss : 0.065299, loss_ce: 0.004014, loss_dice: 0.126584
[08:46:37.011] TRAIN: iteration 6751 : loss : 0.251368, loss_ce: 0.002628, loss_dice: 0.500108
[08:46:37.232] TRAIN: iteration 6752 : loss : 0.119560, loss_ce: 0.004308, loss_dice: 0.234811
[08:46:37.439] TRAIN: iteration 6753 : loss : 0.065165, loss_ce: 0.002613, loss_dice: 0.127718
[08:46:37.647] TRAIN: iteration 6754 : loss : 0.048381, loss_ce: 0.002077, loss_dice: 0.094684
[08:46:37.857] TRAIN: iteration 6755 : loss : 0.246601, loss_ce: 0.003042, loss_dice: 0.490161
[08:46:39.819] TRAIN: iteration 6756 : loss : 0.078012, loss_ce: 0.003321, loss_dice: 0.152702
[08:46:45.841] TRAIN: iteration 6757 : loss : 0.243284, loss_ce: 0.003549, loss_dice: 0.483019
[08:46:46.050] TRAIN: iteration 6758 : loss : 0.122683, loss_ce: 0.004086, loss_dice: 0.241281
[08:46:46.260] TRAIN: iteration 6759 : loss : 0.175906, loss_ce: 0.011136, loss_dice: 0.340676
[08:46:46.467] TRAIN: iteration 6760 : loss : 0.149307, loss_ce: 0.005842, loss_dice: 0.292772
[08:46:46.708] TRAIN: iteration 6761 : loss : 0.182918, loss_ce: 0.007312, loss_dice: 0.358524
[08:46:46.915] TRAIN: iteration 6762 : loss : 0.054359, loss_ce: 0.002775, loss_dice: 0.105943
[08:46:47.123] TRAIN: iteration 6763 : loss : 0.064808, loss_ce: 0.005514, loss_dice: 0.124103
[08:46:48.394] TRAIN: iteration 6764 : loss : 0.115006, loss_ce: 0.005898, loss_dice: 0.224114
[08:46:53.069] TRAIN: iteration 6765 : loss : 0.250070, loss_ce: 0.008735, loss_dice: 0.491405
[08:46:53.276] TRAIN: iteration 6766 : loss : 0.170421, loss_ce: 0.006912, loss_dice: 0.333929
[08:46:53.483] TRAIN: iteration 6767 : loss : 0.252103, loss_ce: 0.003976, loss_dice: 0.500230
[08:46:53.691] TRAIN: iteration 6768 : loss : 0.063518, loss_ce: 0.004760, loss_dice: 0.122276
[08:46:53.899] TRAIN: iteration 6769 : loss : 0.065763, loss_ce: 0.004774, loss_dice: 0.126752
[08:46:54.107] TRAIN: iteration 6770 : loss : 0.038917, loss_ce: 0.002559, loss_dice: 0.075275
[08:46:54.315] TRAIN: iteration 6771 : loss : 0.201470, loss_ce: 0.006647, loss_dice: 0.396293
[08:46:55.809] TRAIN: iteration 6772 : loss : 0.252177, loss_ce: 0.004054, loss_dice: 0.500301
[08:46:58.480] TRAIN: iteration 6773 : loss : 0.069907, loss_ce: 0.005515, loss_dice: 0.134299
[08:46:58.689] TRAIN: iteration 6774 : loss : 0.105180, loss_ce: 0.016359, loss_dice: 0.194001
[08:46:58.897] TRAIN: iteration 6775 : loss : 0.139136, loss_ce: 0.006450, loss_dice: 0.271822
[08:46:59.104] TRAIN: iteration 6776 : loss : 0.116384, loss_ce: 0.008895, loss_dice: 0.223873
[08:46:59.313] TRAIN: iteration 6777 : loss : 0.108191, loss_ce: 0.006982, loss_dice: 0.209400
[08:46:59.519] TRAIN: iteration 6778 : loss : 0.169180, loss_ce: 0.006110, loss_dice: 0.332249
[08:46:59.726] TRAIN: iteration 6779 : loss : 0.197081, loss_ce: 0.007586, loss_dice: 0.386575
[08:47:05.091] TRAIN: iteration 6780 : loss : 0.253507, loss_ce: 0.006504, loss_dice: 0.500510
[08:47:05.091] NaN or Inf found in input tensor.
[08:47:07.592] TRAIN: iteration 6781 : loss : 0.181997, loss_ce: 0.006026, loss_dice: 0.357968
[08:47:07.804] TRAIN: iteration 6782 : loss : 0.040685, loss_ce: 0.003317, loss_dice: 0.078054
[08:47:08.013] TRAIN: iteration 6783 : loss : 0.138260, loss_ce: 0.004493, loss_dice: 0.272027
[08:47:08.220] TRAIN: iteration 6784 : loss : 0.241026, loss_ce: 0.006170, loss_dice: 0.475883
[08:47:08.429] TRAIN: iteration 6785 : loss : 0.241059, loss_ce: 0.006524, loss_dice: 0.475594
[08:47:08.637] TRAIN: iteration 6786 : loss : 0.253278, loss_ce: 0.008214, loss_dice: 0.498342
[08:47:08.844] TRAIN: iteration 6787 : loss : 0.121574, loss_ce: 0.006179, loss_dice: 0.236970
[08:47:14.623] TRAIN: iteration 6788 : loss : 0.252453, loss_ce: 0.004575, loss_dice: 0.500331
[08:47:17.941] TRAIN: iteration 6789 : loss : 0.227178, loss_ce: 0.040850, loss_dice: 0.413506
[08:47:18.149] TRAIN: iteration 6790 : loss : 0.114837, loss_ce: 0.004945, loss_dice: 0.224729
[08:47:18.357] TRAIN: iteration 6791 : loss : 0.250274, loss_ce: 0.004529, loss_dice: 0.496019
[08:47:18.564] TRAIN: iteration 6792 : loss : 0.252113, loss_ce: 0.004015, loss_dice: 0.500210
[08:47:18.772] TRAIN: iteration 6793 : loss : 0.097193, loss_ce: 0.008607, loss_dice: 0.185780
[08:47:18.980] TRAIN: iteration 6794 : loss : 0.148747, loss_ce: 0.011444, loss_dice: 0.286050
[08:47:19.189] TRAIN: iteration 6795 : loss : 0.194973, loss_ce: 0.007444, loss_dice: 0.382503
[08:47:23.501] TRAIN: iteration 6796 : loss : 0.252047, loss_ce: 0.003860, loss_dice: 0.500234
[08:47:24.478] TRAIN: iteration 6797 : loss : 0.055269, loss_ce: 0.003098, loss_dice: 0.107440
[08:47:24.685] TRAIN: iteration 6798 : loss : 0.083788, loss_ce: 0.005395, loss_dice: 0.162180
[08:47:24.894] TRAIN: iteration 6799 : loss : 0.154570, loss_ce: 0.011093, loss_dice: 0.298047
[08:47:25.101] TRAIN: iteration 6800 : loss : 0.242219, loss_ce: 0.004983, loss_dice: 0.479455
[08:47:25.339] TRAIN: iteration 6801 : loss : 0.212522, loss_ce: 0.008791, loss_dice: 0.416253
[08:47:25.546] TRAIN: iteration 6802 : loss : 0.252583, loss_ce: 0.004908, loss_dice: 0.500257
[08:47:25.753] TRAIN: iteration 6803 : loss : 0.188211, loss_ce: 0.007876, loss_dice: 0.368545
[08:47:32.239] TRAIN: iteration 6804 : loss : 0.164527, loss_ce: 0.013551, loss_dice: 0.315502
[08:47:32.992] TRAIN: iteration 6805 : loss : 0.207763, loss_ce: 0.007299, loss_dice: 0.408228
[08:47:33.199] TRAIN: iteration 6806 : loss : 0.116002, loss_ce: 0.005246, loss_dice: 0.226758
[08:47:33.407] TRAIN: iteration 6807 : loss : 0.044397, loss_ce: 0.004912, loss_dice: 0.083882
[08:47:33.613] TRAIN: iteration 6808 : loss : 0.149518, loss_ce: 0.007749, loss_dice: 0.291287
[08:47:33.819] TRAIN: iteration 6809 : loss : 0.070839, loss_ce: 0.005610, loss_dice: 0.136067
[08:47:34.026] TRAIN: iteration 6810 : loss : 0.166892, loss_ce: 0.008288, loss_dice: 0.325496
[08:47:34.233] TRAIN: iteration 6811 : loss : 0.251937, loss_ce: 0.007718, loss_dice: 0.496157
[08:47:40.466] TRAIN: iteration 6812 : loss : 0.208038, loss_ce: 0.006703, loss_dice: 0.409373
[08:47:40.686] TRAIN: iteration 6813 : loss : 0.117493, loss_ce: 0.006339, loss_dice: 0.228646
[08:47:40.893] TRAIN: iteration 6814 : loss : 0.076874, loss_ce: 0.006984, loss_dice: 0.146764
[08:47:41.101] TRAIN: iteration 6815 : loss : 0.029994, loss_ce: 0.002715, loss_dice: 0.057273
[08:47:41.373] TRAIN: iteration 6816 : loss : 0.105078, loss_ce: 0.008644, loss_dice: 0.201511
[08:47:41.580] TRAIN: iteration 6817 : loss : 0.060361, loss_ce: 0.003932, loss_dice: 0.116790
[08:47:41.787] TRAIN: iteration 6818 : loss : 0.237676, loss_ce: 0.010037, loss_dice: 0.465315
[08:47:41.995] TRAIN: iteration 6819 : loss : 0.114520, loss_ce: 0.005920, loss_dice: 0.223121
[08:47:48.005] TRAIN: iteration 6820 : loss : 0.074661, loss_ce: 0.005206, loss_dice: 0.144116
[08:47:49.151] TRAIN: iteration 6821 : loss : 0.106082, loss_ce: 0.008147, loss_dice: 0.204018
[08:47:49.357] TRAIN: iteration 6822 : loss : 0.206401, loss_ce: 0.005784, loss_dice: 0.407017
[08:47:49.563] TRAIN: iteration 6823 : loss : 0.176039, loss_ce: 0.005478, loss_dice: 0.346601
[08:47:49.769] TRAIN: iteration 6824 : loss : 0.128426, loss_ce: 0.009725, loss_dice: 0.247126
[08:47:49.977] TRAIN: iteration 6825 : loss : 0.149436, loss_ce: 0.012813, loss_dice: 0.286058
[08:47:50.185] TRAIN: iteration 6826 : loss : 0.158541, loss_ce: 0.007151, loss_dice: 0.309931
[08:47:50.392] TRAIN: iteration 6827 : loss : 0.228116, loss_ce: 0.006650, loss_dice: 0.449582
[08:47:56.463] TRAIN: iteration 6828 : loss : 0.114315, loss_ce: 0.003860, loss_dice: 0.224770
[08:47:56.672] TRAIN: iteration 6829 : loss : 0.252442, loss_ce: 0.004590, loss_dice: 0.500294
[08:47:56.879] TRAIN: iteration 6830 : loss : 0.169253, loss_ce: 0.013394, loss_dice: 0.325112
[08:47:57.087] TRAIN: iteration 6831 : loss : 0.145064, loss_ce: 0.004409, loss_dice: 0.285719
[08:47:57.295] TRAIN: iteration 6832 : loss : 0.212356, loss_ce: 0.004863, loss_dice: 0.419848
[08:47:57.503] TRAIN: iteration 6833 : loss : 0.250666, loss_ce: 0.005936, loss_dice: 0.495396
[08:47:57.711] TRAIN: iteration 6834 : loss : 0.245459, loss_ce: 0.008449, loss_dice: 0.482468
[08:47:57.919] TRAIN: iteration 6835 : loss : 0.084067, loss_ce: 0.005705, loss_dice: 0.162429
[08:48:06.623] TRAIN: iteration 6836 : loss : 0.252244, loss_ce: 0.004185, loss_dice: 0.500303
[08:48:06.831] TRAIN: iteration 6837 : loss : 0.166025, loss_ce: 0.005242, loss_dice: 0.326809
[08:48:07.037] TRAIN: iteration 6838 : loss : 0.185178, loss_ce: 0.005364, loss_dice: 0.364993
[08:48:07.244] TRAIN: iteration 6839 : loss : 0.175145, loss_ce: 0.011449, loss_dice: 0.338841
[08:48:07.450] TRAIN: iteration 6840 : loss : 0.109793, loss_ce: 0.005186, loss_dice: 0.214401
[08:48:07.685] TRAIN: iteration 6841 : loss : 0.175941, loss_ce: 0.007915, loss_dice: 0.343967
[08:48:07.893] TRAIN: iteration 6842 : loss : 0.108851, loss_ce: 0.005722, loss_dice: 0.211980
[08:48:08.099] TRAIN: iteration 6843 : loss : 0.245727, loss_ce: 0.004885, loss_dice: 0.486569
[08:48:15.904] TRAIN: iteration 6844 : loss : 0.103569, loss_ce: 0.006077, loss_dice: 0.201060
[08:48:16.112] TRAIN: iteration 6845 : loss : 0.252797, loss_ce: 0.005228, loss_dice: 0.500366
[08:48:16.320] TRAIN: iteration 6846 : loss : 0.095665, loss_ce: 0.003976, loss_dice: 0.187353
[08:48:16.529] TRAIN: iteration 6847 : loss : 0.081646, loss_ce: 0.006190, loss_dice: 0.157101
[08:48:16.736] TRAIN: iteration 6848 : loss : 0.252839, loss_ce: 0.005303, loss_dice: 0.500375
[08:48:16.944] TRAIN: iteration 6849 : loss : 0.110495, loss_ce: 0.007057, loss_dice: 0.213933
[08:48:17.152] TRAIN: iteration 6850 : loss : 0.247102, loss_ce: 0.005431, loss_dice: 0.488773
[08:48:17.359] TRAIN: iteration 6851 : loss : 0.159671, loss_ce: 0.010334, loss_dice: 0.309007
[08:48:24.960] TRAIN: iteration 6852 : loss : 0.084246, loss_ce: 0.005838, loss_dice: 0.162655
[08:48:25.167] TRAIN: iteration 6853 : loss : 0.237956, loss_ce: 0.006953, loss_dice: 0.468959
[08:48:25.374] TRAIN: iteration 6854 : loss : 0.244967, loss_ce: 0.005847, loss_dice: 0.484086
[08:48:25.582] TRAIN: iteration 6855 : loss : 0.127950, loss_ce: 0.007433, loss_dice: 0.248468
[08:48:25.789] TRAIN: iteration 6856 : loss : 0.178151, loss_ce: 0.006056, loss_dice: 0.350246
[08:48:25.996] TRAIN: iteration 6857 : loss : 0.153199, loss_ce: 0.006169, loss_dice: 0.300229
[08:48:26.203] TRAIN: iteration 6858 : loss : 0.211631, loss_ce: 0.006472, loss_dice: 0.416789
[08:48:26.411] TRAIN: iteration 6859 : loss : 0.181891, loss_ce: 0.005849, loss_dice: 0.357932
[08:48:32.805] TRAIN: iteration 6860 : loss : 0.140097, loss_ce: 0.006977, loss_dice: 0.273217
[08:48:33.424] TRAIN: iteration 6861 : loss : 0.114062, loss_ce: 0.003737, loss_dice: 0.224387
[08:48:33.631] TRAIN: iteration 6862 : loss : 0.184656, loss_ce: 0.016737, loss_dice: 0.352575
[08:48:33.838] TRAIN: iteration 6863 : loss : 0.252441, loss_ce: 0.004621, loss_dice: 0.500261
[08:48:34.046] TRAIN: iteration 6864 : loss : 0.064650, loss_ce: 0.003645, loss_dice: 0.125654
[08:48:34.256] TRAIN: iteration 6865 : loss : 0.108430, loss_ce: 0.004678, loss_dice: 0.212183
[08:48:34.463] TRAIN: iteration 6866 : loss : 0.097515, loss_ce: 0.008233, loss_dice: 0.186797
[08:48:34.675] TRAIN: iteration 6867 : loss : 0.253068, loss_ce: 0.005853, loss_dice: 0.500283
[08:48:41.753] TRAIN: iteration 6868 : loss : 0.075278, loss_ce: 0.003157, loss_dice: 0.147399
[08:48:41.965] TRAIN: iteration 6869 : loss : 0.228316, loss_ce: 0.004734, loss_dice: 0.451899
[08:48:42.171] TRAIN: iteration 6870 : loss : 0.161081, loss_ce: 0.008297, loss_dice: 0.313865
[08:48:42.378] TRAIN: iteration 6871 : loss : 0.089543, loss_ce: 0.003466, loss_dice: 0.175620
[08:48:42.586] TRAIN: iteration 6872 : loss : 0.080890, loss_ce: 0.005871, loss_dice: 0.155910
[08:48:42.793] TRAIN: iteration 6873 : loss : 0.211057, loss_ce: 0.003873, loss_dice: 0.418241
[08:48:43.010] TRAIN: iteration 6874 : loss : 0.235021, loss_ce: 0.004413, loss_dice: 0.465629
[08:48:43.217] TRAIN: iteration 6875 : loss : 0.181797, loss_ce: 0.008253, loss_dice: 0.355340
[08:48:52.720] TRAIN: iteration 6876 : loss : 0.095312, loss_ce: 0.006563, loss_dice: 0.184061
[08:48:52.927] TRAIN: iteration 6877 : loss : 0.192064, loss_ce: 0.009656, loss_dice: 0.374471
[08:48:53.134] TRAIN: iteration 6878 : loss : 0.101626, loss_ce: 0.004313, loss_dice: 0.198939
[08:48:53.341] TRAIN: iteration 6879 : loss : 0.147604, loss_ce: 0.006134, loss_dice: 0.289075
[08:48:53.548] TRAIN: iteration 6880 : loss : 0.196968, loss_ce: 0.003972, loss_dice: 0.389963
[08:48:53.783] TRAIN: iteration 6881 : loss : 0.254418, loss_ce: 0.008568, loss_dice: 0.500269
[08:48:53.991] TRAIN: iteration 6882 : loss : 0.207198, loss_ce: 0.005187, loss_dice: 0.409209
[08:48:54.328] TRAIN: iteration 6883 : loss : 0.063469, loss_ce: 0.003515, loss_dice: 0.123424
[08:49:03.020] TRAIN: iteration 6884 : loss : 0.049116, loss_ce: 0.003459, loss_dice: 0.094774
[08:49:03.234] TRAIN: iteration 6885 : loss : 0.103212, loss_ce: 0.008853, loss_dice: 0.197571
[08:49:03.441] TRAIN: iteration 6886 : loss : 0.127608, loss_ce: 0.024109, loss_dice: 0.231108
[08:49:03.649] TRAIN: iteration 6887 : loss : 0.044055, loss_ce: 0.003085, loss_dice: 0.085025
[08:49:03.856] TRAIN: iteration 6888 : loss : 0.169558, loss_ce: 0.007007, loss_dice: 0.332109
[08:49:04.064] TRAIN: iteration 6889 : loss : 0.086883, loss_ce: 0.005191, loss_dice: 0.168576
[08:49:04.272] TRAIN: iteration 6890 : loss : 0.167357, loss_ce: 0.005739, loss_dice: 0.328976
[08:49:04.479] TRAIN: iteration 6891 : loss : 0.249154, loss_ce: 0.008848, loss_dice: 0.489460
[08:49:11.750] TRAIN: iteration 6892 : loss : 0.111535, loss_ce: 0.003893, loss_dice: 0.219176
[08:49:11.956] TRAIN: iteration 6893 : loss : 0.153484, loss_ce: 0.005331, loss_dice: 0.301637
[08:49:12.164] TRAIN: iteration 6894 : loss : 0.234937, loss_ce: 0.005911, loss_dice: 0.463963
[08:49:12.372] TRAIN: iteration 6895 : loss : 0.182631, loss_ce: 0.004816, loss_dice: 0.360445
[08:49:12.578] TRAIN: iteration 6896 : loss : 0.253253, loss_ce: 0.006050, loss_dice: 0.500455
[08:49:12.786] TRAIN: iteration 6897 : loss : 0.179128, loss_ce: 0.006897, loss_dice: 0.351359
[08:49:12.993] TRAIN: iteration 6898 : loss : 0.160945, loss_ce: 0.008402, loss_dice: 0.313489
[08:49:13.201] TRAIN: iteration 6899 : loss : 0.119901, loss_ce: 0.004890, loss_dice: 0.234912
[08:49:21.773] TRAIN: iteration 6900 : loss : 0.252923, loss_ce: 0.005583, loss_dice: 0.500264
[08:49:22.013] TRAIN: iteration 6901 : loss : 0.167946, loss_ce: 0.004942, loss_dice: 0.330949
[08:49:22.224] TRAIN: iteration 6902 : loss : 0.162771, loss_ce: 0.006910, loss_dice: 0.318632
[08:49:22.432] TRAIN: iteration 6903 : loss : 0.118435, loss_ce: 0.005844, loss_dice: 0.231026
[08:49:22.639] TRAIN: iteration 6904 : loss : 0.161901, loss_ce: 0.004317, loss_dice: 0.319484
[08:49:22.846] TRAIN: iteration 6905 : loss : 0.163022, loss_ce: 0.021732, loss_dice: 0.304312
[08:49:23.053] TRAIN: iteration 6906 : loss : 0.175727, loss_ce: 0.006233, loss_dice: 0.345220
[08:49:23.264] TRAIN: iteration 6907 : loss : 0.221569, loss_ce: 0.005245, loss_dice: 0.437892
[08:49:31.326] TRAIN: iteration 6908 : loss : 0.226913, loss_ce: 0.004639, loss_dice: 0.449187
[08:49:31.533] TRAIN: iteration 6909 : loss : 0.252203, loss_ce: 0.004168, loss_dice: 0.500238
[08:49:31.741] TRAIN: iteration 6910 : loss : 0.101614, loss_ce: 0.003899, loss_dice: 0.199329
[08:49:31.953] TRAIN: iteration 6911 : loss : 0.094496, loss_ce: 0.005132, loss_dice: 0.183860
[08:49:32.164] TRAIN: iteration 6912 : loss : 0.137476, loss_ce: 0.014404, loss_dice: 0.260547
[08:49:32.371] TRAIN: iteration 6913 : loss : 0.252511, loss_ce: 0.004691, loss_dice: 0.500331
[08:49:32.579] TRAIN: iteration 6914 : loss : 0.164955, loss_ce: 0.004670, loss_dice: 0.325240
[08:49:32.786] TRAIN: iteration 6915 : loss : 0.148854, loss_ce: 0.004320, loss_dice: 0.293388
[08:49:40.426] TRAIN: iteration 6916 : loss : 0.077844, loss_ce: 0.003157, loss_dice: 0.152531
[08:49:40.637] TRAIN: iteration 6917 : loss : 0.131340, loss_ce: 0.007360, loss_dice: 0.255319
[08:49:40.844] TRAIN: iteration 6918 : loss : 0.187742, loss_ce: 0.004664, loss_dice: 0.370820
[08:49:41.051] TRAIN: iteration 6919 : loss : 0.101931, loss_ce: 0.006612, loss_dice: 0.197250
[08:49:41.259] TRAIN: iteration 6920 : loss : 0.252239, loss_ce: 0.004201, loss_dice: 0.500278
[08:49:41.493] TRAIN: iteration 6921 : loss : 0.104707, loss_ce: 0.007342, loss_dice: 0.202072
[08:49:41.700] TRAIN: iteration 6922 : loss : 0.089545, loss_ce: 0.008797, loss_dice: 0.170293
[08:49:41.907] TRAIN: iteration 6923 : loss : 0.130025, loss_ce: 0.005754, loss_dice: 0.254295
[08:49:49.646] TRAIN: iteration 6924 : loss : 0.246500, loss_ce: 0.004383, loss_dice: 0.488616
[08:49:49.857] TRAIN: iteration 6925 : loss : 0.073135, loss_ce: 0.002970, loss_dice: 0.143300
[08:49:50.063] TRAIN: iteration 6926 : loss : 0.178423, loss_ce: 0.005042, loss_dice: 0.351804
[08:49:50.269] TRAIN: iteration 6927 : loss : 0.250207, loss_ce: 0.005219, loss_dice: 0.495195
[08:49:50.476] TRAIN: iteration 6928 : loss : 0.185252, loss_ce: 0.004090, loss_dice: 0.366414
[08:49:50.683] TRAIN: iteration 6929 : loss : 0.131547, loss_ce: 0.011653, loss_dice: 0.251441
[08:49:50.889] TRAIN: iteration 6930 : loss : 0.105061, loss_ce: 0.005077, loss_dice: 0.205044
[08:49:51.097] TRAIN: iteration 6931 : loss : 0.102526, loss_ce: 0.009713, loss_dice: 0.195338
[08:49:59.705] TRAIN: iteration 6932 : loss : 0.173715, loss_ce: 0.003901, loss_dice: 0.343528
[08:49:59.912] TRAIN: iteration 6933 : loss : 0.233123, loss_ce: 0.004109, loss_dice: 0.462136
[08:50:00.119] TRAIN: iteration 6934 : loss : 0.257474, loss_ce: 0.021746, loss_dice: 0.493201
[08:50:00.326] TRAIN: iteration 6935 : loss : 0.105077, loss_ce: 0.006805, loss_dice: 0.203348
[08:50:00.534] TRAIN: iteration 6936 : loss : 0.164287, loss_ce: 0.021641, loss_dice: 0.306933
[08:50:00.741] TRAIN: iteration 6937 : loss : 0.252364, loss_ce: 0.004448, loss_dice: 0.500280
[08:50:00.962] TRAIN: iteration 6938 : loss : 0.234680, loss_ce: 0.006052, loss_dice: 0.463308
[08:50:01.169] TRAIN: iteration 6939 : loss : 0.148363, loss_ce: 0.019111, loss_dice: 0.277614
[08:50:03.066] TRAIN: iteration 6940 : loss : 0.133745, loss_ce: 0.009058, loss_dice: 0.258431
[08:55:55.736] VALIDATION: iteration 3 : loss : 0.154772, loss_ce: 0.006890, loss_dice: 0.302655
[08:55:57.905] TRAIN: iteration 6941 : loss : 0.252226, loss_ce: 0.004158, loss_dice: 0.500293
[08:55:58.115] TRAIN: iteration 6942 : loss : 0.168348, loss_ce: 0.004901, loss_dice: 0.331794
[08:55:58.331] TRAIN: iteration 6943 : loss : 0.120200, loss_ce: 0.007262, loss_dice: 0.233138
[08:55:58.541] TRAIN: iteration 6944 : loss : 0.163372, loss_ce: 0.007101, loss_dice: 0.319643
[08:55:58.757] TRAIN: iteration 6945 : loss : 0.124989, loss_ce: 0.015829, loss_dice: 0.234149
[08:55:59.407] TRAIN: iteration 6946 : loss : 0.252747, loss_ce: 0.005142, loss_dice: 0.500352
[08:55:59.614] TRAIN: iteration 6947 : loss : 0.225915, loss_ce: 0.006017, loss_dice: 0.445813
[08:55:59.822] TRAIN: iteration 6948 : loss : 0.100415, loss_ce: 0.004533, loss_dice: 0.196297
[08:56:00.029] TRAIN: iteration 6949 : loss : 0.094265, loss_ce: 0.009077, loss_dice: 0.179453
[08:56:00.240] TRAIN: iteration 6950 : loss : 0.148364, loss_ce: 0.006396, loss_dice: 0.290333
[08:56:00.450] TRAIN: iteration 6951 : loss : 0.239402, loss_ce: 0.005689, loss_dice: 0.473114
[08:56:00.660] TRAIN: iteration 6952 : loss : 0.143662, loss_ce: 0.008089, loss_dice: 0.279235
[08:56:00.867] TRAIN: iteration 6953 : loss : 0.169926, loss_ce: 0.006934, loss_dice: 0.332918
[08:56:01.076] TRAIN: iteration 6954 : loss : 0.086471, loss_ce: 0.005242, loss_dice: 0.167700
[08:56:01.283] TRAIN: iteration 6955 : loss : 0.115573, loss_ce: 0.007474, loss_dice: 0.223672
[08:56:01.499] TRAIN: iteration 6956 : loss : 0.252135, loss_ce: 0.009461, loss_dice: 0.494808
[08:56:01.713] TRAIN: iteration 6957 : loss : 0.252660, loss_ce: 0.005065, loss_dice: 0.500255
[08:56:01.936] TRAIN: iteration 6958 : loss : 0.104264, loss_ce: 0.005890, loss_dice: 0.202639
[08:56:02.146] TRAIN: iteration 6959 : loss : 0.131704, loss_ce: 0.014003, loss_dice: 0.249405
[08:56:02.359] TRAIN: iteration 6960 : loss : 0.041433, loss_ce: 0.003058, loss_dice: 0.079808
[08:56:02.599] TRAIN: iteration 6961 : loss : 0.251752, loss_ce: 0.005281, loss_dice: 0.498223
[08:56:02.806] TRAIN: iteration 6962 : loss : 0.060834, loss_ce: 0.003588, loss_dice: 0.118080
[08:56:03.015] TRAIN: iteration 6963 : loss : 0.235317, loss_ce: 0.009701, loss_dice: 0.460933
[08:56:03.256] TRAIN: iteration 6964 : loss : 0.248158, loss_ce: 0.005393, loss_dice: 0.490923
[08:56:03.469] TRAIN: iteration 6965 : loss : 0.189768, loss_ce: 0.008828, loss_dice: 0.370707
[08:56:03.676] TRAIN: iteration 6966 : loss : 0.107827, loss_ce: 0.007903, loss_dice: 0.207751
[08:56:03.889] TRAIN: iteration 6967 : loss : 0.100056, loss_ce: 0.004506, loss_dice: 0.195605
[08:56:04.100] TRAIN: iteration 6968 : loss : 0.083698, loss_ce: 0.007204, loss_dice: 0.160193
[08:56:04.315] TRAIN: iteration 6969 : loss : 0.147473, loss_ce: 0.006254, loss_dice: 0.288691
[08:56:04.530] TRAIN: iteration 6970 : loss : 0.125144, loss_ce: 0.009051, loss_dice: 0.241238
[08:56:04.738] TRAIN: iteration 6971 : loss : 0.087085, loss_ce: 0.004862, loss_dice: 0.169307
[08:56:04.948] TRAIN: iteration 6972 : loss : 0.245749, loss_ce: 0.007567, loss_dice: 0.483931
[08:56:05.156] TRAIN: iteration 6973 : loss : 0.193388, loss_ce: 0.004848, loss_dice: 0.381929
[08:56:05.368] TRAIN: iteration 6974 : loss : 0.158184, loss_ce: 0.006395, loss_dice: 0.309974
[08:56:05.576] TRAIN: iteration 6975 : loss : 0.202964, loss_ce: 0.007698, loss_dice: 0.398229
[08:56:05.786] TRAIN: iteration 6976 : loss : 0.230875, loss_ce: 0.005759, loss_dice: 0.455991
[08:56:05.994] TRAIN: iteration 6977 : loss : 0.068270, loss_ce: 0.007566, loss_dice: 0.128974
[08:56:06.202] TRAIN: iteration 6978 : loss : 0.253268, loss_ce: 0.006092, loss_dice: 0.500444
[08:56:06.411] TRAIN: iteration 6979 : loss : 0.107948, loss_ce: 0.006116, loss_dice: 0.209780
[08:56:06.623] TRAIN: iteration 6980 : loss : 0.100519, loss_ce: 0.005050, loss_dice: 0.195988
[08:56:06.860] TRAIN: iteration 6981 : loss : 0.075797, loss_ce: 0.006767, loss_dice: 0.144827
[08:56:07.074] TRAIN: iteration 6982 : loss : 0.131818, loss_ce: 0.005390, loss_dice: 0.258247
[08:56:07.282] TRAIN: iteration 6983 : loss : 0.201812, loss_ce: 0.005088, loss_dice: 0.398537
[08:56:07.491] TRAIN: iteration 6984 : loss : 0.136684, loss_ce: 0.004532, loss_dice: 0.268836
[08:56:07.858] TRAIN: iteration 6985 : loss : 0.252566, loss_ce: 0.004830, loss_dice: 0.500301
[08:56:08.071] TRAIN: iteration 6986 : loss : 0.154204, loss_ce: 0.014385, loss_dice: 0.294022
[08:56:08.282] TRAIN: iteration 6987 : loss : 0.112468, loss_ce: 0.005311, loss_dice: 0.219626
[08:56:08.490] TRAIN: iteration 6988 : loss : 0.233453, loss_ce: 0.004986, loss_dice: 0.461921
[08:56:08.698] TRAIN: iteration 6989 : loss : 0.252088, loss_ce: 0.003965, loss_dice: 0.500212
[08:56:08.906] TRAIN: iteration 6990 : loss : 0.059623, loss_ce: 0.004041, loss_dice: 0.115205
[08:56:09.115] TRAIN: iteration 6991 : loss : 0.092659, loss_ce: 0.007969, loss_dice: 0.177348
[08:56:09.325] TRAIN: iteration 6992 : loss : 0.051453, loss_ce: 0.002772, loss_dice: 0.100134
[08:56:09.535] TRAIN: iteration 6993 : loss : 0.099964, loss_ce: 0.003444, loss_dice: 0.196484
[08:56:09.747] TRAIN: iteration 6994 : loss : 0.101336, loss_ce: 0.003957, loss_dice: 0.198714
[08:56:09.975] TRAIN: iteration 6995 : loss : 0.129564, loss_ce: 0.005261, loss_dice: 0.253867
[08:56:10.189] TRAIN: iteration 6996 : loss : 0.118415, loss_ce: 0.010081, loss_dice: 0.226750
[08:56:10.396] TRAIN: iteration 6997 : loss : 0.185921, loss_ce: 0.004928, loss_dice: 0.366914
[08:56:10.604] TRAIN: iteration 6998 : loss : 0.255028, loss_ce: 0.009834, loss_dice: 0.500222
[08:56:10.811] TRAIN: iteration 6999 : loss : 0.173363, loss_ce: 0.006874, loss_dice: 0.339851
[08:56:11.019] TRAIN: iteration 7000 : loss : 0.227218, loss_ce: 0.005844, loss_dice: 0.448591
[08:56:11.256] TRAIN: iteration 7001 : loss : 0.171726, loss_ce: 0.005568, loss_dice: 0.337884
[08:56:11.465] TRAIN: iteration 7002 : loss : 0.071170, loss_ce: 0.003353, loss_dice: 0.138988
[08:56:11.678] TRAIN: iteration 7003 : loss : 0.091144, loss_ce: 0.007350, loss_dice: 0.174937
[08:56:11.885] TRAIN: iteration 7004 : loss : 0.198370, loss_ce: 0.004604, loss_dice: 0.392135
[08:56:12.091] TRAIN: iteration 7005 : loss : 0.242760, loss_ce: 0.005118, loss_dice: 0.480403
[08:56:12.299] TRAIN: iteration 7006 : loss : 0.110675, loss_ce: 0.006816, loss_dice: 0.214533
[08:56:12.507] TRAIN: iteration 7007 : loss : 0.113502, loss_ce: 0.010588, loss_dice: 0.216416
[08:56:12.714] TRAIN: iteration 7008 : loss : 0.158372, loss_ce: 0.008416, loss_dice: 0.308329
[08:56:12.928] TRAIN: iteration 7009 : loss : 0.103831, loss_ce: 0.004642, loss_dice: 0.203020
[08:56:13.155] TRAIN: iteration 7010 : loss : 0.177516, loss_ce: 0.004458, loss_dice: 0.350575
[08:56:13.367] TRAIN: iteration 7011 : loss : 0.086723, loss_ce: 0.003399, loss_dice: 0.170047
[08:56:13.579] TRAIN: iteration 7012 : loss : 0.252594, loss_ce: 0.004817, loss_dice: 0.500370
[08:56:13.789] TRAIN: iteration 7013 : loss : 0.068187, loss_ce: 0.005377, loss_dice: 0.130997
[08:56:14.025] TRAIN: iteration 7014 : loss : 0.178103, loss_ce: 0.033901, loss_dice: 0.322304
[08:56:14.233] TRAIN: iteration 7015 : loss : 0.080750, loss_ce: 0.004609, loss_dice: 0.156892
[08:56:14.443] TRAIN: iteration 7016 : loss : 0.088094, loss_ce: 0.003640, loss_dice: 0.172549
[08:56:14.652] TRAIN: iteration 7017 : loss : 0.251710, loss_ce: 0.003242, loss_dice: 0.500177
[08:56:14.863] TRAIN: iteration 7018 : loss : 0.069936, loss_ce: 0.005549, loss_dice: 0.134322
[08:56:15.077] TRAIN: iteration 7019 : loss : 0.140956, loss_ce: 0.003433, loss_dice: 0.278479
[08:56:15.288] TRAIN: iteration 7020 : loss : 0.060515, loss_ce: 0.004228, loss_dice: 0.116802
[08:56:15.527] TRAIN: iteration 7021 : loss : 0.244867, loss_ce: 0.010606, loss_dice: 0.479128
[08:56:15.741] TRAIN: iteration 7022 : loss : 0.181064, loss_ce: 0.004318, loss_dice: 0.357810
[08:56:15.951] TRAIN: iteration 7023 : loss : 0.183068, loss_ce: 0.003497, loss_dice: 0.362640
[08:56:16.159] TRAIN: iteration 7024 : loss : 0.251891, loss_ce: 0.003534, loss_dice: 0.500248
[08:56:16.368] TRAIN: iteration 7025 : loss : 0.127341, loss_ce: 0.003294, loss_dice: 0.251388
[08:56:16.576] TRAIN: iteration 7026 : loss : 0.073280, loss_ce: 0.002188, loss_dice: 0.144372
[08:56:16.791] TRAIN: iteration 7027 : loss : 0.072716, loss_ce: 0.002007, loss_dice: 0.143424
[08:56:17.000] TRAIN: iteration 7028 : loss : 0.215555, loss_ce: 0.002491, loss_dice: 0.428618
[08:56:17.213] TRAIN: iteration 7029 : loss : 0.231103, loss_ce: 0.039474, loss_dice: 0.422731
[08:56:17.428] TRAIN: iteration 7030 : loss : 0.247898, loss_ce: 0.002893, loss_dice: 0.492903
[08:56:17.643] TRAIN: iteration 7031 : loss : 0.250236, loss_ce: 0.000471, loss_dice: 0.500001
[08:56:17.851] TRAIN: iteration 7032 : loss : 0.081731, loss_ce: 0.001349, loss_dice: 0.162113
[08:56:18.060] TRAIN: iteration 7033 : loss : 0.250589, loss_ce: 0.001141, loss_dice: 0.500036
[08:56:18.273] TRAIN: iteration 7034 : loss : 0.250654, loss_ce: 0.003141, loss_dice: 0.498167
[08:56:18.480] TRAIN: iteration 7035 : loss : 0.177263, loss_ce: 0.013953, loss_dice: 0.340574
[08:56:18.687] TRAIN: iteration 7036 : loss : 0.105344, loss_ce: 0.006048, loss_dice: 0.204640
[08:56:18.894] TRAIN: iteration 7037 : loss : 0.194318, loss_ce: 0.004620, loss_dice: 0.384017
[08:56:19.102] TRAIN: iteration 7038 : loss : 0.111418, loss_ce: 0.008981, loss_dice: 0.213854
[08:56:19.310] TRAIN: iteration 7039 : loss : 0.112406, loss_ce: 0.005039, loss_dice: 0.219773
[08:56:19.521] TRAIN: iteration 7040 : loss : 0.119757, loss_ce: 0.009001, loss_dice: 0.230513
[08:56:19.773] TRAIN: iteration 7041 : loss : 0.125631, loss_ce: 0.006693, loss_dice: 0.244569
[08:56:19.990] TRAIN: iteration 7042 : loss : 0.214104, loss_ce: 0.007105, loss_dice: 0.421103
[08:56:20.205] TRAIN: iteration 7043 : loss : 0.153630, loss_ce: 0.004505, loss_dice: 0.302755
[08:56:20.414] TRAIN: iteration 7044 : loss : 0.057601, loss_ce: 0.003393, loss_dice: 0.111810
[08:56:20.625] TRAIN: iteration 7045 : loss : 0.089553, loss_ce: 0.004652, loss_dice: 0.174454
[08:56:20.835] TRAIN: iteration 7046 : loss : 0.252926, loss_ce: 0.005695, loss_dice: 0.500156
[08:56:21.045] TRAIN: iteration 7047 : loss : 0.252535, loss_ce: 0.004735, loss_dice: 0.500335
[08:56:21.253] TRAIN: iteration 7048 : loss : 0.076602, loss_ce: 0.008085, loss_dice: 0.145119
[08:56:21.461] TRAIN: iteration 7049 : loss : 0.181705, loss_ce: 0.005065, loss_dice: 0.358346
[08:56:21.669] TRAIN: iteration 7050 : loss : 0.252864, loss_ce: 0.005352, loss_dice: 0.500377
[08:56:21.876] TRAIN: iteration 7051 : loss : 0.078058, loss_ce: 0.005706, loss_dice: 0.150410
[08:56:22.084] TRAIN: iteration 7052 : loss : 0.071000, loss_ce: 0.005724, loss_dice: 0.136275
[08:56:22.292] TRAIN: iteration 7053 : loss : 0.120808, loss_ce: 0.003572, loss_dice: 0.238045
[08:56:22.499] TRAIN: iteration 7054 : loss : 0.185428, loss_ce: 0.005643, loss_dice: 0.365213
[08:56:22.706] TRAIN: iteration 7055 : loss : 0.152905, loss_ce: 0.007583, loss_dice: 0.298227
[08:56:22.915] TRAIN: iteration 7056 : loss : 0.087353, loss_ce: 0.003470, loss_dice: 0.171236
[08:56:23.125] TRAIN: iteration 7057 : loss : 0.233989, loss_ce: 0.004116, loss_dice: 0.463863
[08:56:23.339] TRAIN: iteration 7058 : loss : 0.248784, loss_ce: 0.004655, loss_dice: 0.492913
[08:56:23.555] TRAIN: iteration 7059 : loss : 0.191393, loss_ce: 0.003268, loss_dice: 0.379518
[08:56:23.764] TRAIN: iteration 7060 : loss : 0.081908, loss_ce: 0.006824, loss_dice: 0.156992
[08:56:23.999] TRAIN: iteration 7061 : loss : 0.093480, loss_ce: 0.002764, loss_dice: 0.184195
[08:56:24.205] TRAIN: iteration 7062 : loss : 0.101855, loss_ce: 0.004298, loss_dice: 0.199411
[08:56:24.413] TRAIN: iteration 7063 : loss : 0.097303, loss_ce: 0.003441, loss_dice: 0.191165
[08:56:24.620] TRAIN: iteration 7064 : loss : 0.245162, loss_ce: 0.005076, loss_dice: 0.485248
[08:56:24.830] TRAIN: iteration 7065 : loss : 0.236335, loss_ce: 0.002944, loss_dice: 0.469727
[08:56:25.048] TRAIN: iteration 7066 : loss : 0.113759, loss_ce: 0.006346, loss_dice: 0.221173
[08:56:25.256] TRAIN: iteration 7067 : loss : 0.146990, loss_ce: 0.010003, loss_dice: 0.283977
[08:56:25.466] TRAIN: iteration 7068 : loss : 0.189286, loss_ce: 0.002749, loss_dice: 0.375823
[08:56:25.673] TRAIN: iteration 7069 : loss : 0.120219, loss_ce: 0.003197, loss_dice: 0.237240
[08:56:25.883] TRAIN: iteration 7070 : loss : 0.248486, loss_ce: 0.002451, loss_dice: 0.494521
[08:56:26.097] TRAIN: iteration 7071 : loss : 0.194567, loss_ce: 0.002836, loss_dice: 0.386298
[08:56:26.306] TRAIN: iteration 7072 : loss : 0.164517, loss_ce: 0.005116, loss_dice: 0.323917
[08:56:26.513] TRAIN: iteration 7073 : loss : 0.133870, loss_ce: 0.015192, loss_dice: 0.252548
[08:56:26.723] TRAIN: iteration 7074 : loss : 0.247777, loss_ce: 0.003398, loss_dice: 0.492157
[08:56:26.933] TRAIN: iteration 7075 : loss : 0.160243, loss_ce: 0.012252, loss_dice: 0.308234
[08:56:27.143] TRAIN: iteration 7076 : loss : 0.123640, loss_ce: 0.010245, loss_dice: 0.237035
[08:56:27.352] TRAIN: iteration 7077 : loss : 0.152773, loss_ce: 0.006754, loss_dice: 0.298791
[08:56:27.565] TRAIN: iteration 7078 : loss : 0.251912, loss_ce: 0.003676, loss_dice: 0.500149
[08:56:27.778] TRAIN: iteration 7079 : loss : 0.252594, loss_ce: 0.004857, loss_dice: 0.500330
[08:56:27.986] TRAIN: iteration 7080 : loss : 0.163144, loss_ce: 0.005327, loss_dice: 0.320960
[08:56:28.215] TRAIN: iteration 7081 : loss : 0.244414, loss_ce: 0.006520, loss_dice: 0.482308
[08:56:28.423] TRAIN: iteration 7082 : loss : 0.252880, loss_ce: 0.005390, loss_dice: 0.500370
[08:56:28.632] TRAIN: iteration 7083 : loss : 0.084104, loss_ce: 0.005550, loss_dice: 0.162658
[08:56:28.841] TRAIN: iteration 7084 : loss : 0.191560, loss_ce: 0.011833, loss_dice: 0.371287
[08:56:29.057] TRAIN: iteration 7085 : loss : 0.250940, loss_ce: 0.006869, loss_dice: 0.495012
[08:56:29.268] TRAIN: iteration 7086 : loss : 0.119348, loss_ce: 0.004665, loss_dice: 0.234031
[08:56:29.478] TRAIN: iteration 7087 : loss : 0.115014, loss_ce: 0.013055, loss_dice: 0.216972
[08:56:29.688] TRAIN: iteration 7088 : loss : 0.253226, loss_ce: 0.006030, loss_dice: 0.500422
[08:56:29.900] TRAIN: iteration 7089 : loss : 0.252946, loss_ce: 0.005503, loss_dice: 0.500388
[08:56:30.108] TRAIN: iteration 7090 : loss : 0.074529, loss_ce: 0.003937, loss_dice: 0.145120
[08:56:30.317] TRAIN: iteration 7091 : loss : 0.183005, loss_ce: 0.007005, loss_dice: 0.359005
[08:56:30.524] TRAIN: iteration 7092 : loss : 0.072879, loss_ce: 0.005148, loss_dice: 0.140610
[08:56:30.732] TRAIN: iteration 7093 : loss : 0.104038, loss_ce: 0.005023, loss_dice: 0.203054
[08:56:30.940] TRAIN: iteration 7094 : loss : 0.244558, loss_ce: 0.007723, loss_dice: 0.481393
[08:56:31.147] TRAIN: iteration 7095 : loss : 0.135569, loss_ce: 0.005219, loss_dice: 0.265920
[08:56:31.355] TRAIN: iteration 7096 : loss : 0.115124, loss_ce: 0.004440, loss_dice: 0.225808
[08:56:31.565] TRAIN: iteration 7097 : loss : 0.252265, loss_ce: 0.004292, loss_dice: 0.500238
[08:56:32.164] TRAIN: iteration 7098 : loss : 0.252198, loss_ce: 0.004176, loss_dice: 0.500220
[08:56:32.371] TRAIN: iteration 7099 : loss : 0.057556, loss_ce: 0.003343, loss_dice: 0.111770
[08:56:32.578] TRAIN: iteration 7100 : loss : 0.252053, loss_ce: 0.003900, loss_dice: 0.500205
[08:56:32.814] TRAIN: iteration 7101 : loss : 0.129385, loss_ce: 0.005206, loss_dice: 0.253563
[08:56:33.022] TRAIN: iteration 7102 : loss : 0.131610, loss_ce: 0.003891, loss_dice: 0.259329
[08:56:33.229] TRAIN: iteration 7103 : loss : 0.178410, loss_ce: 0.004983, loss_dice: 0.351837
[08:56:33.438] TRAIN: iteration 7104 : loss : 0.162258, loss_ce: 0.010356, loss_dice: 0.314160
[08:56:33.647] TRAIN: iteration 7105 : loss : 0.087893, loss_ce: 0.003708, loss_dice: 0.172078
[08:56:33.855] TRAIN: iteration 7106 : loss : 0.201288, loss_ce: 0.004271, loss_dice: 0.398305
[08:56:34.063] TRAIN: iteration 7107 : loss : 0.251952, loss_ce: 0.003672, loss_dice: 0.500231
[08:56:34.271] TRAIN: iteration 7108 : loss : 0.082639, loss_ce: 0.003298, loss_dice: 0.161980
[08:56:34.479] TRAIN: iteration 7109 : loss : 0.251721, loss_ce: 0.003280, loss_dice: 0.500162
[08:56:34.687] TRAIN: iteration 7110 : loss : 0.154892, loss_ce: 0.010313, loss_dice: 0.299471
[08:56:34.898] TRAIN: iteration 7111 : loss : 0.085644, loss_ce: 0.003021, loss_dice: 0.168267
[08:56:35.107] TRAIN: iteration 7112 : loss : 0.136929, loss_ce: 0.006598, loss_dice: 0.267260
[08:56:35.326] TRAIN: iteration 7113 : loss : 0.193729, loss_ce: 0.005897, loss_dice: 0.381561
[08:56:35.535] TRAIN: iteration 7114 : loss : 0.124590, loss_ce: 0.008052, loss_dice: 0.241128
[08:56:35.743] TRAIN: iteration 7115 : loss : 0.129366, loss_ce: 0.011737, loss_dice: 0.246996
[08:56:35.951] TRAIN: iteration 7116 : loss : 0.251914, loss_ce: 0.003645, loss_dice: 0.500183
[08:56:36.161] TRAIN: iteration 7117 : loss : 0.149188, loss_ce: 0.004922, loss_dice: 0.293454
[08:56:36.369] TRAIN: iteration 7118 : loss : 0.229316, loss_ce: 0.011791, loss_dice: 0.446841
[08:56:36.579] TRAIN: iteration 7119 : loss : 0.136478, loss_ce: 0.005342, loss_dice: 0.267615
[08:56:36.786] TRAIN: iteration 7120 : loss : 0.169065, loss_ce: 0.007066, loss_dice: 0.331064
[08:56:37.026] TRAIN: iteration 7121 : loss : 0.107130, loss_ce: 0.005530, loss_dice: 0.208730
[08:56:37.234] TRAIN: iteration 7122 : loss : 0.145948, loss_ce: 0.005891, loss_dice: 0.286005
[08:56:37.442] TRAIN: iteration 7123 : loss : 0.176777, loss_ce: 0.008227, loss_dice: 0.345326
[08:56:37.652] TRAIN: iteration 7124 : loss : 0.120704, loss_ce: 0.007096, loss_dice: 0.234312
[08:56:37.866] TRAIN: iteration 7125 : loss : 0.079942, loss_ce: 0.004712, loss_dice: 0.155172
[08:56:38.075] TRAIN: iteration 7126 : loss : 0.155389, loss_ce: 0.005601, loss_dice: 0.305177
[08:56:38.284] TRAIN: iteration 7127 : loss : 0.161806, loss_ce: 0.005655, loss_dice: 0.317956
[08:56:38.496] TRAIN: iteration 7128 : loss : 0.106327, loss_ce: 0.004728, loss_dice: 0.207925
[08:56:38.705] TRAIN: iteration 7129 : loss : 0.252867, loss_ce: 0.005433, loss_dice: 0.500302
[08:56:38.913] TRAIN: iteration 7130 : loss : 0.252091, loss_ce: 0.005341, loss_dice: 0.498841
[08:56:39.125] TRAIN: iteration 7131 : loss : 0.116529, loss_ce: 0.005617, loss_dice: 0.227441
[08:56:39.333] TRAIN: iteration 7132 : loss : 0.091094, loss_ce: 0.006250, loss_dice: 0.175937
[08:56:39.542] TRAIN: iteration 7133 : loss : 0.092783, loss_ce: 0.012904, loss_dice: 0.172661
[08:56:39.781] TRAIN: iteration 7134 : loss : 0.071752, loss_ce: 0.005410, loss_dice: 0.138094
[08:56:39.989] TRAIN: iteration 7135 : loss : 0.211443, loss_ce: 0.006271, loss_dice: 0.416616
[08:56:40.195] TRAIN: iteration 7136 : loss : 0.172600, loss_ce: 0.004840, loss_dice: 0.340360
[08:56:40.410] TRAIN: iteration 7137 : loss : 0.087463, loss_ce: 0.003659, loss_dice: 0.171267
[08:56:40.623] TRAIN: iteration 7138 : loss : 0.252558, loss_ce: 0.004815, loss_dice: 0.500301
[08:56:40.831] TRAIN: iteration 7139 : loss : 0.076909, loss_ce: 0.003979, loss_dice: 0.149839
[08:56:41.040] TRAIN: iteration 7140 : loss : 0.095914, loss_ce: 0.007396, loss_dice: 0.184433
[08:56:41.281] TRAIN: iteration 7141 : loss : 0.251277, loss_ce: 0.004748, loss_dice: 0.497805
[08:56:41.491] TRAIN: iteration 7142 : loss : 0.229117, loss_ce: 0.005560, loss_dice: 0.452674
[08:56:41.698] TRAIN: iteration 7143 : loss : 0.141600, loss_ce: 0.007267, loss_dice: 0.275934
[08:56:41.910] TRAIN: iteration 7144 : loss : 0.184773, loss_ce: 0.034054, loss_dice: 0.335492
[08:56:42.119] TRAIN: iteration 7145 : loss : 0.157293, loss_ce: 0.003863, loss_dice: 0.310722
[08:56:42.327] TRAIN: iteration 7146 : loss : 0.252397, loss_ce: 0.004480, loss_dice: 0.500314
[08:56:42.880] TRAIN: iteration 7147 : loss : 0.250394, loss_ce: 0.006692, loss_dice: 0.494096
[08:56:43.090] TRAIN: iteration 7148 : loss : 0.213752, loss_ce: 0.005238, loss_dice: 0.422267
[08:56:43.297] TRAIN: iteration 7149 : loss : 0.077409, loss_ce: 0.004198, loss_dice: 0.150620
[08:56:43.504] TRAIN: iteration 7150 : loss : 0.252119, loss_ce: 0.004065, loss_dice: 0.500173
[08:56:43.711] TRAIN: iteration 7151 : loss : 0.246491, loss_ce: 0.005311, loss_dice: 0.487671
[08:56:43.961] TRAIN: iteration 7152 : loss : 0.064875, loss_ce: 0.002873, loss_dice: 0.126876
[08:56:44.169] TRAIN: iteration 7153 : loss : 0.116296, loss_ce: 0.003226, loss_dice: 0.229366
[08:56:44.383] TRAIN: iteration 7154 : loss : 0.172701, loss_ce: 0.005020, loss_dice: 0.340382
[08:56:44.591] TRAIN: iteration 7155 : loss : 0.237456, loss_ce: 0.007262, loss_dice: 0.467651
[08:56:44.799] TRAIN: iteration 7156 : loss : 0.168064, loss_ce: 0.017723, loss_dice: 0.318406
[08:56:45.006] TRAIN: iteration 7157 : loss : 0.119278, loss_ce: 0.002586, loss_dice: 0.235971
[08:56:45.213] TRAIN: iteration 7158 : loss : 0.125930, loss_ce: 0.002719, loss_dice: 0.249141
[08:56:45.420] TRAIN: iteration 7159 : loss : 0.111194, loss_ce: 0.008335, loss_dice: 0.214053
[08:56:45.628] TRAIN: iteration 7160 : loss : 0.177007, loss_ce: 0.002048, loss_dice: 0.351965
[08:56:45.869] TRAIN: iteration 7161 : loss : 0.233016, loss_ce: 0.008376, loss_dice: 0.457657
[08:56:46.078] TRAIN: iteration 7162 : loss : 0.244505, loss_ce: 0.006839, loss_dice: 0.482170
[08:56:46.288] TRAIN: iteration 7163 : loss : 0.138408, loss_ce: 0.002185, loss_dice: 0.274631
[08:56:46.496] TRAIN: iteration 7164 : loss : 0.158122, loss_ce: 0.001656, loss_dice: 0.314589
[08:56:46.704] TRAIN: iteration 7165 : loss : 0.193830, loss_ce: 0.002135, loss_dice: 0.385525
[08:56:46.911] TRAIN: iteration 7166 : loss : 0.222146, loss_ce: 0.008008, loss_dice: 0.436284
[08:56:47.118] TRAIN: iteration 7167 : loss : 0.223752, loss_ce: 0.003804, loss_dice: 0.443699
[08:56:47.326] TRAIN: iteration 7168 : loss : 0.166793, loss_ce: 0.005125, loss_dice: 0.328462
[08:56:47.534] TRAIN: iteration 7169 : loss : 0.249008, loss_ce: 0.001500, loss_dice: 0.496516
[08:56:47.741] TRAIN: iteration 7170 : loss : 0.162380, loss_ce: 0.002721, loss_dice: 0.322040
[08:56:47.948] TRAIN: iteration 7171 : loss : 0.169312, loss_ce: 0.005582, loss_dice: 0.333042
[08:56:48.155] TRAIN: iteration 7172 : loss : 0.251310, loss_ce: 0.002477, loss_dice: 0.500143
[08:56:49.141] TRAIN: iteration 7173 : loss : 0.136266, loss_ce: 0.003251, loss_dice: 0.269280
[08:56:49.349] TRAIN: iteration 7174 : loss : 0.111180, loss_ce: 0.002730, loss_dice: 0.219629
[08:56:49.560] TRAIN: iteration 7175 : loss : 0.194660, loss_ce: 0.005198, loss_dice: 0.384123
[08:56:49.767] TRAIN: iteration 7176 : loss : 0.121519, loss_ce: 0.003512, loss_dice: 0.239526
[08:56:49.976] TRAIN: iteration 7177 : loss : 0.229233, loss_ce: 0.005335, loss_dice: 0.453131
[08:56:50.183] TRAIN: iteration 7178 : loss : 0.242316, loss_ce: 0.004711, loss_dice: 0.479920
[08:56:50.391] TRAIN: iteration 7179 : loss : 0.085086, loss_ce: 0.005270, loss_dice: 0.164902
[08:56:50.605] TRAIN: iteration 7180 : loss : 0.184204, loss_ce: 0.003723, loss_dice: 0.364685
[08:56:50.859] TRAIN: iteration 7181 : loss : 0.150534, loss_ce: 0.013290, loss_dice: 0.287777
[08:56:51.067] TRAIN: iteration 7182 : loss : 0.122598, loss_ce: 0.004260, loss_dice: 0.240935
[08:56:51.274] TRAIN: iteration 7183 : loss : 0.230477, loss_ce: 0.005671, loss_dice: 0.455283
[08:56:51.505] TRAIN: iteration 7184 : loss : 0.177262, loss_ce: 0.007058, loss_dice: 0.347466
[08:56:51.720] TRAIN: iteration 7185 : loss : 0.059521, loss_ce: 0.003677, loss_dice: 0.115365
[08:56:51.930] TRAIN: iteration 7186 : loss : 0.143862, loss_ce: 0.005022, loss_dice: 0.282702
[08:56:52.140] TRAIN: iteration 7187 : loss : 0.190196, loss_ce: 0.006922, loss_dice: 0.373471
[08:56:52.352] TRAIN: iteration 7188 : loss : 0.167150, loss_ce: 0.004792, loss_dice: 0.329509
[08:56:52.566] TRAIN: iteration 7189 : loss : 0.100588, loss_ce: 0.004155, loss_dice: 0.197022
[08:56:52.783] TRAIN: iteration 7190 : loss : 0.163504, loss_ce: 0.004052, loss_dice: 0.322956
[08:56:52.995] TRAIN: iteration 7191 : loss : 0.125738, loss_ce: 0.004068, loss_dice: 0.247408
[08:56:53.208] TRAIN: iteration 7192 : loss : 0.104533, loss_ce: 0.004356, loss_dice: 0.204710
[08:56:53.417] TRAIN: iteration 7193 : loss : 0.130557, loss_ce: 0.003304, loss_dice: 0.257811
[08:56:53.627] TRAIN: iteration 7194 : loss : 0.163469, loss_ce: 0.005201, loss_dice: 0.321736
[08:56:53.835] TRAIN: iteration 7195 : loss : 0.148148, loss_ce: 0.008853, loss_dice: 0.287443
[08:56:54.044] TRAIN: iteration 7196 : loss : 0.195332, loss_ce: 0.005190, loss_dice: 0.385474
[08:56:54.251] TRAIN: iteration 7197 : loss : 0.099748, loss_ce: 0.002399, loss_dice: 0.197097
[08:56:54.459] TRAIN: iteration 7198 : loss : 0.192524, loss_ce: 0.030259, loss_dice: 0.354788
[08:56:54.668] TRAIN: iteration 7199 : loss : 0.125980, loss_ce: 0.003715, loss_dice: 0.248245
[08:56:54.875] TRAIN: iteration 7200 : loss : 0.109811, loss_ce: 0.009276, loss_dice: 0.210345
[08:56:55.112] TRAIN: iteration 7201 : loss : 0.247914, loss_ce: 0.004506, loss_dice: 0.491323
[08:56:55.322] TRAIN: iteration 7202 : loss : 0.226655, loss_ce: 0.003399, loss_dice: 0.449910
[08:56:55.534] TRAIN: iteration 7203 : loss : 0.237338, loss_ce: 0.010219, loss_dice: 0.464457
[08:56:55.748] TRAIN: iteration 7204 : loss : 0.153477, loss_ce: 0.009127, loss_dice: 0.297827
[08:56:55.963] TRAIN: iteration 7205 : loss : 0.112653, loss_ce: 0.002767, loss_dice: 0.222540
[08:56:56.175] TRAIN: iteration 7206 : loss : 0.080098, loss_ce: 0.003163, loss_dice: 0.157034
[08:56:56.385] TRAIN: iteration 7207 : loss : 0.105327, loss_ce: 0.004582, loss_dice: 0.206073
[08:56:56.593] TRAIN: iteration 7208 : loss : 0.183420, loss_ce: 0.002870, loss_dice: 0.363970
[08:56:56.808] TRAIN: iteration 7209 : loss : 0.110687, loss_ce: 0.005847, loss_dice: 0.215527
[08:56:57.019] TRAIN: iteration 7210 : loss : 0.068684, loss_ce: 0.002724, loss_dice: 0.134644
[08:56:57.226] TRAIN: iteration 7211 : loss : 0.207149, loss_ce: 0.007241, loss_dice: 0.407056
[08:56:57.433] TRAIN: iteration 7212 : loss : 0.251524, loss_ce: 0.002907, loss_dice: 0.500140
[08:56:57.643] TRAIN: iteration 7213 : loss : 0.201313, loss_ce: 0.026761, loss_dice: 0.375864
[08:56:57.852] TRAIN: iteration 7214 : loss : 0.160585, loss_ce: 0.010857, loss_dice: 0.310312
[08:56:58.060] TRAIN: iteration 7215 : loss : 0.247543, loss_ce: 0.006793, loss_dice: 0.488294
[08:56:58.267] TRAIN: iteration 7216 : loss : 0.122921, loss_ce: 0.007697, loss_dice: 0.238144
[08:56:58.474] TRAIN: iteration 7217 : loss : 0.158235, loss_ce: 0.006841, loss_dice: 0.309629
[08:56:58.682] TRAIN: iteration 7218 : loss : 0.083371, loss_ce: 0.004011, loss_dice: 0.162732
[08:56:58.890] TRAIN: iteration 7219 : loss : 0.159122, loss_ce: 0.024204, loss_dice: 0.294041
[08:56:59.099] TRAIN: iteration 7220 : loss : 0.204986, loss_ce: 0.020790, loss_dice: 0.389183
[08:56:59.335] TRAIN: iteration 7221 : loss : 0.162435, loss_ce: 0.023355, loss_dice: 0.301515
[08:56:59.541] TRAIN: iteration 7222 : loss : 0.212123, loss_ce: 0.007134, loss_dice: 0.417111
[08:56:59.748] TRAIN: iteration 7223 : loss : 0.144452, loss_ce: 0.005959, loss_dice: 0.282945
[08:56:59.955] TRAIN: iteration 7224 : loss : 0.206275, loss_ce: 0.005249, loss_dice: 0.407301
[08:57:00.163] TRAIN: iteration 7225 : loss : 0.210139, loss_ce: 0.007460, loss_dice: 0.412819
[08:57:00.372] TRAIN: iteration 7226 : loss : 0.240258, loss_ce: 0.006916, loss_dice: 0.473601
[08:57:00.579] TRAIN: iteration 7227 : loss : 0.064798, loss_ce: 0.005965, loss_dice: 0.123632
[08:57:00.786] TRAIN: iteration 7228 : loss : 0.093606, loss_ce: 0.007285, loss_dice: 0.179926
[08:57:00.994] TRAIN: iteration 7229 : loss : 0.155256, loss_ce: 0.006530, loss_dice: 0.303982
[08:57:01.204] TRAIN: iteration 7230 : loss : 0.104380, loss_ce: 0.004074, loss_dice: 0.204686
[08:57:01.412] TRAIN: iteration 7231 : loss : 0.184022, loss_ce: 0.008274, loss_dice: 0.359771
[08:57:01.619] TRAIN: iteration 7232 : loss : 0.251400, loss_ce: 0.012652, loss_dice: 0.490148
[08:57:01.826] TRAIN: iteration 7233 : loss : 0.092947, loss_ce: 0.005579, loss_dice: 0.180316
[08:57:02.039] TRAIN: iteration 7234 : loss : 0.189529, loss_ce: 0.006513, loss_dice: 0.372545
[08:57:02.249] TRAIN: iteration 7235 : loss : 0.233357, loss_ce: 0.004563, loss_dice: 0.462152
[08:57:02.461] TRAIN: iteration 7236 : loss : 0.082517, loss_ce: 0.004428, loss_dice: 0.160605
[08:57:02.672] TRAIN: iteration 7237 : loss : 0.247574, loss_ce: 0.004407, loss_dice: 0.490741
[08:57:02.880] TRAIN: iteration 7238 : loss : 0.073221, loss_ce: 0.005370, loss_dice: 0.141073
[08:57:03.088] TRAIN: iteration 7239 : loss : 0.049500, loss_ce: 0.002603, loss_dice: 0.096397
[08:57:03.296] TRAIN: iteration 7240 : loss : 0.249020, loss_ce: 0.005038, loss_dice: 0.493001
[08:57:03.537] TRAIN: iteration 7241 : loss : 0.073473, loss_ce: 0.003142, loss_dice: 0.143805
[08:57:03.748] TRAIN: iteration 7242 : loss : 0.251551, loss_ce: 0.002957, loss_dice: 0.500145
[08:57:03.964] TRAIN: iteration 7243 : loss : 0.150835, loss_ce: 0.009410, loss_dice: 0.292259
[08:57:04.172] TRAIN: iteration 7244 : loss : 0.087958, loss_ce: 0.002826, loss_dice: 0.173091
[08:57:04.381] TRAIN: iteration 7245 : loss : 0.098469, loss_ce: 0.004061, loss_dice: 0.192877
[08:57:04.593] TRAIN: iteration 7246 : loss : 0.126311, loss_ce: 0.010707, loss_dice: 0.241914
[08:57:04.802] TRAIN: iteration 7247 : loss : 0.130043, loss_ce: 0.003503, loss_dice: 0.256584
[08:57:05.010] TRAIN: iteration 7248 : loss : 0.035948, loss_ce: 0.002188, loss_dice: 0.069708
[08:57:05.247] TRAIN: iteration 7249 : loss : 0.113660, loss_ce: 0.006964, loss_dice: 0.220357
[08:57:05.455] TRAIN: iteration 7250 : loss : 0.120204, loss_ce: 0.003161, loss_dice: 0.237248
[08:57:05.664] TRAIN: iteration 7251 : loss : 0.226477, loss_ce: 0.004068, loss_dice: 0.448885
[08:57:05.873] TRAIN: iteration 7252 : loss : 0.184867, loss_ce: 0.010692, loss_dice: 0.359043
[08:57:06.083] TRAIN: iteration 7253 : loss : 0.151044, loss_ce: 0.006799, loss_dice: 0.295290
[08:57:06.292] TRAIN: iteration 7254 : loss : 0.226569, loss_ce: 0.009299, loss_dice: 0.443838
[08:57:06.500] TRAIN: iteration 7255 : loss : 0.036986, loss_ce: 0.002706, loss_dice: 0.071267
[08:57:06.711] TRAIN: iteration 7256 : loss : 0.108580, loss_ce: 0.003577, loss_dice: 0.213583
[08:57:06.921] TRAIN: iteration 7257 : loss : 0.070078, loss_ce: 0.003352, loss_dice: 0.136805
[08:57:07.137] TRAIN: iteration 7258 : loss : 0.164572, loss_ce: 0.004271, loss_dice: 0.324874
[08:57:07.349] TRAIN: iteration 7259 : loss : 0.105868, loss_ce: 0.006242, loss_dice: 0.205493
[08:57:07.556] TRAIN: iteration 7260 : loss : 0.197000, loss_ce: 0.003801, loss_dice: 0.390199
[08:57:07.557] NaN or Inf found in input tensor.
[08:57:07.772] TRAIN: iteration 7261 : loss : 0.112931, loss_ce: 0.005551, loss_dice: 0.220311
[08:57:07.979] TRAIN: iteration 7262 : loss : 0.252023, loss_ce: 0.003826, loss_dice: 0.500221
[08:57:08.188] TRAIN: iteration 7263 : loss : 0.184631, loss_ce: 0.003954, loss_dice: 0.365309
[08:57:08.403] TRAIN: iteration 7264 : loss : 0.072511, loss_ce: 0.002691, loss_dice: 0.142331
[08:57:08.611] TRAIN: iteration 7265 : loss : 0.129251, loss_ce: 0.003587, loss_dice: 0.254915
[08:57:08.823] TRAIN: iteration 7266 : loss : 0.252062, loss_ce: 0.003889, loss_dice: 0.500235
[08:57:09.036] TRAIN: iteration 7267 : loss : 0.251990, loss_ce: 0.003726, loss_dice: 0.500254
[08:57:09.244] TRAIN: iteration 7268 : loss : 0.097149, loss_ce: 0.007620, loss_dice: 0.186677
[08:57:09.464] TRAIN: iteration 7269 : loss : 0.190357, loss_ce: 0.005601, loss_dice: 0.375114
[08:57:09.675] TRAIN: iteration 7270 : loss : 0.234484, loss_ce: 0.004100, loss_dice: 0.464868
[08:57:09.886] TRAIN: iteration 7271 : loss : 0.194063, loss_ce: 0.013491, loss_dice: 0.374634
[08:57:10.102] TRAIN: iteration 7272 : loss : 0.097525, loss_ce: 0.004282, loss_dice: 0.190768
[08:57:10.309] TRAIN: iteration 7273 : loss : 0.133969, loss_ce: 0.003538, loss_dice: 0.264399
[08:57:10.517] TRAIN: iteration 7274 : loss : 0.080873, loss_ce: 0.004047, loss_dice: 0.157698
[08:57:10.728] TRAIN: iteration 7275 : loss : 0.235570, loss_ce: 0.002921, loss_dice: 0.468219
[08:57:10.941] TRAIN: iteration 7276 : loss : 0.254313, loss_ce: 0.008731, loss_dice: 0.499896
[08:57:11.156] TRAIN: iteration 7277 : loss : 0.129909, loss_ce: 0.009203, loss_dice: 0.250615
[08:57:11.365] TRAIN: iteration 7278 : loss : 0.196014, loss_ce: 0.010806, loss_dice: 0.381222
[08:57:11.576] TRAIN: iteration 7279 : loss : 0.125768, loss_ce: 0.003505, loss_dice: 0.248030
[08:57:11.787] TRAIN: iteration 7280 : loss : 0.252073, loss_ce: 0.003891, loss_dice: 0.500255
[08:57:12.052] TRAIN: iteration 7281 : loss : 0.188230, loss_ce: 0.003766, loss_dice: 0.372693
[08:57:12.263] TRAIN: iteration 7282 : loss : 0.250633, loss_ce: 0.003582, loss_dice: 0.497685
[08:57:12.472] TRAIN: iteration 7283 : loss : 0.106413, loss_ce: 0.006000, loss_dice: 0.206826
[08:57:12.716] TRAIN: iteration 7284 : loss : 0.164929, loss_ce: 0.006786, loss_dice: 0.323071
[08:57:12.923] TRAIN: iteration 7285 : loss : 0.240449, loss_ce: 0.004262, loss_dice: 0.476636
[08:57:13.131] TRAIN: iteration 7286 : loss : 0.086049, loss_ce: 0.003413, loss_dice: 0.168686
[08:57:13.338] TRAIN: iteration 7287 : loss : 0.249808, loss_ce: 0.003242, loss_dice: 0.496374
[08:57:13.547] TRAIN: iteration 7288 : loss : 0.197731, loss_ce: 0.008599, loss_dice: 0.386863
[08:57:13.757] TRAIN: iteration 7289 : loss : 0.123684, loss_ce: 0.007973, loss_dice: 0.239395
[08:57:13.965] TRAIN: iteration 7290 : loss : 0.096420, loss_ce: 0.005524, loss_dice: 0.187316
[08:57:14.221] TRAIN: iteration 7291 : loss : 0.139733, loss_ce: 0.014163, loss_dice: 0.265302
[08:57:14.430] TRAIN: iteration 7292 : loss : 0.240189, loss_ce: 0.004269, loss_dice: 0.476109
[08:57:14.640] TRAIN: iteration 7293 : loss : 0.222539, loss_ce: 0.005597, loss_dice: 0.439482
[08:57:14.848] TRAIN: iteration 7294 : loss : 0.258054, loss_ce: 0.018204, loss_dice: 0.497903
[08:57:15.056] TRAIN: iteration 7295 : loss : 0.096705, loss_ce: 0.004391, loss_dice: 0.189018
[08:57:15.267] TRAIN: iteration 7296 : loss : 0.158061, loss_ce: 0.005442, loss_dice: 0.310681
[08:57:15.478] TRAIN: iteration 7297 : loss : 0.105015, loss_ce: 0.008203, loss_dice: 0.201826
[08:57:15.685] TRAIN: iteration 7298 : loss : 0.070071, loss_ce: 0.006777, loss_dice: 0.133365
[08:57:15.893] TRAIN: iteration 7299 : loss : 0.056317, loss_ce: 0.005523, loss_dice: 0.107111
[08:57:16.177] TRAIN: iteration 7300 : loss : 0.253131, loss_ce: 0.005971, loss_dice: 0.500291
[08:57:16.420] TRAIN: iteration 7301 : loss : 0.128806, loss_ce: 0.007413, loss_dice: 0.250198
[08:57:16.629] TRAIN: iteration 7302 : loss : 0.253533, loss_ce: 0.006676, loss_dice: 0.500390
[08:57:16.837] TRAIN: iteration 7303 : loss : 0.196679, loss_ce: 0.010730, loss_dice: 0.382627
[08:57:17.048] TRAIN: iteration 7304 : loss : 0.110424, loss_ce: 0.006348, loss_dice: 0.214501
[08:57:17.256] TRAIN: iteration 7305 : loss : 0.251812, loss_ce: 0.007012, loss_dice: 0.496611
[08:57:17.464] TRAIN: iteration 7306 : loss : 0.134199, loss_ce: 0.008443, loss_dice: 0.259955
[08:57:17.671] TRAIN: iteration 7307 : loss : 0.160715, loss_ce: 0.014856, loss_dice: 0.306574
[08:57:17.880] TRAIN: iteration 7308 : loss : 0.212528, loss_ce: 0.009999, loss_dice: 0.415056
[08:57:18.089] TRAIN: iteration 7309 : loss : 0.071297, loss_ce: 0.009536, loss_dice: 0.133058
[08:57:18.297] TRAIN: iteration 7310 : loss : 0.253093, loss_ce: 0.007405, loss_dice: 0.498781
[08:57:18.504] TRAIN: iteration 7311 : loss : 0.081117, loss_ce: 0.005871, loss_dice: 0.156364
[08:57:18.712] TRAIN: iteration 7312 : loss : 0.254353, loss_ce: 0.008121, loss_dice: 0.500584
[08:57:18.926] TRAIN: iteration 7313 : loss : 0.092951, loss_ce: 0.004980, loss_dice: 0.180922
[08:57:19.135] TRAIN: iteration 7314 : loss : 0.253711, loss_ce: 0.006993, loss_dice: 0.500430
[08:57:19.343] TRAIN: iteration 7315 : loss : 0.137770, loss_ce: 0.005219, loss_dice: 0.270321
[08:57:19.551] TRAIN: iteration 7316 : loss : 0.149353, loss_ce: 0.006076, loss_dice: 0.292629
[08:57:19.758] TRAIN: iteration 7317 : loss : 0.146858, loss_ce: 0.005505, loss_dice: 0.288210
[08:57:19.966] TRAIN: iteration 7318 : loss : 0.109928, loss_ce: 0.006771, loss_dice: 0.213085
[08:57:20.174] TRAIN: iteration 7319 : loss : 0.253015, loss_ce: 0.005663, loss_dice: 0.500368
[08:57:20.381] TRAIN: iteration 7320 : loss : 0.100203, loss_ce: 0.003436, loss_dice: 0.196971
[08:57:20.617] TRAIN: iteration 7321 : loss : 0.169970, loss_ce: 0.005164, loss_dice: 0.334775
[08:57:20.825] TRAIN: iteration 7322 : loss : 0.248537, loss_ce: 0.004612, loss_dice: 0.492463
[08:57:21.035] TRAIN: iteration 7323 : loss : 0.248554, loss_ce: 0.004939, loss_dice: 0.492169
[08:57:21.242] TRAIN: iteration 7324 : loss : 0.110535, loss_ce: 0.003654, loss_dice: 0.217417
[08:57:21.450] TRAIN: iteration 7325 : loss : 0.102145, loss_ce: 0.005653, loss_dice: 0.198637
[08:57:21.658] TRAIN: iteration 7326 : loss : 0.139272, loss_ce: 0.003807, loss_dice: 0.274736
[08:57:21.868] TRAIN: iteration 7327 : loss : 0.182358, loss_ce: 0.004191, loss_dice: 0.360525
[08:57:22.852] TRAIN: iteration 7328 : loss : 0.169915, loss_ce: 0.002689, loss_dice: 0.337141
[08:57:23.062] TRAIN: iteration 7329 : loss : 0.228916, loss_ce: 0.011917, loss_dice: 0.445915
[08:57:23.271] TRAIN: iteration 7330 : loss : 0.034747, loss_ce: 0.001762, loss_dice: 0.067733
[08:57:23.483] TRAIN: iteration 7331 : loss : 0.180209, loss_ce: 0.003182, loss_dice: 0.357236
[08:57:23.691] TRAIN: iteration 7332 : loss : 0.248754, loss_ce: 0.002802, loss_dice: 0.494706
[08:57:23.900] TRAIN: iteration 7333 : loss : 0.129137, loss_ce: 0.006937, loss_dice: 0.251337
[08:57:24.108] TRAIN: iteration 7334 : loss : 0.237552, loss_ce: 0.001625, loss_dice: 0.473480
[08:57:24.316] TRAIN: iteration 7335 : loss : 0.188592, loss_ce: 0.008039, loss_dice: 0.369146
[08:57:24.523] TRAIN: iteration 7336 : loss : 0.117002, loss_ce: 0.004833, loss_dice: 0.229171
[08:57:24.730] TRAIN: iteration 7337 : loss : 0.197692, loss_ce: 0.006691, loss_dice: 0.388693
[08:57:24.939] TRAIN: iteration 7338 : loss : 0.097769, loss_ce: 0.002572, loss_dice: 0.192967
[08:57:25.146] TRAIN: iteration 7339 : loss : 0.092014, loss_ce: 0.004080, loss_dice: 0.179948
[08:57:25.353] TRAIN: iteration 7340 : loss : 0.251589, loss_ce: 0.003010, loss_dice: 0.500168
[08:57:25.602] TRAIN: iteration 7341 : loss : 0.083801, loss_ce: 0.006482, loss_dice: 0.161119
[08:57:25.810] TRAIN: iteration 7342 : loss : 0.131799, loss_ce: 0.014485, loss_dice: 0.249112
[08:57:26.019] TRAIN: iteration 7343 : loss : 0.239699, loss_ce: 0.005627, loss_dice: 0.473772
[08:57:26.229] TRAIN: iteration 7344 : loss : 0.252652, loss_ce: 0.004939, loss_dice: 0.500365
[08:57:26.439] TRAIN: iteration 7345 : loss : 0.157086, loss_ce: 0.018490, loss_dice: 0.295682
[08:57:26.652] TRAIN: iteration 7346 : loss : 0.124147, loss_ce: 0.010933, loss_dice: 0.237362
[08:57:26.859] TRAIN: iteration 7347 : loss : 0.142043, loss_ce: 0.006167, loss_dice: 0.277920
[08:57:27.069] TRAIN: iteration 7348 : loss : 0.114589, loss_ce: 0.005421, loss_dice: 0.223756
[08:57:27.278] TRAIN: iteration 7349 : loss : 0.156499, loss_ce: 0.005487, loss_dice: 0.307512
[08:57:27.487] TRAIN: iteration 7350 : loss : 0.139295, loss_ce: 0.005337, loss_dice: 0.273252
[08:57:27.698] TRAIN: iteration 7351 : loss : 0.057364, loss_ce: 0.006214, loss_dice: 0.108515
[08:57:27.908] TRAIN: iteration 7352 : loss : 0.170248, loss_ce: 0.014331, loss_dice: 0.326164
[08:57:28.118] TRAIN: iteration 7353 : loss : 0.149980, loss_ce: 0.006808, loss_dice: 0.293151
[08:57:28.327] TRAIN: iteration 7354 : loss : 0.187214, loss_ce: 0.008326, loss_dice: 0.366102
[08:57:28.539] TRAIN: iteration 7355 : loss : 0.080569, loss_ce: 0.007224, loss_dice: 0.153913
[08:57:28.749] TRAIN: iteration 7356 : loss : 0.107702, loss_ce: 0.005626, loss_dice: 0.209778
[08:57:28.957] TRAIN: iteration 7357 : loss : 0.117787, loss_ce: 0.008058, loss_dice: 0.227515
[08:57:29.167] TRAIN: iteration 7358 : loss : 0.250734, loss_ce: 0.007001, loss_dice: 0.494466
[08:57:29.376] TRAIN: iteration 7359 : loss : 0.152381, loss_ce: 0.006332, loss_dice: 0.298429
[08:57:29.585] TRAIN: iteration 7360 : loss : 0.211552, loss_ce: 0.008751, loss_dice: 0.414353
[08:57:29.821] TRAIN: iteration 7361 : loss : 0.240678, loss_ce: 0.007740, loss_dice: 0.473616
[08:57:30.029] TRAIN: iteration 7362 : loss : 0.238400, loss_ce: 0.008552, loss_dice: 0.468248
[08:57:30.238] TRAIN: iteration 7363 : loss : 0.199157, loss_ce: 0.004358, loss_dice: 0.393956
[08:57:30.449] TRAIN: iteration 7364 : loss : 0.159636, loss_ce: 0.005524, loss_dice: 0.313749
[08:57:30.664] TRAIN: iteration 7365 : loss : 0.085260, loss_ce: 0.007685, loss_dice: 0.162836
[08:57:30.872] TRAIN: iteration 7366 : loss : 0.170297, loss_ce: 0.011172, loss_dice: 0.329421
[08:57:31.081] TRAIN: iteration 7367 : loss : 0.132953, loss_ce: 0.004086, loss_dice: 0.261819
[08:57:31.289] TRAIN: iteration 7368 : loss : 0.084880, loss_ce: 0.004376, loss_dice: 0.165385
[08:57:31.497] TRAIN: iteration 7369 : loss : 0.134255, loss_ce: 0.019491, loss_dice: 0.249020
[08:57:31.704] TRAIN: iteration 7370 : loss : 0.238334, loss_ce: 0.058613, loss_dice: 0.418054
[08:57:31.913] TRAIN: iteration 7371 : loss : 0.092565, loss_ce: 0.004834, loss_dice: 0.180297
[08:57:32.128] TRAIN: iteration 7372 : loss : 0.230059, loss_ce: 0.003985, loss_dice: 0.456132
[08:57:32.336] TRAIN: iteration 7373 : loss : 0.213783, loss_ce: 0.003249, loss_dice: 0.424317
[08:57:32.544] TRAIN: iteration 7374 : loss : 0.202853, loss_ce: 0.010267, loss_dice: 0.395438
[08:57:32.754] TRAIN: iteration 7375 : loss : 0.054763, loss_ce: 0.005655, loss_dice: 0.103872
[08:57:32.966] TRAIN: iteration 7376 : loss : 0.178686, loss_ce: 0.011273, loss_dice: 0.346099
[08:57:33.175] TRAIN: iteration 7377 : loss : 0.101236, loss_ce: 0.003819, loss_dice: 0.198653
[08:57:34.020] TRAIN: iteration 7378 : loss : 0.227945, loss_ce: 0.010939, loss_dice: 0.444951
[08:57:34.227] TRAIN: iteration 7379 : loss : 0.105657, loss_ce: 0.006204, loss_dice: 0.205110
[08:57:34.435] TRAIN: iteration 7380 : loss : 0.125929, loss_ce: 0.005519, loss_dice: 0.246339
[08:57:34.681] TRAIN: iteration 7381 : loss : 0.132042, loss_ce: 0.007398, loss_dice: 0.256686
[08:57:34.911] TRAIN: iteration 7382 : loss : 0.165409, loss_ce: 0.004839, loss_dice: 0.325978
[08:57:35.119] TRAIN: iteration 7383 : loss : 0.097733, loss_ce: 0.012158, loss_dice: 0.183308
[08:57:35.326] TRAIN: iteration 7384 : loss : 0.074304, loss_ce: 0.003267, loss_dice: 0.145342
[08:57:35.536] TRAIN: iteration 7385 : loss : 0.252168, loss_ce: 0.004116, loss_dice: 0.500220
[08:57:35.744] TRAIN: iteration 7386 : loss : 0.216867, loss_ce: 0.009537, loss_dice: 0.424197
[08:57:35.954] TRAIN: iteration 7387 : loss : 0.133824, loss_ce: 0.007594, loss_dice: 0.260054
[08:57:36.163] TRAIN: iteration 7388 : loss : 0.101269, loss_ce: 0.005228, loss_dice: 0.197309
[08:57:36.371] TRAIN: iteration 7389 : loss : 0.208867, loss_ce: 0.010062, loss_dice: 0.407671
[08:57:36.582] TRAIN: iteration 7390 : loss : 0.074892, loss_ce: 0.003079, loss_dice: 0.146704
[08:57:36.790] TRAIN: iteration 7391 : loss : 0.156390, loss_ce: 0.007982, loss_dice: 0.304797
[08:57:36.999] TRAIN: iteration 7392 : loss : 0.169771, loss_ce: 0.005476, loss_dice: 0.334066
[08:57:37.208] TRAIN: iteration 7393 : loss : 0.125000, loss_ce: 0.007998, loss_dice: 0.242001
[08:57:37.417] TRAIN: iteration 7394 : loss : 0.241155, loss_ce: 0.006441, loss_dice: 0.475868
[08:57:37.625] TRAIN: iteration 7395 : loss : 0.248916, loss_ce: 0.005087, loss_dice: 0.492746
[08:57:37.833] TRAIN: iteration 7396 : loss : 0.199728, loss_ce: 0.005930, loss_dice: 0.393527
[08:57:38.040] TRAIN: iteration 7397 : loss : 0.252639, loss_ce: 0.005039, loss_dice: 0.500239
[08:57:38.248] TRAIN: iteration 7398 : loss : 0.162215, loss_ce: 0.007227, loss_dice: 0.317203
[08:57:38.456] TRAIN: iteration 7399 : loss : 0.054528, loss_ce: 0.003466, loss_dice: 0.105590
[08:57:38.673] TRAIN: iteration 7400 : loss : 0.231847, loss_ce: 0.006979, loss_dice: 0.456716
[08:57:38.674] NaN or Inf found in input tensor.
[08:57:38.898] TRAIN: iteration 7401 : loss : 0.151520, loss_ce: 0.006278, loss_dice: 0.296763
[08:57:39.111] TRAIN: iteration 7402 : loss : 0.110353, loss_ce: 0.009600, loss_dice: 0.211106
[08:57:39.321] TRAIN: iteration 7403 : loss : 0.120990, loss_ce: 0.010757, loss_dice: 0.231223
[08:57:39.529] TRAIN: iteration 7404 : loss : 0.251934, loss_ce: 0.003749, loss_dice: 0.500119
[08:57:39.736] TRAIN: iteration 7405 : loss : 0.228511, loss_ce: 0.006143, loss_dice: 0.450879
[08:57:39.946] TRAIN: iteration 7406 : loss : 0.268682, loss_ce: 0.037880, loss_dice: 0.499485
[08:57:40.160] TRAIN: iteration 7407 : loss : 0.161633, loss_ce: 0.008574, loss_dice: 0.314693
[08:57:40.371] TRAIN: iteration 7408 : loss : 0.152108, loss_ce: 0.006263, loss_dice: 0.297954
[08:57:40.579] TRAIN: iteration 7409 : loss : 0.180986, loss_ce: 0.005054, loss_dice: 0.356918
[08:57:40.788] TRAIN: iteration 7410 : loss : 0.176396, loss_ce: 0.006219, loss_dice: 0.346573
[08:57:40.996] TRAIN: iteration 7411 : loss : 0.174912, loss_ce: 0.025756, loss_dice: 0.324068
[08:57:41.203] TRAIN: iteration 7412 : loss : 0.121309, loss_ce: 0.005374, loss_dice: 0.237244
[08:57:41.410] TRAIN: iteration 7413 : loss : 0.207205, loss_ce: 0.005175, loss_dice: 0.409236
[08:57:41.617] TRAIN: iteration 7414 : loss : 0.053735, loss_ce: 0.002979, loss_dice: 0.104491
[08:57:41.827] TRAIN: iteration 7415 : loss : 0.100227, loss_ce: 0.005798, loss_dice: 0.194655
[08:57:42.034] TRAIN: iteration 7416 : loss : 0.080899, loss_ce: 0.004134, loss_dice: 0.157664
[08:57:42.319] TRAIN: iteration 7417 : loss : 0.242069, loss_ce: 0.005258, loss_dice: 0.478880
[08:57:42.532] TRAIN: iteration 7418 : loss : 0.251632, loss_ce: 0.004657, loss_dice: 0.498608
[08:57:42.742] TRAIN: iteration 7419 : loss : 0.117560, loss_ce: 0.004601, loss_dice: 0.230518
[08:57:42.953] TRAIN: iteration 7420 : loss : 0.142461, loss_ce: 0.005426, loss_dice: 0.279495
[08:57:43.196] TRAIN: iteration 7421 : loss : 0.199947, loss_ce: 0.005805, loss_dice: 0.394089
[08:57:43.415] TRAIN: iteration 7422 : loss : 0.243978, loss_ce: 0.003115, loss_dice: 0.484841
[08:57:43.622] TRAIN: iteration 7423 : loss : 0.079660, loss_ce: 0.006197, loss_dice: 0.153124
[08:57:43.985] TRAIN: iteration 7424 : loss : 0.249616, loss_ce: 0.003437, loss_dice: 0.495796
[08:57:44.192] TRAIN: iteration 7425 : loss : 0.179831, loss_ce: 0.033268, loss_dice: 0.326394
[08:57:44.404] TRAIN: iteration 7426 : loss : 0.211889, loss_ce: 0.003678, loss_dice: 0.420100
[08:57:44.612] TRAIN: iteration 7427 : loss : 0.159705, loss_ce: 0.005075, loss_dice: 0.314335
[08:57:44.820] TRAIN: iteration 7428 : loss : 0.128226, loss_ce: 0.003580, loss_dice: 0.252873
[08:57:45.291] TRAIN: iteration 7429 : loss : 0.163053, loss_ce: 0.004113, loss_dice: 0.321993
[08:57:45.500] TRAIN: iteration 7430 : loss : 0.196228, loss_ce: 0.008599, loss_dice: 0.383857
[08:57:45.709] TRAIN: iteration 7431 : loss : 0.224126, loss_ce: 0.002306, loss_dice: 0.445945
[08:57:46.278] TRAIN: iteration 7432 : loss : 0.137272, loss_ce: 0.004149, loss_dice: 0.270395
[08:57:46.487] TRAIN: iteration 7433 : loss : 0.214405, loss_ce: 0.003568, loss_dice: 0.425242
[08:57:46.695] TRAIN: iteration 7434 : loss : 0.034692, loss_ce: 0.001409, loss_dice: 0.067975
[08:57:46.904] TRAIN: iteration 7435 : loss : 0.171887, loss_ce: 0.008666, loss_dice: 0.335108
[08:57:47.123] TRAIN: iteration 7436 : loss : 0.083717, loss_ce: 0.002517, loss_dice: 0.164917
[08:57:47.333] TRAIN: iteration 7437 : loss : 0.080955, loss_ce: 0.003127, loss_dice: 0.158783
[08:57:47.541] TRAIN: iteration 7438 : loss : 0.251409, loss_ce: 0.002678, loss_dice: 0.500140
[08:57:47.756] TRAIN: iteration 7439 : loss : 0.205564, loss_ce: 0.005297, loss_dice: 0.405830
[08:57:47.966] TRAIN: iteration 7440 : loss : 0.218116, loss_ce: 0.005479, loss_dice: 0.430752
[08:57:48.210] TRAIN: iteration 7441 : loss : 0.114061, loss_ce: 0.017292, loss_dice: 0.210830
[08:57:48.417] TRAIN: iteration 7442 : loss : 0.237338, loss_ce: 0.007522, loss_dice: 0.467154
[08:57:48.624] TRAIN: iteration 7443 : loss : 0.032646, loss_ce: 0.002676, loss_dice: 0.062616
[08:57:48.832] TRAIN: iteration 7444 : loss : 0.105639, loss_ce: 0.003858, loss_dice: 0.207421
[08:57:49.040] TRAIN: iteration 7445 : loss : 0.166183, loss_ce: 0.016574, loss_dice: 0.315792
[08:57:49.251] TRAIN: iteration 7446 : loss : 0.117322, loss_ce: 0.004939, loss_dice: 0.229706
[08:57:49.459] TRAIN: iteration 7447 : loss : 0.113163, loss_ce: 0.006325, loss_dice: 0.220001
[08:57:49.668] TRAIN: iteration 7448 : loss : 0.252750, loss_ce: 0.005145, loss_dice: 0.500354
[08:57:49.879] TRAIN: iteration 7449 : loss : 0.137308, loss_ce: 0.009930, loss_dice: 0.264686
[08:57:50.088] TRAIN: iteration 7450 : loss : 0.081493, loss_ce: 0.003834, loss_dice: 0.159151
[08:57:50.298] TRAIN: iteration 7451 : loss : 0.104359, loss_ce: 0.004954, loss_dice: 0.203763
[08:57:50.506] TRAIN: iteration 7452 : loss : 0.195770, loss_ce: 0.004283, loss_dice: 0.387256
[08:57:50.713] TRAIN: iteration 7453 : loss : 0.252794, loss_ce: 0.005237, loss_dice: 0.500352
[08:57:50.992] TRAIN: iteration 7454 : loss : 0.224571, loss_ce: 0.007176, loss_dice: 0.441966
[08:57:51.202] TRAIN: iteration 7455 : loss : 0.164443, loss_ce: 0.008127, loss_dice: 0.320760
[08:57:51.410] TRAIN: iteration 7456 : loss : 0.153122, loss_ce: 0.012712, loss_dice: 0.293532
[08:57:51.621] TRAIN: iteration 7457 : loss : 0.251366, loss_ce: 0.005455, loss_dice: 0.497276
[08:57:51.834] TRAIN: iteration 7458 : loss : 0.141488, loss_ce: 0.008181, loss_dice: 0.274795
[08:57:52.042] TRAIN: iteration 7459 : loss : 0.251824, loss_ce: 0.004850, loss_dice: 0.498798
[08:57:52.251] TRAIN: iteration 7460 : loss : 0.078307, loss_ce: 0.005381, loss_dice: 0.151233
[08:57:52.497] TRAIN: iteration 7461 : loss : 0.104839, loss_ce: 0.006650, loss_dice: 0.203029
[08:57:54.609] TRAIN: iteration 7462 : loss : 0.239205, loss_ce: 0.004808, loss_dice: 0.473602
[08:57:54.817] TRAIN: iteration 7463 : loss : 0.252320, loss_ce: 0.004411, loss_dice: 0.500230
[08:57:55.025] TRAIN: iteration 7464 : loss : 0.148505, loss_ce: 0.007382, loss_dice: 0.289629
[08:57:55.234] TRAIN: iteration 7465 : loss : 0.231705, loss_ce: 0.007641, loss_dice: 0.455770
[08:57:55.441] TRAIN: iteration 7466 : loss : 0.227801, loss_ce: 0.011469, loss_dice: 0.444134
[08:57:55.652] TRAIN: iteration 7467 : loss : 0.135224, loss_ce: 0.005624, loss_dice: 0.264823
[08:57:55.867] TRAIN: iteration 7468 : loss : 0.145649, loss_ce: 0.004124, loss_dice: 0.287173
[08:57:56.075] TRAIN: iteration 7469 : loss : 0.180217, loss_ce: 0.005223, loss_dice: 0.355210
[08:57:56.282] TRAIN: iteration 7470 : loss : 0.081401, loss_ce: 0.004898, loss_dice: 0.157904
[08:57:56.490] TRAIN: iteration 7471 : loss : 0.089667, loss_ce: 0.005260, loss_dice: 0.174075
[08:57:56.702] TRAIN: iteration 7472 : loss : 0.093460, loss_ce: 0.005067, loss_dice: 0.181854
[08:57:56.910] TRAIN: iteration 7473 : loss : 0.060775, loss_ce: 0.002743, loss_dice: 0.118808
[08:57:57.125] TRAIN: iteration 7474 : loss : 0.076225, loss_ce: 0.003782, loss_dice: 0.148668
[08:57:57.337] TRAIN: iteration 7475 : loss : 0.131739, loss_ce: 0.003917, loss_dice: 0.259561
[08:57:57.552] TRAIN: iteration 7476 : loss : 0.251798, loss_ce: 0.004183, loss_dice: 0.499414
[08:57:57.760] TRAIN: iteration 7477 : loss : 0.246012, loss_ce: 0.003409, loss_dice: 0.488616
[08:57:57.970] TRAIN: iteration 7478 : loss : 0.210928, loss_ce: 0.008351, loss_dice: 0.413505
[08:57:58.179] TRAIN: iteration 7479 : loss : 0.182361, loss_ce: 0.025735, loss_dice: 0.338986
[08:57:58.386] TRAIN: iteration 7480 : loss : 0.163134, loss_ce: 0.012342, loss_dice: 0.313925
[08:57:58.620] TRAIN: iteration 7481 : loss : 0.158184, loss_ce: 0.003296, loss_dice: 0.313071
[08:57:58.828] TRAIN: iteration 7482 : loss : 0.251746, loss_ce: 0.003326, loss_dice: 0.500167
[08:57:59.036] TRAIN: iteration 7483 : loss : 0.037333, loss_ce: 0.001804, loss_dice: 0.072862
[08:57:59.289] TRAIN: iteration 7484 : loss : 0.143303, loss_ce: 0.010427, loss_dice: 0.276179
[08:57:59.501] TRAIN: iteration 7485 : loss : 0.200099, loss_ce: 0.006293, loss_dice: 0.393906
[08:57:59.709] TRAIN: iteration 7486 : loss : 0.177491, loss_ce: 0.003971, loss_dice: 0.351011
[08:57:59.924] TRAIN: iteration 7487 : loss : 0.251793, loss_ce: 0.003402, loss_dice: 0.500183
[08:58:00.133] TRAIN: iteration 7488 : loss : 0.091944, loss_ce: 0.002851, loss_dice: 0.181037
[08:58:00.341] TRAIN: iteration 7489 : loss : 0.226492, loss_ce: 0.004291, loss_dice: 0.448693
[08:58:00.556] TRAIN: iteration 7490 : loss : 0.251502, loss_ce: 0.002872, loss_dice: 0.500132
[08:58:00.771] TRAIN: iteration 7491 : loss : 0.155521, loss_ce: 0.005442, loss_dice: 0.305600
[08:58:00.979] TRAIN: iteration 7492 : loss : 0.242159, loss_ce: 0.003966, loss_dice: 0.480353
[08:58:01.199] TRAIN: iteration 7493 : loss : 0.236547, loss_ce: 0.005193, loss_dice: 0.467902
[08:58:01.407] TRAIN: iteration 7494 : loss : 0.084427, loss_ce: 0.006932, loss_dice: 0.161922
[08:58:01.618] TRAIN: iteration 7495 : loss : 0.228214, loss_ce: 0.002862, loss_dice: 0.453566
[08:58:01.832] TRAIN: iteration 7496 : loss : 0.214121, loss_ce: 0.002986, loss_dice: 0.425255
[08:58:02.043] TRAIN: iteration 7497 : loss : 0.128640, loss_ce: 0.004397, loss_dice: 0.252882
[08:58:02.251] TRAIN: iteration 7498 : loss : 0.251595, loss_ce: 0.003048, loss_dice: 0.500143
[08:58:02.464] TRAIN: iteration 7499 : loss : 0.183177, loss_ce: 0.003949, loss_dice: 0.362405
[08:58:02.671] TRAIN: iteration 7500 : loss : 0.110225, loss_ce: 0.004550, loss_dice: 0.215900
[08:58:02.908] TRAIN: iteration 7501 : loss : 0.249433, loss_ce: 0.003517, loss_dice: 0.495348
[08:58:03.123] TRAIN: iteration 7502 : loss : 0.208660, loss_ce: 0.006015, loss_dice: 0.411304
[08:58:03.345] TRAIN: iteration 7503 : loss : 0.100709, loss_ce: 0.007695, loss_dice: 0.193722
[08:58:03.554] TRAIN: iteration 7504 : loss : 0.252209, loss_ce: 0.004149, loss_dice: 0.500270
[08:58:04.220] TRAIN: iteration 7505 : loss : 0.251698, loss_ce: 0.003225, loss_dice: 0.500172
[08:58:04.428] TRAIN: iteration 7506 : loss : 0.193248, loss_ce: 0.011083, loss_dice: 0.375414
[08:58:04.638] TRAIN: iteration 7507 : loss : 0.198568, loss_ce: 0.004450, loss_dice: 0.392687
[08:58:04.847] TRAIN: iteration 7508 : loss : 0.251967, loss_ce: 0.003749, loss_dice: 0.500185
[08:58:05.059] TRAIN: iteration 7509 : loss : 0.212487, loss_ce: 0.020547, loss_dice: 0.404427
[08:58:05.268] TRAIN: iteration 7510 : loss : 0.103414, loss_ce: 0.004230, loss_dice: 0.202597
[08:58:05.475] TRAIN: iteration 7511 : loss : 0.144446, loss_ce: 0.004467, loss_dice: 0.284425
[08:58:05.690] TRAIN: iteration 7512 : loss : 0.096840, loss_ce: 0.003664, loss_dice: 0.190017
[08:58:05.898] TRAIN: iteration 7513 : loss : 0.105642, loss_ce: 0.003503, loss_dice: 0.207781
[08:58:06.108] TRAIN: iteration 7514 : loss : 0.204610, loss_ce: 0.007670, loss_dice: 0.401550
[08:58:06.317] TRAIN: iteration 7515 : loss : 0.192523, loss_ce: 0.005007, loss_dice: 0.380038
[08:58:06.526] TRAIN: iteration 7516 : loss : 0.229211, loss_ce: 0.010870, loss_dice: 0.447551
[08:58:06.732] TRAIN: iteration 7517 : loss : 0.103884, loss_ce: 0.011135, loss_dice: 0.196633
[08:58:06.939] TRAIN: iteration 7518 : loss : 0.251963, loss_ce: 0.003739, loss_dice: 0.500187
[08:58:07.151] TRAIN: iteration 7519 : loss : 0.156976, loss_ce: 0.005612, loss_dice: 0.308340
[08:58:07.360] TRAIN: iteration 7520 : loss : 0.130714, loss_ce: 0.007428, loss_dice: 0.254000
[08:58:07.600] TRAIN: iteration 7521 : loss : 0.076513, loss_ce: 0.005887, loss_dice: 0.147139
[08:58:07.810] TRAIN: iteration 7522 : loss : 0.207742, loss_ce: 0.005960, loss_dice: 0.409525
[08:58:08.018] TRAIN: iteration 7523 : loss : 0.139421, loss_ce: 0.008111, loss_dice: 0.270731
[08:58:08.226] TRAIN: iteration 7524 : loss : 0.144399, loss_ce: 0.010401, loss_dice: 0.278397
[08:58:08.436] TRAIN: iteration 7525 : loss : 0.203400, loss_ce: 0.017710, loss_dice: 0.389091
[08:58:08.643] TRAIN: iteration 7526 : loss : 0.123294, loss_ce: 0.005629, loss_dice: 0.240960
[08:58:08.851] TRAIN: iteration 7527 : loss : 0.242202, loss_ce: 0.006922, loss_dice: 0.477481
[08:58:09.059] TRAIN: iteration 7528 : loss : 0.253614, loss_ce: 0.007023, loss_dice: 0.500204
[08:58:09.273] TRAIN: iteration 7529 : loss : 0.151052, loss_ce: 0.009137, loss_dice: 0.292967
[08:58:09.482] TRAIN: iteration 7530 : loss : 0.072161, loss_ce: 0.005400, loss_dice: 0.138922
[08:58:09.690] TRAIN: iteration 7531 : loss : 0.134882, loss_ce: 0.005749, loss_dice: 0.264014
[08:58:09.898] TRAIN: iteration 7532 : loss : 0.102850, loss_ce: 0.007748, loss_dice: 0.197952
[08:58:10.123] TRAIN: iteration 7533 : loss : 0.145626, loss_ce: 0.017445, loss_dice: 0.273807
[08:58:10.331] TRAIN: iteration 7534 : loss : 0.253361, loss_ce: 0.006283, loss_dice: 0.500438
[08:58:10.539] TRAIN: iteration 7535 : loss : 0.087183, loss_ce: 0.006045, loss_dice: 0.168322
[08:58:10.747] TRAIN: iteration 7536 : loss : 0.129764, loss_ce: 0.005390, loss_dice: 0.254138
[08:58:10.954] TRAIN: iteration 7537 : loss : 0.155064, loss_ce: 0.005186, loss_dice: 0.304943
[08:58:11.162] TRAIN: iteration 7538 : loss : 0.038045, loss_ce: 0.002276, loss_dice: 0.073814
[08:58:11.370] TRAIN: iteration 7539 : loss : 0.082035, loss_ce: 0.006591, loss_dice: 0.157479
[08:58:11.983] TRAIN: iteration 7540 : loss : 0.178017, loss_ce: 0.005464, loss_dice: 0.350569
[08:58:12.222] TRAIN: iteration 7541 : loss : 0.110712, loss_ce: 0.003407, loss_dice: 0.218018
[08:58:12.429] TRAIN: iteration 7542 : loss : 0.211256, loss_ce: 0.005730, loss_dice: 0.416782
[08:58:12.720] TRAIN: iteration 7543 : loss : 0.127622, loss_ce: 0.004521, loss_dice: 0.250722
[08:58:12.929] TRAIN: iteration 7544 : loss : 0.150496, loss_ce: 0.003504, loss_dice: 0.297487
[08:58:13.136] TRAIN: iteration 7545 : loss : 0.066344, loss_ce: 0.005533, loss_dice: 0.127154
[08:58:13.344] TRAIN: iteration 7546 : loss : 0.191597, loss_ce: 0.002550, loss_dice: 0.380644
[08:58:13.570] TRAIN: iteration 7547 : loss : 0.028983, loss_ce: 0.001233, loss_dice: 0.056734
[08:58:14.028] TRAIN: iteration 7548 : loss : 0.159980, loss_ce: 0.003857, loss_dice: 0.316103
[08:58:15.294] TRAIN: iteration 7549 : loss : 0.212818, loss_ce: 0.002707, loss_dice: 0.422928
[08:58:15.506] TRAIN: iteration 7550 : loss : 0.082633, loss_ce: 0.003014, loss_dice: 0.162252
[08:58:15.717] TRAIN: iteration 7551 : loss : 0.163221, loss_ce: 0.004235, loss_dice: 0.322206
[08:58:15.927] TRAIN: iteration 7552 : loss : 0.237888, loss_ce: 0.011947, loss_dice: 0.463828
[08:58:16.135] TRAIN: iteration 7553 : loss : 0.251900, loss_ce: 0.003916, loss_dice: 0.499884
[08:58:16.350] TRAIN: iteration 7554 : loss : 0.195393, loss_ce: 0.016226, loss_dice: 0.374561
[08:58:16.559] TRAIN: iteration 7555 : loss : 0.169289, loss_ce: 0.018349, loss_dice: 0.320228
[08:58:16.772] TRAIN: iteration 7556 : loss : 0.180573, loss_ce: 0.003884, loss_dice: 0.357262
[08:58:17.548] TRAIN: iteration 7557 : loss : 0.160051, loss_ce: 0.018408, loss_dice: 0.301694
[08:58:17.758] TRAIN: iteration 7558 : loss : 0.082362, loss_ce: 0.001938, loss_dice: 0.162786
[08:58:17.965] TRAIN: iteration 7559 : loss : 0.237332, loss_ce: 0.005172, loss_dice: 0.469492
[08:58:18.173] TRAIN: iteration 7560 : loss : 0.167634, loss_ce: 0.007205, loss_dice: 0.328064
[08:58:18.411] TRAIN: iteration 7561 : loss : 0.164906, loss_ce: 0.008394, loss_dice: 0.321417
[08:58:18.621] TRAIN: iteration 7562 : loss : 0.252446, loss_ce: 0.004576, loss_dice: 0.500315
[08:58:18.853] TRAIN: iteration 7563 : loss : 0.251234, loss_ce: 0.004986, loss_dice: 0.497482
[08:58:19.061] TRAIN: iteration 7564 : loss : 0.087813, loss_ce: 0.005318, loss_dice: 0.170308
[08:58:19.269] TRAIN: iteration 7565 : loss : 0.207242, loss_ce: 0.004806, loss_dice: 0.409679
[08:58:19.476] TRAIN: iteration 7566 : loss : 0.159056, loss_ce: 0.003882, loss_dice: 0.314231
[08:58:19.689] TRAIN: iteration 7567 : loss : 0.063905, loss_ce: 0.003312, loss_dice: 0.124499
[08:58:19.899] TRAIN: iteration 7568 : loss : 0.145352, loss_ce: 0.010580, loss_dice: 0.280124
[08:58:20.107] TRAIN: iteration 7569 : loss : 0.252279, loss_ce: 0.004222, loss_dice: 0.500336
[08:58:20.316] TRAIN: iteration 7570 : loss : 0.052893, loss_ce: 0.003734, loss_dice: 0.102053
[08:58:20.525] TRAIN: iteration 7571 : loss : 0.169661, loss_ce: 0.005304, loss_dice: 0.334018
[08:58:20.733] TRAIN: iteration 7572 : loss : 0.121197, loss_ce: 0.004707, loss_dice: 0.237688
[08:58:20.943] TRAIN: iteration 7573 : loss : 0.124723, loss_ce: 0.012832, loss_dice: 0.236614
[08:58:21.152] TRAIN: iteration 7574 : loss : 0.156689, loss_ce: 0.005941, loss_dice: 0.307436
[08:58:21.368] TRAIN: iteration 7575 : loss : 0.102786, loss_ce: 0.005515, loss_dice: 0.200057
[08:58:21.582] TRAIN: iteration 7576 : loss : 0.242660, loss_ce: 0.004041, loss_dice: 0.481278
[08:58:21.790] TRAIN: iteration 7577 : loss : 0.154945, loss_ce: 0.014505, loss_dice: 0.295384
[08:58:21.997] TRAIN: iteration 7578 : loss : 0.192141, loss_ce: 0.003845, loss_dice: 0.380437
[08:58:22.205] TRAIN: iteration 7579 : loss : 0.252660, loss_ce: 0.004954, loss_dice: 0.500365
[08:58:22.421] TRAIN: iteration 7580 : loss : 0.250327, loss_ce: 0.004611, loss_dice: 0.496043
[08:58:23.075] TRAIN: iteration 7581 : loss : 0.168054, loss_ce: 0.034598, loss_dice: 0.301511
[08:58:24.858] TRAIN: iteration 7582 : loss : 0.227806, loss_ce: 0.004982, loss_dice: 0.450631
[08:58:25.069] TRAIN: iteration 7583 : loss : 0.116897, loss_ce: 0.003994, loss_dice: 0.229800
[08:58:25.279] TRAIN: iteration 7584 : loss : 0.155598, loss_ce: 0.004058, loss_dice: 0.307138
[08:58:25.486] TRAIN: iteration 7585 : loss : 0.047494, loss_ce: 0.003995, loss_dice: 0.090993
[08:58:25.693] TRAIN: iteration 7586 : loss : 0.129350, loss_ce: 0.008106, loss_dice: 0.250594
[08:58:25.901] TRAIN: iteration 7587 : loss : 0.248970, loss_ce: 0.005515, loss_dice: 0.492424
[08:58:26.109] TRAIN: iteration 7588 : loss : 0.154680, loss_ce: 0.003739, loss_dice: 0.305621
[08:58:26.694] TRAIN: iteration 7589 : loss : 0.252086, loss_ce: 0.003949, loss_dice: 0.500223
[08:58:26.903] TRAIN: iteration 7590 : loss : 0.231948, loss_ce: 0.005456, loss_dice: 0.458441
[08:58:27.111] TRAIN: iteration 7591 : loss : 0.174431, loss_ce: 0.009450, loss_dice: 0.339413
[08:58:27.323] TRAIN: iteration 7592 : loss : 0.177238, loss_ce: 0.009961, loss_dice: 0.344516
[08:58:27.532] TRAIN: iteration 7593 : loss : 0.124282, loss_ce: 0.005764, loss_dice: 0.242800
[08:58:27.740] TRAIN: iteration 7594 : loss : 0.126664, loss_ce: 0.004671, loss_dice: 0.248656
[08:58:27.951] TRAIN: iteration 7595 : loss : 0.237108, loss_ce: 0.004066, loss_dice: 0.470151
[08:58:28.165] TRAIN: iteration 7596 : loss : 0.235291, loss_ce: 0.004026, loss_dice: 0.466557
[08:58:29.869] TRAIN: iteration 7597 : loss : 0.118382, loss_ce: 0.005137, loss_dice: 0.231626
[08:58:30.077] TRAIN: iteration 7598 : loss : 0.171114, loss_ce: 0.003450, loss_dice: 0.338779
[08:58:30.286] TRAIN: iteration 7599 : loss : 0.160875, loss_ce: 0.010418, loss_dice: 0.311332
[08:58:30.493] TRAIN: iteration 7600 : loss : 0.195341, loss_ce: 0.005882, loss_dice: 0.384801
[08:58:30.734] TRAIN: iteration 7601 : loss : 0.166919, loss_ce: 0.007048, loss_dice: 0.326791
[08:58:30.943] TRAIN: iteration 7602 : loss : 0.100486, loss_ce: 0.004411, loss_dice: 0.196562
[08:58:31.151] TRAIN: iteration 7603 : loss : 0.120074, loss_ce: 0.006504, loss_dice: 0.233644
[08:58:31.359] TRAIN: iteration 7604 : loss : 0.240919, loss_ce: 0.003732, loss_dice: 0.478107
[08:58:31.833] TRAIN: iteration 7605 : loss : 0.064842, loss_ce: 0.003495, loss_dice: 0.126189
[08:58:32.041] TRAIN: iteration 7606 : loss : 0.109497, loss_ce: 0.003641, loss_dice: 0.215354
[08:58:32.249] TRAIN: iteration 7607 : loss : 0.242776, loss_ce: 0.003317, loss_dice: 0.482236
[08:58:32.461] TRAIN: iteration 7608 : loss : 0.097340, loss_ce: 0.002992, loss_dice: 0.191688
[08:58:32.669] TRAIN: iteration 7609 : loss : 0.089203, loss_ce: 0.003360, loss_dice: 0.175045
[08:58:34.018] TRAIN: iteration 7610 : loss : 0.222657, loss_ce: 0.003791, loss_dice: 0.441523
[08:58:34.225] TRAIN: iteration 7611 : loss : 0.115566, loss_ce: 0.004965, loss_dice: 0.226167
[08:58:34.433] TRAIN: iteration 7612 : loss : 0.187436, loss_ce: 0.003537, loss_dice: 0.371335
[08:58:34.641] TRAIN: iteration 7613 : loss : 0.073369, loss_ce: 0.003977, loss_dice: 0.142762
[08:58:34.851] TRAIN: iteration 7614 : loss : 0.110217, loss_ce: 0.006908, loss_dice: 0.213527
[08:58:35.059] TRAIN: iteration 7615 : loss : 0.253474, loss_ce: 0.007603, loss_dice: 0.499345
[08:58:35.266] TRAIN: iteration 7616 : loss : 0.219745, loss_ce: 0.005165, loss_dice: 0.434325
[08:58:35.482] TRAIN: iteration 7617 : loss : 0.093485, loss_ce: 0.006024, loss_dice: 0.180947
[08:58:36.373] TRAIN: iteration 7618 : loss : 0.239835, loss_ce: 0.007436, loss_dice: 0.472235
[08:58:36.583] TRAIN: iteration 7619 : loss : 0.119523, loss_ce: 0.012499, loss_dice: 0.226546
[08:58:36.791] TRAIN: iteration 7620 : loss : 0.168396, loss_ce: 0.010879, loss_dice: 0.325913
[08:58:37.030] TRAIN: iteration 7621 : loss : 0.114181, loss_ce: 0.016926, loss_dice: 0.211437
[08:58:37.241] TRAIN: iteration 7622 : loss : 0.151969, loss_ce: 0.017572, loss_dice: 0.286366
[08:58:37.448] TRAIN: iteration 7623 : loss : 0.129364, loss_ce: 0.010064, loss_dice: 0.248664
[08:58:37.655] TRAIN: iteration 7624 : loss : 0.249531, loss_ce: 0.004601, loss_dice: 0.494461
[08:58:37.862] TRAIN: iteration 7625 : loss : 0.252532, loss_ce: 0.004705, loss_dice: 0.500360
[08:58:38.070] TRAIN: iteration 7626 : loss : 0.253241, loss_ce: 0.006004, loss_dice: 0.500478
[08:58:38.282] TRAIN: iteration 7627 : loss : 0.195160, loss_ce: 0.007893, loss_dice: 0.382426
[08:58:38.489] TRAIN: iteration 7628 : loss : 0.122964, loss_ce: 0.005020, loss_dice: 0.240908
[08:58:38.698] TRAIN: iteration 7629 : loss : 0.138384, loss_ce: 0.026161, loss_dice: 0.250608
[08:58:38.908] TRAIN: iteration 7630 : loss : 0.245613, loss_ce: 0.008058, loss_dice: 0.483169
[08:58:39.122] TRAIN: iteration 7631 : loss : 0.155092, loss_ce: 0.003907, loss_dice: 0.306277
[08:58:39.333] TRAIN: iteration 7632 : loss : 0.257495, loss_ce: 0.021396, loss_dice: 0.493593
[08:58:39.541] TRAIN: iteration 7633 : loss : 0.060993, loss_ce: 0.005394, loss_dice: 0.116593
[08:58:39.754] TRAIN: iteration 7634 : loss : 0.243442, loss_ce: 0.006702, loss_dice: 0.480182
[08:58:39.962] TRAIN: iteration 7635 : loss : 0.249213, loss_ce: 0.005542, loss_dice: 0.492883
[08:58:40.173] TRAIN: iteration 7636 : loss : 0.079857, loss_ce: 0.004284, loss_dice: 0.155430
[08:58:40.384] TRAIN: iteration 7637 : loss : 0.174913, loss_ce: 0.008677, loss_dice: 0.341150
[08:58:40.592] TRAIN: iteration 7638 : loss : 0.229782, loss_ce: 0.004239, loss_dice: 0.455325
[08:58:40.800] TRAIN: iteration 7639 : loss : 0.062665, loss_ce: 0.005190, loss_dice: 0.120140
[08:58:41.015] TRAIN: iteration 7640 : loss : 0.241629, loss_ce: 0.005001, loss_dice: 0.478257
[08:58:41.260] TRAIN: iteration 7641 : loss : 0.093538, loss_ce: 0.005249, loss_dice: 0.181826
[08:58:41.468] TRAIN: iteration 7642 : loss : 0.136273, loss_ce: 0.003571, loss_dice: 0.268974
[08:58:41.676] TRAIN: iteration 7643 : loss : 0.161113, loss_ce: 0.005325, loss_dice: 0.316900
[08:58:41.893] TRAIN: iteration 7644 : loss : 0.251804, loss_ce: 0.003383, loss_dice: 0.500224
[08:58:42.110] TRAIN: iteration 7645 : loss : 0.243736, loss_ce: 0.004467, loss_dice: 0.483005
[08:58:42.317] TRAIN: iteration 7646 : loss : 0.050143, loss_ce: 0.003329, loss_dice: 0.096957
[08:58:42.525] TRAIN: iteration 7647 : loss : 0.077588, loss_ce: 0.006393, loss_dice: 0.148782
[08:58:42.733] TRAIN: iteration 7648 : loss : 0.177404, loss_ce: 0.002554, loss_dice: 0.352255
[08:58:42.941] TRAIN: iteration 7649 : loss : 0.177684, loss_ce: 0.005831, loss_dice: 0.349536
[08:58:43.881] TRAIN: iteration 7650 : loss : 0.233267, loss_ce: 0.004361, loss_dice: 0.462172
[08:58:44.094] TRAIN: iteration 7651 : loss : 0.081185, loss_ce: 0.004488, loss_dice: 0.157882
[08:58:44.303] TRAIN: iteration 7652 : loss : 0.091568, loss_ce: 0.003753, loss_dice: 0.179382
[08:58:44.512] TRAIN: iteration 7653 : loss : 0.102698, loss_ce: 0.003896, loss_dice: 0.201499
[08:58:44.720] TRAIN: iteration 7654 : loss : 0.108758, loss_ce: 0.003146, loss_dice: 0.214369
[08:58:44.927] TRAIN: iteration 7655 : loss : 0.125326, loss_ce: 0.010097, loss_dice: 0.240555
[08:58:45.136] TRAIN: iteration 7656 : loss : 0.171060, loss_ce: 0.009022, loss_dice: 0.333097
[08:58:45.346] TRAIN: iteration 7657 : loss : 0.251383, loss_ce: 0.002624, loss_dice: 0.500141
[08:58:47.129] TRAIN: iteration 7658 : loss : 0.116520, loss_ce: 0.006959, loss_dice: 0.226080
[08:58:47.338] TRAIN: iteration 7659 : loss : 0.105478, loss_ce: 0.005697, loss_dice: 0.205260
[08:58:47.547] TRAIN: iteration 7660 : loss : 0.075366, loss_ce: 0.005568, loss_dice: 0.145165
[08:58:47.772] TRAIN: iteration 7661 : loss : 0.226296, loss_ce: 0.030443, loss_dice: 0.422150
[08:58:47.987] TRAIN: iteration 7662 : loss : 0.197287, loss_ce: 0.003288, loss_dice: 0.391286
[08:58:48.194] TRAIN: iteration 7663 : loss : 0.222656, loss_ce: 0.005040, loss_dice: 0.440272
[08:58:48.401] TRAIN: iteration 7664 : loss : 0.161006, loss_ce: 0.007900, loss_dice: 0.314112
[08:58:48.609] TRAIN: iteration 7665 : loss : 0.122324, loss_ce: 0.009087, loss_dice: 0.235561
[08:58:48.823] TRAIN: iteration 7666 : loss : 0.111371, loss_ce: 0.004534, loss_dice: 0.218208
[08:58:49.033] TRAIN: iteration 7667 : loss : 0.190408, loss_ce: 0.006083, loss_dice: 0.374732
[08:58:49.244] TRAIN: iteration 7668 : loss : 0.148470, loss_ce: 0.005037, loss_dice: 0.291904
[08:58:49.453] TRAIN: iteration 7669 : loss : 0.111469, loss_ce: 0.004089, loss_dice: 0.218849
[08:58:49.662] TRAIN: iteration 7670 : loss : 0.156166, loss_ce: 0.005792, loss_dice: 0.306539
[08:58:49.871] TRAIN: iteration 7671 : loss : 0.251690, loss_ce: 0.003264, loss_dice: 0.500116
[08:58:50.087] TRAIN: iteration 7672 : loss : 0.080262, loss_ce: 0.003509, loss_dice: 0.157014
[08:58:50.298] TRAIN: iteration 7673 : loss : 0.156562, loss_ce: 0.005223, loss_dice: 0.307900
[08:58:50.846] TRAIN: iteration 7674 : loss : 0.099164, loss_ce: 0.003181, loss_dice: 0.195147
[08:58:51.054] TRAIN: iteration 7675 : loss : 0.173694, loss_ce: 0.003410, loss_dice: 0.343979
[08:58:51.262] TRAIN: iteration 7676 : loss : 0.171605, loss_ce: 0.004172, loss_dice: 0.339038
[08:58:51.469] TRAIN: iteration 7677 : loss : 0.084985, loss_ce: 0.004788, loss_dice: 0.165181
[08:58:51.678] TRAIN: iteration 7678 : loss : 0.188250, loss_ce: 0.003926, loss_dice: 0.372574
[08:58:51.887] TRAIN: iteration 7679 : loss : 0.142793, loss_ce: 0.003717, loss_dice: 0.281870
[08:58:52.097] TRAIN: iteration 7680 : loss : 0.234065, loss_ce: 0.002645, loss_dice: 0.465484
[08:58:52.341] TRAIN: iteration 7681 : loss : 0.224115, loss_ce: 0.003339, loss_dice: 0.444890
[08:58:52.555] TRAIN: iteration 7682 : loss : 0.161166, loss_ce: 0.008757, loss_dice: 0.313576
[08:58:52.764] TRAIN: iteration 7683 : loss : 0.155837, loss_ce: 0.005961, loss_dice: 0.305713
[08:58:53.323] TRAIN: iteration 7684 : loss : 0.149007, loss_ce: 0.006245, loss_dice: 0.291769
[08:58:53.530] TRAIN: iteration 7685 : loss : 0.155346, loss_ce: 0.006241, loss_dice: 0.304452
[08:58:53.738] TRAIN: iteration 7686 : loss : 0.215473, loss_ce: 0.004662, loss_dice: 0.426285
[08:58:53.948] TRAIN: iteration 7687 : loss : 0.253040, loss_ce: 0.009784, loss_dice: 0.496296
[08:58:54.163] TRAIN: iteration 7688 : loss : 0.113938, loss_ce: 0.005050, loss_dice: 0.222826
[08:58:54.374] TRAIN: iteration 7689 : loss : 0.073135, loss_ce: 0.006908, loss_dice: 0.139363
[08:58:55.636] TRAIN: iteration 7690 : loss : 0.055562, loss_ce: 0.004512, loss_dice: 0.106612
[08:58:55.843] TRAIN: iteration 7691 : loss : 0.085289, loss_ce: 0.008115, loss_dice: 0.162463
[08:58:56.051] TRAIN: iteration 7692 : loss : 0.105966, loss_ce: 0.007407, loss_dice: 0.204525
[08:58:56.262] TRAIN: iteration 7693 : loss : 0.105541, loss_ce: 0.007025, loss_dice: 0.204057
[08:58:56.472] TRAIN: iteration 7694 : loss : 0.253891, loss_ce: 0.010340, loss_dice: 0.497441
[08:58:56.681] TRAIN: iteration 7695 : loss : 0.097929, loss_ce: 0.004831, loss_dice: 0.191026
[08:58:56.897] TRAIN: iteration 7696 : loss : 0.253927, loss_ce: 0.007342, loss_dice: 0.500512
[08:58:57.106] TRAIN: iteration 7697 : loss : 0.140814, loss_ce: 0.011473, loss_dice: 0.270155
[08:58:58.394] TRAIN: iteration 7698 : loss : 0.142693, loss_ce: 0.008507, loss_dice: 0.276878
[08:58:58.602] TRAIN: iteration 7699 : loss : 0.117724, loss_ce: 0.005700, loss_dice: 0.229748
[08:58:58.810] TRAIN: iteration 7700 : loss : 0.176264, loss_ce: 0.015570, loss_dice: 0.336959
[08:58:59.055] TRAIN: iteration 7701 : loss : 0.193094, loss_ce: 0.005900, loss_dice: 0.380289
[08:58:59.262] TRAIN: iteration 7702 : loss : 0.208689, loss_ce: 0.004977, loss_dice: 0.412401
[08:58:59.469] TRAIN: iteration 7703 : loss : 0.248866, loss_ce: 0.008328, loss_dice: 0.489404
[08:58:59.676] TRAIN: iteration 7704 : loss : 0.090201, loss_ce: 0.013356, loss_dice: 0.167045
[08:58:59.883] TRAIN: iteration 7705 : loss : 0.147666, loss_ce: 0.007268, loss_dice: 0.288063
[08:59:01.340] TRAIN: iteration 7706 : loss : 0.082948, loss_ce: 0.006350, loss_dice: 0.159547
[08:59:01.549] TRAIN: iteration 7707 : loss : 0.180048, loss_ce: 0.007977, loss_dice: 0.352119
[08:59:01.757] TRAIN: iteration 7708 : loss : 0.125284, loss_ce: 0.009268, loss_dice: 0.241300
[08:59:01.966] TRAIN: iteration 7709 : loss : 0.199061, loss_ce: 0.005903, loss_dice: 0.392218
[08:59:02.173] TRAIN: iteration 7710 : loss : 0.178100, loss_ce: 0.004174, loss_dice: 0.352026
[08:59:02.382] TRAIN: iteration 7711 : loss : 0.216275, loss_ce: 0.010682, loss_dice: 0.421868
[08:59:02.595] TRAIN: iteration 7712 : loss : 0.209289, loss_ce: 0.004187, loss_dice: 0.414391
[08:59:02.803] TRAIN: iteration 7713 : loss : 0.101704, loss_ce: 0.003004, loss_dice: 0.200403
[08:59:03.010] TRAIN: iteration 7714 : loss : 0.252022, loss_ce: 0.003767, loss_dice: 0.500277
[08:59:03.217] TRAIN: iteration 7715 : loss : 0.100385, loss_ce: 0.004004, loss_dice: 0.196765
[08:59:03.425] TRAIN: iteration 7716 : loss : 0.212362, loss_ce: 0.004796, loss_dice: 0.419929
[08:59:03.632] TRAIN: iteration 7717 : loss : 0.128140, loss_ce: 0.005184, loss_dice: 0.251096
[08:59:03.840] TRAIN: iteration 7718 : loss : 0.082117, loss_ce: 0.004362, loss_dice: 0.159872
[08:59:04.047] TRAIN: iteration 7719 : loss : 0.240288, loss_ce: 0.005152, loss_dice: 0.475424
[08:59:04.260] TRAIN: iteration 7720 : loss : 0.187156, loss_ce: 0.024694, loss_dice: 0.349617
[08:59:04.503] TRAIN: iteration 7721 : loss : 0.200322, loss_ce: 0.019130, loss_dice: 0.381514
[08:59:04.711] TRAIN: iteration 7722 : loss : 0.073052, loss_ce: 0.008617, loss_dice: 0.137488
[08:59:04.920] TRAIN: iteration 7723 : loss : 0.252504, loss_ce: 0.004695, loss_dice: 0.500312
[08:59:05.130] TRAIN: iteration 7724 : loss : 0.091643, loss_ce: 0.005094, loss_dice: 0.178192
[08:59:05.341] TRAIN: iteration 7725 : loss : 0.106826, loss_ce: 0.004739, loss_dice: 0.208913
[08:59:05.551] TRAIN: iteration 7726 : loss : 0.102680, loss_ce: 0.004627, loss_dice: 0.200733
[08:59:06.082] TRAIN: iteration 7727 : loss : 0.096097, loss_ce: 0.005775, loss_dice: 0.186418
[08:59:06.292] TRAIN: iteration 7728 : loss : 0.095941, loss_ce: 0.011008, loss_dice: 0.180874
[08:59:06.503] TRAIN: iteration 7729 : loss : 0.134020, loss_ce: 0.013380, loss_dice: 0.254661
[08:59:06.710] TRAIN: iteration 7730 : loss : 0.108370, loss_ce: 0.015896, loss_dice: 0.200844
[08:59:06.938] TRAIN: iteration 7731 : loss : 0.194362, loss_ce: 0.006664, loss_dice: 0.382060
[08:59:07.149] TRAIN: iteration 7732 : loss : 0.252816, loss_ce: 0.005265, loss_dice: 0.500367
[08:59:07.359] TRAIN: iteration 7733 : loss : 0.169439, loss_ce: 0.009189, loss_dice: 0.329689
[08:59:07.567] TRAIN: iteration 7734 : loss : 0.250661, loss_ce: 0.006055, loss_dice: 0.495266
[08:59:07.782] TRAIN: iteration 7735 : loss : 0.252809, loss_ce: 0.005283, loss_dice: 0.500335
[08:59:07.989] TRAIN: iteration 7736 : loss : 0.091093, loss_ce: 0.013296, loss_dice: 0.168890
[08:59:08.196] TRAIN: iteration 7737 : loss : 0.233990, loss_ce: 0.005674, loss_dice: 0.462307
[08:59:08.405] TRAIN: iteration 7738 : loss : 0.149127, loss_ce: 0.008235, loss_dice: 0.290020
[08:59:08.616] TRAIN: iteration 7739 : loss : 0.249695, loss_ce: 0.007545, loss_dice: 0.491846
[08:59:08.843] TRAIN: iteration 7740 : loss : 0.073682, loss_ce: 0.006188, loss_dice: 0.141176
[08:59:09.086] TRAIN: iteration 7741 : loss : 0.131020, loss_ce: 0.007366, loss_dice: 0.254673
[08:59:09.295] TRAIN: iteration 7742 : loss : 0.081224, loss_ce: 0.004701, loss_dice: 0.157746
[08:59:09.509] TRAIN: iteration 7743 : loss : 0.184915, loss_ce: 0.007443, loss_dice: 0.362387
[08:59:09.716] TRAIN: iteration 7744 : loss : 0.250954, loss_ce: 0.007943, loss_dice: 0.493966
[08:59:09.933] TRAIN: iteration 7745 : loss : 0.073865, loss_ce: 0.005003, loss_dice: 0.142728
[08:59:10.846] TRAIN: iteration 7746 : loss : 0.236341, loss_ce: 0.006534, loss_dice: 0.466149
[08:59:11.052] TRAIN: iteration 7747 : loss : 0.221264, loss_ce: 0.006565, loss_dice: 0.435964
[08:59:11.260] TRAIN: iteration 7748 : loss : 0.130646, loss_ce: 0.004911, loss_dice: 0.256381
[08:59:11.467] TRAIN: iteration 7749 : loss : 0.234992, loss_ce: 0.008451, loss_dice: 0.461534
[08:59:11.674] TRAIN: iteration 7750 : loss : 0.133062, loss_ce: 0.007466, loss_dice: 0.258659
[08:59:11.882] TRAIN: iteration 7751 : loss : 0.183568, loss_ce: 0.005761, loss_dice: 0.361375
[08:59:12.090] TRAIN: iteration 7752 : loss : 0.135034, loss_ce: 0.004767, loss_dice: 0.265301
[08:59:12.298] TRAIN: iteration 7753 : loss : 0.252585, loss_ce: 0.004866, loss_dice: 0.500305
[08:59:12.508] TRAIN: iteration 7754 : loss : 0.198258, loss_ce: 0.007322, loss_dice: 0.389194
[08:59:12.717] TRAIN: iteration 7755 : loss : 0.166245, loss_ce: 0.018336, loss_dice: 0.314155
[08:59:12.927] TRAIN: iteration 7756 : loss : 0.238372, loss_ce: 0.008835, loss_dice: 0.467910
[08:59:13.138] TRAIN: iteration 7757 : loss : 0.229055, loss_ce: 0.005482, loss_dice: 0.452629
[08:59:13.348] TRAIN: iteration 7758 : loss : 0.155755, loss_ce: 0.009348, loss_dice: 0.302161
[08:59:13.558] TRAIN: iteration 7759 : loss : 0.070419, loss_ce: 0.005773, loss_dice: 0.135064
[08:59:13.765] TRAIN: iteration 7760 : loss : 0.251562, loss_ce: 0.004965, loss_dice: 0.498160
[08:59:13.987] TRAIN: iteration 7761 : loss : 0.240314, loss_ce: 0.010335, loss_dice: 0.470293
[08:59:14.194] TRAIN: iteration 7762 : loss : 0.242936, loss_ce: 0.007663, loss_dice: 0.478209
[08:59:14.401] TRAIN: iteration 7763 : loss : 0.107756, loss_ce: 0.007056, loss_dice: 0.208456
[08:59:14.609] TRAIN: iteration 7764 : loss : 0.156403, loss_ce: 0.005287, loss_dice: 0.307519
[08:59:14.817] TRAIN: iteration 7765 : loss : 0.147610, loss_ce: 0.005488, loss_dice: 0.289732
[08:59:15.025] TRAIN: iteration 7766 : loss : 0.076837, loss_ce: 0.007749, loss_dice: 0.145925
[08:59:15.234] TRAIN: iteration 7767 : loss : 0.246010, loss_ce: 0.006698, loss_dice: 0.485321
[08:59:15.442] TRAIN: iteration 7768 : loss : 0.111521, loss_ce: 0.006048, loss_dice: 0.216994
[08:59:16.117] TRAIN: iteration 7769 : loss : 0.074653, loss_ce: 0.003664, loss_dice: 0.145642
[08:59:16.332] TRAIN: iteration 7770 : loss : 0.149381, loss_ce: 0.009458, loss_dice: 0.289304
[08:59:16.541] TRAIN: iteration 7771 : loss : 0.252854, loss_ce: 0.005392, loss_dice: 0.500316
[08:59:16.749] TRAIN: iteration 7772 : loss : 0.065093, loss_ce: 0.005826, loss_dice: 0.124359
[08:59:17.006] TRAIN: iteration 7773 : loss : 0.252562, loss_ce: 0.004856, loss_dice: 0.500268
[08:59:17.216] TRAIN: iteration 7774 : loss : 0.182339, loss_ce: 0.016922, loss_dice: 0.347756
[08:59:17.430] TRAIN: iteration 7775 : loss : 0.239726, loss_ce: 0.004492, loss_dice: 0.474960
[08:59:17.640] TRAIN: iteration 7776 : loss : 0.068653, loss_ce: 0.004163, loss_dice: 0.133143
[08:59:17.849] TRAIN: iteration 7777 : loss : 0.184511, loss_ce: 0.021182, loss_dice: 0.347841
[08:59:18.057] TRAIN: iteration 7778 : loss : 0.120281, loss_ce: 0.006255, loss_dice: 0.234308
[08:59:18.265] TRAIN: iteration 7779 : loss : 0.154385, loss_ce: 0.004477, loss_dice: 0.304294
[08:59:18.473] TRAIN: iteration 7780 : loss : 0.217964, loss_ce: 0.004623, loss_dice: 0.431306
[08:59:18.474] NaN or Inf found in input tensor.
[08:59:19.395] TRAIN: iteration 7781 : loss : 0.202134, loss_ce: 0.009920, loss_dice: 0.394347
[08:59:19.603] TRAIN: iteration 7782 : loss : 0.117330, loss_ce: 0.003837, loss_dice: 0.230823
[08:59:19.810] TRAIN: iteration 7783 : loss : 0.111101, loss_ce: 0.006226, loss_dice: 0.215975
[08:59:20.019] TRAIN: iteration 7784 : loss : 0.252526, loss_ce: 0.004715, loss_dice: 0.500336
[08:59:21.165] TRAIN: iteration 7785 : loss : 0.215630, loss_ce: 0.009500, loss_dice: 0.421761
[08:59:21.374] TRAIN: iteration 7786 : loss : 0.116454, loss_ce: 0.005199, loss_dice: 0.227710
[08:59:21.584] TRAIN: iteration 7787 : loss : 0.144907, loss_ce: 0.024869, loss_dice: 0.264946
[08:59:21.793] TRAIN: iteration 7788 : loss : 0.202973, loss_ce: 0.032116, loss_dice: 0.373831
[08:59:22.000] TRAIN: iteration 7789 : loss : 0.188212, loss_ce: 0.013827, loss_dice: 0.362598
[08:59:22.208] TRAIN: iteration 7790 : loss : 0.251838, loss_ce: 0.003498, loss_dice: 0.500178
[08:59:22.415] TRAIN: iteration 7791 : loss : 0.115966, loss_ce: 0.003715, loss_dice: 0.228216
[08:59:22.622] TRAIN: iteration 7792 : loss : 0.251650, loss_ce: 0.003512, loss_dice: 0.499788
[08:59:23.368] TRAIN: iteration 7793 : loss : 0.241194, loss_ce: 0.005267, loss_dice: 0.477121
[08:59:23.574] TRAIN: iteration 7794 : loss : 0.083355, loss_ce: 0.006455, loss_dice: 0.160256
[08:59:24.457] TRAIN: iteration 7795 : loss : 0.179532, loss_ce: 0.010909, loss_dice: 0.348154
[08:59:24.664] TRAIN: iteration 7796 : loss : 0.174610, loss_ce: 0.007095, loss_dice: 0.342126
[08:59:24.872] TRAIN: iteration 7797 : loss : 0.225011, loss_ce: 0.006560, loss_dice: 0.443461
[08:59:25.081] TRAIN: iteration 7798 : loss : 0.132974, loss_ce: 0.004558, loss_dice: 0.261390
[08:59:25.290] TRAIN: iteration 7799 : loss : 0.252828, loss_ce: 0.005369, loss_dice: 0.500287
[08:59:25.498] TRAIN: iteration 7800 : loss : 0.087751, loss_ce: 0.004822, loss_dice: 0.170680
[08:59:25.750] TRAIN: iteration 7801 : loss : 0.113100, loss_ce: 0.005291, loss_dice: 0.220908
[08:59:25.960] TRAIN: iteration 7802 : loss : 0.135041, loss_ce: 0.004933, loss_dice: 0.265149
[08:59:26.169] TRAIN: iteration 7803 : loss : 0.153317, loss_ce: 0.018360, loss_dice: 0.288274
[08:59:26.378] TRAIN: iteration 7804 : loss : 0.077750, loss_ce: 0.004107, loss_dice: 0.151393
[08:59:26.585] TRAIN: iteration 7805 : loss : 0.234873, loss_ce: 0.004881, loss_dice: 0.464866
[08:59:26.793] TRAIN: iteration 7806 : loss : 0.144945, loss_ce: 0.004212, loss_dice: 0.285677
[08:59:27.002] TRAIN: iteration 7807 : loss : 0.125361, loss_ce: 0.004173, loss_dice: 0.246549
[08:59:27.210] TRAIN: iteration 7808 : loss : 0.106637, loss_ce: 0.005712, loss_dice: 0.207561
[08:59:27.418] TRAIN: iteration 7809 : loss : 0.229330, loss_ce: 0.005611, loss_dice: 0.453049
[08:59:27.624] TRAIN: iteration 7810 : loss : 0.142340, loss_ce: 0.005025, loss_dice: 0.279656
[08:59:27.832] TRAIN: iteration 7811 : loss : 0.088710, loss_ce: 0.008093, loss_dice: 0.169327
[08:59:28.079] TRAIN: iteration 7812 : loss : 0.252172, loss_ce: 0.004082, loss_dice: 0.500261
[08:59:28.288] TRAIN: iteration 7813 : loss : 0.103816, loss_ce: 0.009614, loss_dice: 0.198017
[08:59:28.506] TRAIN: iteration 7814 : loss : 0.158740, loss_ce: 0.004262, loss_dice: 0.313218
[08:59:28.717] TRAIN: iteration 7815 : loss : 0.184270, loss_ce: 0.004700, loss_dice: 0.363839
[08:59:28.926] TRAIN: iteration 7816 : loss : 0.136035, loss_ce: 0.002990, loss_dice: 0.269080
[08:59:29.475] TRAIN: iteration 7817 : loss : 0.124016, loss_ce: 0.010957, loss_dice: 0.237076
[08:59:29.683] TRAIN: iteration 7818 : loss : 0.252192, loss_ce: 0.009118, loss_dice: 0.495265
[08:59:29.894] TRAIN: iteration 7819 : loss : 0.077018, loss_ce: 0.004575, loss_dice: 0.149462
[08:59:30.106] TRAIN: iteration 7820 : loss : 0.197017, loss_ce: 0.007238, loss_dice: 0.386796
[08:59:30.345] TRAIN: iteration 7821 : loss : 0.168499, loss_ce: 0.005475, loss_dice: 0.331524
[08:59:30.553] TRAIN: iteration 7822 : loss : 0.198670, loss_ce: 0.007795, loss_dice: 0.389545
[08:59:30.767] TRAIN: iteration 7823 : loss : 0.252779, loss_ce: 0.005168, loss_dice: 0.500390
[08:59:30.976] TRAIN: iteration 7824 : loss : 0.055279, loss_ce: 0.002556, loss_dice: 0.108002
[08:59:31.944] TRAIN: iteration 7825 : loss : 0.050849, loss_ce: 0.006204, loss_dice: 0.095494
[08:59:32.152] TRAIN: iteration 7826 : loss : 0.212227, loss_ce: 0.011196, loss_dice: 0.413257
[08:59:32.361] TRAIN: iteration 7827 : loss : 0.207795, loss_ce: 0.004390, loss_dice: 0.411200
[08:59:32.569] TRAIN: iteration 7828 : loss : 0.063171, loss_ce: 0.004841, loss_dice: 0.121500
[08:59:32.777] TRAIN: iteration 7829 : loss : 0.181222, loss_ce: 0.005233, loss_dice: 0.357212
[08:59:32.994] TRAIN: iteration 7830 : loss : 0.099364, loss_ce: 0.003051, loss_dice: 0.195677
[08:59:33.209] TRAIN: iteration 7831 : loss : 0.045650, loss_ce: 0.004593, loss_dice: 0.086708
[08:59:33.418] TRAIN: iteration 7832 : loss : 0.166920, loss_ce: 0.005912, loss_dice: 0.327928
[08:59:33.625] TRAIN: iteration 7833 : loss : 0.172682, loss_ce: 0.022802, loss_dice: 0.322561
[08:59:33.835] TRAIN: iteration 7834 : loss : 0.128265, loss_ce: 0.006323, loss_dice: 0.250207
[08:59:34.043] TRAIN: iteration 7835 : loss : 0.111067, loss_ce: 0.013881, loss_dice: 0.208253
[08:59:34.251] TRAIN: iteration 7836 : loss : 0.058228, loss_ce: 0.004044, loss_dice: 0.112412
[08:59:34.459] TRAIN: iteration 7837 : loss : 0.097561, loss_ce: 0.007579, loss_dice: 0.187542
[08:59:34.669] TRAIN: iteration 7838 : loss : 0.057607, loss_ce: 0.005108, loss_dice: 0.110105
[08:59:34.879] TRAIN: iteration 7839 : loss : 0.158836, loss_ce: 0.014581, loss_dice: 0.303091
[08:59:36.294] TRAIN: iteration 7840 : loss : 0.172461, loss_ce: 0.006835, loss_dice: 0.338086
[08:59:36.527] TRAIN: iteration 7841 : loss : 0.102343, loss_ce: 0.009495, loss_dice: 0.195190
[08:59:36.736] TRAIN: iteration 7842 : loss : 0.190951, loss_ce: 0.005537, loss_dice: 0.376365
[08:59:36.946] TRAIN: iteration 7843 : loss : 0.161197, loss_ce: 0.004871, loss_dice: 0.317523
[08:59:37.154] TRAIN: iteration 7844 : loss : 0.079757, loss_ce: 0.007012, loss_dice: 0.152501
[08:59:37.363] TRAIN: iteration 7845 : loss : 0.252992, loss_ce: 0.006217, loss_dice: 0.499767
[08:59:37.572] TRAIN: iteration 7846 : loss : 0.210020, loss_ce: 0.004504, loss_dice: 0.415536
[08:59:37.780] TRAIN: iteration 7847 : loss : 0.238641, loss_ce: 0.004733, loss_dice: 0.472549
[08:59:39.187] TRAIN: iteration 7848 : loss : 0.139146, loss_ce: 0.004370, loss_dice: 0.273922
[08:59:39.395] TRAIN: iteration 7849 : loss : 0.241131, loss_ce: 0.006787, loss_dice: 0.475475
[08:59:39.603] TRAIN: iteration 7850 : loss : 0.138023, loss_ce: 0.003541, loss_dice: 0.272505
[08:59:39.811] TRAIN: iteration 7851 : loss : 0.209819, loss_ce: 0.003919, loss_dice: 0.415718
[08:59:40.025] TRAIN: iteration 7852 : loss : 0.251843, loss_ce: 0.003490, loss_dice: 0.500196
[08:59:40.235] TRAIN: iteration 7853 : loss : 0.248200, loss_ce: 0.004876, loss_dice: 0.491525
[08:59:40.443] TRAIN: iteration 7854 : loss : 0.251757, loss_ce: 0.003321, loss_dice: 0.500194
[08:59:40.653] TRAIN: iteration 7855 : loss : 0.140190, loss_ce: 0.006622, loss_dice: 0.273757
[08:59:40.865] TRAIN: iteration 7856 : loss : 0.227204, loss_ce: 0.006674, loss_dice: 0.447733
[08:59:41.076] TRAIN: iteration 7857 : loss : 0.120443, loss_ce: 0.007780, loss_dice: 0.233106
[08:59:41.286] TRAIN: iteration 7858 : loss : 0.114525, loss_ce: 0.003882, loss_dice: 0.225168
[08:59:41.495] TRAIN: iteration 7859 : loss : 0.251392, loss_ce: 0.002710, loss_dice: 0.500075
[08:59:41.727] TRAIN: iteration 7860 : loss : 0.121149, loss_ce: 0.006411, loss_dice: 0.235886
[08:59:41.972] TRAIN: iteration 7861 : loss : 0.040523, loss_ce: 0.003183, loss_dice: 0.077864
[08:59:42.180] TRAIN: iteration 7862 : loss : 0.186613, loss_ce: 0.004344, loss_dice: 0.368882
[08:59:42.391] TRAIN: iteration 7863 : loss : 0.062163, loss_ce: 0.003221, loss_dice: 0.121104
[08:59:42.603] TRAIN: iteration 7864 : loss : 0.212767, loss_ce: 0.004557, loss_dice: 0.420978
[08:59:42.920] TRAIN: iteration 7865 : loss : 0.252536, loss_ce: 0.004793, loss_dice: 0.500278
[08:59:43.130] TRAIN: iteration 7866 : loss : 0.112982, loss_ce: 0.005028, loss_dice: 0.220936
[08:59:43.339] TRAIN: iteration 7867 : loss : 0.091171, loss_ce: 0.007296, loss_dice: 0.175047
[08:59:43.547] TRAIN: iteration 7868 : loss : 0.104913, loss_ce: 0.005999, loss_dice: 0.203827
[08:59:43.754] TRAIN: iteration 7869 : loss : 0.197506, loss_ce: 0.006283, loss_dice: 0.388730
[08:59:43.961] TRAIN: iteration 7870 : loss : 0.080136, loss_ce: 0.004134, loss_dice: 0.156138
[08:59:44.178] TRAIN: iteration 7871 : loss : 0.116481, loss_ce: 0.004920, loss_dice: 0.228041
[08:59:44.394] TRAIN: iteration 7872 : loss : 0.079297, loss_ce: 0.004043, loss_dice: 0.154551
[08:59:44.603] TRAIN: iteration 7873 : loss : 0.201899, loss_ce: 0.004829, loss_dice: 0.398969
[08:59:44.813] TRAIN: iteration 7874 : loss : 0.114873, loss_ce: 0.004832, loss_dice: 0.224915
[08:59:45.021] TRAIN: iteration 7875 : loss : 0.139551, loss_ce: 0.006893, loss_dice: 0.272208
[08:59:45.633] TRAIN: iteration 7876 : loss : 0.234545, loss_ce: 0.005313, loss_dice: 0.463777
[08:59:45.841] TRAIN: iteration 7877 : loss : 0.073362, loss_ce: 0.005477, loss_dice: 0.141246
[08:59:46.050] TRAIN: iteration 7878 : loss : 0.242578, loss_ce: 0.005410, loss_dice: 0.479746
[08:59:46.259] TRAIN: iteration 7879 : loss : 0.217284, loss_ce: 0.005406, loss_dice: 0.429161
[08:59:46.739] TRAIN: iteration 7880 : loss : 0.127806, loss_ce: 0.002829, loss_dice: 0.252784
[08:59:46.986] TRAIN: iteration 7881 : loss : 0.252830, loss_ce: 0.005427, loss_dice: 0.500233
[08:59:47.195] TRAIN: iteration 7882 : loss : 0.088726, loss_ce: 0.003122, loss_dice: 0.174329
[08:59:47.406] TRAIN: iteration 7883 : loss : 0.155509, loss_ce: 0.004166, loss_dice: 0.306851
[08:59:47.619] TRAIN: iteration 7884 : loss : 0.186508, loss_ce: 0.003618, loss_dice: 0.369397
[08:59:47.834] TRAIN: iteration 7885 : loss : 0.095625, loss_ce: 0.011905, loss_dice: 0.179344
[08:59:48.043] TRAIN: iteration 7886 : loss : 0.108328, loss_ce: 0.007109, loss_dice: 0.209548
[08:59:48.256] TRAIN: iteration 7887 : loss : 0.062616, loss_ce: 0.003221, loss_dice: 0.122010
[08:59:48.464] TRAIN: iteration 7888 : loss : 0.159816, loss_ce: 0.003236, loss_dice: 0.316396
[08:59:48.671] TRAIN: iteration 7889 : loss : 0.154177, loss_ce: 0.004257, loss_dice: 0.304096
[08:59:48.886] TRAIN: iteration 7890 : loss : 0.098872, loss_ce: 0.003531, loss_dice: 0.194212
[08:59:49.097] TRAIN: iteration 7891 : loss : 0.064036, loss_ce: 0.004920, loss_dice: 0.123152
[08:59:49.306] TRAIN: iteration 7892 : loss : 0.255439, loss_ce: 0.015678, loss_dice: 0.495199
[08:59:49.515] TRAIN: iteration 7893 : loss : 0.251356, loss_ce: 0.002556, loss_dice: 0.500156
[08:59:49.730] TRAIN: iteration 7894 : loss : 0.251445, loss_ce: 0.002732, loss_dice: 0.500158
[08:59:49.937] TRAIN: iteration 7895 : loss : 0.216164, loss_ce: 0.005853, loss_dice: 0.426475
[08:59:50.145] TRAIN: iteration 7896 : loss : 0.169887, loss_ce: 0.003783, loss_dice: 0.335990
[08:59:50.354] TRAIN: iteration 7897 : loss : 0.215772, loss_ce: 0.003167, loss_dice: 0.428377
[08:59:50.564] TRAIN: iteration 7898 : loss : 0.142690, loss_ce: 0.022752, loss_dice: 0.262628
[08:59:50.771] TRAIN: iteration 7899 : loss : 0.258645, loss_ce: 0.016796, loss_dice: 0.500494
[08:59:50.982] TRAIN: iteration 7900 : loss : 0.234773, loss_ce: 0.008652, loss_dice: 0.460894
[08:59:51.219] TRAIN: iteration 7901 : loss : 0.131907, loss_ce: 0.011425, loss_dice: 0.252389
[08:59:51.430] TRAIN: iteration 7902 : loss : 0.078626, loss_ce: 0.002853, loss_dice: 0.154399
[08:59:51.638] TRAIN: iteration 7903 : loss : 0.250251, loss_ce: 0.011755, loss_dice: 0.488746
[08:59:51.847] TRAIN: iteration 7904 : loss : 0.125093, loss_ce: 0.010676, loss_dice: 0.239509
[08:59:52.055] TRAIN: iteration 7905 : loss : 0.174198, loss_ce: 0.028586, loss_dice: 0.319810
[08:59:52.266] TRAIN: iteration 7906 : loss : 0.146403, loss_ce: 0.005725, loss_dice: 0.287081
[08:59:52.476] TRAIN: iteration 7907 : loss : 0.252397, loss_ce: 0.004440, loss_dice: 0.500353
[08:59:52.687] TRAIN: iteration 7908 : loss : 0.115337, loss_ce: 0.006983, loss_dice: 0.223691
[08:59:52.956] TRAIN: iteration 7909 : loss : 0.139626, loss_ce: 0.005783, loss_dice: 0.273469
[08:59:53.166] TRAIN: iteration 7910 : loss : 0.091943, loss_ce: 0.005550, loss_dice: 0.178336
[08:59:54.016] TRAIN: iteration 7911 : loss : 0.124572, loss_ce: 0.005935, loss_dice: 0.243209
[08:59:54.227] TRAIN: iteration 7912 : loss : 0.232591, loss_ce: 0.006610, loss_dice: 0.458572
[08:59:54.438] TRAIN: iteration 7913 : loss : 0.214020, loss_ce: 0.004850, loss_dice: 0.423189
[08:59:54.646] TRAIN: iteration 7914 : loss : 0.104443, loss_ce: 0.006926, loss_dice: 0.201960
[08:59:54.853] TRAIN: iteration 7915 : loss : 0.176751, loss_ce: 0.034586, loss_dice: 0.318917
[08:59:55.066] TRAIN: iteration 7916 : loss : 0.067278, loss_ce: 0.008711, loss_dice: 0.125845
[08:59:55.273] TRAIN: iteration 7917 : loss : 0.139569, loss_ce: 0.024693, loss_dice: 0.254445
[08:59:55.481] TRAIN: iteration 7918 : loss : 0.069829, loss_ce: 0.003692, loss_dice: 0.135966
[08:59:55.689] TRAIN: iteration 7919 : loss : 0.252236, loss_ce: 0.004269, loss_dice: 0.500203
[08:59:55.900] TRAIN: iteration 7920 : loss : 0.196997, loss_ce: 0.012664, loss_dice: 0.381330
[08:59:56.141] TRAIN: iteration 7921 : loss : 0.252330, loss_ce: 0.004429, loss_dice: 0.500230
[08:59:56.349] TRAIN: iteration 7922 : loss : 0.057516, loss_ce: 0.004531, loss_dice: 0.110502
[08:59:56.564] TRAIN: iteration 7923 : loss : 0.077754, loss_ce: 0.006872, loss_dice: 0.148636
[08:59:56.774] TRAIN: iteration 7924 : loss : 0.172171, loss_ce: 0.005108, loss_dice: 0.339233
[08:59:57.723] TRAIN: iteration 7925 : loss : 0.129210, loss_ce: 0.006602, loss_dice: 0.251818
[08:59:57.930] TRAIN: iteration 7926 : loss : 0.048970, loss_ce: 0.003416, loss_dice: 0.094524
[08:59:59.830] TRAIN: iteration 7927 : loss : 0.164052, loss_ce: 0.004473, loss_dice: 0.323630
[09:00:00.039] TRAIN: iteration 7928 : loss : 0.248721, loss_ce: 0.005098, loss_dice: 0.492344
[09:00:00.248] TRAIN: iteration 7929 : loss : 0.143045, loss_ce: 0.006602, loss_dice: 0.279489
[09:00:00.462] TRAIN: iteration 7930 : loss : 0.178669, loss_ce: 0.007783, loss_dice: 0.349554
[09:00:00.670] TRAIN: iteration 7931 : loss : 0.112533, loss_ce: 0.007667, loss_dice: 0.217399
[09:00:00.884] TRAIN: iteration 7932 : loss : 0.045272, loss_ce: 0.003729, loss_dice: 0.086816
[09:00:01.091] TRAIN: iteration 7933 : loss : 0.066279, loss_ce: 0.002131, loss_dice: 0.130426
[09:00:01.303] TRAIN: iteration 7934 : loss : 0.250031, loss_ce: 0.003514, loss_dice: 0.496549
[09:00:01.939] TRAIN: iteration 7935 : loss : 0.089207, loss_ce: 0.004801, loss_dice: 0.173613
[09:00:02.154] TRAIN: iteration 7936 : loss : 0.073997, loss_ce: 0.003603, loss_dice: 0.144392
[09:00:02.363] TRAIN: iteration 7937 : loss : 0.249418, loss_ce: 0.013634, loss_dice: 0.485201
[09:00:02.572] TRAIN: iteration 7938 : loss : 0.100723, loss_ce: 0.003903, loss_dice: 0.197543
[09:00:02.783] TRAIN: iteration 7939 : loss : 0.080273, loss_ce: 0.003106, loss_dice: 0.157441
[09:00:02.992] TRAIN: iteration 7940 : loss : 0.048379, loss_ce: 0.003026, loss_dice: 0.093732
[09:00:03.222] TRAIN: iteration 7941 : loss : 0.103094, loss_ce: 0.006728, loss_dice: 0.199460
[09:00:03.432] TRAIN: iteration 7942 : loss : 0.148955, loss_ce: 0.007093, loss_dice: 0.290817
[09:00:04.155] TRAIN: iteration 7943 : loss : 0.123966, loss_ce: 0.004760, loss_dice: 0.243172
[09:00:04.364] TRAIN: iteration 7944 : loss : 0.036504, loss_ce: 0.003737, loss_dice: 0.069271
[09:00:04.685] TRAIN: iteration 7945 : loss : 0.077933, loss_ce: 0.003083, loss_dice: 0.152783
[09:00:05.628] TRAIN: iteration 7946 : loss : 0.153018, loss_ce: 0.010735, loss_dice: 0.295301
[09:00:05.839] TRAIN: iteration 7947 : loss : 0.106373, loss_ce: 0.004096, loss_dice: 0.208650
[09:00:06.050] TRAIN: iteration 7948 : loss : 0.220754, loss_ce: 0.008996, loss_dice: 0.432512
[09:00:06.259] TRAIN: iteration 7949 : loss : 0.132846, loss_ce: 0.002512, loss_dice: 0.263181
[09:00:06.471] TRAIN: iteration 7950 : loss : 0.250158, loss_ce: 0.003261, loss_dice: 0.497055
[09:00:06.686] TRAIN: iteration 7951 : loss : 0.250914, loss_ce: 0.004205, loss_dice: 0.497623
[09:00:06.893] TRAIN: iteration 7952 : loss : 0.218175, loss_ce: 0.003020, loss_dice: 0.433330
[09:00:07.104] TRAIN: iteration 7953 : loss : 0.120210, loss_ce: 0.003421, loss_dice: 0.236998
[09:00:09.103] TRAIN: iteration 7954 : loss : 0.116964, loss_ce: 0.003028, loss_dice: 0.230899
[09:00:09.311] TRAIN: iteration 7955 : loss : 0.171386, loss_ce: 0.003697, loss_dice: 0.339075
[09:00:09.520] TRAIN: iteration 7956 : loss : 0.183810, loss_ce: 0.002815, loss_dice: 0.364804
[09:00:09.728] TRAIN: iteration 7957 : loss : 0.170269, loss_ce: 0.026060, loss_dice: 0.314478
[09:00:09.935] TRAIN: iteration 7958 : loss : 0.232902, loss_ce: 0.004799, loss_dice: 0.461005
[09:00:10.142] TRAIN: iteration 7959 : loss : 0.123607, loss_ce: 0.010836, loss_dice: 0.236378
[09:00:10.349] TRAIN: iteration 7960 : loss : 0.197912, loss_ce: 0.001788, loss_dice: 0.394035
[09:00:10.601] TRAIN: iteration 7961 : loss : 0.239945, loss_ce: 0.003133, loss_dice: 0.476757
[09:00:11.073] TRAIN: iteration 7962 : loss : 0.156972, loss_ce: 0.003488, loss_dice: 0.310456
[09:00:11.282] TRAIN: iteration 7963 : loss : 0.094209, loss_ce: 0.009307, loss_dice: 0.179111
[09:00:11.491] TRAIN: iteration 7964 : loss : 0.145663, loss_ce: 0.003601, loss_dice: 0.287724
[09:00:11.705] TRAIN: iteration 7965 : loss : 0.230304, loss_ce: 0.014030, loss_dice: 0.446578
[09:00:11.913] TRAIN: iteration 7966 : loss : 0.187338, loss_ce: 0.002537, loss_dice: 0.372139
[09:00:12.144] TRAIN: iteration 7967 : loss : 0.063516, loss_ce: 0.003602, loss_dice: 0.123429
[09:00:12.353] TRAIN: iteration 7968 : loss : 0.104030, loss_ce: 0.002763, loss_dice: 0.205297
[09:00:12.562] TRAIN: iteration 7969 : loss : 0.072036, loss_ce: 0.003415, loss_dice: 0.140656
[09:00:12.773] TRAIN: iteration 7970 : loss : 0.251318, loss_ce: 0.002524, loss_dice: 0.500111
[09:00:12.988] TRAIN: iteration 7971 : loss : 0.238093, loss_ce: 0.009511, loss_dice: 0.466676
[09:00:13.198] TRAIN: iteration 7972 : loss : 0.105087, loss_ce: 0.004660, loss_dice: 0.205514
[09:00:13.405] TRAIN: iteration 7973 : loss : 0.122283, loss_ce: 0.003367, loss_dice: 0.241199
[09:00:13.614] TRAIN: iteration 7974 : loss : 0.227192, loss_ce: 0.002165, loss_dice: 0.452220
[09:00:13.821] TRAIN: iteration 7975 : loss : 0.189619, loss_ce: 0.004481, loss_dice: 0.374757
[09:00:14.030] TRAIN: iteration 7976 : loss : 0.123528, loss_ce: 0.006326, loss_dice: 0.240730
[09:00:14.240] TRAIN: iteration 7977 : loss : 0.253168, loss_ce: 0.006656, loss_dice: 0.499679
[09:00:14.531] TRAIN: iteration 7978 : loss : 0.126481, loss_ce: 0.015851, loss_dice: 0.237111
[09:00:15.147] TRAIN: iteration 7979 : loss : 0.046385, loss_ce: 0.003181, loss_dice: 0.089588
[09:00:15.355] TRAIN: iteration 7980 : loss : 0.171612, loss_ce: 0.013070, loss_dice: 0.330155
[09:00:15.587] TRAIN: iteration 7981 : loss : 0.104454, loss_ce: 0.007337, loss_dice: 0.201571
[09:00:15.795] TRAIN: iteration 7982 : loss : 0.247879, loss_ce: 0.004862, loss_dice: 0.490896
[09:00:16.002] TRAIN: iteration 7983 : loss : 0.175471, loss_ce: 0.004892, loss_dice: 0.346051
[09:00:16.214] TRAIN: iteration 7984 : loss : 0.249009, loss_ce: 0.020579, loss_dice: 0.477438
[09:00:16.422] TRAIN: iteration 7985 : loss : 0.249596, loss_ce: 0.005006, loss_dice: 0.494186
[09:00:17.390] TRAIN: iteration 7986 : loss : 0.146799, loss_ce: 0.007713, loss_dice: 0.285885
[09:00:17.597] TRAIN: iteration 7987 : loss : 0.186995, loss_ce: 0.003915, loss_dice: 0.370076
[09:00:17.808] TRAIN: iteration 7988 : loss : 0.251639, loss_ce: 0.003085, loss_dice: 0.500192
[09:00:18.025] TRAIN: iteration 7989 : loss : 0.121498, loss_ce: 0.012836, loss_dice: 0.230161
[09:00:18.232] TRAIN: iteration 7990 : loss : 0.125247, loss_ce: 0.004308, loss_dice: 0.246186
[09:00:18.443] TRAIN: iteration 7991 : loss : 0.155778, loss_ce: 0.007987, loss_dice: 0.303568
[09:00:18.656] TRAIN: iteration 7992 : loss : 0.111459, loss_ce: 0.004892, loss_dice: 0.218026
[09:00:18.864] TRAIN: iteration 7993 : loss : 0.192020, loss_ce: 0.005508, loss_dice: 0.378533
[09:00:19.534] TRAIN: iteration 7994 : loss : 0.122892, loss_ce: 0.004243, loss_dice: 0.241541
[09:00:19.834] TRAIN: iteration 7995 : loss : 0.121991, loss_ce: 0.005965, loss_dice: 0.238018
[09:00:20.041] TRAIN: iteration 7996 : loss : 0.136475, loss_ce: 0.004794, loss_dice: 0.268155
[09:00:21.440] TRAIN: iteration 7997 : loss : 0.195605, loss_ce: 0.005138, loss_dice: 0.386072
[09:00:21.648] TRAIN: iteration 7998 : loss : 0.123125, loss_ce: 0.007814, loss_dice: 0.238435
[09:00:21.855] TRAIN: iteration 7999 : loss : 0.151383, loss_ce: 0.003620, loss_dice: 0.299145
[09:00:22.061] TRAIN: iteration 8000 : loss : 0.233651, loss_ce: 0.004757, loss_dice: 0.462544
[09:00:22.303] TRAIN: iteration 8001 : loss : 0.251731, loss_ce: 0.003329, loss_dice: 0.500133
[09:00:22.512] TRAIN: iteration 8002 : loss : 0.225970, loss_ce: 0.003767, loss_dice: 0.448173
[09:00:22.719] TRAIN: iteration 8003 : loss : 0.129366, loss_ce: 0.004233, loss_dice: 0.254499
[09:00:22.930] TRAIN: iteration 8004 : loss : 0.160707, loss_ce: 0.015261, loss_dice: 0.306154
[09:00:23.275] TRAIN: iteration 8005 : loss : 0.081248, loss_ce: 0.004427, loss_dice: 0.158069
[09:00:23.482] TRAIN: iteration 8006 : loss : 0.105654, loss_ce: 0.003792, loss_dice: 0.207515
[09:00:23.691] TRAIN: iteration 8007 : loss : 0.251618, loss_ce: 0.003104, loss_dice: 0.500132
[09:00:23.938] TRAIN: iteration 8008 : loss : 0.062249, loss_ce: 0.003048, loss_dice: 0.121450
[09:00:24.145] TRAIN: iteration 8009 : loss : 0.128140, loss_ce: 0.012899, loss_dice: 0.243380
[09:00:24.353] TRAIN: iteration 8010 : loss : 0.080428, loss_ce: 0.003698, loss_dice: 0.157157
[09:00:24.668] TRAIN: iteration 8011 : loss : 0.118756, loss_ce: 0.011961, loss_dice: 0.225550
[09:00:24.875] TRAIN: iteration 8012 : loss : 0.106529, loss_ce: 0.005516, loss_dice: 0.207543
[09:00:25.347] TRAIN: iteration 8013 : loss : 0.125015, loss_ce: 0.007870, loss_dice: 0.242160
[09:00:25.555] TRAIN: iteration 8014 : loss : 0.037761, loss_ce: 0.002653, loss_dice: 0.072868
[09:00:25.767] TRAIN: iteration 8015 : loss : 0.092814, loss_ce: 0.005568, loss_dice: 0.180061
[09:00:27.179] TRAIN: iteration 8016 : loss : 0.142918, loss_ce: 0.005127, loss_dice: 0.280709
[09:00:27.385] TRAIN: iteration 8017 : loss : 0.216177, loss_ce: 0.014530, loss_dice: 0.417825
[09:00:27.592] TRAIN: iteration 8018 : loss : 0.225955, loss_ce: 0.004196, loss_dice: 0.447713
[09:00:27.843] TRAIN: iteration 8019 : loss : 0.144868, loss_ce: 0.004826, loss_dice: 0.284909
[09:00:28.051] TRAIN: iteration 8020 : loss : 0.239082, loss_ce: 0.004296, loss_dice: 0.473868
[09:00:28.289] TRAIN: iteration 8021 : loss : 0.072973, loss_ce: 0.005822, loss_dice: 0.140123
[09:00:28.496] TRAIN: iteration 8022 : loss : 0.252076, loss_ce: 0.003897, loss_dice: 0.500255
[09:00:28.704] TRAIN: iteration 8023 : loss : 0.103374, loss_ce: 0.004539, loss_dice: 0.202209
[09:00:28.911] TRAIN: iteration 8024 : loss : 0.080651, loss_ce: 0.005879, loss_dice: 0.155424
[09:00:29.677] TRAIN: iteration 8025 : loss : 0.249483, loss_ce: 0.004127, loss_dice: 0.494840
[09:00:29.886] TRAIN: iteration 8026 : loss : 0.235599, loss_ce: 0.003159, loss_dice: 0.468039
[09:00:30.099] TRAIN: iteration 8027 : loss : 0.251117, loss_ce: 0.002163, loss_dice: 0.500071
[09:00:30.307] TRAIN: iteration 8028 : loss : 0.098157, loss_ce: 0.002630, loss_dice: 0.193684
[09:00:30.848] TRAIN: iteration 8029 : loss : 0.134900, loss_ce: 0.004358, loss_dice: 0.265441
[09:00:31.061] TRAIN: iteration 8030 : loss : 0.179009, loss_ce: 0.005576, loss_dice: 0.352441
[09:00:31.269] TRAIN: iteration 8031 : loss : 0.203593, loss_ce: 0.008844, loss_dice: 0.398343
[09:00:33.762] TRAIN: iteration 8032 : loss : 0.145800, loss_ce: 0.009543, loss_dice: 0.282056
[09:00:33.969] TRAIN: iteration 8033 : loss : 0.155926, loss_ce: 0.003731, loss_dice: 0.308122
[09:00:34.177] TRAIN: iteration 8034 : loss : 0.079029, loss_ce: 0.005475, loss_dice: 0.152583
[09:00:34.386] TRAIN: iteration 8035 : loss : 0.250950, loss_ce: 0.001853, loss_dice: 0.500048
[09:00:34.593] TRAIN: iteration 8036 : loss : 0.092502, loss_ce: 0.002485, loss_dice: 0.182519
[09:00:34.802] TRAIN: iteration 8037 : loss : 0.103189, loss_ce: 0.008716, loss_dice: 0.197662
[09:00:35.018] TRAIN: iteration 8038 : loss : 0.227753, loss_ce: 0.011178, loss_dice: 0.444327
[09:00:35.227] TRAIN: iteration 8039 : loss : 0.052650, loss_ce: 0.002108, loss_dice: 0.103193
[09:00:35.437] TRAIN: iteration 8040 : loss : 0.252527, loss_ce: 0.004741, loss_dice: 0.500313
[09:00:35.672] TRAIN: iteration 8041 : loss : 0.251887, loss_ce: 0.003568, loss_dice: 0.500206
[09:00:35.880] TRAIN: iteration 8042 : loss : 0.206777, loss_ce: 0.005540, loss_dice: 0.408014
[09:00:36.092] TRAIN: iteration 8043 : loss : 0.240370, loss_ce: 0.005017, loss_dice: 0.475722
[09:00:36.301] TRAIN: iteration 8044 : loss : 0.095948, loss_ce: 0.004149, loss_dice: 0.187746
[09:00:36.513] TRAIN: iteration 8045 : loss : 0.076482, loss_ce: 0.005355, loss_dice: 0.147608
[09:00:36.729] TRAIN: iteration 8046 : loss : 0.151237, loss_ce: 0.009779, loss_dice: 0.292696
[09:00:36.936] TRAIN: iteration 8047 : loss : 0.128375, loss_ce: 0.005778, loss_dice: 0.250972
[09:00:37.148] TRAIN: iteration 8048 : loss : 0.119387, loss_ce: 0.014045, loss_dice: 0.224730
[09:00:37.358] TRAIN: iteration 8049 : loss : 0.128259, loss_ce: 0.004995, loss_dice: 0.251523
[09:00:37.569] TRAIN: iteration 8050 : loss : 0.131116, loss_ce: 0.004864, loss_dice: 0.257367
[09:00:38.722] TRAIN: iteration 8051 : loss : 0.202339, loss_ce: 0.005746, loss_dice: 0.398932
[09:00:38.930] TRAIN: iteration 8052 : loss : 0.120620, loss_ce: 0.004265, loss_dice: 0.236975
[09:00:39.144] TRAIN: iteration 8053 : loss : 0.089533, loss_ce: 0.005146, loss_dice: 0.173920
[09:00:39.353] TRAIN: iteration 8054 : loss : 0.151433, loss_ce: 0.005776, loss_dice: 0.297090
[09:00:40.076] TRAIN: iteration 8055 : loss : 0.233695, loss_ce: 0.010300, loss_dice: 0.457090
[09:00:40.285] TRAIN: iteration 8056 : loss : 0.207192, loss_ce: 0.007256, loss_dice: 0.407127
[09:00:40.492] TRAIN: iteration 8057 : loss : 0.058610, loss_ce: 0.002556, loss_dice: 0.114665
[09:00:40.699] TRAIN: iteration 8058 : loss : 0.251051, loss_ce: 0.002040, loss_dice: 0.500061
[09:00:43.009] TRAIN: iteration 8059 : loss : 0.124674, loss_ce: 0.007515, loss_dice: 0.241833
[09:00:43.224] TRAIN: iteration 8060 : loss : 0.244501, loss_ce: 0.003019, loss_dice: 0.485983
[09:00:43.463] TRAIN: iteration 8061 : loss : 0.236115, loss_ce: 0.009828, loss_dice: 0.462402
[09:00:43.673] TRAIN: iteration 8062 : loss : 0.126286, loss_ce: 0.008419, loss_dice: 0.244153
[09:00:43.882] TRAIN: iteration 8063 : loss : 0.244506, loss_ce: 0.004808, loss_dice: 0.484204
[09:00:44.175] TRAIN: iteration 8064 : loss : 0.119067, loss_ce: 0.005538, loss_dice: 0.232596
[09:00:44.385] TRAIN: iteration 8065 : loss : 0.087978, loss_ce: 0.005201, loss_dice: 0.170756
[09:00:44.592] TRAIN: iteration 8066 : loss : 0.251558, loss_ce: 0.003264, loss_dice: 0.499853
[09:00:47.524] TRAIN: iteration 8067 : loss : 0.085701, loss_ce: 0.003018, loss_dice: 0.168384
[09:00:47.734] TRAIN: iteration 8068 : loss : 0.181608, loss_ce: 0.007804, loss_dice: 0.355413
[09:00:47.942] TRAIN: iteration 8069 : loss : 0.031519, loss_ce: 0.002421, loss_dice: 0.060617
[09:00:48.149] TRAIN: iteration 8070 : loss : 0.202523, loss_ce: 0.006784, loss_dice: 0.398262
[09:00:48.356] TRAIN: iteration 8071 : loss : 0.119098, loss_ce: 0.006034, loss_dice: 0.232161
[09:00:48.563] TRAIN: iteration 8072 : loss : 0.147082, loss_ce: 0.003902, loss_dice: 0.290261
[09:00:48.777] TRAIN: iteration 8073 : loss : 0.252104, loss_ce: 0.005151, loss_dice: 0.499056
[09:00:48.985] TRAIN: iteration 8074 : loss : 0.221939, loss_ce: 0.005881, loss_dice: 0.437997
[09:00:49.998] TRAIN: iteration 8075 : loss : 0.252687, loss_ce: 0.004998, loss_dice: 0.500376
[09:00:50.206] TRAIN: iteration 8076 : loss : 0.121423, loss_ce: 0.004701, loss_dice: 0.238146
[09:00:50.414] TRAIN: iteration 8077 : loss : 0.096351, loss_ce: 0.004394, loss_dice: 0.188308
[09:00:50.621] TRAIN: iteration 8078 : loss : 0.222058, loss_ce: 0.007159, loss_dice: 0.436957
[09:00:50.829] TRAIN: iteration 8079 : loss : 0.252717, loss_ce: 0.005082, loss_dice: 0.500352
[09:00:51.036] TRAIN: iteration 8080 : loss : 0.180705, loss_ce: 0.006277, loss_dice: 0.355133
[09:00:51.271] TRAIN: iteration 8081 : loss : 0.161374, loss_ce: 0.004246, loss_dice: 0.318502
[09:00:51.479] TRAIN: iteration 8082 : loss : 0.118898, loss_ce: 0.008249, loss_dice: 0.229546
[09:00:53.878] TRAIN: iteration 8083 : loss : 0.094194, loss_ce: 0.006163, loss_dice: 0.182226
[09:00:54.090] TRAIN: iteration 8084 : loss : 0.203865, loss_ce: 0.004680, loss_dice: 0.403050
[09:00:54.303] TRAIN: iteration 8085 : loss : 0.134328, loss_ce: 0.004305, loss_dice: 0.264351
[09:00:54.511] TRAIN: iteration 8086 : loss : 0.120195, loss_ce: 0.005319, loss_dice: 0.235071
[09:00:54.721] TRAIN: iteration 8087 : loss : 0.193646, loss_ce: 0.003433, loss_dice: 0.383859
[09:00:54.930] TRAIN: iteration 8088 : loss : 0.061936, loss_ce: 0.007646, loss_dice: 0.116225
[09:00:55.141] TRAIN: iteration 8089 : loss : 0.252521, loss_ce: 0.004757, loss_dice: 0.500285
[09:00:55.353] TRAIN: iteration 8090 : loss : 0.115032, loss_ce: 0.005962, loss_dice: 0.224103
[09:00:55.561] TRAIN: iteration 8091 : loss : 0.153146, loss_ce: 0.007373, loss_dice: 0.298919
[09:00:55.775] TRAIN: iteration 8092 : loss : 0.227492, loss_ce: 0.004951, loss_dice: 0.450034
[09:00:55.989] TRAIN: iteration 8093 : loss : 0.112291, loss_ce: 0.015417, loss_dice: 0.209165
[09:00:56.197] TRAIN: iteration 8094 : loss : 0.182381, loss_ce: 0.004781, loss_dice: 0.359981
[09:00:56.404] TRAIN: iteration 8095 : loss : 0.243650, loss_ce: 0.003802, loss_dice: 0.483498
[09:00:56.614] TRAIN: iteration 8096 : loss : 0.065513, loss_ce: 0.004457, loss_dice: 0.126568
[09:00:56.821] TRAIN: iteration 8097 : loss : 0.113360, loss_ce: 0.003660, loss_dice: 0.223060
[09:00:57.030] TRAIN: iteration 8098 : loss : 0.096890, loss_ce: 0.005267, loss_dice: 0.188513
[09:00:57.239] TRAIN: iteration 8099 : loss : 0.105306, loss_ce: 0.004024, loss_dice: 0.206587
[09:00:58.033] TRAIN: iteration 8100 : loss : 0.251953, loss_ce: 0.003866, loss_dice: 0.500040
[09:00:58.275] TRAIN: iteration 8101 : loss : 0.199081, loss_ce: 0.005044, loss_dice: 0.393118
[09:00:58.483] TRAIN: iteration 8102 : loss : 0.129004, loss_ce: 0.005023, loss_dice: 0.252985
[09:00:58.696] TRAIN: iteration 8103 : loss : 0.143113, loss_ce: 0.012857, loss_dice: 0.273368
[09:00:58.903] TRAIN: iteration 8104 : loss : 0.207556, loss_ce: 0.002553, loss_dice: 0.412559
[09:00:59.113] TRAIN: iteration 8105 : loss : 0.175329, loss_ce: 0.003771, loss_dice: 0.346886
[09:00:59.322] TRAIN: iteration 8106 : loss : 0.069783, loss_ce: 0.002500, loss_dice: 0.137066
[09:00:59.529] TRAIN: iteration 8107 : loss : 0.121754, loss_ce: 0.003695, loss_dice: 0.239812
[09:00:59.837] TRAIN: iteration 8108 : loss : 0.205697, loss_ce: 0.031370, loss_dice: 0.380025
[09:01:00.046] TRAIN: iteration 8109 : loss : 0.076277, loss_ce: 0.005576, loss_dice: 0.146978
[09:01:00.257] TRAIN: iteration 8110 : loss : 0.057536, loss_ce: 0.001888, loss_dice: 0.113185
[09:01:00.571] TRAIN: iteration 8111 : loss : 0.236278, loss_ce: 0.003856, loss_dice: 0.468699
[09:01:01.618] TRAIN: iteration 8112 : loss : 0.157223, loss_ce: 0.004442, loss_dice: 0.310003
[09:01:01.825] TRAIN: iteration 8113 : loss : 0.166335, loss_ce: 0.002456, loss_dice: 0.330213
[09:01:02.033] TRAIN: iteration 8114 : loss : 0.197276, loss_ce: 0.040401, loss_dice: 0.354151
[09:01:02.244] TRAIN: iteration 8115 : loss : 0.106238, loss_ce: 0.002634, loss_dice: 0.209843
[09:01:02.453] TRAIN: iteration 8116 : loss : 0.251507, loss_ce: 0.002827, loss_dice: 0.500186
[09:01:02.661] TRAIN: iteration 8117 : loss : 0.251238, loss_ce: 0.002330, loss_dice: 0.500146
[09:01:02.870] TRAIN: iteration 8118 : loss : 0.251593, loss_ce: 0.002977, loss_dice: 0.500208
[09:01:03.080] TRAIN: iteration 8119 : loss : 0.252236, loss_ce: 0.004151, loss_dice: 0.500320
[09:01:03.288] TRAIN: iteration 8120 : loss : 0.211317, loss_ce: 0.004564, loss_dice: 0.418070
[09:01:03.520] TRAIN: iteration 8121 : loss : 0.118122, loss_ce: 0.017881, loss_dice: 0.218362
[09:01:03.728] TRAIN: iteration 8122 : loss : 0.138985, loss_ce: 0.006319, loss_dice: 0.271652
[09:01:03.939] TRAIN: iteration 8123 : loss : 0.155019, loss_ce: 0.003219, loss_dice: 0.306819
[09:01:06.127] TRAIN: iteration 8124 : loss : 0.169042, loss_ce: 0.009861, loss_dice: 0.328224
[09:01:06.334] TRAIN: iteration 8125 : loss : 0.132695, loss_ce: 0.004815, loss_dice: 0.260575
[09:01:06.542] TRAIN: iteration 8126 : loss : 0.253471, loss_ce: 0.006409, loss_dice: 0.500533
[09:01:06.751] TRAIN: iteration 8127 : loss : 0.251576, loss_ce: 0.002950, loss_dice: 0.500201
[09:01:06.960] TRAIN: iteration 8128 : loss : 0.123497, loss_ce: 0.011159, loss_dice: 0.235834
[09:01:07.168] TRAIN: iteration 8129 : loss : 0.201905, loss_ce: 0.003541, loss_dice: 0.400270
[09:01:07.375] TRAIN: iteration 8130 : loss : 0.091442, loss_ce: 0.006291, loss_dice: 0.176594
[09:01:07.584] TRAIN: iteration 8131 : loss : 0.178835, loss_ce: 0.029309, loss_dice: 0.328362
[09:01:08.495] TRAIN: iteration 8132 : loss : 0.058554, loss_ce: 0.006246, loss_dice: 0.110862
[09:01:08.705] TRAIN: iteration 8133 : loss : 0.131330, loss_ce: 0.004049, loss_dice: 0.258611
[09:01:08.913] TRAIN: iteration 8134 : loss : 0.081149, loss_ce: 0.011070, loss_dice: 0.151227
[09:01:09.120] TRAIN: iteration 8135 : loss : 0.056526, loss_ce: 0.005235, loss_dice: 0.107816
[09:01:09.327] TRAIN: iteration 8136 : loss : 0.252144, loss_ce: 0.003979, loss_dice: 0.500309
[09:01:09.535] TRAIN: iteration 8137 : loss : 0.256764, loss_ce: 0.012941, loss_dice: 0.500587
[09:01:11.394] TRAIN: iteration 8138 : loss : 0.246603, loss_ce: 0.005216, loss_dice: 0.487990
[09:01:11.611] TRAIN: iteration 8139 : loss : 0.191309, loss_ce: 0.008582, loss_dice: 0.374035
[09:01:11.833] TRAIN: iteration 8140 : loss : 0.209792, loss_ce: 0.003494, loss_dice: 0.416090
[09:01:12.271] TRAIN: iteration 8141 : loss : 0.106786, loss_ce: 0.006144, loss_dice: 0.207429
[09:01:12.478] TRAIN: iteration 8142 : loss : 0.074390, loss_ce: 0.003738, loss_dice: 0.145043
[09:01:12.685] TRAIN: iteration 8143 : loss : 0.041445, loss_ce: 0.003950, loss_dice: 0.078940
[09:01:12.892] TRAIN: iteration 8144 : loss : 0.072726, loss_ce: 0.011470, loss_dice: 0.133981
[09:01:13.099] TRAIN: iteration 8145 : loss : 0.251285, loss_ce: 0.007547, loss_dice: 0.495022
[09:01:13.803] TRAIN: iteration 8146 : loss : 0.248098, loss_ce: 0.004188, loss_dice: 0.492009
[09:01:14.010] TRAIN: iteration 8147 : loss : 0.122241, loss_ce: 0.011016, loss_dice: 0.233466
[09:01:14.217] TRAIN: iteration 8148 : loss : 0.218152, loss_ce: 0.006859, loss_dice: 0.429445
[09:01:14.424] TRAIN: iteration 8149 : loss : 0.148941, loss_ce: 0.010668, loss_dice: 0.287215
[09:01:14.634] TRAIN: iteration 8150 : loss : 0.232480, loss_ce: 0.004368, loss_dice: 0.460591
[09:01:14.841] TRAIN: iteration 8151 : loss : 0.097773, loss_ce: 0.010643, loss_dice: 0.184903
[09:01:15.049] TRAIN: iteration 8152 : loss : 0.120168, loss_ce: 0.004997, loss_dice: 0.235339
[09:01:15.256] TRAIN: iteration 8153 : loss : 0.131812, loss_ce: 0.005238, loss_dice: 0.258386
[09:01:15.523] TRAIN: iteration 8154 : loss : 0.254226, loss_ce: 0.007822, loss_dice: 0.500630
[09:01:15.730] TRAIN: iteration 8155 : loss : 0.169221, loss_ce: 0.008112, loss_dice: 0.330329
[09:01:15.939] TRAIN: iteration 8156 : loss : 0.251944, loss_ce: 0.003674, loss_dice: 0.500213
[09:01:16.146] TRAIN: iteration 8157 : loss : 0.251395, loss_ce: 0.002675, loss_dice: 0.500115
[09:01:16.370] TRAIN: iteration 8158 : loss : 0.124293, loss_ce: 0.007941, loss_dice: 0.240645
[09:01:16.577] TRAIN: iteration 8159 : loss : 0.199870, loss_ce: 0.006254, loss_dice: 0.393487
[09:01:16.785] TRAIN: iteration 8160 : loss : 0.089581, loss_ce: 0.005622, loss_dice: 0.173541
[09:01:17.014] TRAIN: iteration 8161 : loss : 0.208418, loss_ce: 0.005830, loss_dice: 0.411005
[09:01:17.395] TRAIN: iteration 8162 : loss : 0.252078, loss_ce: 0.003934, loss_dice: 0.500222
[09:01:17.604] TRAIN: iteration 8163 : loss : 0.084585, loss_ce: 0.006070, loss_dice: 0.163100
[09:01:18.169] TRAIN: iteration 8164 : loss : 0.185836, loss_ce: 0.009152, loss_dice: 0.362520
[09:01:18.553] TRAIN: iteration 8165 : loss : 0.251785, loss_ce: 0.005622, loss_dice: 0.497947
[09:01:18.763] TRAIN: iteration 8166 : loss : 0.251978, loss_ce: 0.003777, loss_dice: 0.500180
[09:01:18.973] TRAIN: iteration 8167 : loss : 0.252238, loss_ce: 0.004252, loss_dice: 0.500224
[09:01:19.658] TRAIN: iteration 8168 : loss : 0.207317, loss_ce: 0.005719, loss_dice: 0.408914
[09:01:19.866] TRAIN: iteration 8169 : loss : 0.251708, loss_ce: 0.003255, loss_dice: 0.500161
[09:01:20.079] TRAIN: iteration 8170 : loss : 0.060604, loss_ce: 0.003519, loss_dice: 0.117689
[09:01:20.286] TRAIN: iteration 8171 : loss : 0.183105, loss_ce: 0.004675, loss_dice: 0.361536
[09:01:21.291] TRAIN: iteration 8172 : loss : 0.253471, loss_ce: 0.006745, loss_dice: 0.500197
[09:01:21.498] TRAIN: iteration 8173 : loss : 0.160210, loss_ce: 0.004988, loss_dice: 0.315431
[09:01:22.413] TRAIN: iteration 8174 : loss : 0.130613, loss_ce: 0.004253, loss_dice: 0.256973
[09:01:22.621] TRAIN: iteration 8175 : loss : 0.094955, loss_ce: 0.003455, loss_dice: 0.186454
[09:01:23.948] TRAIN: iteration 8176 : loss : 0.169008, loss_ce: 0.012590, loss_dice: 0.325427
[09:01:24.155] TRAIN: iteration 8177 : loss : 0.132291, loss_ce: 0.003466, loss_dice: 0.261117
[09:01:24.363] TRAIN: iteration 8178 : loss : 0.076074, loss_ce: 0.002489, loss_dice: 0.149658
[09:01:24.572] TRAIN: iteration 8179 : loss : 0.209473, loss_ce: 0.002471, loss_dice: 0.416475
[09:01:25.180] TRAIN: iteration 8180 : loss : 0.147555, loss_ce: 0.005560, loss_dice: 0.289550
[09:01:25.421] TRAIN: iteration 8181 : loss : 0.166797, loss_ce: 0.008227, loss_dice: 0.325367
[09:01:25.710] TRAIN: iteration 8182 : loss : 0.096821, loss_ce: 0.002306, loss_dice: 0.191335
[09:01:25.918] TRAIN: iteration 8183 : loss : 0.248804, loss_ce: 0.001877, loss_dice: 0.495731
[09:01:27.228] TRAIN: iteration 8184 : loss : 0.183643, loss_ce: 0.003392, loss_dice: 0.363893
[09:01:27.436] TRAIN: iteration 8185 : loss : 0.105238, loss_ce: 0.003419, loss_dice: 0.207057
[09:01:27.643] TRAIN: iteration 8186 : loss : 0.142256, loss_ce: 0.002513, loss_dice: 0.282000
[09:01:27.858] TRAIN: iteration 8187 : loss : 0.164119, loss_ce: 0.009732, loss_dice: 0.318506
[09:01:28.065] TRAIN: iteration 8188 : loss : 0.241538, loss_ce: 0.013596, loss_dice: 0.469479
[09:01:28.272] TRAIN: iteration 8189 : loss : 0.137440, loss_ce: 0.002142, loss_dice: 0.272738
[09:01:28.739] TRAIN: iteration 8190 : loss : 0.215296, loss_ce: 0.003540, loss_dice: 0.427052
[09:01:28.945] TRAIN: iteration 8191 : loss : 0.107401, loss_ce: 0.003301, loss_dice: 0.211500
[09:01:29.957] TRAIN: iteration 8192 : loss : 0.146502, loss_ce: 0.005184, loss_dice: 0.287819
[09:01:30.165] TRAIN: iteration 8193 : loss : 0.194926, loss_ce: 0.010608, loss_dice: 0.379245
[09:01:30.380] TRAIN: iteration 8194 : loss : 0.117091, loss_ce: 0.005091, loss_dice: 0.229091
[09:01:30.620] TRAIN: iteration 8195 : loss : 0.141228, loss_ce: 0.006642, loss_dice: 0.275814
[09:01:30.828] TRAIN: iteration 8196 : loss : 0.226886, loss_ce: 0.004916, loss_dice: 0.448856
[09:01:31.037] TRAIN: iteration 8197 : loss : 0.241556, loss_ce: 0.005232, loss_dice: 0.477880
[09:01:31.246] TRAIN: iteration 8198 : loss : 0.215400, loss_ce: 0.006039, loss_dice: 0.424760
[09:01:31.458] TRAIN: iteration 8199 : loss : 0.217280, loss_ce: 0.007082, loss_dice: 0.427479
[09:01:32.702] TRAIN: iteration 8200 : loss : 0.124934, loss_ce: 0.006882, loss_dice: 0.242986
[09:01:32.940] TRAIN: iteration 8201 : loss : 0.104380, loss_ce: 0.005458, loss_dice: 0.203303
[09:01:33.151] TRAIN: iteration 8202 : loss : 0.083027, loss_ce: 0.004101, loss_dice: 0.161954
[09:01:33.360] TRAIN: iteration 8203 : loss : 0.185651, loss_ce: 0.006888, loss_dice: 0.364414
[09:01:33.567] TRAIN: iteration 8204 : loss : 0.163470, loss_ce: 0.004753, loss_dice: 0.322188
[09:01:33.780] TRAIN: iteration 8205 : loss : 0.061984, loss_ce: 0.005365, loss_dice: 0.118603
[09:01:33.987] TRAIN: iteration 8206 : loss : 0.230358, loss_ce: 0.005018, loss_dice: 0.455698
[09:01:34.196] TRAIN: iteration 8207 : loss : 0.057475, loss_ce: 0.002804, loss_dice: 0.112146
[09:01:35.696] TRAIN: iteration 8208 : loss : 0.139193, loss_ce: 0.004999, loss_dice: 0.273387
[09:01:35.903] TRAIN: iteration 8209 : loss : 0.226865, loss_ce: 0.003945, loss_dice: 0.449784
[09:01:36.115] TRAIN: iteration 8210 : loss : 0.244219, loss_ce: 0.017446, loss_dice: 0.470991
[09:01:36.324] TRAIN: iteration 8211 : loss : 0.118622, loss_ce: 0.005208, loss_dice: 0.232035
[09:01:36.538] TRAIN: iteration 8212 : loss : 0.207942, loss_ce: 0.043845, loss_dice: 0.372039
[09:01:36.813] TRAIN: iteration 8213 : loss : 0.100218, loss_ce: 0.005952, loss_dice: 0.194484
[09:01:37.469] TRAIN: iteration 8214 : loss : 0.070660, loss_ce: 0.004854, loss_dice: 0.136465
[09:01:37.676] TRAIN: iteration 8215 : loss : 0.168269, loss_ce: 0.003343, loss_dice: 0.333195
[09:01:37.883] TRAIN: iteration 8216 : loss : 0.252327, loss_ce: 0.004338, loss_dice: 0.500316
[09:01:38.099] TRAIN: iteration 8217 : loss : 0.141885, loss_ce: 0.005365, loss_dice: 0.278404
[09:01:38.306] TRAIN: iteration 8218 : loss : 0.103669, loss_ce: 0.006672, loss_dice: 0.200666
[09:01:38.514] TRAIN: iteration 8219 : loss : 0.144077, loss_ce: 0.021772, loss_dice: 0.266382
[09:01:39.642] TRAIN: iteration 8220 : loss : 0.176420, loss_ce: 0.009021, loss_dice: 0.343819
[09:01:39.643] NaN or Inf found in input tensor.
[09:01:39.857] TRAIN: iteration 8221 : loss : 0.103366, loss_ce: 0.004592, loss_dice: 0.202140
[09:01:40.063] TRAIN: iteration 8222 : loss : 0.058399, loss_ce: 0.004017, loss_dice: 0.112780
[09:01:40.271] TRAIN: iteration 8223 : loss : 0.173525, loss_ce: 0.004185, loss_dice: 0.342865
[09:01:41.417] TRAIN: iteration 8224 : loss : 0.128358, loss_ce: 0.023199, loss_dice: 0.233517
[09:01:41.623] TRAIN: iteration 8225 : loss : 0.170064, loss_ce: 0.023448, loss_dice: 0.316680
[09:01:41.831] TRAIN: iteration 8226 : loss : 0.103600, loss_ce: 0.003717, loss_dice: 0.203482
[09:01:42.040] TRAIN: iteration 8227 : loss : 0.252493, loss_ce: 0.004704, loss_dice: 0.500281
[09:01:43.268] TRAIN: iteration 8228 : loss : 0.138409, loss_ce: 0.003374, loss_dice: 0.273444
[09:01:43.483] TRAIN: iteration 8229 : loss : 0.252428, loss_ce: 0.004535, loss_dice: 0.500320
[09:01:43.691] TRAIN: iteration 8230 : loss : 0.087031, loss_ce: 0.011798, loss_dice: 0.162263
[09:01:43.904] TRAIN: iteration 8231 : loss : 0.139328, loss_ce: 0.004892, loss_dice: 0.273764
[09:01:44.115] TRAIN: iteration 8232 : loss : 0.162980, loss_ce: 0.008114, loss_dice: 0.317846
[09:01:44.323] TRAIN: iteration 8233 : loss : 0.115276, loss_ce: 0.007214, loss_dice: 0.223338
[09:01:44.532] TRAIN: iteration 8234 : loss : 0.199338, loss_ce: 0.004374, loss_dice: 0.394302
[09:01:44.740] TRAIN: iteration 8235 : loss : 0.252344, loss_ce: 0.004400, loss_dice: 0.500288
[09:01:47.225] TRAIN: iteration 8236 : loss : 0.193645, loss_ce: 0.006344, loss_dice: 0.380945
[09:01:47.434] TRAIN: iteration 8237 : loss : 0.161586, loss_ce: 0.004784, loss_dice: 0.318389
[09:01:47.643] TRAIN: iteration 8238 : loss : 0.212820, loss_ce: 0.022088, loss_dice: 0.403553
[09:01:47.850] TRAIN: iteration 8239 : loss : 0.154337, loss_ce: 0.005545, loss_dice: 0.303130
[09:01:48.056] TRAIN: iteration 8240 : loss : 0.159097, loss_ce: 0.005290, loss_dice: 0.312904
[09:01:48.287] TRAIN: iteration 8241 : loss : 0.077104, loss_ce: 0.005125, loss_dice: 0.149083
[09:01:48.496] TRAIN: iteration 8242 : loss : 0.251057, loss_ce: 0.002073, loss_dice: 0.500041
[09:01:48.703] TRAIN: iteration 8243 : loss : 0.113356, loss_ce: 0.002699, loss_dice: 0.224012
[09:01:50.649] TRAIN: iteration 8244 : loss : 0.251018, loss_ce: 0.001997, loss_dice: 0.500039
[09:01:50.856] TRAIN: iteration 8245 : loss : 0.156902, loss_ce: 0.005165, loss_dice: 0.308638
[09:01:51.064] TRAIN: iteration 8246 : loss : 0.089886, loss_ce: 0.004171, loss_dice: 0.175601
[09:01:51.270] TRAIN: iteration 8247 : loss : 0.261531, loss_ce: 0.023968, loss_dice: 0.499093
[09:01:52.047] TRAIN: iteration 8248 : loss : 0.100779, loss_ce: 0.007253, loss_dice: 0.194305
[09:01:52.254] TRAIN: iteration 8249 : loss : 0.089262, loss_ce: 0.009895, loss_dice: 0.168628
[09:01:52.462] TRAIN: iteration 8250 : loss : 0.107813, loss_ce: 0.016561, loss_dice: 0.199065
[09:01:52.669] TRAIN: iteration 8251 : loss : 0.077560, loss_ce: 0.011498, loss_dice: 0.143621
[09:01:54.451] TRAIN: iteration 8252 : loss : 0.042018, loss_ce: 0.003456, loss_dice: 0.080579
[09:01:54.664] TRAIN: iteration 8253 : loss : 0.247144, loss_ce: 0.007595, loss_dice: 0.486694
[09:01:54.871] TRAIN: iteration 8254 : loss : 0.141792, loss_ce: 0.007532, loss_dice: 0.276052
[09:01:55.082] TRAIN: iteration 8255 : loss : 0.227842, loss_ce: 0.005029, loss_dice: 0.450655
[09:01:55.290] TRAIN: iteration 8256 : loss : 0.250875, loss_ce: 0.007057, loss_dice: 0.494693
[09:01:55.504] TRAIN: iteration 8257 : loss : 0.253905, loss_ce: 0.007322, loss_dice: 0.500488
[09:01:55.718] TRAIN: iteration 8258 : loss : 0.151162, loss_ce: 0.010478, loss_dice: 0.291847
[09:01:55.932] TRAIN: iteration 8259 : loss : 0.123172, loss_ce: 0.005133, loss_dice: 0.241210
[09:01:58.102] TRAIN: iteration 8260 : loss : 0.253850, loss_ce: 0.008413, loss_dice: 0.499288
[09:01:58.340] TRAIN: iteration 8261 : loss : 0.192497, loss_ce: 0.008493, loss_dice: 0.376501
[09:01:58.547] TRAIN: iteration 8262 : loss : 0.115250, loss_ce: 0.005333, loss_dice: 0.225166
[09:01:58.754] TRAIN: iteration 8263 : loss : 0.190684, loss_ce: 0.006481, loss_dice: 0.374886
[09:01:58.962] TRAIN: iteration 8264 : loss : 0.211442, loss_ce: 0.004741, loss_dice: 0.418143
[09:01:59.171] TRAIN: iteration 8265 : loss : 0.251512, loss_ce: 0.005411, loss_dice: 0.497613
[09:01:59.379] TRAIN: iteration 8266 : loss : 0.211926, loss_ce: 0.007629, loss_dice: 0.416224
[09:01:59.586] TRAIN: iteration 8267 : loss : 0.148048, loss_ce: 0.003507, loss_dice: 0.292588
[09:02:02.071] TRAIN: iteration 8268 : loss : 0.251879, loss_ce: 0.003574, loss_dice: 0.500184
[09:02:02.280] TRAIN: iteration 8269 : loss : 0.183187, loss_ce: 0.010796, loss_dice: 0.355578
[09:02:02.486] TRAIN: iteration 8270 : loss : 0.240892, loss_ce: 0.004533, loss_dice: 0.477251
[09:02:02.693] TRAIN: iteration 8271 : loss : 0.135102, loss_ce: 0.002440, loss_dice: 0.267763
[09:02:02.900] TRAIN: iteration 8272 : loss : 0.251007, loss_ce: 0.001960, loss_dice: 0.500054
[09:02:03.118] TRAIN: iteration 8273 : loss : 0.225548, loss_ce: 0.002398, loss_dice: 0.448698
[09:02:03.335] TRAIN: iteration 8274 : loss : 0.250925, loss_ce: 0.001792, loss_dice: 0.500057
[09:02:03.546] TRAIN: iteration 8275 : loss : 0.045985, loss_ce: 0.001710, loss_dice: 0.090260
[09:02:05.431] TRAIN: iteration 8276 : loss : 0.114147, loss_ce: 0.003984, loss_dice: 0.224309
[09:02:05.646] TRAIN: iteration 8277 : loss : 0.250705, loss_ce: 0.001378, loss_dice: 0.500031
[09:02:05.854] TRAIN: iteration 8278 : loss : 0.129028, loss_ce: 0.002709, loss_dice: 0.255347
[09:02:06.062] TRAIN: iteration 8279 : loss : 0.223910, loss_ce: 0.003157, loss_dice: 0.444662
[09:02:06.269] TRAIN: iteration 8280 : loss : 0.223024, loss_ce: 0.002410, loss_dice: 0.443639
[09:02:06.511] TRAIN: iteration 8281 : loss : 0.098304, loss_ce: 0.003348, loss_dice: 0.193260
[09:02:06.718] TRAIN: iteration 8282 : loss : 0.251021, loss_ce: 0.001974, loss_dice: 0.500067
[09:02:06.926] TRAIN: iteration 8283 : loss : 0.121345, loss_ce: 0.004166, loss_dice: 0.238524
[09:02:07.587] TRAIN: iteration 8284 : loss : 0.108041, loss_ce: 0.004182, loss_dice: 0.211901
[09:02:07.978] TRAIN: iteration 8285 : loss : 0.126096, loss_ce: 0.007243, loss_dice: 0.244950
[09:02:08.185] TRAIN: iteration 8286 : loss : 0.079176, loss_ce: 0.003725, loss_dice: 0.154628
[09:02:08.393] TRAIN: iteration 8287 : loss : 0.178490, loss_ce: 0.004341, loss_dice: 0.352638
[09:02:11.336] TRAIN: iteration 8288 : loss : 0.132415, loss_ce: 0.004167, loss_dice: 0.260663
[09:02:11.544] TRAIN: iteration 8289 : loss : 0.134738, loss_ce: 0.003522, loss_dice: 0.265954
[09:02:11.793] TRAIN: iteration 8290 : loss : 0.137132, loss_ce: 0.004528, loss_dice: 0.269737
[09:02:12.000] TRAIN: iteration 8291 : loss : 0.053611, loss_ce: 0.003302, loss_dice: 0.103921
[09:02:12.208] TRAIN: iteration 8292 : loss : 0.099290, loss_ce: 0.005851, loss_dice: 0.192729
[09:02:12.423] TRAIN: iteration 8293 : loss : 0.101452, loss_ce: 0.013988, loss_dice: 0.188916
[09:02:12.630] TRAIN: iteration 8294 : loss : 0.145646, loss_ce: 0.018850, loss_dice: 0.272443
[09:02:12.836] TRAIN: iteration 8295 : loss : 0.088722, loss_ce: 0.006448, loss_dice: 0.170996
[09:02:14.225] TRAIN: iteration 8296 : loss : 0.148744, loss_ce: 0.008172, loss_dice: 0.289316
[09:02:14.433] TRAIN: iteration 8297 : loss : 0.172208, loss_ce: 0.004261, loss_dice: 0.340156
[09:02:14.641] TRAIN: iteration 8298 : loss : 0.063905, loss_ce: 0.004437, loss_dice: 0.123372
[09:02:14.848] TRAIN: iteration 8299 : loss : 0.054942, loss_ce: 0.003256, loss_dice: 0.106628
[09:02:15.056] TRAIN: iteration 8300 : loss : 0.125549, loss_ce: 0.004178, loss_dice: 0.246920
[09:02:16.945] TRAIN: iteration 8301 : loss : 0.112597, loss_ce: 0.005432, loss_dice: 0.219761
[09:02:17.152] TRAIN: iteration 8302 : loss : 0.224623, loss_ce: 0.007636, loss_dice: 0.441609
[09:02:17.359] TRAIN: iteration 8303 : loss : 0.101820, loss_ce: 0.005157, loss_dice: 0.198484
[09:02:20.794] TRAIN: iteration 8304 : loss : 0.028352, loss_ce: 0.001942, loss_dice: 0.054761
[09:02:21.002] TRAIN: iteration 8305 : loss : 0.240222, loss_ce: 0.006203, loss_dice: 0.474242
[09:02:21.215] TRAIN: iteration 8306 : loss : 0.241166, loss_ce: 0.003286, loss_dice: 0.479046
[09:02:21.424] TRAIN: iteration 8307 : loss : 0.251478, loss_ce: 0.007234, loss_dice: 0.495721
[09:02:21.632] TRAIN: iteration 8308 : loss : 0.107919, loss_ce: 0.005268, loss_dice: 0.210570
[09:02:21.970] TRAIN: iteration 8309 : loss : 0.157253, loss_ce: 0.006857, loss_dice: 0.307648
[09:02:22.178] TRAIN: iteration 8310 : loss : 0.208756, loss_ce: 0.002748, loss_dice: 0.414765
[09:02:22.386] TRAIN: iteration 8311 : loss : 0.220420, loss_ce: 0.001837, loss_dice: 0.439002
[09:02:27.495] TRAIN: iteration 8312 : loss : 0.189669, loss_ce: 0.003958, loss_dice: 0.375381
[09:02:27.709] TRAIN: iteration 8313 : loss : 0.198391, loss_ce: 0.017928, loss_dice: 0.378854
[09:02:27.915] TRAIN: iteration 8314 : loss : 0.080989, loss_ce: 0.007875, loss_dice: 0.154103
[09:02:28.121] TRAIN: iteration 8315 : loss : 0.227725, loss_ce: 0.002988, loss_dice: 0.452462
[09:02:28.332] TRAIN: iteration 8316 : loss : 0.212713, loss_ce: 0.004136, loss_dice: 0.421291
[09:02:28.539] TRAIN: iteration 8317 : loss : 0.159166, loss_ce: 0.004121, loss_dice: 0.314211
[09:02:28.745] TRAIN: iteration 8318 : loss : 0.194362, loss_ce: 0.003149, loss_dice: 0.385576
[09:02:28.952] TRAIN: iteration 8319 : loss : 0.127980, loss_ce: 0.008238, loss_dice: 0.247722
[09:02:30.892] TRAIN: iteration 8320 : loss : 0.178026, loss_ce: 0.004948, loss_dice: 0.351103
[09:02:31.132] TRAIN: iteration 8321 : loss : 0.233333, loss_ce: 0.003329, loss_dice: 0.463338
[09:02:31.348] TRAIN: iteration 8322 : loss : 0.102591, loss_ce: 0.002727, loss_dice: 0.202454
[09:02:31.557] TRAIN: iteration 8323 : loss : 0.094889, loss_ce: 0.002133, loss_dice: 0.187645
[09:02:31.764] TRAIN: iteration 8324 : loss : 0.081404, loss_ce: 0.003004, loss_dice: 0.159804
[09:02:31.975] TRAIN: iteration 8325 : loss : 0.074621, loss_ce: 0.004803, loss_dice: 0.144439
[09:02:32.184] TRAIN: iteration 8326 : loss : 0.128278, loss_ce: 0.006161, loss_dice: 0.250396
[09:02:32.392] TRAIN: iteration 8327 : loss : 0.107143, loss_ce: 0.001262, loss_dice: 0.213023
[09:02:37.471] TRAIN: iteration 8328 : loss : 0.250353, loss_ce: 0.000684, loss_dice: 0.500021
[09:02:37.678] TRAIN: iteration 8329 : loss : 0.104595, loss_ce: 0.008708, loss_dice: 0.200482
[09:02:37.887] TRAIN: iteration 8330 : loss : 0.250525, loss_ce: 0.000992, loss_dice: 0.500058
[09:02:38.095] TRAIN: iteration 8331 : loss : 0.227779, loss_ce: 0.030819, loss_dice: 0.424739
[09:02:38.305] TRAIN: iteration 8332 : loss : 0.135241, loss_ce: 0.002574, loss_dice: 0.267907
[09:02:38.512] TRAIN: iteration 8333 : loss : 0.135242, loss_ce: 0.010117, loss_dice: 0.260368
[09:02:38.719] TRAIN: iteration 8334 : loss : 0.234547, loss_ce: 0.000741, loss_dice: 0.468353
[09:02:38.926] TRAIN: iteration 8335 : loss : 0.250601, loss_ce: 0.001666, loss_dice: 0.499537
[09:02:42.272] TRAIN: iteration 8336 : loss : 0.249342, loss_ce: 0.008549, loss_dice: 0.490134
[09:02:42.483] TRAIN: iteration 8337 : loss : 0.137767, loss_ce: 0.003771, loss_dice: 0.271763
[09:02:42.804] TRAIN: iteration 8338 : loss : 0.058462, loss_ce: 0.001197, loss_dice: 0.115728
[09:02:43.012] TRAIN: iteration 8339 : loss : 0.111449, loss_ce: 0.007769, loss_dice: 0.215128
[09:02:43.222] TRAIN: iteration 8340 : loss : 0.248390, loss_ce: 0.003869, loss_dice: 0.492911
[09:02:43.453] TRAIN: iteration 8341 : loss : 0.169137, loss_ce: 0.005797, loss_dice: 0.332477
[09:02:43.662] TRAIN: iteration 8342 : loss : 0.098619, loss_ce: 0.004813, loss_dice: 0.192424
[09:02:43.870] TRAIN: iteration 8343 : loss : 0.155399, loss_ce: 0.006173, loss_dice: 0.304625
[09:02:45.221] TRAIN: iteration 8344 : loss : 0.132785, loss_ce: 0.009972, loss_dice: 0.255598
[09:02:45.432] TRAIN: iteration 8345 : loss : 0.252748, loss_ce: 0.005120, loss_dice: 0.500376
[09:02:45.646] TRAIN: iteration 8346 : loss : 0.111504, loss_ce: 0.004832, loss_dice: 0.218177
[09:02:45.853] TRAIN: iteration 8347 : loss : 0.180848, loss_ce: 0.009572, loss_dice: 0.352125
[09:02:46.061] TRAIN: iteration 8348 : loss : 0.082843, loss_ce: 0.008734, loss_dice: 0.156951
[09:02:46.269] TRAIN: iteration 8349 : loss : 0.081578, loss_ce: 0.005200, loss_dice: 0.157955
[09:02:46.477] TRAIN: iteration 8350 : loss : 0.251773, loss_ce: 0.006842, loss_dice: 0.496703
[09:02:46.684] TRAIN: iteration 8351 : loss : 0.064696, loss_ce: 0.010894, loss_dice: 0.118499
[09:02:47.611] TRAIN: iteration 8352 : loss : 0.098959, loss_ce: 0.009133, loss_dice: 0.188784
[09:02:47.819] TRAIN: iteration 8353 : loss : 0.254847, loss_ce: 0.008999, loss_dice: 0.500694
[09:02:48.154] TRAIN: iteration 8354 : loss : 0.079890, loss_ce: 0.007045, loss_dice: 0.152735
[09:02:49.513] TRAIN: iteration 8355 : loss : 0.253359, loss_ce: 0.006300, loss_dice: 0.500417
[09:02:49.720] TRAIN: iteration 8356 : loss : 0.070334, loss_ce: 0.005174, loss_dice: 0.135495
[09:02:49.927] TRAIN: iteration 8357 : loss : 0.254155, loss_ce: 0.008023, loss_dice: 0.500286
[09:02:50.135] TRAIN: iteration 8358 : loss : 0.136167, loss_ce: 0.007500, loss_dice: 0.264834
[09:02:50.349] TRAIN: iteration 8359 : loss : 0.086075, loss_ce: 0.004784, loss_dice: 0.167365
[09:02:50.561] TRAIN: iteration 8360 : loss : 0.158814, loss_ce: 0.008971, loss_dice: 0.308658
[09:02:50.797] TRAIN: iteration 8361 : loss : 0.229461, loss_ce: 0.005392, loss_dice: 0.453531
[09:02:54.961] TRAIN: iteration 8362 : loss : 0.127181, loss_ce: 0.015502, loss_dice: 0.238861
[09:02:55.173] TRAIN: iteration 8363 : loss : 0.142627, loss_ce: 0.006092, loss_dice: 0.279162
[09:02:55.380] TRAIN: iteration 8364 : loss : 0.193345, loss_ce: 0.005136, loss_dice: 0.381554
[09:02:55.588] TRAIN: iteration 8365 : loss : 0.062706, loss_ce: 0.003307, loss_dice: 0.122106
[09:02:55.797] TRAIN: iteration 8366 : loss : 0.112071, loss_ce: 0.008354, loss_dice: 0.215789
[09:02:56.004] TRAIN: iteration 8367 : loss : 0.129030, loss_ce: 0.004164, loss_dice: 0.253896
[09:02:56.427] TRAIN: iteration 8368 : loss : 0.100101, loss_ce: 0.004795, loss_dice: 0.195408
[09:02:56.635] TRAIN: iteration 8369 : loss : 0.252269, loss_ce: 0.004232, loss_dice: 0.500306
[09:03:00.168] TRAIN: iteration 8370 : loss : 0.105081, loss_ce: 0.005104, loss_dice: 0.205059
[09:03:00.387] TRAIN: iteration 8371 : loss : 0.092051, loss_ce: 0.007366, loss_dice: 0.176737
[09:03:00.597] TRAIN: iteration 8372 : loss : 0.096075, loss_ce: 0.005594, loss_dice: 0.186555
[09:03:00.805] TRAIN: iteration 8373 : loss : 0.074763, loss_ce: 0.004774, loss_dice: 0.144751
[09:03:01.013] TRAIN: iteration 8374 : loss : 0.181839, loss_ce: 0.019754, loss_dice: 0.343924
[09:03:01.221] TRAIN: iteration 8375 : loss : 0.147383, loss_ce: 0.004855, loss_dice: 0.289912
[09:03:01.429] TRAIN: iteration 8376 : loss : 0.075509, loss_ce: 0.006394, loss_dice: 0.144624
[09:03:01.637] TRAIN: iteration 8377 : loss : 0.104376, loss_ce: 0.006811, loss_dice: 0.201942
[09:03:03.819] TRAIN: iteration 8378 : loss : 0.212699, loss_ce: 0.008019, loss_dice: 0.417379
[09:03:04.029] TRAIN: iteration 8379 : loss : 0.074031, loss_ce: 0.006487, loss_dice: 0.141575
[09:03:04.237] TRAIN: iteration 8380 : loss : 0.092421, loss_ce: 0.008495, loss_dice: 0.176346
[09:03:04.475] TRAIN: iteration 8381 : loss : 0.220907, loss_ce: 0.012228, loss_dice: 0.429587
[09:03:04.683] TRAIN: iteration 8382 : loss : 0.248570, loss_ce: 0.009151, loss_dice: 0.487989
[09:03:04.890] TRAIN: iteration 8383 : loss : 0.122707, loss_ce: 0.005527, loss_dice: 0.239887
[09:03:05.098] TRAIN: iteration 8384 : loss : 0.149797, loss_ce: 0.008394, loss_dice: 0.291201
[09:03:05.311] TRAIN: iteration 8385 : loss : 0.253348, loss_ce: 0.006264, loss_dice: 0.500433
[09:03:08.472] TRAIN: iteration 8386 : loss : 0.244178, loss_ce: 0.010015, loss_dice: 0.478341
[09:03:08.681] TRAIN: iteration 8387 : loss : 0.123455, loss_ce: 0.012482, loss_dice: 0.234428
[09:03:08.891] TRAIN: iteration 8388 : loss : 0.252606, loss_ce: 0.006535, loss_dice: 0.498677
[09:03:09.099] TRAIN: iteration 8389 : loss : 0.225905, loss_ce: 0.011877, loss_dice: 0.439932
[09:03:09.307] TRAIN: iteration 8390 : loss : 0.127446, loss_ce: 0.006798, loss_dice: 0.248093
[09:03:09.515] TRAIN: iteration 8391 : loss : 0.114747, loss_ce: 0.009076, loss_dice: 0.220418
[09:03:09.728] TRAIN: iteration 8392 : loss : 0.218004, loss_ce: 0.006992, loss_dice: 0.429016
[09:03:09.938] TRAIN: iteration 8393 : loss : 0.214436, loss_ce: 0.006527, loss_dice: 0.422345
[09:03:15.560] TRAIN: iteration 8394 : loss : 0.104470, loss_ce: 0.010464, loss_dice: 0.198475
[09:03:15.769] TRAIN: iteration 8395 : loss : 0.108437, loss_ce: 0.007426, loss_dice: 0.209447
[09:03:15.977] TRAIN: iteration 8396 : loss : 0.097706, loss_ce: 0.006377, loss_dice: 0.189034
[09:03:16.185] TRAIN: iteration 8397 : loss : 0.056145, loss_ce: 0.006492, loss_dice: 0.105797
[09:03:16.392] TRAIN: iteration 8398 : loss : 0.197887, loss_ce: 0.009391, loss_dice: 0.386382
[09:03:16.600] TRAIN: iteration 8399 : loss : 0.213312, loss_ce: 0.007962, loss_dice: 0.418662
[09:03:16.808] TRAIN: iteration 8400 : loss : 0.101298, loss_ce: 0.005292, loss_dice: 0.197305
[09:03:17.050] TRAIN: iteration 8401 : loss : 0.247099, loss_ce: 0.007023, loss_dice: 0.487176
[09:03:17.864] TRAIN: iteration 8402 : loss : 0.252929, loss_ce: 0.005484, loss_dice: 0.500373
[09:03:19.515] TRAIN: iteration 8403 : loss : 0.056587, loss_ce: 0.005661, loss_dice: 0.107513
[09:03:19.721] TRAIN: iteration 8404 : loss : 0.232350, loss_ce: 0.031517, loss_dice: 0.433183
[09:03:19.930] TRAIN: iteration 8405 : loss : 0.189111, loss_ce: 0.005501, loss_dice: 0.372722
[09:03:20.139] TRAIN: iteration 8406 : loss : 0.071281, loss_ce: 0.007042, loss_dice: 0.135520
[09:03:20.347] TRAIN: iteration 8407 : loss : 0.070021, loss_ce: 0.004039, loss_dice: 0.136002
[09:03:20.555] TRAIN: iteration 8408 : loss : 0.161411, loss_ce: 0.006439, loss_dice: 0.316384
[09:03:20.763] TRAIN: iteration 8409 : loss : 0.047362, loss_ce: 0.003275, loss_dice: 0.091449
[09:03:22.964] TRAIN: iteration 8410 : loss : 0.252815, loss_ce: 0.005267, loss_dice: 0.500363
[09:03:26.750] TRAIN: iteration 8411 : loss : 0.253093, loss_ce: 0.006676, loss_dice: 0.499510
[09:03:26.961] TRAIN: iteration 8412 : loss : 0.061490, loss_ce: 0.004466, loss_dice: 0.118513
[09:03:27.169] TRAIN: iteration 8413 : loss : 0.248219, loss_ce: 0.005638, loss_dice: 0.490800
[09:03:27.378] TRAIN: iteration 8414 : loss : 0.245532, loss_ce: 0.005960, loss_dice: 0.485104
[09:03:27.585] TRAIN: iteration 8415 : loss : 0.231874, loss_ce: 0.004852, loss_dice: 0.458895
[09:03:27.792] TRAIN: iteration 8416 : loss : 0.120652, loss_ce: 0.008298, loss_dice: 0.233006
[09:03:27.999] TRAIN: iteration 8417 : loss : 0.138081, loss_ce: 0.003441, loss_dice: 0.272721
[09:03:28.251] TRAIN: iteration 8418 : loss : 0.124245, loss_ce: 0.007915, loss_dice: 0.240575
[09:03:31.194] TRAIN: iteration 8419 : loss : 0.074294, loss_ce: 0.007241, loss_dice: 0.141347
[09:03:31.405] TRAIN: iteration 8420 : loss : 0.110936, loss_ce: 0.004462, loss_dice: 0.217411
[09:03:31.645] TRAIN: iteration 8421 : loss : 0.126654, loss_ce: 0.008767, loss_dice: 0.244541
[09:03:31.852] TRAIN: iteration 8422 : loss : 0.045799, loss_ce: 0.002066, loss_dice: 0.089532
[09:03:32.063] TRAIN: iteration 8423 : loss : 0.141276, loss_ce: 0.012726, loss_dice: 0.269827
[09:03:32.271] TRAIN: iteration 8424 : loss : 0.173990, loss_ce: 0.017863, loss_dice: 0.330118
[09:03:32.486] TRAIN: iteration 8425 : loss : 0.042195, loss_ce: 0.002493, loss_dice: 0.081897
[09:03:36.494] TRAIN: iteration 8426 : loss : 0.252050, loss_ce: 0.003845, loss_dice: 0.500255
[09:03:36.702] TRAIN: iteration 8427 : loss : 0.108541, loss_ce: 0.005090, loss_dice: 0.211992
[09:03:36.911] TRAIN: iteration 8428 : loss : 0.077492, loss_ce: 0.004643, loss_dice: 0.150342
[09:03:37.119] TRAIN: iteration 8429 : loss : 0.171684, loss_ce: 0.018606, loss_dice: 0.324763
[09:03:37.326] TRAIN: iteration 8430 : loss : 0.249161, loss_ce: 0.006797, loss_dice: 0.491525
[09:03:37.533] TRAIN: iteration 8431 : loss : 0.052371, loss_ce: 0.004161, loss_dice: 0.100582
[09:03:37.740] TRAIN: iteration 8432 : loss : 0.115074, loss_ce: 0.005827, loss_dice: 0.224321
[09:03:37.948] TRAIN: iteration 8433 : loss : 0.129003, loss_ce: 0.007883, loss_dice: 0.250123
[09:03:41.374] TRAIN: iteration 8434 : loss : 0.195156, loss_ce: 0.005732, loss_dice: 0.384581
[09:03:41.581] TRAIN: iteration 8435 : loss : 0.153167, loss_ce: 0.004550, loss_dice: 0.301783
[09:03:41.788] TRAIN: iteration 8436 : loss : 0.046175, loss_ce: 0.005471, loss_dice: 0.086878
[09:03:41.996] TRAIN: iteration 8437 : loss : 0.214527, loss_ce: 0.007846, loss_dice: 0.421209
[09:03:42.205] TRAIN: iteration 8438 : loss : 0.215967, loss_ce: 0.006216, loss_dice: 0.425718
[09:03:45.211] TRAIN: iteration 8439 : loss : 0.090248, loss_ce: 0.011803, loss_dice: 0.168693
[09:03:45.422] TRAIN: iteration 8440 : loss : 0.253191, loss_ce: 0.005979, loss_dice: 0.500403
[09:03:45.673] TRAIN: iteration 8441 : loss : 0.189350, loss_ce: 0.010444, loss_dice: 0.368257
[09:03:47.178] TRAIN: iteration 8442 : loss : 0.058749, loss_ce: 0.006171, loss_dice: 0.111328
[09:03:49.447] TRAIN: iteration 8443 : loss : 0.178541, loss_ce: 0.008008, loss_dice: 0.349074
[09:03:49.664] TRAIN: iteration 8444 : loss : 0.094358, loss_ce: 0.006218, loss_dice: 0.182498
[09:03:49.873] TRAIN: iteration 8445 : loss : 0.252764, loss_ce: 0.005219, loss_dice: 0.500308
[09:03:50.092] TRAIN: iteration 8446 : loss : 0.200333, loss_ce: 0.006681, loss_dice: 0.393985
[09:03:50.615] TRAIN: iteration 8447 : loss : 0.221591, loss_ce: 0.010647, loss_dice: 0.432534
[09:03:50.822] TRAIN: iteration 8448 : loss : 0.253097, loss_ce: 0.005799, loss_dice: 0.500396
[09:03:51.030] TRAIN: iteration 8449 : loss : 0.132931, loss_ce: 0.007069, loss_dice: 0.258793
[09:03:54.527] TRAIN: iteration 8450 : loss : 0.198334, loss_ce: 0.007412, loss_dice: 0.389257
[09:03:56.624] TRAIN: iteration 8451 : loss : 0.252968, loss_ce: 0.005583, loss_dice: 0.500354
[09:03:56.832] TRAIN: iteration 8452 : loss : 0.240052, loss_ce: 0.005415, loss_dice: 0.474690
[09:03:57.048] TRAIN: iteration 8453 : loss : 0.094048, loss_ce: 0.008745, loss_dice: 0.179350
[09:03:57.257] TRAIN: iteration 8454 : loss : 0.122839, loss_ce: 0.007503, loss_dice: 0.238176
[09:03:57.466] TRAIN: iteration 8455 : loss : 0.204504, loss_ce: 0.021389, loss_dice: 0.387618
[09:03:57.675] TRAIN: iteration 8456 : loss : 0.204791, loss_ce: 0.012187, loss_dice: 0.397395
[09:03:57.889] TRAIN: iteration 8457 : loss : 0.138578, loss_ce: 0.016036, loss_dice: 0.261120
[09:04:02.472] TRAIN: iteration 8458 : loss : 0.121827, loss_ce: 0.016933, loss_dice: 0.226722
[09:04:02.728] TRAIN: iteration 8459 : loss : 0.241806, loss_ce: 0.008056, loss_dice: 0.475555
[09:04:02.935] TRAIN: iteration 8460 : loss : 0.164653, loss_ce: 0.011195, loss_dice: 0.318111
[09:04:03.177] TRAIN: iteration 8461 : loss : 0.191944, loss_ce: 0.008113, loss_dice: 0.375775
[09:04:03.385] TRAIN: iteration 8462 : loss : 0.056904, loss_ce: 0.006025, loss_dice: 0.107784
[09:04:04.137] TRAIN: iteration 8463 : loss : 0.253559, loss_ce: 0.006667, loss_dice: 0.500451
[09:04:04.344] TRAIN: iteration 8464 : loss : 0.199318, loss_ce: 0.006656, loss_dice: 0.391981
[09:04:04.556] TRAIN: iteration 8465 : loss : 0.144139, loss_ce: 0.009196, loss_dice: 0.279081
[09:04:13.125] TRAIN: iteration 8466 : loss : 0.222034, loss_ce: 0.005236, loss_dice: 0.438832
[09:04:13.332] TRAIN: iteration 8467 : loss : 0.252824, loss_ce: 0.005340, loss_dice: 0.500308
[09:04:13.540] TRAIN: iteration 8468 : loss : 0.189715, loss_ce: 0.007731, loss_dice: 0.371700
[09:04:13.748] TRAIN: iteration 8469 : loss : 0.244499, loss_ce: 0.008238, loss_dice: 0.480760
[09:04:13.956] TRAIN: iteration 8470 : loss : 0.169257, loss_ce: 0.004898, loss_dice: 0.333616
[09:04:14.164] TRAIN: iteration 8471 : loss : 0.170733, loss_ce: 0.006694, loss_dice: 0.334773
[09:04:14.373] TRAIN: iteration 8472 : loss : 0.070382, loss_ce: 0.004419, loss_dice: 0.136346
[09:04:14.583] TRAIN: iteration 8473 : loss : 0.101201, loss_ce: 0.004098, loss_dice: 0.198304
[09:04:19.169] TRAIN: iteration 8474 : loss : 0.214296, loss_ce: 0.004147, loss_dice: 0.424445
[09:04:19.377] TRAIN: iteration 8475 : loss : 0.043766, loss_ce: 0.004048, loss_dice: 0.083483
[09:04:19.585] TRAIN: iteration 8476 : loss : 0.115648, loss_ce: 0.015672, loss_dice: 0.215625
[09:04:19.793] TRAIN: iteration 8477 : loss : 0.149888, loss_ce: 0.004750, loss_dice: 0.295027
[09:04:20.000] TRAIN: iteration 8478 : loss : 0.125031, loss_ce: 0.003065, loss_dice: 0.246998
[09:04:20.208] TRAIN: iteration 8479 : loss : 0.102694, loss_ce: 0.003567, loss_dice: 0.201821
[09:04:20.415] TRAIN: iteration 8480 : loss : 0.094488, loss_ce: 0.003762, loss_dice: 0.185214
[09:04:20.649] TRAIN: iteration 8481 : loss : 0.139775, loss_ce: 0.002807, loss_dice: 0.276743
[09:04:26.346] TRAIN: iteration 8482 : loss : 0.178156, loss_ce: 0.004824, loss_dice: 0.351488
[09:04:26.553] TRAIN: iteration 8483 : loss : 0.243952, loss_ce: 0.004021, loss_dice: 0.483883
[09:04:26.761] TRAIN: iteration 8484 : loss : 0.251820, loss_ce: 0.003427, loss_dice: 0.500212
[09:04:26.968] TRAIN: iteration 8485 : loss : 0.131115, loss_ce: 0.016950, loss_dice: 0.245281
[09:04:27.175] TRAIN: iteration 8486 : loss : 0.150228, loss_ce: 0.011421, loss_dice: 0.289036
[09:04:27.382] TRAIN: iteration 8487 : loss : 0.147995, loss_ce: 0.010306, loss_dice: 0.285684
[09:04:27.589] TRAIN: iteration 8488 : loss : 0.254878, loss_ce: 0.009594, loss_dice: 0.500162
[09:04:27.796] TRAIN: iteration 8489 : loss : 0.110569, loss_ce: 0.006504, loss_dice: 0.214634
[09:04:32.234] TRAIN: iteration 8490 : loss : 0.253577, loss_ce: 0.008470, loss_dice: 0.498683
[09:04:32.440] TRAIN: iteration 8491 : loss : 0.075896, loss_ce: 0.007486, loss_dice: 0.144306
[09:04:32.648] TRAIN: iteration 8492 : loss : 0.047448, loss_ce: 0.005248, loss_dice: 0.089648
[09:04:32.856] TRAIN: iteration 8493 : loss : 0.206163, loss_ce: 0.015026, loss_dice: 0.397300
[09:04:33.064] TRAIN: iteration 8494 : loss : 0.210972, loss_ce: 0.008113, loss_dice: 0.413830
[09:04:34.254] TRAIN: iteration 8495 : loss : 0.253756, loss_ce: 0.006989, loss_dice: 0.500524
[09:04:34.463] TRAIN: iteration 8496 : loss : 0.075306, loss_ce: 0.004862, loss_dice: 0.145751
[09:04:34.675] TRAIN: iteration 8497 : loss : 0.137609, loss_ce: 0.005399, loss_dice: 0.269818
[09:04:38.015] TRAIN: iteration 8498 : loss : 0.071563, loss_ce: 0.006250, loss_dice: 0.136876
[09:04:38.222] TRAIN: iteration 8499 : loss : 0.253655, loss_ce: 0.006793, loss_dice: 0.500516
[09:04:38.429] TRAIN: iteration 8500 : loss : 0.251049, loss_ce: 0.006129, loss_dice: 0.495969
[09:04:38.659] TRAIN: iteration 8501 : loss : 0.096080, loss_ce: 0.009971, loss_dice: 0.182188
[09:04:38.866] TRAIN: iteration 8502 : loss : 0.189712, loss_ce: 0.012501, loss_dice: 0.366922
[09:04:42.273] TRAIN: iteration 8503 : loss : 0.129535, loss_ce: 0.005783, loss_dice: 0.253287
[09:04:42.486] TRAIN: iteration 8504 : loss : 0.067285, loss_ce: 0.003321, loss_dice: 0.131248
[09:04:42.693] TRAIN: iteration 8505 : loss : 0.226612, loss_ce: 0.006134, loss_dice: 0.447091
[09:04:44.120] TRAIN: iteration 8506 : loss : 0.122658, loss_ce: 0.004760, loss_dice: 0.240557
[09:04:44.327] TRAIN: iteration 8507 : loss : 0.187664, loss_ce: 0.007453, loss_dice: 0.367875
[09:04:44.540] TRAIN: iteration 8508 : loss : 0.103144, loss_ce: 0.006865, loss_dice: 0.199423
[09:04:44.754] TRAIN: iteration 8509 : loss : 0.173474, loss_ce: 0.012179, loss_dice: 0.334770
[09:04:44.963] TRAIN: iteration 8510 : loss : 0.141724, loss_ce: 0.023247, loss_dice: 0.260200
[09:04:48.421] TRAIN: iteration 8511 : loss : 0.209431, loss_ce: 0.005541, loss_dice: 0.413321
[09:04:48.633] TRAIN: iteration 8512 : loss : 0.117199, loss_ce: 0.005949, loss_dice: 0.228450
[09:04:48.839] TRAIN: iteration 8513 : loss : 0.252530, loss_ce: 0.004712, loss_dice: 0.500349
[09:04:51.254] TRAIN: iteration 8514 : loss : 0.171591, loss_ce: 0.005951, loss_dice: 0.337231
[09:04:51.465] TRAIN: iteration 8515 : loss : 0.247212, loss_ce: 0.005891, loss_dice: 0.488534
[09:04:51.673] TRAIN: iteration 8516 : loss : 0.133236, loss_ce: 0.008642, loss_dice: 0.257830
[09:04:52.077] TRAIN: iteration 8517 : loss : 0.219408, loss_ce: 0.004832, loss_dice: 0.433984
[09:04:52.284] TRAIN: iteration 8518 : loss : 0.131668, loss_ce: 0.005276, loss_dice: 0.258061
[09:04:56.383] TRAIN: iteration 8519 : loss : 0.076577, loss_ce: 0.005321, loss_dice: 0.147833
[09:04:56.590] TRAIN: iteration 8520 : loss : 0.183156, loss_ce: 0.022676, loss_dice: 0.343636
[09:04:56.828] TRAIN: iteration 8521 : loss : 0.163276, loss_ce: 0.008134, loss_dice: 0.318419
[09:04:57.065] TRAIN: iteration 8522 : loss : 0.250142, loss_ce: 0.004058, loss_dice: 0.496226
[09:04:57.275] TRAIN: iteration 8523 : loss : 0.204447, loss_ce: 0.006510, loss_dice: 0.402383
[09:04:57.482] TRAIN: iteration 8524 : loss : 0.158833, loss_ce: 0.012750, loss_dice: 0.304916
[09:04:59.107] TRAIN: iteration 8525 : loss : 0.096729, loss_ce: 0.004859, loss_dice: 0.188600
[09:04:59.964] TRAIN: iteration 8526 : loss : 0.090567, loss_ce: 0.006321, loss_dice: 0.174812
[09:05:03.195] TRAIN: iteration 8527 : loss : 0.169634, loss_ce: 0.005734, loss_dice: 0.333534
[09:05:03.402] TRAIN: iteration 8528 : loss : 0.196356, loss_ce: 0.008234, loss_dice: 0.384479
[09:05:04.366] TRAIN: iteration 8529 : loss : 0.111259, loss_ce: 0.005271, loss_dice: 0.217247
[09:05:04.958] TRAIN: iteration 8530 : loss : 0.190639, loss_ce: 0.004514, loss_dice: 0.376765
[09:05:05.165] TRAIN: iteration 8531 : loss : 0.190412, loss_ce: 0.007875, loss_dice: 0.372950
[09:05:05.372] TRAIN: iteration 8532 : loss : 0.212547, loss_ce: 0.004679, loss_dice: 0.420415
[09:05:09.641] TRAIN: iteration 8533 : loss : 0.061627, loss_ce: 0.003412, loss_dice: 0.119842
[09:05:09.849] TRAIN: iteration 8534 : loss : 0.252005, loss_ce: 0.004276, loss_dice: 0.499735
[09:05:12.003] TRAIN: iteration 8535 : loss : 0.195123, loss_ce: 0.011395, loss_dice: 0.378851
[09:05:12.212] TRAIN: iteration 8536 : loss : 0.087179, loss_ce: 0.003940, loss_dice: 0.170419
[09:05:12.969] TRAIN: iteration 8537 : loss : 0.123435, loss_ce: 0.007751, loss_dice: 0.239119
[09:05:13.176] TRAIN: iteration 8538 : loss : 0.134525, loss_ce: 0.005655, loss_dice: 0.263395
[09:05:13.384] TRAIN: iteration 8539 : loss : 0.172949, loss_ce: 0.004967, loss_dice: 0.340930
[09:05:13.592] TRAIN: iteration 8540 : loss : 0.077520, loss_ce: 0.004467, loss_dice: 0.150572
[09:05:18.528] TRAIN: iteration 8541 : loss : 0.245299, loss_ce: 0.003814, loss_dice: 0.486784
[09:05:18.736] TRAIN: iteration 8542 : loss : 0.226693, loss_ce: 0.004287, loss_dice: 0.449098
[09:05:20.597] TRAIN: iteration 8543 : loss : 0.143898, loss_ce: 0.014222, loss_dice: 0.273574
[09:05:20.805] TRAIN: iteration 8544 : loss : 0.134711, loss_ce: 0.007368, loss_dice: 0.262054
[09:05:21.021] TRAIN: iteration 8545 : loss : 0.251877, loss_ce: 0.003546, loss_dice: 0.500207
[09:05:21.235] TRAIN: iteration 8546 : loss : 0.081017, loss_ce: 0.006963, loss_dice: 0.155071
[09:05:21.449] TRAIN: iteration 8547 : loss : 0.166005, loss_ce: 0.003785, loss_dice: 0.328224
[09:05:21.659] TRAIN: iteration 8548 : loss : 0.152582, loss_ce: 0.013220, loss_dice: 0.291944
[09:05:26.042] TRAIN: iteration 8549 : loss : 0.106279, loss_ce: 0.003999, loss_dice: 0.208560
[09:05:26.257] TRAIN: iteration 8550 : loss : 0.252184, loss_ce: 0.004177, loss_dice: 0.500192
[09:05:29.789] TRAIN: iteration 8551 : loss : 0.252153, loss_ce: 0.004075, loss_dice: 0.500232
[09:05:29.996] TRAIN: iteration 8552 : loss : 0.102558, loss_ce: 0.005254, loss_dice: 0.199861
[09:05:30.203] TRAIN: iteration 8553 : loss : 0.091895, loss_ce: 0.005367, loss_dice: 0.178424
[09:05:30.410] TRAIN: iteration 8554 : loss : 0.141739, loss_ce: 0.007929, loss_dice: 0.275548
[09:05:30.618] TRAIN: iteration 8555 : loss : 0.134906, loss_ce: 0.005481, loss_dice: 0.264332
[09:05:30.826] TRAIN: iteration 8556 : loss : 0.252873, loss_ce: 0.005409, loss_dice: 0.500336
[09:05:33.746] TRAIN: iteration 8557 : loss : 0.129433, loss_ce: 0.004375, loss_dice: 0.254492
[09:05:33.954] TRAIN: iteration 8558 : loss : 0.118975, loss_ce: 0.015642, loss_dice: 0.222309
[09:05:40.664] TRAIN: iteration 8559 : loss : 0.091782, loss_ce: 0.006042, loss_dice: 0.177523
[09:05:40.872] TRAIN: iteration 8560 : loss : 0.252282, loss_ce: 0.004305, loss_dice: 0.500259
[09:05:41.108] TRAIN: iteration 8561 : loss : 0.197227, loss_ce: 0.005497, loss_dice: 0.388957
[09:05:41.318] TRAIN: iteration 8562 : loss : 0.139450, loss_ce: 0.005512, loss_dice: 0.273389
[09:05:41.525] TRAIN: iteration 8563 : loss : 0.249404, loss_ce: 0.008461, loss_dice: 0.490347
[09:05:41.732] TRAIN: iteration 8564 : loss : 0.095749, loss_ce: 0.006053, loss_dice: 0.185445
[09:05:43.450] TRAIN: iteration 8565 : loss : 0.167901, loss_ce: 0.013825, loss_dice: 0.321976
[09:05:43.657] TRAIN: iteration 8566 : loss : 0.250131, loss_ce: 0.004261, loss_dice: 0.496002
[09:05:48.020] TRAIN: iteration 8567 : loss : 0.139945, loss_ce: 0.014151, loss_dice: 0.265738
[09:05:48.228] TRAIN: iteration 8568 : loss : 0.230353, loss_ce: 0.004575, loss_dice: 0.456132
[09:05:48.435] TRAIN: iteration 8569 : loss : 0.252900, loss_ce: 0.005406, loss_dice: 0.500394
[09:05:48.642] TRAIN: iteration 8570 : loss : 0.207132, loss_ce: 0.004967, loss_dice: 0.409298
[09:05:48.851] TRAIN: iteration 8571 : loss : 0.100338, loss_ce: 0.004950, loss_dice: 0.195726
[09:05:49.061] TRAIN: iteration 8572 : loss : 0.252407, loss_ce: 0.004852, loss_dice: 0.499961
[09:05:50.949] TRAIN: iteration 8573 : loss : 0.187964, loss_ce: 0.014643, loss_dice: 0.361285
[09:05:51.156] TRAIN: iteration 8574 : loss : 0.102004, loss_ce: 0.008177, loss_dice: 0.195830
[09:05:55.780] TRAIN: iteration 8575 : loss : 0.091291, loss_ce: 0.005214, loss_dice: 0.177368
[09:05:55.990] TRAIN: iteration 8576 : loss : 0.072758, loss_ce: 0.004916, loss_dice: 0.140600
[09:05:56.200] TRAIN: iteration 8577 : loss : 0.140403, loss_ce: 0.013325, loss_dice: 0.267482
[09:05:56.407] TRAIN: iteration 8578 : loss : 0.252275, loss_ce: 0.004294, loss_dice: 0.500256
[09:05:56.614] TRAIN: iteration 8579 : loss : 0.206243, loss_ce: 0.006489, loss_dice: 0.405996
[09:05:56.821] TRAIN: iteration 8580 : loss : 0.086844, loss_ce: 0.003258, loss_dice: 0.170431
[09:05:58.450] TRAIN: iteration 8581 : loss : 0.104535, loss_ce: 0.003940, loss_dice: 0.205131
[09:05:58.657] TRAIN: iteration 8582 : loss : 0.246735, loss_ce: 0.004620, loss_dice: 0.488849
[09:06:04.424] TRAIN: iteration 8583 : loss : 0.090929, loss_ce: 0.006096, loss_dice: 0.175762
[09:06:04.634] TRAIN: iteration 8584 : loss : 0.170554, loss_ce: 0.013854, loss_dice: 0.327254
[09:06:04.841] TRAIN: iteration 8585 : loss : 0.039005, loss_ce: 0.002372, loss_dice: 0.075638
[09:06:05.047] TRAIN: iteration 8586 : loss : 0.182888, loss_ce: 0.004895, loss_dice: 0.360881
[09:06:05.256] TRAIN: iteration 8587 : loss : 0.098995, loss_ce: 0.003152, loss_dice: 0.194839
[09:06:05.464] TRAIN: iteration 8588 : loss : 0.105194, loss_ce: 0.010608, loss_dice: 0.199780
[09:06:07.526] TRAIN: iteration 8589 : loss : 0.070859, loss_ce: 0.006691, loss_dice: 0.135028
[09:06:07.733] TRAIN: iteration 8590 : loss : 0.121113, loss_ce: 0.007434, loss_dice: 0.234792
[09:06:11.809] TRAIN: iteration 8591 : loss : 0.035182, loss_ce: 0.002380, loss_dice: 0.067984
[09:06:12.017] TRAIN: iteration 8592 : loss : 0.132710, loss_ce: 0.009515, loss_dice: 0.255904
[09:06:12.224] TRAIN: iteration 8593 : loss : 0.225950, loss_ce: 0.012833, loss_dice: 0.439066
[09:06:12.431] TRAIN: iteration 8594 : loss : 0.211618, loss_ce: 0.014173, loss_dice: 0.409063
[09:06:12.641] TRAIN: iteration 8595 : loss : 0.101612, loss_ce: 0.005664, loss_dice: 0.197560
[09:06:13.251] TRAIN: iteration 8596 : loss : 0.250472, loss_ce: 0.004510, loss_dice: 0.496434
[09:06:15.206] TRAIN: iteration 8597 : loss : 0.244683, loss_ce: 0.004416, loss_dice: 0.484951
[09:06:15.414] TRAIN: iteration 8598 : loss : 0.148684, loss_ce: 0.019151, loss_dice: 0.278218
[09:06:22.626] TRAIN: iteration 8599 : loss : 0.182044, loss_ce: 0.004439, loss_dice: 0.359650
[09:06:22.833] TRAIN: iteration 8600 : loss : 0.111285, loss_ce: 0.008363, loss_dice: 0.214207
[09:06:23.069] TRAIN: iteration 8601 : loss : 0.168333, loss_ce: 0.005719, loss_dice: 0.330946
[09:06:23.276] TRAIN: iteration 8602 : loss : 0.171336, loss_ce: 0.006343, loss_dice: 0.336328
[09:06:23.486] TRAIN: iteration 8603 : loss : 0.162285, loss_ce: 0.005495, loss_dice: 0.319075
[09:06:23.693] TRAIN: iteration 8604 : loss : 0.122275, loss_ce: 0.003871, loss_dice: 0.240679
[09:06:23.901] TRAIN: iteration 8605 : loss : 0.248403, loss_ce: 0.005260, loss_dice: 0.491545
[09:06:24.109] TRAIN: iteration 8606 : loss : 0.240522, loss_ce: 0.003918, loss_dice: 0.477125
[09:06:31.623] TRAIN: iteration 8607 : loss : 0.087767, loss_ce: 0.004711, loss_dice: 0.170824
[09:06:31.835] TRAIN: iteration 8608 : loss : 0.136199, loss_ce: 0.003591, loss_dice: 0.268808
[09:06:32.042] TRAIN: iteration 8609 : loss : 0.251624, loss_ce: 0.003136, loss_dice: 0.500111
[09:06:32.249] TRAIN: iteration 8610 : loss : 0.124521, loss_ce: 0.004128, loss_dice: 0.244913
[09:06:32.457] TRAIN: iteration 8611 : loss : 0.055920, loss_ce: 0.003233, loss_dice: 0.108607
[09:06:32.663] TRAIN: iteration 8612 : loss : 0.176749, loss_ce: 0.004194, loss_dice: 0.349303
[09:06:32.870] TRAIN: iteration 8613 : loss : 0.132120, loss_ce: 0.005402, loss_dice: 0.258838
[09:06:33.083] TRAIN: iteration 8614 : loss : 0.196975, loss_ce: 0.003262, loss_dice: 0.390687
[09:06:40.755] TRAIN: iteration 8615 : loss : 0.248457, loss_ce: 0.001731, loss_dice: 0.495182
[09:06:40.962] TRAIN: iteration 8616 : loss : 0.204879, loss_ce: 0.005039, loss_dice: 0.404719
[09:06:41.169] TRAIN: iteration 8617 : loss : 0.114615, loss_ce: 0.005039, loss_dice: 0.224192
[09:06:41.376] TRAIN: iteration 8618 : loss : 0.064213, loss_ce: 0.002881, loss_dice: 0.125544
[09:06:41.584] TRAIN: iteration 8619 : loss : 0.188978, loss_ce: 0.002221, loss_dice: 0.375735
[09:06:41.790] TRAIN: iteration 8620 : loss : 0.088270, loss_ce: 0.005243, loss_dice: 0.171298
[09:06:42.028] TRAIN: iteration 8621 : loss : 0.203286, loss_ce: 0.032540, loss_dice: 0.374032
[09:06:42.234] TRAIN: iteration 8622 : loss : 0.128515, loss_ce: 0.003573, loss_dice: 0.253458
[09:06:47.927] TRAIN: iteration 8623 : loss : 0.050335, loss_ce: 0.002411, loss_dice: 0.098259
[09:06:48.137] TRAIN: iteration 8624 : loss : 0.119118, loss_ce: 0.011843, loss_dice: 0.226393
[09:06:48.344] TRAIN: iteration 8625 : loss : 0.251438, loss_ce: 0.002751, loss_dice: 0.500125
[09:06:48.556] TRAIN: iteration 8626 : loss : 0.063856, loss_ce: 0.002839, loss_dice: 0.124872
[09:06:48.764] TRAIN: iteration 8627 : loss : 0.079947, loss_ce: 0.004275, loss_dice: 0.155618
[09:06:48.970] TRAIN: iteration 8628 : loss : 0.252716, loss_ce: 0.005073, loss_dice: 0.500359
[09:06:49.178] TRAIN: iteration 8629 : loss : 0.182072, loss_ce: 0.007924, loss_dice: 0.356219
[09:06:49.386] TRAIN: iteration 8630 : loss : 0.076261, loss_ce: 0.008466, loss_dice: 0.144056
[09:06:56.636] TRAIN: iteration 8631 : loss : 0.081296, loss_ce: 0.006781, loss_dice: 0.155811
[09:06:56.842] TRAIN: iteration 8632 : loss : 0.089126, loss_ce: 0.004874, loss_dice: 0.173379
[09:06:57.053] TRAIN: iteration 8633 : loss : 0.252713, loss_ce: 0.005052, loss_dice: 0.500373
[09:06:57.260] TRAIN: iteration 8634 : loss : 0.251633, loss_ce: 0.008434, loss_dice: 0.494833
[09:06:57.468] TRAIN: iteration 8635 : loss : 0.117302, loss_ce: 0.011728, loss_dice: 0.222876
[09:06:57.674] TRAIN: iteration 8636 : loss : 0.252710, loss_ce: 0.005108, loss_dice: 0.500312
[09:06:57.881] TRAIN: iteration 8637 : loss : 0.255143, loss_ce: 0.010190, loss_dice: 0.500096
[09:06:58.092] TRAIN: iteration 8638 : loss : 0.105610, loss_ce: 0.016923, loss_dice: 0.194298
[09:07:06.340] TRAIN: iteration 8639 : loss : 0.056396, loss_ce: 0.003733, loss_dice: 0.109059
[09:07:06.546] TRAIN: iteration 8640 : loss : 0.161295, loss_ce: 0.011348, loss_dice: 0.311242
[09:07:06.782] TRAIN: iteration 8641 : loss : 0.252695, loss_ce: 0.005044, loss_dice: 0.500347
[09:07:06.989] TRAIN: iteration 8642 : loss : 0.253448, loss_ce: 0.006383, loss_dice: 0.500513
[09:07:07.196] TRAIN: iteration 8643 : loss : 0.198164, loss_ce: 0.003908, loss_dice: 0.392420
[09:07:08.477] TRAIN: iteration 8644 : loss : 0.253139, loss_ce: 0.005807, loss_dice: 0.500471
[09:07:08.685] TRAIN: iteration 8645 : loss : 0.249247, loss_ce: 0.005204, loss_dice: 0.493290
[09:07:08.892] TRAIN: iteration 8646 : loss : 0.142745, loss_ce: 0.004287, loss_dice: 0.281203
[09:07:14.807] TRAIN: iteration 8647 : loss : 0.098888, loss_ce: 0.003500, loss_dice: 0.194276
[09:07:15.014] TRAIN: iteration 8648 : loss : 0.104339, loss_ce: 0.008935, loss_dice: 0.199743
[09:07:15.221] TRAIN: iteration 8649 : loss : 0.252388, loss_ce: 0.004444, loss_dice: 0.500332
[09:07:15.430] TRAIN: iteration 8650 : loss : 0.078388, loss_ce: 0.003157, loss_dice: 0.153619
[09:07:15.636] TRAIN: iteration 8651 : loss : 0.132830, loss_ce: 0.007593, loss_dice: 0.258067
[09:07:18.701] TRAIN: iteration 8652 : loss : 0.166627, loss_ce: 0.003176, loss_dice: 0.330078
[09:07:18.910] TRAIN: iteration 8653 : loss : 0.237706, loss_ce: 0.008207, loss_dice: 0.467206
[09:07:19.767] TRAIN: iteration 8654 : loss : 0.059613, loss_ce: 0.003406, loss_dice: 0.115821
[09:07:23.797] TRAIN: iteration 8655 : loss : 0.144308, loss_ce: 0.009648, loss_dice: 0.278968
[09:07:24.005] TRAIN: iteration 8656 : loss : 0.186406, loss_ce: 0.004264, loss_dice: 0.368547
[09:07:24.219] TRAIN: iteration 8657 : loss : 0.143971, loss_ce: 0.004675, loss_dice: 0.283267
[09:07:24.426] TRAIN: iteration 8658 : loss : 0.110689, loss_ce: 0.002971, loss_dice: 0.218406
[09:07:24.635] TRAIN: iteration 8659 : loss : 0.252589, loss_ce: 0.004827, loss_dice: 0.500351
[09:07:28.151] TRAIN: iteration 8660 : loss : 0.252688, loss_ce: 0.004998, loss_dice: 0.500378
[09:07:28.388] TRAIN: iteration 8661 : loss : 0.117881, loss_ce: 0.009006, loss_dice: 0.226755
[09:07:30.019] TRAIN: iteration 8662 : loss : 0.249116, loss_ce: 0.006000, loss_dice: 0.492233
[09:07:32.016] TRAIN: iteration 8663 : loss : 0.087722, loss_ce: 0.009139, loss_dice: 0.166306
[09:07:32.224] TRAIN: iteration 8664 : loss : 0.236994, loss_ce: 0.005237, loss_dice: 0.468752
[09:07:32.431] TRAIN: iteration 8665 : loss : 0.118043, loss_ce: 0.004837, loss_dice: 0.231248
[09:07:32.638] TRAIN: iteration 8666 : loss : 0.209429, loss_ce: 0.010468, loss_dice: 0.408390
[09:07:32.844] TRAIN: iteration 8667 : loss : 0.134976, loss_ce: 0.003032, loss_dice: 0.266920
[09:07:37.136] TRAIN: iteration 8668 : loss : 0.181331, loss_ce: 0.004317, loss_dice: 0.358344
[09:07:37.343] TRAIN: iteration 8669 : loss : 0.169199, loss_ce: 0.004301, loss_dice: 0.334097
[09:07:40.439] TRAIN: iteration 8670 : loss : 0.253452, loss_ce: 0.007809, loss_dice: 0.499095
[09:07:41.538] TRAIN: iteration 8671 : loss : 0.232256, loss_ce: 0.010261, loss_dice: 0.454252
[09:07:41.745] TRAIN: iteration 8672 : loss : 0.250060, loss_ce: 0.004890, loss_dice: 0.495231
[09:07:41.952] TRAIN: iteration 8673 : loss : 0.109063, loss_ce: 0.004249, loss_dice: 0.213877
[09:07:42.158] TRAIN: iteration 8674 : loss : 0.232824, loss_ce: 0.014410, loss_dice: 0.451238
[09:07:42.253] TRAIN: iteration 8675 : loss : 0.192573, loss_ce: 0.009943, loss_dice: 0.375203
[09:13:13.403] VALIDATION: iteration 4 : loss : 0.153184, loss_ce: 0.005976, loss_dice: 0.300392
[09:13:14.763] TRAIN: iteration 8676 : loss : 0.180141, loss_ce: 0.007276, loss_dice: 0.353005
[09:13:14.976] TRAIN: iteration 8677 : loss : 0.251929, loss_ce: 0.003627, loss_dice: 0.500232
[09:13:15.186] TRAIN: iteration 8678 : loss : 0.075375, loss_ce: 0.004006, loss_dice: 0.146744
[09:13:16.448] TRAIN: iteration 8679 : loss : 0.251411, loss_ce: 0.002714, loss_dice: 0.500108
[09:13:16.662] TRAIN: iteration 8680 : loss : 0.191642, loss_ce: 0.003697, loss_dice: 0.379587
[09:13:16.900] TRAIN: iteration 8681 : loss : 0.098708, loss_ce: 0.004455, loss_dice: 0.192961
[09:13:17.115] TRAIN: iteration 8682 : loss : 0.095165, loss_ce: 0.006271, loss_dice: 0.184059
[09:13:17.323] TRAIN: iteration 8683 : loss : 0.242329, loss_ce: 0.003480, loss_dice: 0.481178
[09:13:17.697] TRAIN: iteration 8684 : loss : 0.147797, loss_ce: 0.007249, loss_dice: 0.288345
[09:13:17.914] TRAIN: iteration 8685 : loss : 0.230430, loss_ce: 0.004099, loss_dice: 0.456761
[09:13:18.123] TRAIN: iteration 8686 : loss : 0.065259, loss_ce: 0.003956, loss_dice: 0.126562
[09:13:18.333] TRAIN: iteration 8687 : loss : 0.195678, loss_ce: 0.006103, loss_dice: 0.385254
[09:13:18.546] TRAIN: iteration 8688 : loss : 0.199217, loss_ce: 0.004157, loss_dice: 0.394277
[09:13:18.754] TRAIN: iteration 8689 : loss : 0.252790, loss_ce: 0.005220, loss_dice: 0.500360
[09:13:18.970] TRAIN: iteration 8690 : loss : 0.140131, loss_ce: 0.008443, loss_dice: 0.271818
[09:13:19.179] TRAIN: iteration 8691 : loss : 0.116063, loss_ce: 0.006756, loss_dice: 0.225370
[09:13:19.387] TRAIN: iteration 8692 : loss : 0.047407, loss_ce: 0.003561, loss_dice: 0.091253
[09:13:19.600] TRAIN: iteration 8693 : loss : 0.052994, loss_ce: 0.002940, loss_dice: 0.103047
[09:13:19.807] TRAIN: iteration 8694 : loss : 0.120751, loss_ce: 0.004266, loss_dice: 0.237236
[09:13:20.017] TRAIN: iteration 8695 : loss : 0.117995, loss_ce: 0.007410, loss_dice: 0.228580
[09:13:20.225] TRAIN: iteration 8696 : loss : 0.152795, loss_ce: 0.018451, loss_dice: 0.287139
[09:13:20.432] TRAIN: iteration 8697 : loss : 0.083136, loss_ce: 0.005344, loss_dice: 0.160928
[09:13:20.640] TRAIN: iteration 8698 : loss : 0.157325, loss_ce: 0.004739, loss_dice: 0.309911
[09:13:20.847] TRAIN: iteration 8699 : loss : 0.145075, loss_ce: 0.004972, loss_dice: 0.285178
[09:13:21.056] TRAIN: iteration 8700 : loss : 0.223057, loss_ce: 0.003688, loss_dice: 0.442426
[09:13:21.291] TRAIN: iteration 8701 : loss : 0.113424, loss_ce: 0.003190, loss_dice: 0.223659
[09:13:21.500] TRAIN: iteration 8702 : loss : 0.183883, loss_ce: 0.008153, loss_dice: 0.359613
[09:13:21.716] TRAIN: iteration 8703 : loss : 0.252120, loss_ce: 0.003959, loss_dice: 0.500281
[09:13:21.923] TRAIN: iteration 8704 : loss : 0.095890, loss_ce: 0.002651, loss_dice: 0.189130
[09:13:22.131] TRAIN: iteration 8705 : loss : 0.081182, loss_ce: 0.002093, loss_dice: 0.160271
[09:13:22.339] TRAIN: iteration 8706 : loss : 0.257844, loss_ce: 0.035419, loss_dice: 0.480269
[09:13:22.548] TRAIN: iteration 8707 : loss : 0.225335, loss_ce: 0.002143, loss_dice: 0.448527
[09:13:22.756] TRAIN: iteration 8708 : loss : 0.149750, loss_ce: 0.003294, loss_dice: 0.296206
[09:13:22.964] TRAIN: iteration 8709 : loss : 0.178004, loss_ce: 0.027665, loss_dice: 0.328344
[09:13:23.176] TRAIN: iteration 8710 : loss : 0.079266, loss_ce: 0.002461, loss_dice: 0.156071
[09:13:23.388] TRAIN: iteration 8711 : loss : 0.113692, loss_ce: 0.005987, loss_dice: 0.221398
[09:13:23.594] TRAIN: iteration 8712 : loss : 0.167074, loss_ce: 0.026546, loss_dice: 0.307603
[09:13:23.802] TRAIN: iteration 8713 : loss : 0.250780, loss_ce: 0.001521, loss_dice: 0.500040
[09:13:24.010] TRAIN: iteration 8714 : loss : 0.248704, loss_ce: 0.002300, loss_dice: 0.495109
[09:13:24.225] TRAIN: iteration 8715 : loss : 0.251029, loss_ce: 0.001986, loss_dice: 0.500072
[09:13:24.434] TRAIN: iteration 8716 : loss : 0.184855, loss_ce: 0.018367, loss_dice: 0.351343
[09:13:24.642] TRAIN: iteration 8717 : loss : 0.041518, loss_ce: 0.003013, loss_dice: 0.080023
[09:13:24.850] TRAIN: iteration 8718 : loss : 0.146508, loss_ce: 0.008515, loss_dice: 0.284501
[09:13:25.063] TRAIN: iteration 8719 : loss : 0.164764, loss_ce: 0.006005, loss_dice: 0.323522
[09:13:25.270] TRAIN: iteration 8720 : loss : 0.251597, loss_ce: 0.003045, loss_dice: 0.500149
[09:13:25.507] TRAIN: iteration 8721 : loss : 0.171731, loss_ce: 0.006288, loss_dice: 0.337174
[09:13:25.720] TRAIN: iteration 8722 : loss : 0.251728, loss_ce: 0.003291, loss_dice: 0.500164
[09:13:25.926] TRAIN: iteration 8723 : loss : 0.252904, loss_ce: 0.005391, loss_dice: 0.500417
[09:13:26.134] TRAIN: iteration 8724 : loss : 0.112032, loss_ce: 0.004184, loss_dice: 0.219879
[09:13:26.343] TRAIN: iteration 8725 : loss : 0.148029, loss_ce: 0.006064, loss_dice: 0.289995
[09:13:26.552] TRAIN: iteration 8726 : loss : 0.252871, loss_ce: 0.005339, loss_dice: 0.500403
[09:13:26.762] TRAIN: iteration 8727 : loss : 0.252298, loss_ce: 0.005094, loss_dice: 0.499501
[09:13:26.971] TRAIN: iteration 8728 : loss : 0.101404, loss_ce: 0.005620, loss_dice: 0.197189
[09:13:27.187] TRAIN: iteration 8729 : loss : 0.067258, loss_ce: 0.010274, loss_dice: 0.124242
[09:13:27.397] TRAIN: iteration 8730 : loss : 0.053208, loss_ce: 0.004043, loss_dice: 0.102374
[09:13:27.604] TRAIN: iteration 8731 : loss : 0.119246, loss_ce: 0.003453, loss_dice: 0.235039
[09:13:27.813] TRAIN: iteration 8732 : loss : 0.047699, loss_ce: 0.005681, loss_dice: 0.089718
[09:13:28.026] TRAIN: iteration 8733 : loss : 0.160541, loss_ce: 0.010036, loss_dice: 0.311046
[09:13:28.307] TRAIN: iteration 8734 : loss : 0.087156, loss_ce: 0.003389, loss_dice: 0.170924
[09:13:28.517] TRAIN: iteration 8735 : loss : 0.112143, loss_ce: 0.004435, loss_dice: 0.219850
[09:13:28.731] TRAIN: iteration 8736 : loss : 0.153160, loss_ce: 0.010497, loss_dice: 0.295824
[09:13:28.938] TRAIN: iteration 8737 : loss : 0.087794, loss_ce: 0.005096, loss_dice: 0.170491
[09:13:29.147] TRAIN: iteration 8738 : loss : 0.150544, loss_ce: 0.010914, loss_dice: 0.290175
[09:13:29.356] TRAIN: iteration 8739 : loss : 0.244066, loss_ce: 0.005184, loss_dice: 0.482948
[09:13:29.568] TRAIN: iteration 8740 : loss : 0.252468, loss_ce: 0.004620, loss_dice: 0.500317
[09:13:29.805] TRAIN: iteration 8741 : loss : 0.249900, loss_ce: 0.005471, loss_dice: 0.494329
[09:13:30.014] TRAIN: iteration 8742 : loss : 0.240355, loss_ce: 0.004270, loss_dice: 0.476440
[09:13:30.224] TRAIN: iteration 8743 : loss : 0.128470, loss_ce: 0.002789, loss_dice: 0.254150
[09:13:30.434] TRAIN: iteration 8744 : loss : 0.072774, loss_ce: 0.002411, loss_dice: 0.143137
[09:13:30.643] TRAIN: iteration 8745 : loss : 0.245248, loss_ce: 0.004273, loss_dice: 0.486224
[09:13:30.854] TRAIN: iteration 8746 : loss : 0.144757, loss_ce: 0.008644, loss_dice: 0.280871
[09:13:31.063] TRAIN: iteration 8747 : loss : 0.124246, loss_ce: 0.003891, loss_dice: 0.244600
[09:13:31.275] TRAIN: iteration 8748 : loss : 0.111433, loss_ce: 0.007605, loss_dice: 0.215261
[09:13:31.482] TRAIN: iteration 8749 : loss : 0.089971, loss_ce: 0.014206, loss_dice: 0.165737
[09:13:31.693] TRAIN: iteration 8750 : loss : 0.171649, loss_ce: 0.006029, loss_dice: 0.337269
[09:13:31.899] TRAIN: iteration 8751 : loss : 0.251837, loss_ce: 0.003449, loss_dice: 0.500225
[09:13:32.107] TRAIN: iteration 8752 : loss : 0.251882, loss_ce: 0.003545, loss_dice: 0.500218
[09:13:32.316] TRAIN: iteration 8753 : loss : 0.101039, loss_ce: 0.004592, loss_dice: 0.197486
[09:13:32.530] TRAIN: iteration 8754 : loss : 0.208689, loss_ce: 0.014512, loss_dice: 0.402866
[09:13:32.737] TRAIN: iteration 8755 : loss : 0.065685, loss_ce: 0.006737, loss_dice: 0.124633
[09:13:33.018] TRAIN: iteration 8756 : loss : 0.146280, loss_ce: 0.006418, loss_dice: 0.286141
[09:13:33.226] TRAIN: iteration 8757 : loss : 0.098214, loss_ce: 0.005795, loss_dice: 0.190633
[09:13:33.434] TRAIN: iteration 8758 : loss : 0.242436, loss_ce: 0.005329, loss_dice: 0.479542
[09:13:33.647] TRAIN: iteration 8759 : loss : 0.227403, loss_ce: 0.006714, loss_dice: 0.448093
[09:13:33.854] TRAIN: iteration 8760 : loss : 0.131248, loss_ce: 0.005501, loss_dice: 0.256994
[09:13:34.095] TRAIN: iteration 8761 : loss : 0.253486, loss_ce: 0.006528, loss_dice: 0.500444
[09:13:34.302] TRAIN: iteration 8762 : loss : 0.083477, loss_ce: 0.008122, loss_dice: 0.158832
[09:13:34.509] TRAIN: iteration 8763 : loss : 0.253577, loss_ce: 0.006695, loss_dice: 0.500459
[09:13:34.715] TRAIN: iteration 8764 : loss : 0.078807, loss_ce: 0.010582, loss_dice: 0.147031
[09:13:34.925] TRAIN: iteration 8765 : loss : 0.053632, loss_ce: 0.004396, loss_dice: 0.102869
[09:13:35.458] TRAIN: iteration 8766 : loss : 0.090278, loss_ce: 0.007497, loss_dice: 0.173060
[09:13:35.664] TRAIN: iteration 8767 : loss : 0.099640, loss_ce: 0.007033, loss_dice: 0.192248
[09:13:35.872] TRAIN: iteration 8768 : loss : 0.219120, loss_ce: 0.009357, loss_dice: 0.428883
[09:13:36.081] TRAIN: iteration 8769 : loss : 0.195986, loss_ce: 0.006533, loss_dice: 0.385439
[09:13:36.288] TRAIN: iteration 8770 : loss : 0.104032, loss_ce: 0.007561, loss_dice: 0.200503
[09:13:36.496] TRAIN: iteration 8771 : loss : 0.127239, loss_ce: 0.004514, loss_dice: 0.249965
[09:13:36.706] TRAIN: iteration 8772 : loss : 0.075678, loss_ce: 0.003633, loss_dice: 0.147722
[09:13:36.913] TRAIN: iteration 8773 : loss : 0.147515, loss_ce: 0.005463, loss_dice: 0.289566
[09:13:37.120] TRAIN: iteration 8774 : loss : 0.158787, loss_ce: 0.006485, loss_dice: 0.311088
[09:13:37.328] TRAIN: iteration 8775 : loss : 0.039366, loss_ce: 0.004169, loss_dice: 0.074562
[09:13:37.536] TRAIN: iteration 8776 : loss : 0.087401, loss_ce: 0.009468, loss_dice: 0.165334
[09:13:37.750] TRAIN: iteration 8777 : loss : 0.081190, loss_ce: 0.002348, loss_dice: 0.160032
[09:13:37.959] TRAIN: iteration 8778 : loss : 0.162811, loss_ce: 0.030439, loss_dice: 0.295183
[09:13:38.173] TRAIN: iteration 8779 : loss : 0.217270, loss_ce: 0.004639, loss_dice: 0.429902
[09:13:38.381] TRAIN: iteration 8780 : loss : 0.166864, loss_ce: 0.003120, loss_dice: 0.330609
[09:13:38.625] TRAIN: iteration 8781 : loss : 0.091847, loss_ce: 0.005163, loss_dice: 0.178531
[09:13:38.833] TRAIN: iteration 8782 : loss : 0.096441, loss_ce: 0.004291, loss_dice: 0.188591
[09:13:39.042] TRAIN: iteration 8783 : loss : 0.249116, loss_ce: 0.002898, loss_dice: 0.495334
[09:13:39.252] TRAIN: iteration 8784 : loss : 0.097509, loss_ce: 0.002869, loss_dice: 0.192149
[09:13:39.459] TRAIN: iteration 8785 : loss : 0.251549, loss_ce: 0.004917, loss_dice: 0.498180
[09:13:39.666] TRAIN: iteration 8786 : loss : 0.084175, loss_ce: 0.002283, loss_dice: 0.166066
[09:13:39.875] TRAIN: iteration 8787 : loss : 0.094995, loss_ce: 0.004185, loss_dice: 0.185804
[09:13:40.134] TRAIN: iteration 8788 : loss : 0.252806, loss_ce: 0.008162, loss_dice: 0.497450
[09:13:40.342] TRAIN: iteration 8789 : loss : 0.143633, loss_ce: 0.010189, loss_dice: 0.277078
[09:13:40.550] TRAIN: iteration 8790 : loss : 0.250837, loss_ce: 0.001612, loss_dice: 0.500062
[09:13:40.759] TRAIN: iteration 8791 : loss : 0.086357, loss_ce: 0.005648, loss_dice: 0.167067
[09:13:40.967] TRAIN: iteration 8792 : loss : 0.067454, loss_ce: 0.003703, loss_dice: 0.131204
[09:13:41.174] TRAIN: iteration 8793 : loss : 0.062093, loss_ce: 0.004041, loss_dice: 0.120144
[09:13:41.381] TRAIN: iteration 8794 : loss : 0.098277, loss_ce: 0.002585, loss_dice: 0.193968
[09:13:41.588] TRAIN: iteration 8795 : loss : 0.244127, loss_ce: 0.004113, loss_dice: 0.484141
[09:13:41.796] TRAIN: iteration 8796 : loss : 0.227430, loss_ce: 0.005059, loss_dice: 0.449801
[09:13:42.002] TRAIN: iteration 8797 : loss : 0.082996, loss_ce: 0.007859, loss_dice: 0.158133
[09:13:42.211] TRAIN: iteration 8798 : loss : 0.104006, loss_ce: 0.005007, loss_dice: 0.203005
[09:13:42.423] TRAIN: iteration 8799 : loss : 0.203719, loss_ce: 0.007389, loss_dice: 0.400049
[09:13:42.631] TRAIN: iteration 8800 : loss : 0.139839, loss_ce: 0.027518, loss_dice: 0.252160
[09:13:42.867] TRAIN: iteration 8801 : loss : 0.198920, loss_ce: 0.004570, loss_dice: 0.393269
[09:13:43.075] TRAIN: iteration 8802 : loss : 0.087000, loss_ce: 0.004984, loss_dice: 0.169015
[09:13:43.284] TRAIN: iteration 8803 : loss : 0.158626, loss_ce: 0.007670, loss_dice: 0.309582
[09:13:43.492] TRAIN: iteration 8804 : loss : 0.136204, loss_ce: 0.009595, loss_dice: 0.262813
[09:13:43.701] TRAIN: iteration 8805 : loss : 0.222557, loss_ce: 0.006701, loss_dice: 0.438412
[09:13:43.908] TRAIN: iteration 8806 : loss : 0.254606, loss_ce: 0.008507, loss_dice: 0.500706
[09:13:44.116] TRAIN: iteration 8807 : loss : 0.085931, loss_ce: 0.002169, loss_dice: 0.169693
[09:13:44.324] TRAIN: iteration 8808 : loss : 0.252513, loss_ce: 0.004698, loss_dice: 0.500328
[09:13:44.532] TRAIN: iteration 8809 : loss : 0.252604, loss_ce: 0.004882, loss_dice: 0.500327
[09:13:44.740] TRAIN: iteration 8810 : loss : 0.134349, loss_ce: 0.006372, loss_dice: 0.262326
[09:13:44.954] TRAIN: iteration 8811 : loss : 0.066050, loss_ce: 0.003570, loss_dice: 0.128531
[09:13:45.171] TRAIN: iteration 8812 : loss : 0.101639, loss_ce: 0.006936, loss_dice: 0.196342
[09:13:45.381] TRAIN: iteration 8813 : loss : 0.252052, loss_ce: 0.003855, loss_dice: 0.500249
[09:13:45.589] TRAIN: iteration 8814 : loss : 0.156229, loss_ce: 0.012323, loss_dice: 0.300135
[09:13:45.798] TRAIN: iteration 8815 : loss : 0.126833, loss_ce: 0.005050, loss_dice: 0.248616
[09:13:46.005] TRAIN: iteration 8816 : loss : 0.131422, loss_ce: 0.004871, loss_dice: 0.257973
[09:13:46.217] TRAIN: iteration 8817 : loss : 0.253388, loss_ce: 0.006503, loss_dice: 0.500274
[09:13:46.430] TRAIN: iteration 8818 : loss : 0.053857, loss_ce: 0.003157, loss_dice: 0.104558
[09:13:46.661] TRAIN: iteration 8819 : loss : 0.248854, loss_ce: 0.003899, loss_dice: 0.493809
[09:13:46.869] TRAIN: iteration 8820 : loss : 0.072321, loss_ce: 0.003331, loss_dice: 0.141310
[09:13:47.111] TRAIN: iteration 8821 : loss : 0.195488, loss_ce: 0.003473, loss_dice: 0.387504
[09:13:47.323] TRAIN: iteration 8822 : loss : 0.252163, loss_ce: 0.004038, loss_dice: 0.500287
[09:13:47.533] TRAIN: iteration 8823 : loss : 0.161533, loss_ce: 0.005608, loss_dice: 0.317458
[09:13:47.743] TRAIN: iteration 8824 : loss : 0.239469, loss_ce: 0.014072, loss_dice: 0.464865
[09:13:47.954] TRAIN: iteration 8825 : loss : 0.063089, loss_ce: 0.002729, loss_dice: 0.123450
[09:13:48.162] TRAIN: iteration 8826 : loss : 0.190611, loss_ce: 0.003971, loss_dice: 0.377250
[09:13:48.369] TRAIN: iteration 8827 : loss : 0.250999, loss_ce: 0.001920, loss_dice: 0.500077
[09:13:48.576] TRAIN: iteration 8828 : loss : 0.252375, loss_ce: 0.004416, loss_dice: 0.500333
[09:13:49.111] TRAIN: iteration 8829 : loss : 0.101765, loss_ce: 0.002777, loss_dice: 0.200752
[09:13:49.324] TRAIN: iteration 8830 : loss : 0.109577, loss_ce: 0.003136, loss_dice: 0.216017
[09:13:49.535] TRAIN: iteration 8831 : loss : 0.109644, loss_ce: 0.002877, loss_dice: 0.216411
[09:13:49.744] TRAIN: iteration 8832 : loss : 0.215017, loss_ce: 0.010093, loss_dice: 0.419941
[09:13:49.959] TRAIN: iteration 8833 : loss : 0.153197, loss_ce: 0.016334, loss_dice: 0.290060
[09:13:50.168] TRAIN: iteration 8834 : loss : 0.171127, loss_ce: 0.004446, loss_dice: 0.337808
[09:13:50.379] TRAIN: iteration 8835 : loss : 0.224407, loss_ce: 0.011853, loss_dice: 0.436961
[09:13:50.596] TRAIN: iteration 8836 : loss : 0.143235, loss_ce: 0.006010, loss_dice: 0.280461
[09:13:50.803] TRAIN: iteration 8837 : loss : 0.180151, loss_ce: 0.003008, loss_dice: 0.357293
[09:13:51.013] TRAIN: iteration 8838 : loss : 0.251746, loss_ce: 0.003442, loss_dice: 0.500049
[09:13:51.221] TRAIN: iteration 8839 : loss : 0.136800, loss_ce: 0.004701, loss_dice: 0.268899
[09:13:51.428] TRAIN: iteration 8840 : loss : 0.116777, loss_ce: 0.006110, loss_dice: 0.227444
[09:13:51.677] TRAIN: iteration 8841 : loss : 0.088529, loss_ce: 0.005790, loss_dice: 0.171268
[09:13:51.886] TRAIN: iteration 8842 : loss : 0.153524, loss_ce: 0.004875, loss_dice: 0.302172
[09:13:52.100] TRAIN: iteration 8843 : loss : 0.104190, loss_ce: 0.003300, loss_dice: 0.205080
[09:13:52.315] TRAIN: iteration 8844 : loss : 0.129329, loss_ce: 0.004196, loss_dice: 0.254463
[09:13:52.524] TRAIN: iteration 8845 : loss : 0.101235, loss_ce: 0.002782, loss_dice: 0.199689
[09:13:52.738] TRAIN: iteration 8846 : loss : 0.071141, loss_ce: 0.002052, loss_dice: 0.140230
[09:13:52.952] TRAIN: iteration 8847 : loss : 0.086525, loss_ce: 0.003803, loss_dice: 0.169248
[09:13:53.160] TRAIN: iteration 8848 : loss : 0.094369, loss_ce: 0.003574, loss_dice: 0.185164
[09:13:53.375] TRAIN: iteration 8849 : loss : 0.251019, loss_ce: 0.001945, loss_dice: 0.500093
[09:13:53.584] TRAIN: iteration 8850 : loss : 0.231989, loss_ce: 0.009224, loss_dice: 0.454754
[09:13:53.791] TRAIN: iteration 8851 : loss : 0.223799, loss_ce: 0.003658, loss_dice: 0.443940
[09:13:54.000] TRAIN: iteration 8852 : loss : 0.240416, loss_ce: 0.005738, loss_dice: 0.475094
[09:13:54.209] TRAIN: iteration 8853 : loss : 0.181979, loss_ce: 0.014072, loss_dice: 0.349886
[09:13:54.418] TRAIN: iteration 8854 : loss : 0.101838, loss_ce: 0.009921, loss_dice: 0.193755
[09:13:54.626] TRAIN: iteration 8855 : loss : 0.250753, loss_ce: 0.001472, loss_dice: 0.500033
[09:13:54.834] TRAIN: iteration 8856 : loss : 0.093952, loss_ce: 0.003164, loss_dice: 0.184740
[09:13:55.042] TRAIN: iteration 8857 : loss : 0.189823, loss_ce: 0.005988, loss_dice: 0.373657
[09:13:55.251] TRAIN: iteration 8858 : loss : 0.133328, loss_ce: 0.009900, loss_dice: 0.256756
[09:13:55.458] TRAIN: iteration 8859 : loss : 0.190068, loss_ce: 0.006675, loss_dice: 0.373461
[09:13:55.671] TRAIN: iteration 8860 : loss : 0.071262, loss_ce: 0.004123, loss_dice: 0.138400
[09:13:55.910] TRAIN: iteration 8861 : loss : 0.193875, loss_ce: 0.005432, loss_dice: 0.382318
[09:13:56.119] TRAIN: iteration 8862 : loss : 0.054563, loss_ce: 0.002931, loss_dice: 0.106196
[09:13:56.326] TRAIN: iteration 8863 : loss : 0.153230, loss_ce: 0.005190, loss_dice: 0.301269
[09:13:56.534] TRAIN: iteration 8864 : loss : 0.207819, loss_ce: 0.003788, loss_dice: 0.411849
[09:13:56.745] TRAIN: iteration 8865 : loss : 0.049392, loss_ce: 0.003487, loss_dice: 0.095297
[09:13:56.952] TRAIN: iteration 8866 : loss : 0.063764, loss_ce: 0.002281, loss_dice: 0.125246
[09:13:57.158] TRAIN: iteration 8867 : loss : 0.120174, loss_ce: 0.003733, loss_dice: 0.236616
[09:13:57.365] TRAIN: iteration 8868 : loss : 0.238393, loss_ce: 0.003374, loss_dice: 0.473412
[09:13:57.573] TRAIN: iteration 8869 : loss : 0.101125, loss_ce: 0.002601, loss_dice: 0.199650
[09:13:57.783] TRAIN: iteration 8870 : loss : 0.043870, loss_ce: 0.002013, loss_dice: 0.085728
[09:13:57.998] TRAIN: iteration 8871 : loss : 0.251580, loss_ce: 0.002972, loss_dice: 0.500188
[09:13:58.206] TRAIN: iteration 8872 : loss : 0.164262, loss_ce: 0.014943, loss_dice: 0.313581
[09:13:58.417] TRAIN: iteration 8873 : loss : 0.112003, loss_ce: 0.004111, loss_dice: 0.219895
[09:13:58.625] TRAIN: iteration 8874 : loss : 0.251554, loss_ce: 0.002914, loss_dice: 0.500194
[09:13:58.833] TRAIN: iteration 8875 : loss : 0.172745, loss_ce: 0.003590, loss_dice: 0.341900
[09:13:59.041] TRAIN: iteration 8876 : loss : 0.241158, loss_ce: 0.005971, loss_dice: 0.476345
[09:13:59.249] TRAIN: iteration 8877 : loss : 0.226388, loss_ce: 0.006118, loss_dice: 0.446658
[09:13:59.456] TRAIN: iteration 8878 : loss : 0.071798, loss_ce: 0.003144, loss_dice: 0.140452
[09:13:59.663] TRAIN: iteration 8879 : loss : 0.102068, loss_ce: 0.003888, loss_dice: 0.200247
[09:13:59.870] TRAIN: iteration 8880 : loss : 0.132452, loss_ce: 0.006243, loss_dice: 0.258661
[09:14:00.110] TRAIN: iteration 8881 : loss : 0.251313, loss_ce: 0.002531, loss_dice: 0.500095
[09:14:00.319] TRAIN: iteration 8882 : loss : 0.214149, loss_ce: 0.054807, loss_dice: 0.373491
[09:14:00.527] TRAIN: iteration 8883 : loss : 0.108963, loss_ce: 0.003625, loss_dice: 0.214301
[09:14:00.734] TRAIN: iteration 8884 : loss : 0.251739, loss_ce: 0.003262, loss_dice: 0.500216
[09:14:00.943] TRAIN: iteration 8885 : loss : 0.059832, loss_ce: 0.002413, loss_dice: 0.117252
[09:14:01.152] TRAIN: iteration 8886 : loss : 0.109095, loss_ce: 0.009568, loss_dice: 0.208622
[09:14:01.359] TRAIN: iteration 8887 : loss : 0.251640, loss_ce: 0.003166, loss_dice: 0.500115
[09:14:01.570] TRAIN: iteration 8888 : loss : 0.089422, loss_ce: 0.010706, loss_dice: 0.168137
[09:14:01.779] TRAIN: iteration 8889 : loss : 0.251965, loss_ce: 0.003749, loss_dice: 0.500181
[09:14:01.989] TRAIN: iteration 8890 : loss : 0.229936, loss_ce: 0.005399, loss_dice: 0.454472
[09:14:02.197] TRAIN: iteration 8891 : loss : 0.091535, loss_ce: 0.003457, loss_dice: 0.179612
[09:14:02.409] TRAIN: iteration 8892 : loss : 0.252716, loss_ce: 0.005111, loss_dice: 0.500320
[09:14:02.619] TRAIN: iteration 8893 : loss : 0.151283, loss_ce: 0.009733, loss_dice: 0.292834
[09:14:02.826] TRAIN: iteration 8894 : loss : 0.252742, loss_ce: 0.005165, loss_dice: 0.500319
[09:14:03.035] TRAIN: iteration 8895 : loss : 0.168899, loss_ce: 0.005140, loss_dice: 0.332658
[09:14:03.247] TRAIN: iteration 8896 : loss : 0.045684, loss_ce: 0.004252, loss_dice: 0.087116
[09:14:03.459] TRAIN: iteration 8897 : loss : 0.049795, loss_ce: 0.003563, loss_dice: 0.096026
[09:14:03.670] TRAIN: iteration 8898 : loss : 0.164284, loss_ce: 0.009708, loss_dice: 0.318860
[09:14:03.879] TRAIN: iteration 8899 : loss : 0.031494, loss_ce: 0.002721, loss_dice: 0.060267
[09:14:04.092] TRAIN: iteration 8900 : loss : 0.082104, loss_ce: 0.008534, loss_dice: 0.155675
[09:14:04.334] TRAIN: iteration 8901 : loss : 0.056930, loss_ce: 0.003392, loss_dice: 0.110469
[09:14:04.542] TRAIN: iteration 8902 : loss : 0.103013, loss_ce: 0.005728, loss_dice: 0.200297
[09:14:04.750] TRAIN: iteration 8903 : loss : 0.086288, loss_ce: 0.003742, loss_dice: 0.168834
[09:14:04.960] TRAIN: iteration 8904 : loss : 0.251808, loss_ce: 0.003476, loss_dice: 0.500139
[09:14:05.174] TRAIN: iteration 8905 : loss : 0.252291, loss_ce: 0.004290, loss_dice: 0.500292
[09:14:05.384] TRAIN: iteration 8906 : loss : 0.106815, loss_ce: 0.003372, loss_dice: 0.210258
[09:14:05.593] TRAIN: iteration 8907 : loss : 0.066340, loss_ce: 0.004159, loss_dice: 0.128521
[09:14:05.812] TRAIN: iteration 8908 : loss : 0.175815, loss_ce: 0.004408, loss_dice: 0.347223
[09:14:06.023] TRAIN: iteration 8909 : loss : 0.136544, loss_ce: 0.003239, loss_dice: 0.269849
[09:14:06.231] TRAIN: iteration 8910 : loss : 0.088340, loss_ce: 0.006146, loss_dice: 0.170534
[09:14:06.438] TRAIN: iteration 8911 : loss : 0.242825, loss_ce: 0.003652, loss_dice: 0.481998
[09:14:06.648] TRAIN: iteration 8912 : loss : 0.134374, loss_ce: 0.013580, loss_dice: 0.255168
[09:14:06.859] TRAIN: iteration 8913 : loss : 0.214384, loss_ce: 0.004034, loss_dice: 0.424733
[09:14:07.070] TRAIN: iteration 8914 : loss : 0.148972, loss_ce: 0.011831, loss_dice: 0.286113
[09:14:07.282] TRAIN: iteration 8915 : loss : 0.149192, loss_ce: 0.005491, loss_dice: 0.292894
[09:14:07.499] TRAIN: iteration 8916 : loss : 0.201197, loss_ce: 0.029629, loss_dice: 0.372765
[09:14:07.713] TRAIN: iteration 8917 : loss : 0.251173, loss_ce: 0.002269, loss_dice: 0.500078
[09:14:07.920] TRAIN: iteration 8918 : loss : 0.165836, loss_ce: 0.009746, loss_dice: 0.321927
[09:14:08.135] TRAIN: iteration 8919 : loss : 0.152612, loss_ce: 0.002956, loss_dice: 0.302268
[09:14:08.342] TRAIN: iteration 8920 : loss : 0.084227, loss_ce: 0.006488, loss_dice: 0.161965
[09:14:08.620] TRAIN: iteration 8921 : loss : 0.249230, loss_ce: 0.003329, loss_dice: 0.495130
[09:14:08.829] TRAIN: iteration 8922 : loss : 0.254710, loss_ce: 0.009547, loss_dice: 0.499873
[09:14:09.038] TRAIN: iteration 8923 : loss : 0.182906, loss_ce: 0.007371, loss_dice: 0.358441
[09:14:09.246] TRAIN: iteration 8924 : loss : 0.252535, loss_ce: 0.017607, loss_dice: 0.487463
[09:14:09.458] TRAIN: iteration 8925 : loss : 0.251871, loss_ce: 0.003569, loss_dice: 0.500173
[09:14:09.667] TRAIN: iteration 8926 : loss : 0.141664, loss_ce: 0.015060, loss_dice: 0.268268
[09:14:09.877] TRAIN: iteration 8927 : loss : 0.108089, loss_ce: 0.018207, loss_dice: 0.197971
[09:14:10.085] TRAIN: iteration 8928 : loss : 0.097629, loss_ce: 0.010627, loss_dice: 0.184632
[09:14:10.294] TRAIN: iteration 8929 : loss : 0.090801, loss_ce: 0.006565, loss_dice: 0.175036
[09:14:10.503] TRAIN: iteration 8930 : loss : 0.128569, loss_ce: 0.025128, loss_dice: 0.232010
[09:14:10.711] TRAIN: iteration 8931 : loss : 0.254130, loss_ce: 0.007700, loss_dice: 0.500560
[09:14:10.920] TRAIN: iteration 8932 : loss : 0.093411, loss_ce: 0.008221, loss_dice: 0.178601
[09:14:11.129] TRAIN: iteration 8933 : loss : 0.254676, loss_ce: 0.011797, loss_dice: 0.497555
[09:14:11.337] TRAIN: iteration 8934 : loss : 0.139664, loss_ce: 0.009333, loss_dice: 0.269996
[09:14:11.546] TRAIN: iteration 8935 : loss : 0.254696, loss_ce: 0.008773, loss_dice: 0.500619
[09:14:11.754] TRAIN: iteration 8936 : loss : 0.126945, loss_ce: 0.007766, loss_dice: 0.246124
[09:14:11.962] TRAIN: iteration 8937 : loss : 0.144859, loss_ce: 0.010339, loss_dice: 0.279379
[09:14:12.169] TRAIN: iteration 8938 : loss : 0.112782, loss_ce: 0.006690, loss_dice: 0.218874
[09:14:12.384] TRAIN: iteration 8939 : loss : 0.071917, loss_ce: 0.005817, loss_dice: 0.138017
[09:14:12.596] TRAIN: iteration 8940 : loss : 0.117039, loss_ce: 0.009049, loss_dice: 0.225028
[09:14:12.826] TRAIN: iteration 8941 : loss : 0.122233, loss_ce: 0.008292, loss_dice: 0.236173
[09:14:13.035] TRAIN: iteration 8942 : loss : 0.040724, loss_ce: 0.005542, loss_dice: 0.075906
[09:14:13.242] TRAIN: iteration 8943 : loss : 0.210443, loss_ce: 0.033960, loss_dice: 0.386927
[09:14:13.449] TRAIN: iteration 8944 : loss : 0.190805, loss_ce: 0.012636, loss_dice: 0.368974
[09:14:13.717] TRAIN: iteration 8945 : loss : 0.089825, loss_ce: 0.006640, loss_dice: 0.173010
[09:14:13.925] TRAIN: iteration 8946 : loss : 0.036823, loss_ce: 0.002262, loss_dice: 0.071384
[09:14:14.133] TRAIN: iteration 8947 : loss : 0.065888, loss_ce: 0.004738, loss_dice: 0.127039
[09:14:14.340] TRAIN: iteration 8948 : loss : 0.206018, loss_ce: 0.009961, loss_dice: 0.402075
[09:14:14.554] TRAIN: iteration 8949 : loss : 0.122686, loss_ce: 0.005732, loss_dice: 0.239641
[09:14:14.762] TRAIN: iteration 8950 : loss : 0.251998, loss_ce: 0.003818, loss_dice: 0.500177
[09:14:14.971] TRAIN: iteration 8951 : loss : 0.144286, loss_ce: 0.003959, loss_dice: 0.284612
[09:14:15.179] TRAIN: iteration 8952 : loss : 0.188938, loss_ce: 0.004054, loss_dice: 0.373822
[09:14:15.388] TRAIN: iteration 8953 : loss : 0.067908, loss_ce: 0.003794, loss_dice: 0.132022
[09:14:15.603] TRAIN: iteration 8954 : loss : 0.106554, loss_ce: 0.002735, loss_dice: 0.210372
[09:14:15.811] TRAIN: iteration 8955 : loss : 0.115357, loss_ce: 0.004317, loss_dice: 0.226397
[09:14:16.019] TRAIN: iteration 8956 : loss : 0.210300, loss_ce: 0.003521, loss_dice: 0.417080
[09:14:16.229] TRAIN: iteration 8957 : loss : 0.123655, loss_ce: 0.003463, loss_dice: 0.243848
[09:14:16.437] TRAIN: iteration 8958 : loss : 0.218557, loss_ce: 0.003550, loss_dice: 0.433565
[09:14:16.648] TRAIN: iteration 8959 : loss : 0.245439, loss_ce: 0.003290, loss_dice: 0.487589
[09:14:16.865] TRAIN: iteration 8960 : loss : 0.121887, loss_ce: 0.002723, loss_dice: 0.241052
[09:14:16.865] NaN or Inf found in input tensor.
[09:14:17.086] TRAIN: iteration 8961 : loss : 0.085886, loss_ce: 0.004714, loss_dice: 0.167057
[09:14:17.298] TRAIN: iteration 8962 : loss : 0.214058, loss_ce: 0.002776, loss_dice: 0.425340
[09:14:17.511] TRAIN: iteration 8963 : loss : 0.173961, loss_ce: 0.010473, loss_dice: 0.337450
[09:14:17.720] TRAIN: iteration 8964 : loss : 0.176815, loss_ce: 0.006105, loss_dice: 0.347524
[09:14:17.933] TRAIN: iteration 8965 : loss : 0.104676, loss_ce: 0.004835, loss_dice: 0.204518
[09:14:18.141] TRAIN: iteration 8966 : loss : 0.048969, loss_ce: 0.002267, loss_dice: 0.095671
[09:14:18.347] TRAIN: iteration 8967 : loss : 0.169782, loss_ce: 0.003813, loss_dice: 0.335751
[09:14:18.553] TRAIN: iteration 8968 : loss : 0.125671, loss_ce: 0.005310, loss_dice: 0.246032
[09:14:18.760] TRAIN: iteration 8969 : loss : 0.115248, loss_ce: 0.002905, loss_dice: 0.227590
[09:14:18.979] TRAIN: iteration 8970 : loss : 0.115729, loss_ce: 0.005827, loss_dice: 0.225632
[09:14:19.187] TRAIN: iteration 8971 : loss : 0.140430, loss_ce: 0.007309, loss_dice: 0.273551
[09:14:19.393] TRAIN: iteration 8972 : loss : 0.147242, loss_ce: 0.002778, loss_dice: 0.291707
[09:14:19.601] TRAIN: iteration 8973 : loss : 0.224729, loss_ce: 0.002537, loss_dice: 0.446921
[09:14:20.606] TRAIN: iteration 8974 : loss : 0.147741, loss_ce: 0.009241, loss_dice: 0.286241
[09:14:20.814] TRAIN: iteration 8975 : loss : 0.085898, loss_ce: 0.009813, loss_dice: 0.161983
[09:14:21.022] TRAIN: iteration 8976 : loss : 0.052893, loss_ce: 0.002813, loss_dice: 0.102973
[09:14:21.232] TRAIN: iteration 8977 : loss : 0.213337, loss_ce: 0.009550, loss_dice: 0.417125
[09:14:21.445] TRAIN: iteration 8978 : loss : 0.105360, loss_ce: 0.002909, loss_dice: 0.207811
[09:14:21.654] TRAIN: iteration 8979 : loss : 0.098871, loss_ce: 0.007295, loss_dice: 0.190447
[09:14:22.075] TRAIN: iteration 8980 : loss : 0.133736, loss_ce: 0.013373, loss_dice: 0.254098
[09:14:22.311] TRAIN: iteration 8981 : loss : 0.252852, loss_ce: 0.005716, loss_dice: 0.499987
[09:14:22.758] TRAIN: iteration 8982 : loss : 0.249369, loss_ce: 0.003278, loss_dice: 0.495459
[09:14:22.966] TRAIN: iteration 8983 : loss : 0.211344, loss_ce: 0.008083, loss_dice: 0.414605
[09:14:23.174] TRAIN: iteration 8984 : loss : 0.105863, loss_ce: 0.010407, loss_dice: 0.201320
[09:14:23.382] TRAIN: iteration 8985 : loss : 0.098954, loss_ce: 0.006264, loss_dice: 0.191644
[09:14:23.590] TRAIN: iteration 8986 : loss : 0.074595, loss_ce: 0.003594, loss_dice: 0.145596
[09:14:23.798] TRAIN: iteration 8987 : loss : 0.050318, loss_ce: 0.004604, loss_dice: 0.096032
[09:14:24.942] TRAIN: iteration 8988 : loss : 0.159312, loss_ce: 0.003663, loss_dice: 0.314961
[09:14:25.149] TRAIN: iteration 8989 : loss : 0.185104, loss_ce: 0.005807, loss_dice: 0.364400
[09:14:25.357] TRAIN: iteration 8990 : loss : 0.130375, loss_ce: 0.005258, loss_dice: 0.255492
[09:14:25.567] TRAIN: iteration 8991 : loss : 0.112845, loss_ce: 0.002572, loss_dice: 0.223117
[09:14:25.782] TRAIN: iteration 8992 : loss : 0.116528, loss_ce: 0.003297, loss_dice: 0.229760
[09:14:25.995] TRAIN: iteration 8993 : loss : 0.170845, loss_ce: 0.007477, loss_dice: 0.334213
[09:14:26.206] TRAIN: iteration 8994 : loss : 0.096656, loss_ce: 0.002592, loss_dice: 0.190720
[09:14:26.419] TRAIN: iteration 8995 : loss : 0.218598, loss_ce: 0.002628, loss_dice: 0.434567
[09:14:26.627] TRAIN: iteration 8996 : loss : 0.112771, loss_ce: 0.012898, loss_dice: 0.212643
[09:14:26.838] TRAIN: iteration 8997 : loss : 0.251321, loss_ce: 0.002495, loss_dice: 0.500146
[09:14:27.050] TRAIN: iteration 8998 : loss : 0.123313, loss_ce: 0.003548, loss_dice: 0.243079
[09:14:27.257] TRAIN: iteration 8999 : loss : 0.178095, loss_ce: 0.006619, loss_dice: 0.349572
[09:14:27.464] TRAIN: iteration 9000 : loss : 0.150807, loss_ce: 0.004859, loss_dice: 0.296755
[09:14:27.705] TRAIN: iteration 9001 : loss : 0.086004, loss_ce: 0.003574, loss_dice: 0.168434
[09:14:27.914] TRAIN: iteration 9002 : loss : 0.148984, loss_ce: 0.006807, loss_dice: 0.291161
[09:14:28.128] TRAIN: iteration 9003 : loss : 0.252798, loss_ce: 0.006859, loss_dice: 0.498738
[09:14:28.344] TRAIN: iteration 9004 : loss : 0.142894, loss_ce: 0.008048, loss_dice: 0.277740
[09:14:28.552] TRAIN: iteration 9005 : loss : 0.073340, loss_ce: 0.004074, loss_dice: 0.142605
[09:14:28.765] TRAIN: iteration 9006 : loss : 0.230618, loss_ce: 0.004320, loss_dice: 0.456915
[09:14:28.972] TRAIN: iteration 9007 : loss : 0.239316, loss_ce: 0.005976, loss_dice: 0.472655
[09:14:29.186] TRAIN: iteration 9008 : loss : 0.203746, loss_ce: 0.003743, loss_dice: 0.403749
[09:14:29.394] TRAIN: iteration 9009 : loss : 0.085353, loss_ce: 0.002358, loss_dice: 0.168349
[09:14:29.609] TRAIN: iteration 9010 : loss : 0.119259, loss_ce: 0.023113, loss_dice: 0.215406
[09:14:29.825] TRAIN: iteration 9011 : loss : 0.244755, loss_ce: 0.002583, loss_dice: 0.486928
[09:14:30.032] TRAIN: iteration 9012 : loss : 0.140418, loss_ce: 0.002858, loss_dice: 0.277977
[09:14:30.239] TRAIN: iteration 9013 : loss : 0.250721, loss_ce: 0.001372, loss_dice: 0.500071
[09:14:30.448] TRAIN: iteration 9014 : loss : 0.086388, loss_ce: 0.004886, loss_dice: 0.167889
[09:14:30.655] TRAIN: iteration 9015 : loss : 0.104293, loss_ce: 0.002049, loss_dice: 0.206537
[09:14:30.864] TRAIN: iteration 9016 : loss : 0.099884, loss_ce: 0.003783, loss_dice: 0.195984
[09:14:31.073] TRAIN: iteration 9017 : loss : 0.233452, loss_ce: 0.008963, loss_dice: 0.457940
[09:14:31.291] TRAIN: iteration 9018 : loss : 0.095446, loss_ce: 0.004024, loss_dice: 0.186869
[09:14:31.500] TRAIN: iteration 9019 : loss : 0.204716, loss_ce: 0.023446, loss_dice: 0.385986
[09:14:31.710] TRAIN: iteration 9020 : loss : 0.120388, loss_ce: 0.003142, loss_dice: 0.237634
[09:14:31.948] TRAIN: iteration 9021 : loss : 0.238316, loss_ce: 0.010792, loss_dice: 0.465840
[09:14:32.156] TRAIN: iteration 9022 : loss : 0.103432, loss_ce: 0.003252, loss_dice: 0.203613
[09:14:32.364] TRAIN: iteration 9023 : loss : 0.124852, loss_ce: 0.007000, loss_dice: 0.242705
[09:14:32.578] TRAIN: iteration 9024 : loss : 0.156933, loss_ce: 0.003690, loss_dice: 0.310175
[09:14:32.786] TRAIN: iteration 9025 : loss : 0.251401, loss_ce: 0.002675, loss_dice: 0.500127
[09:14:32.992] TRAIN: iteration 9026 : loss : 0.158552, loss_ce: 0.009655, loss_dice: 0.307448
[09:14:33.206] TRAIN: iteration 9027 : loss : 0.169068, loss_ce: 0.004190, loss_dice: 0.333947
[09:14:33.416] TRAIN: iteration 9028 : loss : 0.201921, loss_ce: 0.009681, loss_dice: 0.394161
[09:14:33.627] TRAIN: iteration 9029 : loss : 0.251839, loss_ce: 0.003519, loss_dice: 0.500159
[09:14:33.839] TRAIN: iteration 9030 : loss : 0.242324, loss_ce: 0.003801, loss_dice: 0.480846
[09:14:34.047] TRAIN: iteration 9031 : loss : 0.167455, loss_ce: 0.032502, loss_dice: 0.302409
[09:14:34.254] TRAIN: iteration 9032 : loss : 0.155287, loss_ce: 0.003699, loss_dice: 0.306874
[09:14:34.464] TRAIN: iteration 9033 : loss : 0.099908, loss_ce: 0.005791, loss_dice: 0.194025
[09:14:34.674] TRAIN: iteration 9034 : loss : 0.040568, loss_ce: 0.003813, loss_dice: 0.077324
[09:14:34.881] TRAIN: iteration 9035 : loss : 0.107630, loss_ce: 0.004283, loss_dice: 0.210976
[09:14:35.091] TRAIN: iteration 9036 : loss : 0.251760, loss_ce: 0.003328, loss_dice: 0.500192
[09:14:35.301] TRAIN: iteration 9037 : loss : 0.133342, loss_ce: 0.009154, loss_dice: 0.257529
[09:14:35.508] TRAIN: iteration 9038 : loss : 0.184955, loss_ce: 0.007959, loss_dice: 0.361951
[09:14:35.715] TRAIN: iteration 9039 : loss : 0.131828, loss_ce: 0.005971, loss_dice: 0.257686
[09:14:35.925] TRAIN: iteration 9040 : loss : 0.157844, loss_ce: 0.004596, loss_dice: 0.311091
[09:14:36.165] TRAIN: iteration 9041 : loss : 0.124613, loss_ce: 0.006799, loss_dice: 0.242427
[09:14:36.375] TRAIN: iteration 9042 : loss : 0.244523, loss_ce: 0.005857, loss_dice: 0.483188
[09:14:36.583] TRAIN: iteration 9043 : loss : 0.244531, loss_ce: 0.006633, loss_dice: 0.482428
[09:14:36.791] TRAIN: iteration 9044 : loss : 0.215389, loss_ce: 0.011953, loss_dice: 0.418826
[09:14:36.998] TRAIN: iteration 9045 : loss : 0.250063, loss_ce: 0.009690, loss_dice: 0.490436
[09:14:37.206] TRAIN: iteration 9046 : loss : 0.068830, loss_ce: 0.005416, loss_dice: 0.132243
[09:14:37.417] TRAIN: iteration 9047 : loss : 0.083574, loss_ce: 0.005425, loss_dice: 0.161722
[09:14:37.624] TRAIN: iteration 9048 : loss : 0.252459, loss_ce: 0.004667, loss_dice: 0.500252
[09:14:37.831] TRAIN: iteration 9049 : loss : 0.168932, loss_ce: 0.038477, loss_dice: 0.299387
[09:14:38.047] TRAIN: iteration 9050 : loss : 0.197539, loss_ce: 0.005709, loss_dice: 0.389368
[09:14:38.254] TRAIN: iteration 9051 : loss : 0.133464, loss_ce: 0.005060, loss_dice: 0.261867
[09:14:38.460] TRAIN: iteration 9052 : loss : 0.179448, loss_ce: 0.006614, loss_dice: 0.352283
[09:14:38.667] TRAIN: iteration 9053 : loss : 0.172997, loss_ce: 0.005211, loss_dice: 0.340782
[09:14:38.881] TRAIN: iteration 9054 : loss : 0.064753, loss_ce: 0.007182, loss_dice: 0.122323
[09:14:39.094] TRAIN: iteration 9055 : loss : 0.146244, loss_ce: 0.006033, loss_dice: 0.286455
[09:14:39.309] TRAIN: iteration 9056 : loss : 0.051802, loss_ce: 0.003752, loss_dice: 0.099853
[09:14:39.516] TRAIN: iteration 9057 : loss : 0.105835, loss_ce: 0.006481, loss_dice: 0.205190
[09:14:39.724] TRAIN: iteration 9058 : loss : 0.234213, loss_ce: 0.004979, loss_dice: 0.463447
[09:14:39.931] TRAIN: iteration 9059 : loss : 0.135940, loss_ce: 0.003635, loss_dice: 0.268246
[09:14:40.143] TRAIN: iteration 9060 : loss : 0.110061, loss_ce: 0.004016, loss_dice: 0.216107
[09:14:40.143] NaN or Inf found in input tensor.
[09:14:40.357] TRAIN: iteration 9061 : loss : 0.177713, loss_ce: 0.006050, loss_dice: 0.349376
[09:14:40.571] TRAIN: iteration 9062 : loss : 0.109842, loss_ce: 0.005910, loss_dice: 0.213775
[09:14:40.784] TRAIN: iteration 9063 : loss : 0.097489, loss_ce: 0.003651, loss_dice: 0.191328
[09:14:40.991] TRAIN: iteration 9064 : loss : 0.096027, loss_ce: 0.003986, loss_dice: 0.188067
[09:14:41.199] TRAIN: iteration 9065 : loss : 0.233105, loss_ce: 0.003837, loss_dice: 0.462373
[09:14:41.412] TRAIN: iteration 9066 : loss : 0.085830, loss_ce: 0.005031, loss_dice: 0.166629
[09:14:41.622] TRAIN: iteration 9067 : loss : 0.072851, loss_ce: 0.002027, loss_dice: 0.143675
[09:14:41.831] TRAIN: iteration 9068 : loss : 0.174260, loss_ce: 0.003223, loss_dice: 0.345298
[09:14:42.045] TRAIN: iteration 9069 : loss : 0.073726, loss_ce: 0.003700, loss_dice: 0.143751
[09:14:42.259] TRAIN: iteration 9070 : loss : 0.190288, loss_ce: 0.001880, loss_dice: 0.378696
[09:14:42.470] TRAIN: iteration 9071 : loss : 0.108559, loss_ce: 0.003892, loss_dice: 0.213225
[09:14:42.677] TRAIN: iteration 9072 : loss : 0.154558, loss_ce: 0.019822, loss_dice: 0.289293
[09:14:42.884] TRAIN: iteration 9073 : loss : 0.145244, loss_ce: 0.007938, loss_dice: 0.282550
[09:14:43.092] TRAIN: iteration 9074 : loss : 0.199401, loss_ce: 0.001500, loss_dice: 0.397303
[09:14:43.300] TRAIN: iteration 9075 : loss : 0.131065, loss_ce: 0.006820, loss_dice: 0.255311
[09:14:43.507] TRAIN: iteration 9076 : loss : 0.067989, loss_ce: 0.005032, loss_dice: 0.130947
[09:14:43.719] TRAIN: iteration 9077 : loss : 0.056437, loss_ce: 0.002342, loss_dice: 0.110532
[09:14:43.928] TRAIN: iteration 9078 : loss : 0.083718, loss_ce: 0.004813, loss_dice: 0.162623
[09:14:44.138] TRAIN: iteration 9079 : loss : 0.076675, loss_ce: 0.004050, loss_dice: 0.149300
[09:14:44.345] TRAIN: iteration 9080 : loss : 0.199537, loss_ce: 0.004306, loss_dice: 0.394769
[09:14:45.283] TRAIN: iteration 9081 : loss : 0.219184, loss_ce: 0.004737, loss_dice: 0.433632
[09:14:45.491] TRAIN: iteration 9082 : loss : 0.192906, loss_ce: 0.005428, loss_dice: 0.380383
[09:14:45.698] TRAIN: iteration 9083 : loss : 0.246446, loss_ce: 0.005573, loss_dice: 0.487318
[09:14:45.906] TRAIN: iteration 9084 : loss : 0.208554, loss_ce: 0.005163, loss_dice: 0.411946
[09:14:46.121] TRAIN: iteration 9085 : loss : 0.099696, loss_ce: 0.007202, loss_dice: 0.192189
[09:14:46.328] TRAIN: iteration 9086 : loss : 0.148688, loss_ce: 0.003261, loss_dice: 0.294115
[09:14:46.540] TRAIN: iteration 9087 : loss : 0.252463, loss_ce: 0.004586, loss_dice: 0.500341
[09:14:46.756] TRAIN: iteration 9088 : loss : 0.253025, loss_ce: 0.005621, loss_dice: 0.500429
[09:14:46.966] TRAIN: iteration 9089 : loss : 0.103911, loss_ce: 0.003722, loss_dice: 0.204099
[09:14:47.176] TRAIN: iteration 9090 : loss : 0.123046, loss_ce: 0.003017, loss_dice: 0.243075
[09:14:47.386] TRAIN: iteration 9091 : loss : 0.124655, loss_ce: 0.003870, loss_dice: 0.245441
[09:14:47.596] TRAIN: iteration 9092 : loss : 0.114181, loss_ce: 0.004134, loss_dice: 0.224228
[09:14:47.805] TRAIN: iteration 9093 : loss : 0.248970, loss_ce: 0.003009, loss_dice: 0.494931
[09:14:48.012] TRAIN: iteration 9094 : loss : 0.051429, loss_ce: 0.002384, loss_dice: 0.100473
[09:14:48.238] TRAIN: iteration 9095 : loss : 0.250923, loss_ce: 0.001755, loss_dice: 0.500091
[09:14:48.445] TRAIN: iteration 9096 : loss : 0.233147, loss_ce: 0.002619, loss_dice: 0.463675
[09:14:48.653] TRAIN: iteration 9097 : loss : 0.110436, loss_ce: 0.011364, loss_dice: 0.209508
[09:14:48.860] TRAIN: iteration 9098 : loss : 0.106981, loss_ce: 0.003925, loss_dice: 0.210036
[09:14:49.067] TRAIN: iteration 9099 : loss : 0.222670, loss_ce: 0.011165, loss_dice: 0.434175
[09:14:49.274] TRAIN: iteration 9100 : loss : 0.253337, loss_ce: 0.010006, loss_dice: 0.496667
[09:14:49.532] TRAIN: iteration 9101 : loss : 0.123492, loss_ce: 0.007248, loss_dice: 0.239736
[09:14:49.738] TRAIN: iteration 9102 : loss : 0.130816, loss_ce: 0.007993, loss_dice: 0.253639
[09:14:49.954] TRAIN: iteration 9103 : loss : 0.070152, loss_ce: 0.002614, loss_dice: 0.137690
[09:14:50.164] TRAIN: iteration 9104 : loss : 0.081789, loss_ce: 0.006204, loss_dice: 0.157374
[09:14:50.382] TRAIN: iteration 9105 : loss : 0.183935, loss_ce: 0.007160, loss_dice: 0.360710
[09:14:50.589] TRAIN: iteration 9106 : loss : 0.156659, loss_ce: 0.003948, loss_dice: 0.309371
[09:14:50.796] TRAIN: iteration 9107 : loss : 0.210561, loss_ce: 0.005974, loss_dice: 0.415148
[09:14:51.008] TRAIN: iteration 9108 : loss : 0.248843, loss_ce: 0.004426, loss_dice: 0.493260
[09:14:51.221] TRAIN: iteration 9109 : loss : 0.202176, loss_ce: 0.005155, loss_dice: 0.399198
[09:14:51.429] TRAIN: iteration 9110 : loss : 0.067828, loss_ce: 0.004718, loss_dice: 0.130938
[09:14:51.639] TRAIN: iteration 9111 : loss : 0.061048, loss_ce: 0.003183, loss_dice: 0.118913
[09:14:51.855] TRAIN: iteration 9112 : loss : 0.059572, loss_ce: 0.006882, loss_dice: 0.112262
[09:14:52.074] TRAIN: iteration 9113 : loss : 0.211041, loss_ce: 0.004271, loss_dice: 0.417811
[09:14:52.281] TRAIN: iteration 9114 : loss : 0.131312, loss_ce: 0.004237, loss_dice: 0.258387
[09:14:52.489] TRAIN: iteration 9115 : loss : 0.252411, loss_ce: 0.004526, loss_dice: 0.500296
[09:14:52.697] TRAIN: iteration 9116 : loss : 0.094068, loss_ce: 0.006049, loss_dice: 0.182086
[09:14:52.913] TRAIN: iteration 9117 : loss : 0.102655, loss_ce: 0.007931, loss_dice: 0.197380
[09:14:53.124] TRAIN: iteration 9118 : loss : 0.069511, loss_ce: 0.003671, loss_dice: 0.135351
[09:14:53.342] TRAIN: iteration 9119 : loss : 0.119689, loss_ce: 0.005697, loss_dice: 0.233681
[09:14:53.551] TRAIN: iteration 9120 : loss : 0.161778, loss_ce: 0.005853, loss_dice: 0.317702
[09:14:53.787] TRAIN: iteration 9121 : loss : 0.097878, loss_ce: 0.004543, loss_dice: 0.191213
[09:14:53.994] TRAIN: iteration 9122 : loss : 0.110900, loss_ce: 0.009930, loss_dice: 0.211869
[09:14:54.201] TRAIN: iteration 9123 : loss : 0.039707, loss_ce: 0.002786, loss_dice: 0.076627
[09:14:54.407] TRAIN: iteration 9124 : loss : 0.121543, loss_ce: 0.006764, loss_dice: 0.236322
[09:14:54.616] TRAIN: iteration 9125 : loss : 0.086767, loss_ce: 0.005370, loss_dice: 0.168164
[09:14:54.823] TRAIN: iteration 9126 : loss : 0.113142, loss_ce: 0.005920, loss_dice: 0.220365
[09:14:55.036] TRAIN: iteration 9127 : loss : 0.142855, loss_ce: 0.008668, loss_dice: 0.277041
[09:14:55.250] TRAIN: iteration 9128 : loss : 0.146065, loss_ce: 0.003891, loss_dice: 0.288240
[09:14:55.459] TRAIN: iteration 9129 : loss : 0.161890, loss_ce: 0.006220, loss_dice: 0.317560
[09:14:55.666] TRAIN: iteration 9130 : loss : 0.252343, loss_ce: 0.004382, loss_dice: 0.500304
[09:14:55.873] TRAIN: iteration 9131 : loss : 0.226108, loss_ce: 0.005497, loss_dice: 0.446718
[09:14:56.084] TRAIN: iteration 9132 : loss : 0.063345, loss_ce: 0.003555, loss_dice: 0.123135
[09:14:56.292] TRAIN: iteration 9133 : loss : 0.239306, loss_ce: 0.005324, loss_dice: 0.473287
[09:14:56.502] TRAIN: iteration 9134 : loss : 0.053116, loss_ce: 0.003302, loss_dice: 0.102931
[09:14:56.711] TRAIN: iteration 9135 : loss : 0.251596, loss_ce: 0.004185, loss_dice: 0.499007
[09:14:56.925] TRAIN: iteration 9136 : loss : 0.140271, loss_ce: 0.005587, loss_dice: 0.274955
[09:14:57.133] TRAIN: iteration 9137 : loss : 0.139143, loss_ce: 0.003265, loss_dice: 0.275021
[09:14:57.347] TRAIN: iteration 9138 : loss : 0.163080, loss_ce: 0.005667, loss_dice: 0.320492
[09:14:57.557] TRAIN: iteration 9139 : loss : 0.251842, loss_ce: 0.003432, loss_dice: 0.500252
[09:14:57.764] TRAIN: iteration 9140 : loss : 0.187333, loss_ce: 0.002885, loss_dice: 0.371782
[09:14:57.765] NaN or Inf found in input tensor.
[09:14:57.979] TRAIN: iteration 9141 : loss : 0.087943, loss_ce: 0.002273, loss_dice: 0.173614
[09:14:58.187] TRAIN: iteration 9142 : loss : 0.236526, loss_ce: 0.012558, loss_dice: 0.460494
[09:14:58.395] TRAIN: iteration 9143 : loss : 0.101046, loss_ce: 0.002883, loss_dice: 0.199209
[09:14:58.604] TRAIN: iteration 9144 : loss : 0.124978, loss_ce: 0.009157, loss_dice: 0.240798
[09:14:58.812] TRAIN: iteration 9145 : loss : 0.086694, loss_ce: 0.001868, loss_dice: 0.171519
[09:14:59.018] TRAIN: iteration 9146 : loss : 0.144159, loss_ce: 0.002858, loss_dice: 0.285461
[09:14:59.232] TRAIN: iteration 9147 : loss : 0.218631, loss_ce: 0.001955, loss_dice: 0.435307
[09:14:59.442] TRAIN: iteration 9148 : loss : 0.101120, loss_ce: 0.002697, loss_dice: 0.199542
[09:14:59.649] TRAIN: iteration 9149 : loss : 0.250680, loss_ce: 0.001294, loss_dice: 0.500066
[09:14:59.856] TRAIN: iteration 9150 : loss : 0.139703, loss_ce: 0.001130, loss_dice: 0.278276
[09:15:00.063] TRAIN: iteration 9151 : loss : 0.080352, loss_ce: 0.001445, loss_dice: 0.159259
[09:15:00.272] TRAIN: iteration 9152 : loss : 0.250370, loss_ce: 0.000722, loss_dice: 0.500018
[09:15:00.491] TRAIN: iteration 9153 : loss : 0.110887, loss_ce: 0.003969, loss_dice: 0.217805
[09:15:00.704] TRAIN: iteration 9154 : loss : 0.092848, loss_ce: 0.003943, loss_dice: 0.181752
[09:15:00.913] TRAIN: iteration 9155 : loss : 0.166280, loss_ce: 0.004824, loss_dice: 0.327736
[09:15:01.126] TRAIN: iteration 9156 : loss : 0.184723, loss_ce: 0.003575, loss_dice: 0.365870
[09:15:01.334] TRAIN: iteration 9157 : loss : 0.229739, loss_ce: 0.001804, loss_dice: 0.457674
[09:15:01.545] TRAIN: iteration 9158 : loss : 0.097653, loss_ce: 0.002653, loss_dice: 0.192652
[09:15:01.751] TRAIN: iteration 9159 : loss : 0.251726, loss_ce: 0.003196, loss_dice: 0.500257
[09:15:01.965] TRAIN: iteration 9160 : loss : 0.119384, loss_ce: 0.004118, loss_dice: 0.234650
[09:15:02.207] TRAIN: iteration 9161 : loss : 0.068418, loss_ce: 0.002940, loss_dice: 0.133896
[09:15:02.413] TRAIN: iteration 9162 : loss : 0.180175, loss_ce: 0.003318, loss_dice: 0.357033
[09:15:02.620] TRAIN: iteration 9163 : loss : 0.247830, loss_ce: 0.005887, loss_dice: 0.489772
[09:15:02.826] TRAIN: iteration 9164 : loss : 0.156428, loss_ce: 0.003123, loss_dice: 0.309732
[09:15:03.035] TRAIN: iteration 9165 : loss : 0.077706, loss_ce: 0.007159, loss_dice: 0.148252
[09:15:03.244] TRAIN: iteration 9166 : loss : 0.225164, loss_ce: 0.010964, loss_dice: 0.439365
[09:15:03.451] TRAIN: iteration 9167 : loss : 0.133460, loss_ce: 0.004442, loss_dice: 0.262477
[09:15:03.659] TRAIN: iteration 9168 : loss : 0.253970, loss_ce: 0.010982, loss_dice: 0.496957
[09:15:03.873] TRAIN: iteration 9169 : loss : 0.251970, loss_ce: 0.003673, loss_dice: 0.500266
[09:15:04.080] TRAIN: iteration 9170 : loss : 0.097217, loss_ce: 0.005428, loss_dice: 0.189006
[09:15:06.123] TRAIN: iteration 9171 : loss : 0.143174, loss_ce: 0.014994, loss_dice: 0.271354
[09:15:06.332] TRAIN: iteration 9172 : loss : 0.252339, loss_ce: 0.004345, loss_dice: 0.500332
[09:15:06.570] TRAIN: iteration 9173 : loss : 0.078344, loss_ce: 0.003015, loss_dice: 0.153673
[09:15:06.778] TRAIN: iteration 9174 : loss : 0.134787, loss_ce: 0.006183, loss_dice: 0.263391
[09:15:06.985] TRAIN: iteration 9175 : loss : 0.245096, loss_ce: 0.005425, loss_dice: 0.484767
[09:15:07.200] TRAIN: iteration 9176 : loss : 0.252168, loss_ce: 0.004061, loss_dice: 0.500274
[09:15:07.408] TRAIN: iteration 9177 : loss : 0.216447, loss_ce: 0.007371, loss_dice: 0.425522
[09:15:07.617] TRAIN: iteration 9178 : loss : 0.082022, loss_ce: 0.006150, loss_dice: 0.157894
[09:15:07.916] TRAIN: iteration 9179 : loss : 0.225736, loss_ce: 0.007276, loss_dice: 0.444195
[09:15:08.131] TRAIN: iteration 9180 : loss : 0.051008, loss_ce: 0.005062, loss_dice: 0.096954
[09:15:08.370] TRAIN: iteration 9181 : loss : 0.217047, loss_ce: 0.022688, loss_dice: 0.411406
[09:15:08.577] TRAIN: iteration 9182 : loss : 0.172450, loss_ce: 0.004393, loss_dice: 0.340507
[09:15:08.785] TRAIN: iteration 9183 : loss : 0.252717, loss_ce: 0.005082, loss_dice: 0.500352
[09:15:08.992] TRAIN: iteration 9184 : loss : 0.064838, loss_ce: 0.009200, loss_dice: 0.120475
[09:15:09.199] TRAIN: iteration 9185 : loss : 0.118726, loss_ce: 0.005901, loss_dice: 0.231550
[09:15:09.409] TRAIN: iteration 9186 : loss : 0.250168, loss_ce: 0.007830, loss_dice: 0.492506
[09:15:09.617] TRAIN: iteration 9187 : loss : 0.156152, loss_ce: 0.006149, loss_dice: 0.306155
[09:15:09.832] TRAIN: iteration 9188 : loss : 0.154900, loss_ce: 0.010888, loss_dice: 0.298911
[09:15:10.040] TRAIN: iteration 9189 : loss : 0.105478, loss_ce: 0.004739, loss_dice: 0.206216
[09:15:10.250] TRAIN: iteration 9190 : loss : 0.070489, loss_ce: 0.003363, loss_dice: 0.137615
[09:15:10.456] TRAIN: iteration 9191 : loss : 0.067922, loss_ce: 0.003455, loss_dice: 0.132390
[09:15:10.663] TRAIN: iteration 9192 : loss : 0.131080, loss_ce: 0.007828, loss_dice: 0.254332
[09:15:12.003] TRAIN: iteration 9193 : loss : 0.200109, loss_ce: 0.005687, loss_dice: 0.394531
[09:15:12.210] TRAIN: iteration 9194 : loss : 0.027988, loss_ce: 0.002133, loss_dice: 0.053843
[09:15:12.416] TRAIN: iteration 9195 : loss : 0.188126, loss_ce: 0.003312, loss_dice: 0.372939
[09:15:12.630] TRAIN: iteration 9196 : loss : 0.169143, loss_ce: 0.011094, loss_dice: 0.327193
[09:15:12.838] TRAIN: iteration 9197 : loss : 0.138976, loss_ce: 0.004110, loss_dice: 0.273842
[09:15:13.049] TRAIN: iteration 9198 : loss : 0.111488, loss_ce: 0.004587, loss_dice: 0.218389
[09:15:13.262] TRAIN: iteration 9199 : loss : 0.108043, loss_ce: 0.010703, loss_dice: 0.205384
[09:15:13.469] TRAIN: iteration 9200 : loss : 0.084315, loss_ce: 0.003212, loss_dice: 0.165418
[09:15:13.708] TRAIN: iteration 9201 : loss : 0.250079, loss_ce: 0.003437, loss_dice: 0.496721
[09:15:13.914] TRAIN: iteration 9202 : loss : 0.145803, loss_ce: 0.005437, loss_dice: 0.286169
[09:15:14.123] TRAIN: iteration 9203 : loss : 0.082079, loss_ce: 0.003944, loss_dice: 0.160215
[09:15:14.331] TRAIN: iteration 9204 : loss : 0.126810, loss_ce: 0.003004, loss_dice: 0.250615
[09:15:14.537] TRAIN: iteration 9205 : loss : 0.233256, loss_ce: 0.008071, loss_dice: 0.458441
[09:15:14.745] TRAIN: iteration 9206 : loss : 0.109979, loss_ce: 0.007344, loss_dice: 0.212615
[09:15:14.953] TRAIN: iteration 9207 : loss : 0.167171, loss_ce: 0.022082, loss_dice: 0.312260
[09:15:15.161] TRAIN: iteration 9208 : loss : 0.169184, loss_ce: 0.006520, loss_dice: 0.331848
[09:15:15.944] TRAIN: iteration 9209 : loss : 0.251651, loss_ce: 0.003117, loss_dice: 0.500185
[09:15:16.161] TRAIN: iteration 9210 : loss : 0.094744, loss_ce: 0.007262, loss_dice: 0.182226
[09:15:16.369] TRAIN: iteration 9211 : loss : 0.062802, loss_ce: 0.003672, loss_dice: 0.121933
[09:15:16.579] TRAIN: iteration 9212 : loss : 0.200759, loss_ce: 0.006674, loss_dice: 0.394844
[09:15:16.786] TRAIN: iteration 9213 : loss : 0.218578, loss_ce: 0.007705, loss_dice: 0.429451
[09:15:16.995] TRAIN: iteration 9214 : loss : 0.049401, loss_ce: 0.005700, loss_dice: 0.093101
[09:15:17.204] TRAIN: iteration 9215 : loss : 0.094162, loss_ce: 0.006213, loss_dice: 0.182110
[09:15:17.411] TRAIN: iteration 9216 : loss : 0.186346, loss_ce: 0.005566, loss_dice: 0.367126
[09:15:18.643] TRAIN: iteration 9217 : loss : 0.142329, loss_ce: 0.008109, loss_dice: 0.276549
[09:15:18.850] TRAIN: iteration 9218 : loss : 0.094526, loss_ce: 0.005846, loss_dice: 0.183207
[09:15:19.060] TRAIN: iteration 9219 : loss : 0.230233, loss_ce: 0.005161, loss_dice: 0.455306
[09:15:19.272] TRAIN: iteration 9220 : loss : 0.206866, loss_ce: 0.007663, loss_dice: 0.406070
[09:15:19.273] NaN or Inf found in input tensor.
[09:15:19.491] TRAIN: iteration 9221 : loss : 0.075036, loss_ce: 0.006218, loss_dice: 0.143854
[09:15:19.700] TRAIN: iteration 9222 : loss : 0.144014, loss_ce: 0.006790, loss_dice: 0.281239
[09:15:19.907] TRAIN: iteration 9223 : loss : 0.231807, loss_ce: 0.004300, loss_dice: 0.459314
[09:15:20.120] TRAIN: iteration 9224 : loss : 0.090673, loss_ce: 0.003611, loss_dice: 0.177734
[09:15:20.333] TRAIN: iteration 9225 : loss : 0.100763, loss_ce: 0.006417, loss_dice: 0.195110
[09:15:20.547] TRAIN: iteration 9226 : loss : 0.047686, loss_ce: 0.002583, loss_dice: 0.092789
[09:15:20.754] TRAIN: iteration 9227 : loss : 0.093988, loss_ce: 0.006787, loss_dice: 0.181190
[09:15:20.961] TRAIN: iteration 9228 : loss : 0.252688, loss_ce: 0.006575, loss_dice: 0.498800
[09:15:21.168] TRAIN: iteration 9229 : loss : 0.122771, loss_ce: 0.004111, loss_dice: 0.241431
[09:15:21.375] TRAIN: iteration 9230 : loss : 0.114415, loss_ce: 0.002922, loss_dice: 0.225908
[09:15:21.584] TRAIN: iteration 9231 : loss : 0.206641, loss_ce: 0.003416, loss_dice: 0.409866
[09:15:21.794] TRAIN: iteration 9232 : loss : 0.098296, loss_ce: 0.006180, loss_dice: 0.190411
[09:15:22.002] TRAIN: iteration 9233 : loss : 0.184433, loss_ce: 0.005674, loss_dice: 0.363191
[09:15:22.217] TRAIN: iteration 9234 : loss : 0.129619, loss_ce: 0.009494, loss_dice: 0.249744
[09:15:22.428] TRAIN: iteration 9235 : loss : 0.204583, loss_ce: 0.017331, loss_dice: 0.391835
[09:15:22.636] TRAIN: iteration 9236 : loss : 0.120823, loss_ce: 0.010107, loss_dice: 0.231538
[09:15:22.844] TRAIN: iteration 9237 : loss : 0.165563, loss_ce: 0.003497, loss_dice: 0.327630
[09:15:23.052] TRAIN: iteration 9238 : loss : 0.220839, loss_ce: 0.002921, loss_dice: 0.438756
[09:15:23.259] TRAIN: iteration 9239 : loss : 0.089170, loss_ce: 0.003216, loss_dice: 0.175124
[09:15:23.465] TRAIN: iteration 9240 : loss : 0.182467, loss_ce: 0.002649, loss_dice: 0.362286
[09:15:23.713] TRAIN: iteration 9241 : loss : 0.085458, loss_ce: 0.003710, loss_dice: 0.167207
[09:15:23.921] TRAIN: iteration 9242 : loss : 0.092911, loss_ce: 0.007568, loss_dice: 0.178254
[09:15:24.128] TRAIN: iteration 9243 : loss : 0.108168, loss_ce: 0.005941, loss_dice: 0.210395
[09:15:24.613] TRAIN: iteration 9244 : loss : 0.101934, loss_ce: 0.004069, loss_dice: 0.199798
[09:15:24.820] TRAIN: iteration 9245 : loss : 0.029782, loss_ce: 0.001466, loss_dice: 0.058097
[09:15:25.029] TRAIN: iteration 9246 : loss : 0.223094, loss_ce: 0.007797, loss_dice: 0.438392
[09:15:25.239] TRAIN: iteration 9247 : loss : 0.194456, loss_ce: 0.005700, loss_dice: 0.383212
[09:15:25.448] TRAIN: iteration 9248 : loss : 0.125966, loss_ce: 0.005433, loss_dice: 0.246499
[09:15:25.655] TRAIN: iteration 9249 : loss : 0.245058, loss_ce: 0.002889, loss_dice: 0.487226
[09:15:25.871] TRAIN: iteration 9250 : loss : 0.144370, loss_ce: 0.010349, loss_dice: 0.278391
[09:15:27.009] TRAIN: iteration 9251 : loss : 0.073886, loss_ce: 0.006676, loss_dice: 0.141097
[09:15:27.216] TRAIN: iteration 9252 : loss : 0.045752, loss_ce: 0.003339, loss_dice: 0.088164
[09:15:27.425] TRAIN: iteration 9253 : loss : 0.191736, loss_ce: 0.006563, loss_dice: 0.376908
[09:15:27.634] TRAIN: iteration 9254 : loss : 0.067004, loss_ce: 0.005492, loss_dice: 0.128516
[09:15:27.847] TRAIN: iteration 9255 : loss : 0.047080, loss_ce: 0.003641, loss_dice: 0.090519
[09:15:28.055] TRAIN: iteration 9256 : loss : 0.253211, loss_ce: 0.005989, loss_dice: 0.500433
[09:15:28.269] TRAIN: iteration 9257 : loss : 0.109070, loss_ce: 0.007899, loss_dice: 0.210241
[09:15:28.477] TRAIN: iteration 9258 : loss : 0.118384, loss_ce: 0.020886, loss_dice: 0.215882
[09:15:28.847] TRAIN: iteration 9259 : loss : 0.051009, loss_ce: 0.003487, loss_dice: 0.098530
[09:15:29.057] TRAIN: iteration 9260 : loss : 0.132291, loss_ce: 0.006116, loss_dice: 0.258467
[09:15:29.299] TRAIN: iteration 9261 : loss : 0.196830, loss_ce: 0.006159, loss_dice: 0.387501
[09:15:29.506] TRAIN: iteration 9262 : loss : 0.253464, loss_ce: 0.006490, loss_dice: 0.500439
[09:15:29.714] TRAIN: iteration 9263 : loss : 0.120783, loss_ce: 0.010003, loss_dice: 0.231563
[09:15:29.921] TRAIN: iteration 9264 : loss : 0.251585, loss_ce: 0.005799, loss_dice: 0.497371
[09:15:30.134] TRAIN: iteration 9265 : loss : 0.145174, loss_ce: 0.009220, loss_dice: 0.281127
[09:15:30.345] TRAIN: iteration 9266 : loss : 0.172327, loss_ce: 0.008411, loss_dice: 0.336242
[09:15:30.555] TRAIN: iteration 9267 : loss : 0.211151, loss_ce: 0.006617, loss_dice: 0.415684
[09:15:30.762] TRAIN: iteration 9268 : loss : 0.188149, loss_ce: 0.010819, loss_dice: 0.365479
[09:15:30.969] TRAIN: iteration 9269 : loss : 0.059033, loss_ce: 0.008398, loss_dice: 0.109668
[09:15:31.176] TRAIN: iteration 9270 : loss : 0.253136, loss_ce: 0.005842, loss_dice: 0.500430
[09:15:31.439] TRAIN: iteration 9271 : loss : 0.219137, loss_ce: 0.005759, loss_dice: 0.432515
[09:15:31.650] TRAIN: iteration 9272 : loss : 0.211080, loss_ce: 0.009107, loss_dice: 0.413053
[09:15:31.860] TRAIN: iteration 9273 : loss : 0.072749, loss_ce: 0.005643, loss_dice: 0.139855
[09:15:32.068] TRAIN: iteration 9274 : loss : 0.238056, loss_ce: 0.005778, loss_dice: 0.470333
[09:15:32.349] TRAIN: iteration 9275 : loss : 0.068351, loss_ce: 0.006694, loss_dice: 0.130009
[09:15:32.558] TRAIN: iteration 9276 : loss : 0.126244, loss_ce: 0.006670, loss_dice: 0.245818
[09:15:32.767] TRAIN: iteration 9277 : loss : 0.253373, loss_ce: 0.006291, loss_dice: 0.500455
[09:15:32.975] TRAIN: iteration 9278 : loss : 0.091061, loss_ce: 0.007168, loss_dice: 0.174954
[09:15:33.188] TRAIN: iteration 9279 : loss : 0.090560, loss_ce: 0.003812, loss_dice: 0.177307
[09:15:33.398] TRAIN: iteration 9280 : loss : 0.178669, loss_ce: 0.010307, loss_dice: 0.347030
[09:15:33.632] TRAIN: iteration 9281 : loss : 0.165545, loss_ce: 0.007405, loss_dice: 0.323685
[09:15:33.844] TRAIN: iteration 9282 : loss : 0.252110, loss_ce: 0.003984, loss_dice: 0.500237
[09:15:34.053] TRAIN: iteration 9283 : loss : 0.248118, loss_ce: 0.009616, loss_dice: 0.486619
[09:15:34.263] TRAIN: iteration 9284 : loss : 0.046562, loss_ce: 0.002884, loss_dice: 0.090240
[09:15:34.477] TRAIN: iteration 9285 : loss : 0.175230, loss_ce: 0.009202, loss_dice: 0.341258
[09:15:34.685] TRAIN: iteration 9286 : loss : 0.174941, loss_ce: 0.028400, loss_dice: 0.321483
[09:15:34.892] TRAIN: iteration 9287 : loss : 0.203690, loss_ce: 0.005265, loss_dice: 0.402115
[09:15:35.100] TRAIN: iteration 9288 : loss : 0.201252, loss_ce: 0.004293, loss_dice: 0.398210
[09:15:35.307] TRAIN: iteration 9289 : loss : 0.150175, loss_ce: 0.008215, loss_dice: 0.292135
[09:15:35.514] TRAIN: iteration 9290 : loss : 0.094298, loss_ce: 0.006585, loss_dice: 0.182012
[09:15:35.720] TRAIN: iteration 9291 : loss : 0.121885, loss_ce: 0.016227, loss_dice: 0.227543
[09:15:36.409] TRAIN: iteration 9292 : loss : 0.083284, loss_ce: 0.002842, loss_dice: 0.163727
[09:15:36.617] TRAIN: iteration 9293 : loss : 0.078191, loss_ce: 0.003698, loss_dice: 0.152685
[09:15:36.824] TRAIN: iteration 9294 : loss : 0.051812, loss_ce: 0.005125, loss_dice: 0.098499
[09:15:37.031] TRAIN: iteration 9295 : loss : 0.089774, loss_ce: 0.003323, loss_dice: 0.176225
[09:15:37.239] TRAIN: iteration 9296 : loss : 0.068936, loss_ce: 0.004976, loss_dice: 0.132895
[09:15:37.448] TRAIN: iteration 9297 : loss : 0.145446, loss_ce: 0.008267, loss_dice: 0.282625
[09:15:37.659] TRAIN: iteration 9298 : loss : 0.224628, loss_ce: 0.006038, loss_dice: 0.443217
[09:15:37.873] TRAIN: iteration 9299 : loss : 0.057502, loss_ce: 0.003465, loss_dice: 0.111539
[09:15:39.252] TRAIN: iteration 9300 : loss : 0.157667, loss_ce: 0.011373, loss_dice: 0.303962
[09:15:39.488] TRAIN: iteration 9301 : loss : 0.111296, loss_ce: 0.004264, loss_dice: 0.218327
[09:15:39.695] TRAIN: iteration 9302 : loss : 0.136007, loss_ce: 0.005730, loss_dice: 0.266283
[09:15:39.903] TRAIN: iteration 9303 : loss : 0.162270, loss_ce: 0.007198, loss_dice: 0.317343
[09:15:40.112] TRAIN: iteration 9304 : loss : 0.251479, loss_ce: 0.002834, loss_dice: 0.500125
[09:15:40.322] TRAIN: iteration 9305 : loss : 0.114570, loss_ce: 0.011018, loss_dice: 0.218122
[09:15:40.536] TRAIN: iteration 9306 : loss : 0.073901, loss_ce: 0.004995, loss_dice: 0.142806
[09:15:40.744] TRAIN: iteration 9307 : loss : 0.165447, loss_ce: 0.003255, loss_dice: 0.327640
[09:15:40.954] TRAIN: iteration 9308 : loss : 0.162625, loss_ce: 0.004010, loss_dice: 0.321240
[09:15:41.162] TRAIN: iteration 9309 : loss : 0.224202, loss_ce: 0.004580, loss_dice: 0.443823
[09:15:41.370] TRAIN: iteration 9310 : loss : 0.112711, loss_ce: 0.017353, loss_dice: 0.208070
[09:15:41.585] TRAIN: iteration 9311 : loss : 0.192212, loss_ce: 0.008297, loss_dice: 0.376126
[09:15:41.792] TRAIN: iteration 9312 : loss : 0.053579, loss_ce: 0.003252, loss_dice: 0.103907
[09:15:41.999] TRAIN: iteration 9313 : loss : 0.067216, loss_ce: 0.002135, loss_dice: 0.132298
[09:15:42.215] TRAIN: iteration 9314 : loss : 0.139887, loss_ce: 0.015299, loss_dice: 0.264474
[09:15:42.431] TRAIN: iteration 9315 : loss : 0.074964, loss_ce: 0.002342, loss_dice: 0.147587
[09:15:42.639] TRAIN: iteration 9316 : loss : 0.167640, loss_ce: 0.008204, loss_dice: 0.327075
[09:15:42.849] TRAIN: iteration 9317 : loss : 0.068368, loss_ce: 0.005200, loss_dice: 0.131537
[09:15:43.057] TRAIN: iteration 9318 : loss : 0.173311, loss_ce: 0.014750, loss_dice: 0.331872
[09:15:43.266] TRAIN: iteration 9319 : loss : 0.077830, loss_ce: 0.011077, loss_dice: 0.144583
[09:15:43.474] TRAIN: iteration 9320 : loss : 0.127108, loss_ce: 0.009944, loss_dice: 0.244271
[09:15:43.475] NaN or Inf found in input tensor.
[09:15:43.697] TRAIN: iteration 9321 : loss : 0.211573, loss_ce: 0.004314, loss_dice: 0.418832
[09:15:43.904] TRAIN: iteration 9322 : loss : 0.054126, loss_ce: 0.004312, loss_dice: 0.103940
[09:15:44.112] TRAIN: iteration 9323 : loss : 0.148017, loss_ce: 0.005624, loss_dice: 0.290411
[09:15:44.322] TRAIN: iteration 9324 : loss : 0.121694, loss_ce: 0.006796, loss_dice: 0.236593
[09:15:44.529] TRAIN: iteration 9325 : loss : 0.227265, loss_ce: 0.007091, loss_dice: 0.447438
[09:15:44.736] TRAIN: iteration 9326 : loss : 0.126677, loss_ce: 0.004975, loss_dice: 0.248379
[09:15:44.945] TRAIN: iteration 9327 : loss : 0.253503, loss_ce: 0.006533, loss_dice: 0.500474
[09:15:45.156] TRAIN: iteration 9328 : loss : 0.253623, loss_ce: 0.006773, loss_dice: 0.500473
[09:15:45.370] TRAIN: iteration 9329 : loss : 0.047876, loss_ce: 0.006848, loss_dice: 0.088905
[09:15:45.580] TRAIN: iteration 9330 : loss : 0.062262, loss_ce: 0.006842, loss_dice: 0.117682
[09:15:45.787] TRAIN: iteration 9331 : loss : 0.160654, loss_ce: 0.026648, loss_dice: 0.294660
[09:15:45.993] TRAIN: iteration 9332 : loss : 0.209795, loss_ce: 0.010222, loss_dice: 0.409367
[09:15:46.200] TRAIN: iteration 9333 : loss : 0.191816, loss_ce: 0.013007, loss_dice: 0.370626
[09:15:46.415] TRAIN: iteration 9334 : loss : 0.227064, loss_ce: 0.010352, loss_dice: 0.443776
[09:15:46.626] TRAIN: iteration 9335 : loss : 0.087416, loss_ce: 0.006350, loss_dice: 0.168483
[09:15:46.833] TRAIN: iteration 9336 : loss : 0.241688, loss_ce: 0.007334, loss_dice: 0.476042
[09:15:47.042] TRAIN: iteration 9337 : loss : 0.168922, loss_ce: 0.036438, loss_dice: 0.301405
[09:15:47.252] TRAIN: iteration 9338 : loss : 0.253780, loss_ce: 0.008672, loss_dice: 0.498888
[09:15:47.461] TRAIN: iteration 9339 : loss : 0.058753, loss_ce: 0.004429, loss_dice: 0.113077
[09:15:47.670] TRAIN: iteration 9340 : loss : 0.078978, loss_ce: 0.007226, loss_dice: 0.150730
[09:15:47.917] TRAIN: iteration 9341 : loss : 0.128916, loss_ce: 0.007684, loss_dice: 0.250148
[09:15:48.125] TRAIN: iteration 9342 : loss : 0.066401, loss_ce: 0.005420, loss_dice: 0.127383
[09:15:48.333] TRAIN: iteration 9343 : loss : 0.179266, loss_ce: 0.004840, loss_dice: 0.353691
[09:15:48.543] TRAIN: iteration 9344 : loss : 0.207540, loss_ce: 0.005393, loss_dice: 0.409687
[09:15:48.753] TRAIN: iteration 9345 : loss : 0.111729, loss_ce: 0.005730, loss_dice: 0.217728
[09:15:48.961] TRAIN: iteration 9346 : loss : 0.068371, loss_ce: 0.006525, loss_dice: 0.130217
[09:15:49.171] TRAIN: iteration 9347 : loss : 0.100613, loss_ce: 0.007256, loss_dice: 0.193970
[09:15:49.381] TRAIN: iteration 9348 : loss : 0.075310, loss_ce: 0.004562, loss_dice: 0.146058
[09:15:49.906] TRAIN: iteration 9349 : loss : 0.174498, loss_ce: 0.008940, loss_dice: 0.340056
[09:15:50.115] TRAIN: iteration 9350 : loss : 0.106762, loss_ce: 0.006442, loss_dice: 0.207083
[09:15:50.322] TRAIN: iteration 9351 : loss : 0.251052, loss_ce: 0.001998, loss_dice: 0.500106
[09:15:50.532] TRAIN: iteration 9352 : loss : 0.064308, loss_ce: 0.003801, loss_dice: 0.124815
[09:15:50.739] TRAIN: iteration 9353 : loss : 0.251424, loss_ce: 0.002665, loss_dice: 0.500183
[09:15:50.946] TRAIN: iteration 9354 : loss : 0.183340, loss_ce: 0.009484, loss_dice: 0.357195
[09:15:51.154] TRAIN: iteration 9355 : loss : 0.233934, loss_ce: 0.009518, loss_dice: 0.458349
[09:15:51.361] TRAIN: iteration 9356 : loss : 0.143212, loss_ce: 0.017855, loss_dice: 0.268570
[09:15:51.903] TRAIN: iteration 9357 : loss : 0.130280, loss_ce: 0.004968, loss_dice: 0.255592
[09:15:52.110] TRAIN: iteration 9358 : loss : 0.229444, loss_ce: 0.010107, loss_dice: 0.448781
[09:15:52.317] TRAIN: iteration 9359 : loss : 0.080403, loss_ce: 0.007103, loss_dice: 0.153704
[09:15:52.524] TRAIN: iteration 9360 : loss : 0.132096, loss_ce: 0.013692, loss_dice: 0.250499
[09:15:52.524] NaN or Inf found in input tensor.
[09:15:52.738] TRAIN: iteration 9361 : loss : 0.148106, loss_ce: 0.008804, loss_dice: 0.287408
[09:15:52.946] TRAIN: iteration 9362 : loss : 0.090194, loss_ce: 0.004211, loss_dice: 0.176177
[09:15:53.155] TRAIN: iteration 9363 : loss : 0.124421, loss_ce: 0.005367, loss_dice: 0.243475
[09:15:53.363] TRAIN: iteration 9364 : loss : 0.211224, loss_ce: 0.006893, loss_dice: 0.415555
[09:15:53.576] TRAIN: iteration 9365 : loss : 0.251155, loss_ce: 0.005630, loss_dice: 0.496681
[09:15:53.788] TRAIN: iteration 9366 : loss : 0.145076, loss_ce: 0.009301, loss_dice: 0.280850
[09:15:53.996] TRAIN: iteration 9367 : loss : 0.252647, loss_ce: 0.005077, loss_dice: 0.500217
[09:15:54.212] TRAIN: iteration 9368 : loss : 0.184589, loss_ce: 0.008716, loss_dice: 0.360462
[09:15:54.421] TRAIN: iteration 9369 : loss : 0.124175, loss_ce: 0.004834, loss_dice: 0.243516
[09:15:54.629] TRAIN: iteration 9370 : loss : 0.189145, loss_ce: 0.005700, loss_dice: 0.372590
[09:15:54.838] TRAIN: iteration 9371 : loss : 0.231491, loss_ce: 0.006052, loss_dice: 0.456930
[09:15:55.053] TRAIN: iteration 9372 : loss : 0.252824, loss_ce: 0.005384, loss_dice: 0.500264
[09:15:55.263] TRAIN: iteration 9373 : loss : 0.167514, loss_ce: 0.007563, loss_dice: 0.327465
[09:15:55.470] TRAIN: iteration 9374 : loss : 0.116917, loss_ce: 0.006590, loss_dice: 0.227244
[09:15:55.681] TRAIN: iteration 9375 : loss : 0.138889, loss_ce: 0.004085, loss_dice: 0.273693
[09:15:55.892] TRAIN: iteration 9376 : loss : 0.134125, loss_ce: 0.005980, loss_dice: 0.262270
[09:15:56.103] TRAIN: iteration 9377 : loss : 0.146605, loss_ce: 0.007672, loss_dice: 0.285538
[09:15:56.311] TRAIN: iteration 9378 : loss : 0.239135, loss_ce: 0.004546, loss_dice: 0.473724
[09:15:56.521] TRAIN: iteration 9379 : loss : 0.240209, loss_ce: 0.004625, loss_dice: 0.475793
[09:15:56.733] TRAIN: iteration 9380 : loss : 0.251492, loss_ce: 0.002887, loss_dice: 0.500097
[09:15:57.003] TRAIN: iteration 9381 : loss : 0.216170, loss_ce: 0.007878, loss_dice: 0.424462
[09:15:57.217] TRAIN: iteration 9382 : loss : 0.103937, loss_ce: 0.003232, loss_dice: 0.204642
[09:15:57.425] TRAIN: iteration 9383 : loss : 0.245810, loss_ce: 0.003024, loss_dice: 0.488597
[09:15:57.633] TRAIN: iteration 9384 : loss : 0.047927, loss_ce: 0.003301, loss_dice: 0.092553
[09:15:57.842] TRAIN: iteration 9385 : loss : 0.203581, loss_ce: 0.005121, loss_dice: 0.402041
[09:15:58.049] TRAIN: iteration 9386 : loss : 0.055616, loss_ce: 0.002211, loss_dice: 0.109022
[09:15:58.259] TRAIN: iteration 9387 : loss : 0.139492, loss_ce: 0.007395, loss_dice: 0.271588
[09:15:58.470] TRAIN: iteration 9388 : loss : 0.065212, loss_ce: 0.004832, loss_dice: 0.125591
[09:15:58.676] TRAIN: iteration 9389 : loss : 0.239495, loss_ce: 0.003071, loss_dice: 0.475920
[09:15:58.883] TRAIN: iteration 9390 : loss : 0.143748, loss_ce: 0.003447, loss_dice: 0.284049
[09:16:01.384] TRAIN: iteration 9391 : loss : 0.155355, loss_ce: 0.012686, loss_dice: 0.298024
[09:16:01.591] TRAIN: iteration 9392 : loss : 0.063549, loss_ce: 0.005734, loss_dice: 0.121363
[09:16:01.797] TRAIN: iteration 9393 : loss : 0.062820, loss_ce: 0.003044, loss_dice: 0.122595
[09:16:02.008] TRAIN: iteration 9394 : loss : 0.235446, loss_ce: 0.005231, loss_dice: 0.465662
[09:16:02.217] TRAIN: iteration 9395 : loss : 0.184027, loss_ce: 0.006305, loss_dice: 0.361748
[09:16:02.426] TRAIN: iteration 9396 : loss : 0.252043, loss_ce: 0.003886, loss_dice: 0.500199
[09:16:02.638] TRAIN: iteration 9397 : loss : 0.249058, loss_ce: 0.007448, loss_dice: 0.490667
[09:16:02.844] TRAIN: iteration 9398 : loss : 0.242719, loss_ce: 0.009098, loss_dice: 0.476339
[09:16:03.267] TRAIN: iteration 9399 : loss : 0.096430, loss_ce: 0.011153, loss_dice: 0.181707
[09:16:03.481] TRAIN: iteration 9400 : loss : 0.115451, loss_ce: 0.005899, loss_dice: 0.225003
[09:16:03.717] TRAIN: iteration 9401 : loss : 0.253525, loss_ce: 0.006585, loss_dice: 0.500465
[09:16:03.933] TRAIN: iteration 9402 : loss : 0.099389, loss_ce: 0.008370, loss_dice: 0.190408
[09:16:04.143] TRAIN: iteration 9403 : loss : 0.200890, loss_ce: 0.009045, loss_dice: 0.392735
[09:16:04.359] TRAIN: iteration 9404 : loss : 0.221965, loss_ce: 0.006831, loss_dice: 0.437099
[09:16:04.570] TRAIN: iteration 9405 : loss : 0.053077, loss_ce: 0.005951, loss_dice: 0.100203
[09:16:04.777] TRAIN: iteration 9406 : loss : 0.053494, loss_ce: 0.003154, loss_dice: 0.103833
[09:16:04.984] TRAIN: iteration 9407 : loss : 0.253398, loss_ce: 0.006334, loss_dice: 0.500463
[09:16:05.197] TRAIN: iteration 9408 : loss : 0.193044, loss_ce: 0.006818, loss_dice: 0.379270
[09:16:05.405] TRAIN: iteration 9409 : loss : 0.161758, loss_ce: 0.004785, loss_dice: 0.318730
[09:16:05.614] TRAIN: iteration 9410 : loss : 0.080756, loss_ce: 0.005351, loss_dice: 0.156162
[09:16:05.828] TRAIN: iteration 9411 : loss : 0.045409, loss_ce: 0.002679, loss_dice: 0.088139
[09:16:06.042] TRAIN: iteration 9412 : loss : 0.097336, loss_ce: 0.009964, loss_dice: 0.184708
[09:16:06.322] TRAIN: iteration 9413 : loss : 0.248423, loss_ce: 0.005546, loss_dice: 0.491300
[09:16:06.531] TRAIN: iteration 9414 : loss : 0.177350, loss_ce: 0.012810, loss_dice: 0.341890
[09:16:06.783] TRAIN: iteration 9415 : loss : 0.252580, loss_ce: 0.004839, loss_dice: 0.500321
[09:16:06.998] TRAIN: iteration 9416 : loss : 0.184799, loss_ce: 0.037736, loss_dice: 0.331863
[09:16:07.206] TRAIN: iteration 9417 : loss : 0.177461, loss_ce: 0.005902, loss_dice: 0.349020
[09:16:07.418] TRAIN: iteration 9418 : loss : 0.252643, loss_ce: 0.005070, loss_dice: 0.500216
[09:16:07.627] TRAIN: iteration 9419 : loss : 0.095220, loss_ce: 0.003031, loss_dice: 0.187409
[09:16:07.842] TRAIN: iteration 9420 : loss : 0.085520, loss_ce: 0.006747, loss_dice: 0.164292
[09:16:08.079] TRAIN: iteration 9421 : loss : 0.155203, loss_ce: 0.009663, loss_dice: 0.300743
[09:16:08.294] TRAIN: iteration 9422 : loss : 0.070620, loss_ce: 0.007472, loss_dice: 0.133768
[09:16:08.961] TRAIN: iteration 9423 : loss : 0.200618, loss_ce: 0.007436, loss_dice: 0.393801
[09:16:09.171] TRAIN: iteration 9424 : loss : 0.094013, loss_ce: 0.010779, loss_dice: 0.177247
[09:16:09.379] TRAIN: iteration 9425 : loss : 0.131329, loss_ce: 0.005751, loss_dice: 0.256907
[09:16:09.588] TRAIN: iteration 9426 : loss : 0.090507, loss_ce: 0.007994, loss_dice: 0.173019
[09:16:09.806] TRAIN: iteration 9427 : loss : 0.058598, loss_ce: 0.004841, loss_dice: 0.112355
[09:16:10.013] TRAIN: iteration 9428 : loss : 0.255724, loss_ce: 0.012315, loss_dice: 0.499132
[09:16:10.225] TRAIN: iteration 9429 : loss : 0.186323, loss_ce: 0.004502, loss_dice: 0.368144
[09:16:10.433] TRAIN: iteration 9430 : loss : 0.154151, loss_ce: 0.005204, loss_dice: 0.303098
[09:16:10.640] TRAIN: iteration 9431 : loss : 0.107143, loss_ce: 0.006528, loss_dice: 0.207759
[09:16:10.850] TRAIN: iteration 9432 : loss : 0.177873, loss_ce: 0.005759, loss_dice: 0.349988
[09:16:11.396] TRAIN: iteration 9433 : loss : 0.132538, loss_ce: 0.005172, loss_dice: 0.259904
[09:16:11.605] TRAIN: iteration 9434 : loss : 0.042224, loss_ce: 0.003274, loss_dice: 0.081174
[09:16:11.815] TRAIN: iteration 9435 : loss : 0.170560, loss_ce: 0.020734, loss_dice: 0.320385
[09:16:12.026] TRAIN: iteration 9436 : loss : 0.202326, loss_ce: 0.005501, loss_dice: 0.399151
[09:16:12.232] TRAIN: iteration 9437 : loss : 0.236443, loss_ce: 0.006671, loss_dice: 0.466216
[09:16:12.439] TRAIN: iteration 9438 : loss : 0.063352, loss_ce: 0.006738, loss_dice: 0.119966
[09:16:12.655] TRAIN: iteration 9439 : loss : 0.252217, loss_ce: 0.004201, loss_dice: 0.500233
[09:16:12.868] TRAIN: iteration 9440 : loss : 0.140638, loss_ce: 0.006153, loss_dice: 0.275124
[09:16:13.853] TRAIN: iteration 9441 : loss : 0.213729, loss_ce: 0.010158, loss_dice: 0.417300
[09:16:14.060] TRAIN: iteration 9442 : loss : 0.252252, loss_ce: 0.004243, loss_dice: 0.500261
[09:16:14.279] TRAIN: iteration 9443 : loss : 0.185351, loss_ce: 0.005264, loss_dice: 0.365437
[09:16:14.485] TRAIN: iteration 9444 : loss : 0.253062, loss_ce: 0.005875, loss_dice: 0.500250
[09:16:14.706] TRAIN: iteration 9445 : loss : 0.192761, loss_ce: 0.006494, loss_dice: 0.379028
[09:16:14.913] TRAIN: iteration 9446 : loss : 0.100889, loss_ce: 0.003906, loss_dice: 0.197872
[09:16:15.120] TRAIN: iteration 9447 : loss : 0.252722, loss_ce: 0.005111, loss_dice: 0.500333
[09:16:15.333] TRAIN: iteration 9448 : loss : 0.225531, loss_ce: 0.004016, loss_dice: 0.447046
[09:16:15.546] TRAIN: iteration 9449 : loss : 0.250549, loss_ce: 0.004136, loss_dice: 0.496961
[09:16:15.775] TRAIN: iteration 9450 : loss : 0.216589, loss_ce: 0.004228, loss_dice: 0.428951
[09:16:15.983] TRAIN: iteration 9451 : loss : 0.050936, loss_ce: 0.003812, loss_dice: 0.098060
[09:16:16.191] TRAIN: iteration 9452 : loss : 0.166183, loss_ce: 0.003925, loss_dice: 0.328441
[09:16:16.400] TRAIN: iteration 9453 : loss : 0.203566, loss_ce: 0.007017, loss_dice: 0.400114
[09:16:16.607] TRAIN: iteration 9454 : loss : 0.088034, loss_ce: 0.004984, loss_dice: 0.171083
[09:16:16.819] TRAIN: iteration 9455 : loss : 0.172003, loss_ce: 0.009573, loss_dice: 0.334432
[09:16:17.028] TRAIN: iteration 9456 : loss : 0.247603, loss_ce: 0.004289, loss_dice: 0.490916
[09:16:17.237] TRAIN: iteration 9457 : loss : 0.139746, loss_ce: 0.004132, loss_dice: 0.275361
[09:16:17.696] TRAIN: iteration 9458 : loss : 0.251814, loss_ce: 0.003452, loss_dice: 0.500177
[09:16:17.904] TRAIN: iteration 9459 : loss : 0.080191, loss_ce: 0.003051, loss_dice: 0.157330
[09:16:18.116] TRAIN: iteration 9460 : loss : 0.090534, loss_ce: 0.005563, loss_dice: 0.175504
[09:16:18.368] TRAIN: iteration 9461 : loss : 0.129653, loss_ce: 0.007241, loss_dice: 0.252065
[09:16:18.584] TRAIN: iteration 9462 : loss : 0.094770, loss_ce: 0.003205, loss_dice: 0.186334
[09:16:18.791] TRAIN: iteration 9463 : loss : 0.083108, loss_ce: 0.004874, loss_dice: 0.161341
[09:16:19.011] TRAIN: iteration 9464 : loss : 0.251560, loss_ce: 0.002946, loss_dice: 0.500174
[09:16:19.219] TRAIN: iteration 9465 : loss : 0.252122, loss_ce: 0.003968, loss_dice: 0.500276
[09:16:19.429] TRAIN: iteration 9466 : loss : 0.109349, loss_ce: 0.003586, loss_dice: 0.215113
[09:16:19.643] TRAIN: iteration 9467 : loss : 0.079320, loss_ce: 0.007795, loss_dice: 0.150845
[09:16:19.852] TRAIN: iteration 9468 : loss : 0.071281, loss_ce: 0.003914, loss_dice: 0.138649
[09:16:20.059] TRAIN: iteration 9469 : loss : 0.073205, loss_ce: 0.008434, loss_dice: 0.137975
[09:16:20.267] TRAIN: iteration 9470 : loss : 0.068299, loss_ce: 0.003991, loss_dice: 0.132607
[09:16:20.482] TRAIN: iteration 9471 : loss : 0.110352, loss_ce: 0.004147, loss_dice: 0.216557
[09:16:20.693] TRAIN: iteration 9472 : loss : 0.251501, loss_ce: 0.002851, loss_dice: 0.500150
[09:16:20.900] TRAIN: iteration 9473 : loss : 0.099284, loss_ce: 0.006405, loss_dice: 0.192164
[09:16:21.108] TRAIN: iteration 9474 : loss : 0.249401, loss_ce: 0.003047, loss_dice: 0.495756
[09:16:21.317] TRAIN: iteration 9475 : loss : 0.236037, loss_ce: 0.003414, loss_dice: 0.468660
[09:16:21.527] TRAIN: iteration 9476 : loss : 0.113182, loss_ce: 0.004612, loss_dice: 0.221753
[09:16:21.736] TRAIN: iteration 9477 : loss : 0.144708, loss_ce: 0.005152, loss_dice: 0.284263
[09:16:21.950] TRAIN: iteration 9478 : loss : 0.139866, loss_ce: 0.005306, loss_dice: 0.274427
[09:16:22.419] TRAIN: iteration 9479 : loss : 0.116190, loss_ce: 0.003132, loss_dice: 0.229248
[09:16:22.630] TRAIN: iteration 9480 : loss : 0.158022, loss_ce: 0.005694, loss_dice: 0.310349
[09:16:22.870] TRAIN: iteration 9481 : loss : 0.044399, loss_ce: 0.003984, loss_dice: 0.084814
[09:16:23.078] TRAIN: iteration 9482 : loss : 0.086093, loss_ce: 0.006344, loss_dice: 0.165843
[09:16:23.290] TRAIN: iteration 9483 : loss : 0.129738, loss_ce: 0.031269, loss_dice: 0.228208
[09:16:23.497] TRAIN: iteration 9484 : loss : 0.086535, loss_ce: 0.001058, loss_dice: 0.172013
[09:16:23.705] TRAIN: iteration 9485 : loss : 0.115803, loss_ce: 0.004886, loss_dice: 0.226719
[09:16:23.913] TRAIN: iteration 9486 : loss : 0.072327, loss_ce: 0.004223, loss_dice: 0.140430
[09:16:26.138] TRAIN: iteration 9487 : loss : 0.077816, loss_ce: 0.009795, loss_dice: 0.145838
[09:16:26.345] TRAIN: iteration 9488 : loss : 0.191868, loss_ce: 0.015197, loss_dice: 0.368539
[09:16:26.553] TRAIN: iteration 9489 : loss : 0.091850, loss_ce: 0.003925, loss_dice: 0.179775
[09:16:26.761] TRAIN: iteration 9490 : loss : 0.231410, loss_ce: 0.002915, loss_dice: 0.459905
[09:16:26.969] TRAIN: iteration 9491 : loss : 0.176352, loss_ce: 0.002890, loss_dice: 0.349813
[09:16:27.175] TRAIN: iteration 9492 : loss : 0.128733, loss_ce: 0.002978, loss_dice: 0.254487
[09:16:27.384] TRAIN: iteration 9493 : loss : 0.250389, loss_ce: 0.003477, loss_dice: 0.497301
[09:16:27.592] TRAIN: iteration 9494 : loss : 0.141758, loss_ce: 0.005624, loss_dice: 0.277893
[09:16:27.802] TRAIN: iteration 9495 : loss : 0.095326, loss_ce: 0.012227, loss_dice: 0.178426
[09:16:28.034] TRAIN: iteration 9496 : loss : 0.131315, loss_ce: 0.008540, loss_dice: 0.254090
[09:16:28.242] TRAIN: iteration 9497 : loss : 0.091197, loss_ce: 0.003813, loss_dice: 0.178581
[09:16:28.453] TRAIN: iteration 9498 : loss : 0.222385, loss_ce: 0.004017, loss_dice: 0.440753
[09:16:28.663] TRAIN: iteration 9499 : loss : 0.075711, loss_ce: 0.012145, loss_dice: 0.139277
[09:16:28.873] TRAIN: iteration 9500 : loss : 0.085256, loss_ce: 0.012151, loss_dice: 0.158361
[09:16:29.126] TRAIN: iteration 9501 : loss : 0.116731, loss_ce: 0.005566, loss_dice: 0.227895
[09:16:29.333] TRAIN: iteration 9502 : loss : 0.235718, loss_ce: 0.007610, loss_dice: 0.463827
[09:16:29.541] TRAIN: iteration 9503 : loss : 0.129770, loss_ce: 0.007433, loss_dice: 0.252108
[09:16:29.978] TRAIN: iteration 9504 : loss : 0.067523, loss_ce: 0.004057, loss_dice: 0.130988
[09:16:30.187] TRAIN: iteration 9505 : loss : 0.252135, loss_ce: 0.004027, loss_dice: 0.500242
[09:16:30.397] TRAIN: iteration 9506 : loss : 0.241216, loss_ce: 0.003541, loss_dice: 0.478891
[09:16:30.604] TRAIN: iteration 9507 : loss : 0.059901, loss_ce: 0.004479, loss_dice: 0.115323
[09:16:30.811] TRAIN: iteration 9508 : loss : 0.251701, loss_ce: 0.003225, loss_dice: 0.500176
[09:16:31.025] TRAIN: iteration 9509 : loss : 0.077892, loss_ce: 0.004612, loss_dice: 0.151172
[09:16:31.236] TRAIN: iteration 9510 : loss : 0.101841, loss_ce: 0.010708, loss_dice: 0.192973
[09:16:31.444] TRAIN: iteration 9511 : loss : 0.129315, loss_ce: 0.005203, loss_dice: 0.253428
[09:16:31.652] TRAIN: iteration 9512 : loss : 0.225078, loss_ce: 0.010485, loss_dice: 0.439670
[09:16:31.860] TRAIN: iteration 9513 : loss : 0.125169, loss_ce: 0.004177, loss_dice: 0.246161
[09:16:32.067] TRAIN: iteration 9514 : loss : 0.157815, loss_ce: 0.003811, loss_dice: 0.311820
[09:16:32.275] TRAIN: iteration 9515 : loss : 0.247342, loss_ce: 0.004159, loss_dice: 0.490526
[09:16:32.482] TRAIN: iteration 9516 : loss : 0.061332, loss_ce: 0.003968, loss_dice: 0.118697
[09:16:32.691] TRAIN: iteration 9517 : loss : 0.235931, loss_ce: 0.008772, loss_dice: 0.463091
[09:16:32.898] TRAIN: iteration 9518 : loss : 0.073474, loss_ce: 0.003272, loss_dice: 0.143675
[09:16:33.106] TRAIN: iteration 9519 : loss : 0.229055, loss_ce: 0.004028, loss_dice: 0.454081
[09:16:33.313] TRAIN: iteration 9520 : loss : 0.234037, loss_ce: 0.003669, loss_dice: 0.464404
[09:16:33.550] TRAIN: iteration 9521 : loss : 0.092118, loss_ce: 0.004280, loss_dice: 0.179956
[09:16:33.759] TRAIN: iteration 9522 : loss : 0.251764, loss_ce: 0.003372, loss_dice: 0.500157
[09:16:33.967] TRAIN: iteration 9523 : loss : 0.151384, loss_ce: 0.008596, loss_dice: 0.294172
[09:16:34.174] TRAIN: iteration 9524 : loss : 0.240170, loss_ce: 0.006127, loss_dice: 0.474213
[09:16:34.382] TRAIN: iteration 9525 : loss : 0.073209, loss_ce: 0.004933, loss_dice: 0.141486
[09:16:34.591] TRAIN: iteration 9526 : loss : 0.100680, loss_ce: 0.007195, loss_dice: 0.194165
[09:16:34.806] TRAIN: iteration 9527 : loss : 0.131766, loss_ce: 0.005095, loss_dice: 0.258437
[09:16:35.553] TRAIN: iteration 9528 : loss : 0.056393, loss_ce: 0.003484, loss_dice: 0.109303
[09:16:35.760] TRAIN: iteration 9529 : loss : 0.189023, loss_ce: 0.013587, loss_dice: 0.364459
[09:16:35.967] TRAIN: iteration 9530 : loss : 0.233998, loss_ce: 0.009920, loss_dice: 0.458077
[09:16:36.174] TRAIN: iteration 9531 : loss : 0.195667, loss_ce: 0.015105, loss_dice: 0.376228
[09:16:36.381] TRAIN: iteration 9532 : loss : 0.253194, loss_ce: 0.006009, loss_dice: 0.500379
[09:16:36.587] TRAIN: iteration 9533 : loss : 0.141517, loss_ce: 0.005704, loss_dice: 0.277330
[09:16:36.795] TRAIN: iteration 9534 : loss : 0.253406, loss_ce: 0.006387, loss_dice: 0.500425
[09:16:37.005] TRAIN: iteration 9535 : loss : 0.248658, loss_ce: 0.007759, loss_dice: 0.489556
[09:16:37.306] TRAIN: iteration 9536 : loss : 0.253519, loss_ce: 0.006775, loss_dice: 0.500263
[09:16:37.515] TRAIN: iteration 9537 : loss : 0.171678, loss_ce: 0.033912, loss_dice: 0.309443
[09:16:37.774] TRAIN: iteration 9538 : loss : 0.154974, loss_ce: 0.005666, loss_dice: 0.304283
[09:16:37.982] TRAIN: iteration 9539 : loss : 0.115651, loss_ce: 0.005168, loss_dice: 0.226135
[09:16:38.195] TRAIN: iteration 9540 : loss : 0.074955, loss_ce: 0.004825, loss_dice: 0.145086
[09:16:38.433] TRAIN: iteration 9541 : loss : 0.248776, loss_ce: 0.006283, loss_dice: 0.491268
[09:16:38.640] TRAIN: iteration 9542 : loss : 0.058984, loss_ce: 0.005779, loss_dice: 0.112190
[09:16:38.851] TRAIN: iteration 9543 : loss : 0.045682, loss_ce: 0.003483, loss_dice: 0.087882
[09:16:39.060] TRAIN: iteration 9544 : loss : 0.038312, loss_ce: 0.002628, loss_dice: 0.073996
[09:16:39.268] TRAIN: iteration 9545 : loss : 0.115236, loss_ce: 0.004338, loss_dice: 0.226135
[09:16:39.476] TRAIN: iteration 9546 : loss : 0.083571, loss_ce: 0.002817, loss_dice: 0.164326
[09:16:39.685] TRAIN: iteration 9547 : loss : 0.101765, loss_ce: 0.006827, loss_dice: 0.196702
[09:16:39.893] TRAIN: iteration 9548 : loss : 0.184318, loss_ce: 0.002930, loss_dice: 0.365706
[09:16:40.101] TRAIN: iteration 9549 : loss : 0.222832, loss_ce: 0.004646, loss_dice: 0.441018
[09:16:40.315] TRAIN: iteration 9550 : loss : 0.178588, loss_ce: 0.002693, loss_dice: 0.354483
[09:16:40.523] TRAIN: iteration 9551 : loss : 0.250909, loss_ce: 0.001757, loss_dice: 0.500061
[09:16:40.736] TRAIN: iteration 9552 : loss : 0.190478, loss_ce: 0.003629, loss_dice: 0.377327
[09:16:40.944] TRAIN: iteration 9553 : loss : 0.066945, loss_ce: 0.002568, loss_dice: 0.131322
[09:16:41.152] TRAIN: iteration 9554 : loss : 0.039686, loss_ce: 0.001818, loss_dice: 0.077555
[09:16:42.080] TRAIN: iteration 9555 : loss : 0.141283, loss_ce: 0.003008, loss_dice: 0.279559
[09:16:42.289] TRAIN: iteration 9556 : loss : 0.186028, loss_ce: 0.014152, loss_dice: 0.357904
[09:16:42.498] TRAIN: iteration 9557 : loss : 0.144448, loss_ce: 0.009257, loss_dice: 0.279639
[09:16:42.712] TRAIN: iteration 9558 : loss : 0.179492, loss_ce: 0.005548, loss_dice: 0.353436
[09:16:42.920] TRAIN: iteration 9559 : loss : 0.251690, loss_ce: 0.003187, loss_dice: 0.500193
[09:16:43.128] TRAIN: iteration 9560 : loss : 0.247861, loss_ce: 0.004205, loss_dice: 0.491516
[09:16:43.377] TRAIN: iteration 9561 : loss : 0.095657, loss_ce: 0.003750, loss_dice: 0.187564
[09:16:43.585] TRAIN: iteration 9562 : loss : 0.097427, loss_ce: 0.003151, loss_dice: 0.191703
[09:16:43.792] TRAIN: iteration 9563 : loss : 0.107368, loss_ce: 0.006710, loss_dice: 0.208026
[09:16:44.000] TRAIN: iteration 9564 : loss : 0.230138, loss_ce: 0.020669, loss_dice: 0.439608
[09:16:44.208] TRAIN: iteration 9565 : loss : 0.028114, loss_ce: 0.003160, loss_dice: 0.053068
[09:16:44.426] TRAIN: iteration 9566 : loss : 0.177238, loss_ce: 0.003787, loss_dice: 0.350690
[09:16:44.636] TRAIN: iteration 9567 : loss : 0.162916, loss_ce: 0.009582, loss_dice: 0.316249
[09:16:44.848] TRAIN: iteration 9568 : loss : 0.177878, loss_ce: 0.009088, loss_dice: 0.346667
[09:16:45.058] TRAIN: iteration 9569 : loss : 0.127192, loss_ce: 0.008557, loss_dice: 0.245828
[09:16:45.267] TRAIN: iteration 9570 : loss : 0.184830, loss_ce: 0.004041, loss_dice: 0.365618
[09:16:45.474] TRAIN: iteration 9571 : loss : 0.192605, loss_ce: 0.004336, loss_dice: 0.380874
[09:16:45.685] TRAIN: iteration 9572 : loss : 0.057739, loss_ce: 0.002265, loss_dice: 0.113214
[09:16:45.895] TRAIN: iteration 9573 : loss : 0.229527, loss_ce: 0.003993, loss_dice: 0.455062
[09:16:46.103] TRAIN: iteration 9574 : loss : 0.119688, loss_ce: 0.005776, loss_dice: 0.233599
[09:16:46.312] TRAIN: iteration 9575 : loss : 0.118627, loss_ce: 0.004715, loss_dice: 0.232539
[09:16:46.523] TRAIN: iteration 9576 : loss : 0.230598, loss_ce: 0.004263, loss_dice: 0.456933
[09:16:46.732] TRAIN: iteration 9577 : loss : 0.102735, loss_ce: 0.002825, loss_dice: 0.202645
[09:16:46.941] TRAIN: iteration 9578 : loss : 0.153029, loss_ce: 0.007168, loss_dice: 0.298890
[09:16:47.150] TRAIN: iteration 9579 : loss : 0.126811, loss_ce: 0.002582, loss_dice: 0.251041
[09:16:47.358] TRAIN: iteration 9580 : loss : 0.120338, loss_ce: 0.004649, loss_dice: 0.236027
[09:16:47.596] TRAIN: iteration 9581 : loss : 0.215023, loss_ce: 0.011360, loss_dice: 0.418686
[09:16:47.803] TRAIN: iteration 9582 : loss : 0.083704, loss_ce: 0.003982, loss_dice: 0.163426
[09:16:48.015] TRAIN: iteration 9583 : loss : 0.099139, loss_ce: 0.002996, loss_dice: 0.195283
[09:16:48.223] TRAIN: iteration 9584 : loss : 0.142339, loss_ce: 0.008724, loss_dice: 0.275955
[09:16:48.430] TRAIN: iteration 9585 : loss : 0.047583, loss_ce: 0.002183, loss_dice: 0.092983
[09:16:51.204] TRAIN: iteration 9586 : loss : 0.230277, loss_ce: 0.005176, loss_dice: 0.455378
[09:16:51.413] TRAIN: iteration 9587 : loss : 0.251631, loss_ce: 0.003073, loss_dice: 0.500189
[09:16:51.619] TRAIN: iteration 9588 : loss : 0.109850, loss_ce: 0.013789, loss_dice: 0.205912
[09:16:51.828] TRAIN: iteration 9589 : loss : 0.173310, loss_ce: 0.004048, loss_dice: 0.342573
[09:16:52.036] TRAIN: iteration 9590 : loss : 0.246924, loss_ce: 0.002427, loss_dice: 0.491421
[09:16:52.245] TRAIN: iteration 9591 : loss : 0.069667, loss_ce: 0.003022, loss_dice: 0.136313
[09:16:52.454] TRAIN: iteration 9592 : loss : 0.252247, loss_ce: 0.005050, loss_dice: 0.499443
[09:16:52.662] TRAIN: iteration 9593 : loss : 0.058790, loss_ce: 0.003591, loss_dice: 0.113988
[09:16:52.870] TRAIN: iteration 9594 : loss : 0.121741, loss_ce: 0.010172, loss_dice: 0.233311
[09:16:53.077] TRAIN: iteration 9595 : loss : 0.252001, loss_ce: 0.003816, loss_dice: 0.500186
[09:16:53.285] TRAIN: iteration 9596 : loss : 0.077087, loss_ce: 0.004589, loss_dice: 0.149585
[09:16:53.496] TRAIN: iteration 9597 : loss : 0.178195, loss_ce: 0.006781, loss_dice: 0.349609
[09:16:53.708] TRAIN: iteration 9598 : loss : 0.132589, loss_ce: 0.005779, loss_dice: 0.259399
[09:16:53.917] TRAIN: iteration 9599 : loss : 0.177440, loss_ce: 0.024807, loss_dice: 0.330074
[09:16:54.126] TRAIN: iteration 9600 : loss : 0.164463, loss_ce: 0.003894, loss_dice: 0.325031
[09:16:54.367] TRAIN: iteration 9601 : loss : 0.229202, loss_ce: 0.008170, loss_dice: 0.450234
[09:16:54.580] TRAIN: iteration 9602 : loss : 0.106138, loss_ce: 0.003734, loss_dice: 0.208542
[09:16:54.789] TRAIN: iteration 9603 : loss : 0.180032, loss_ce: 0.004402, loss_dice: 0.355662
[09:16:54.999] TRAIN: iteration 9604 : loss : 0.099402, loss_ce: 0.002933, loss_dice: 0.195870
[09:16:55.214] TRAIN: iteration 9605 : loss : 0.048906, loss_ce: 0.001933, loss_dice: 0.095879
[09:16:55.428] TRAIN: iteration 9606 : loss : 0.229358, loss_ce: 0.004673, loss_dice: 0.454042
[09:16:55.637] TRAIN: iteration 9607 : loss : 0.074654, loss_ce: 0.003363, loss_dice: 0.145946
[09:16:55.845] TRAIN: iteration 9608 : loss : 0.090827, loss_ce: 0.006802, loss_dice: 0.174852
[09:16:56.053] TRAIN: iteration 9609 : loss : 0.208821, loss_ce: 0.003369, loss_dice: 0.414273
[09:16:56.271] TRAIN: iteration 9610 : loss : 0.064180, loss_ce: 0.005095, loss_dice: 0.123266
[09:16:56.478] TRAIN: iteration 9611 : loss : 0.166908, loss_ce: 0.034396, loss_dice: 0.299420
[09:16:57.277] TRAIN: iteration 9612 : loss : 0.244020, loss_ce: 0.002948, loss_dice: 0.485092
[09:16:57.486] TRAIN: iteration 9613 : loss : 0.255624, loss_ce: 0.012827, loss_dice: 0.498420
[09:16:57.693] TRAIN: iteration 9614 : loss : 0.113831, loss_ce: 0.004873, loss_dice: 0.222788
[09:16:57.905] TRAIN: iteration 9615 : loss : 0.252950, loss_ce: 0.006107, loss_dice: 0.499793
[09:16:58.118] TRAIN: iteration 9616 : loss : 0.123097, loss_ce: 0.008040, loss_dice: 0.238154
[09:16:58.326] TRAIN: iteration 9617 : loss : 0.199932, loss_ce: 0.007554, loss_dice: 0.392310
[09:16:58.539] TRAIN: iteration 9618 : loss : 0.166036, loss_ce: 0.006622, loss_dice: 0.325449
[09:16:58.748] TRAIN: iteration 9619 : loss : 0.251493, loss_ce: 0.005048, loss_dice: 0.497938
[09:16:58.955] TRAIN: iteration 9620 : loss : 0.164774, loss_ce: 0.007700, loss_dice: 0.321849
[09:16:59.194] TRAIN: iteration 9621 : loss : 0.076522, loss_ce: 0.009705, loss_dice: 0.143339
[09:16:59.402] TRAIN: iteration 9622 : loss : 0.216923, loss_ce: 0.004710, loss_dice: 0.429136
[09:16:59.610] TRAIN: iteration 9623 : loss : 0.163956, loss_ce: 0.005302, loss_dice: 0.322609
[09:16:59.826] TRAIN: iteration 9624 : loss : 0.062083, loss_ce: 0.003747, loss_dice: 0.120418
[09:17:00.036] TRAIN: iteration 9625 : loss : 0.144828, loss_ce: 0.010118, loss_dice: 0.279539
[09:17:00.246] TRAIN: iteration 9626 : loss : 0.229850, loss_ce: 0.007936, loss_dice: 0.451764
[09:17:00.454] TRAIN: iteration 9627 : loss : 0.252263, loss_ce: 0.004263, loss_dice: 0.500263
[09:17:01.975] TRAIN: iteration 9628 : loss : 0.105252, loss_ce: 0.004489, loss_dice: 0.206015
[09:17:02.182] TRAIN: iteration 9629 : loss : 0.252142, loss_ce: 0.004100, loss_dice: 0.500183
[09:17:02.389] TRAIN: iteration 9630 : loss : 0.211190, loss_ce: 0.007104, loss_dice: 0.415275
[09:17:02.597] TRAIN: iteration 9631 : loss : 0.252285, loss_ce: 0.004348, loss_dice: 0.500222
[09:17:02.805] TRAIN: iteration 9632 : loss : 0.249887, loss_ce: 0.027183, loss_dice: 0.472591
[09:17:03.012] TRAIN: iteration 9633 : loss : 0.078393, loss_ce: 0.004668, loss_dice: 0.152119
[09:17:03.220] TRAIN: iteration 9634 : loss : 0.134993, loss_ce: 0.004466, loss_dice: 0.265520
[09:17:03.433] TRAIN: iteration 9635 : loss : 0.251732, loss_ce: 0.003984, loss_dice: 0.499480
[09:17:03.832] TRAIN: iteration 9636 : loss : 0.238429, loss_ce: 0.007523, loss_dice: 0.469336
[09:17:04.040] TRAIN: iteration 9637 : loss : 0.091692, loss_ce: 0.008295, loss_dice: 0.175088
[09:17:04.256] TRAIN: iteration 9638 : loss : 0.252109, loss_ce: 0.004008, loss_dice: 0.500211
[09:17:04.464] TRAIN: iteration 9639 : loss : 0.156496, loss_ce: 0.005082, loss_dice: 0.307909
[09:17:04.672] TRAIN: iteration 9640 : loss : 0.252098, loss_ce: 0.004019, loss_dice: 0.500177
[09:17:04.901] TRAIN: iteration 9641 : loss : 0.054372, loss_ce: 0.002617, loss_dice: 0.106126
[09:17:05.111] TRAIN: iteration 9642 : loss : 0.152304, loss_ce: 0.006782, loss_dice: 0.297827
[09:17:05.327] TRAIN: iteration 9643 : loss : 0.174881, loss_ce: 0.009190, loss_dice: 0.340572
[09:17:06.659] TRAIN: iteration 9644 : loss : 0.074517, loss_ce: 0.005149, loss_dice: 0.143886
[09:17:06.866] TRAIN: iteration 9645 : loss : 0.232895, loss_ce: 0.008471, loss_dice: 0.457318
[09:17:07.076] TRAIN: iteration 9646 : loss : 0.248599, loss_ce: 0.004470, loss_dice: 0.492729
[09:17:07.376] TRAIN: iteration 9647 : loss : 0.087222, loss_ce: 0.011391, loss_dice: 0.163053
[09:17:07.584] TRAIN: iteration 9648 : loss : 0.251531, loss_ce: 0.006271, loss_dice: 0.496791
[09:17:07.791] TRAIN: iteration 9649 : loss : 0.130709, loss_ce: 0.005703, loss_dice: 0.255716
[09:17:07.999] TRAIN: iteration 9650 : loss : 0.083033, loss_ce: 0.010326, loss_dice: 0.155741
[09:17:08.215] TRAIN: iteration 9651 : loss : 0.071080, loss_ce: 0.006572, loss_dice: 0.135588
[09:17:09.667] TRAIN: iteration 9652 : loss : 0.173760, loss_ce: 0.010746, loss_dice: 0.336775
[09:17:09.873] TRAIN: iteration 9653 : loss : 0.088907, loss_ce: 0.005714, loss_dice: 0.172100
[09:17:10.083] TRAIN: iteration 9654 : loss : 0.230993, loss_ce: 0.006232, loss_dice: 0.455754
[09:17:10.544] TRAIN: iteration 9655 : loss : 0.096994, loss_ce: 0.004686, loss_dice: 0.189301
[09:17:10.758] TRAIN: iteration 9656 : loss : 0.117738, loss_ce: 0.005376, loss_dice: 0.230100
[09:17:10.965] TRAIN: iteration 9657 : loss : 0.178826, loss_ce: 0.005317, loss_dice: 0.352336
[09:17:11.173] TRAIN: iteration 9658 : loss : 0.063617, loss_ce: 0.005807, loss_dice: 0.121428
[09:17:11.381] TRAIN: iteration 9659 : loss : 0.252439, loss_ce: 0.004657, loss_dice: 0.500222
[09:17:12.006] TRAIN: iteration 9660 : loss : 0.049815, loss_ce: 0.003279, loss_dice: 0.096350
[09:17:12.006] NaN or Inf found in input tensor.
[09:17:12.219] TRAIN: iteration 9661 : loss : 0.048318, loss_ce: 0.002738, loss_dice: 0.093898
[09:17:12.427] TRAIN: iteration 9662 : loss : 0.097478, loss_ce: 0.008538, loss_dice: 0.186419
[09:17:14.588] TRAIN: iteration 9663 : loss : 0.123223, loss_ce: 0.010924, loss_dice: 0.235522
[09:17:14.794] TRAIN: iteration 9664 : loss : 0.251658, loss_ce: 0.003183, loss_dice: 0.500134
[09:17:15.001] TRAIN: iteration 9665 : loss : 0.252372, loss_ce: 0.004446, loss_dice: 0.500299
[09:17:15.210] TRAIN: iteration 9666 : loss : 0.115984, loss_ce: 0.010528, loss_dice: 0.221440
[09:17:15.418] TRAIN: iteration 9667 : loss : 0.251790, loss_ce: 0.003384, loss_dice: 0.500197
[09:17:15.654] TRAIN: iteration 9668 : loss : 0.123191, loss_ce: 0.005383, loss_dice: 0.240998
[09:17:15.862] TRAIN: iteration 9669 : loss : 0.085057, loss_ce: 0.004463, loss_dice: 0.165650
[09:17:16.070] TRAIN: iteration 9670 : loss : 0.116918, loss_ce: 0.009451, loss_dice: 0.224384
[09:17:18.056] TRAIN: iteration 9671 : loss : 0.135560, loss_ce: 0.004216, loss_dice: 0.266904
[09:17:18.265] TRAIN: iteration 9672 : loss : 0.251706, loss_ce: 0.003224, loss_dice: 0.500187
[09:17:18.472] TRAIN: iteration 9673 : loss : 0.129406, loss_ce: 0.003099, loss_dice: 0.255713
[09:17:18.679] TRAIN: iteration 9674 : loss : 0.098799, loss_ce: 0.003232, loss_dice: 0.194367
[09:17:18.888] TRAIN: iteration 9675 : loss : 0.252356, loss_ce: 0.005984, loss_dice: 0.498727
[09:17:19.097] TRAIN: iteration 9676 : loss : 0.209248, loss_ce: 0.002622, loss_dice: 0.415873
[09:17:19.307] TRAIN: iteration 9677 : loss : 0.092493, loss_ce: 0.003277, loss_dice: 0.181710
[09:17:19.515] TRAIN: iteration 9678 : loss : 0.115171, loss_ce: 0.003196, loss_dice: 0.227147
[09:17:19.895] TRAIN: iteration 9679 : loss : 0.128461, loss_ce: 0.006163, loss_dice: 0.250759
[09:17:20.106] TRAIN: iteration 9680 : loss : 0.122925, loss_ce: 0.005926, loss_dice: 0.239923
[09:17:20.374] TRAIN: iteration 9681 : loss : 0.057177, loss_ce: 0.003041, loss_dice: 0.111314
[09:17:20.582] TRAIN: iteration 9682 : loss : 0.133840, loss_ce: 0.023196, loss_dice: 0.244485
[09:17:20.792] TRAIN: iteration 9683 : loss : 0.250897, loss_ce: 0.006082, loss_dice: 0.495712
[09:17:20.999] TRAIN: iteration 9684 : loss : 0.232258, loss_ce: 0.002508, loss_dice: 0.462008
[09:17:21.209] TRAIN: iteration 9685 : loss : 0.232655, loss_ce: 0.003460, loss_dice: 0.461850
[09:17:21.423] TRAIN: iteration 9686 : loss : 0.123695, loss_ce: 0.003895, loss_dice: 0.243495
[09:17:21.632] TRAIN: iteration 9687 : loss : 0.101313, loss_ce: 0.004081, loss_dice: 0.198545
[09:17:21.839] TRAIN: iteration 9688 : loss : 0.096421, loss_ce: 0.003775, loss_dice: 0.189067
[09:17:22.050] TRAIN: iteration 9689 : loss : 0.130449, loss_ce: 0.005701, loss_dice: 0.255196
[09:17:22.263] TRAIN: iteration 9690 : loss : 0.136402, loss_ce: 0.006445, loss_dice: 0.266360
[09:17:22.470] TRAIN: iteration 9691 : loss : 0.183758, loss_ce: 0.004719, loss_dice: 0.362798
[09:17:22.679] TRAIN: iteration 9692 : loss : 0.238338, loss_ce: 0.004800, loss_dice: 0.471876
[09:17:22.886] TRAIN: iteration 9693 : loss : 0.178330, loss_ce: 0.014634, loss_dice: 0.342027
[09:17:23.178] TRAIN: iteration 9694 : loss : 0.230359, loss_ce: 0.003090, loss_dice: 0.457628
[09:17:23.855] TRAIN: iteration 9695 : loss : 0.073444, loss_ce: 0.003390, loss_dice: 0.143497
[09:17:25.072] TRAIN: iteration 9696 : loss : 0.196583, loss_ce: 0.003270, loss_dice: 0.389897
[09:17:25.279] TRAIN: iteration 9697 : loss : 0.073851, loss_ce: 0.002618, loss_dice: 0.145084
[09:17:25.675] TRAIN: iteration 9698 : loss : 0.206950, loss_ce: 0.003277, loss_dice: 0.410622
[09:17:25.883] TRAIN: iteration 9699 : loss : 0.230043, loss_ce: 0.002996, loss_dice: 0.457090
[09:17:26.091] TRAIN: iteration 9700 : loss : 0.237398, loss_ce: 0.010223, loss_dice: 0.464573
[09:17:26.091] NaN or Inf found in input tensor.
[09:17:26.306] TRAIN: iteration 9701 : loss : 0.202307, loss_ce: 0.009534, loss_dice: 0.395080
[09:17:26.514] TRAIN: iteration 9702 : loss : 0.159842, loss_ce: 0.015203, loss_dice: 0.304481
[09:17:26.845] TRAIN: iteration 9703 : loss : 0.111098, loss_ce: 0.002677, loss_dice: 0.219520
[09:17:27.053] TRAIN: iteration 9704 : loss : 0.146426, loss_ce: 0.002852, loss_dice: 0.289999
[09:17:27.261] TRAIN: iteration 9705 : loss : 0.160509, loss_ce: 0.007204, loss_dice: 0.313815
[09:17:28.453] TRAIN: iteration 9706 : loss : 0.080406, loss_ce: 0.001999, loss_dice: 0.158814
[09:17:28.660] TRAIN: iteration 9707 : loss : 0.219118, loss_ce: 0.013752, loss_dice: 0.424484
[09:17:28.868] TRAIN: iteration 9708 : loss : 0.258930, loss_ce: 0.046089, loss_dice: 0.471771
[09:17:29.077] TRAIN: iteration 9709 : loss : 0.126182, loss_ce: 0.004551, loss_dice: 0.247813
[09:17:29.285] TRAIN: iteration 9710 : loss : 0.124053, loss_ce: 0.003782, loss_dice: 0.244324
[09:17:29.601] TRAIN: iteration 9711 : loss : 0.131426, loss_ce: 0.004416, loss_dice: 0.258436
[09:17:29.811] TRAIN: iteration 9712 : loss : 0.111282, loss_ce: 0.003652, loss_dice: 0.218913
[09:17:30.018] TRAIN: iteration 9713 : loss : 0.251429, loss_ce: 0.002754, loss_dice: 0.500104
[09:17:32.781] TRAIN: iteration 9714 : loss : 0.082407, loss_ce: 0.004758, loss_dice: 0.160055
[09:17:32.990] TRAIN: iteration 9715 : loss : 0.073827, loss_ce: 0.003241, loss_dice: 0.144413
[09:17:33.199] TRAIN: iteration 9716 : loss : 0.088372, loss_ce: 0.003375, loss_dice: 0.173368
[09:17:33.408] TRAIN: iteration 9717 : loss : 0.236729, loss_ce: 0.008096, loss_dice: 0.465362
[09:17:33.615] TRAIN: iteration 9718 : loss : 0.252097, loss_ce: 0.003963, loss_dice: 0.500231
[09:17:33.825] TRAIN: iteration 9719 : loss : 0.198180, loss_ce: 0.028677, loss_dice: 0.367684
[09:17:34.032] TRAIN: iteration 9720 : loss : 0.108081, loss_ce: 0.004316, loss_dice: 0.211847
[09:17:34.268] TRAIN: iteration 9721 : loss : 0.133425, loss_ce: 0.004395, loss_dice: 0.262455
[09:17:34.475] TRAIN: iteration 9722 : loss : 0.234333, loss_ce: 0.005773, loss_dice: 0.462893
[09:17:34.683] TRAIN: iteration 9723 : loss : 0.110087, loss_ce: 0.004483, loss_dice: 0.215692
[09:17:34.891] TRAIN: iteration 9724 : loss : 0.241561, loss_ce: 0.006623, loss_dice: 0.476498
[09:17:35.099] TRAIN: iteration 9725 : loss : 0.196827, loss_ce: 0.004432, loss_dice: 0.389221
[09:17:35.310] TRAIN: iteration 9726 : loss : 0.195566, loss_ce: 0.004680, loss_dice: 0.386452
[09:17:35.516] TRAIN: iteration 9727 : loss : 0.222562, loss_ce: 0.007925, loss_dice: 0.437199
[09:17:35.724] TRAIN: iteration 9728 : loss : 0.157502, loss_ce: 0.009398, loss_dice: 0.305605
[09:17:35.931] TRAIN: iteration 9729 : loss : 0.188389, loss_ce: 0.004758, loss_dice: 0.372019
[09:17:37.092] TRAIN: iteration 9730 : loss : 0.158324, loss_ce: 0.007783, loss_dice: 0.308865
[09:17:37.301] TRAIN: iteration 9731 : loss : 0.252784, loss_ce: 0.006397, loss_dice: 0.499171
[09:17:37.508] TRAIN: iteration 9732 : loss : 0.144044, loss_ce: 0.009421, loss_dice: 0.278668
[09:17:37.721] TRAIN: iteration 9733 : loss : 0.249880, loss_ce: 0.005334, loss_dice: 0.494426
[09:17:37.928] TRAIN: iteration 9734 : loss : 0.109016, loss_ce: 0.004964, loss_dice: 0.213069
[09:17:38.136] TRAIN: iteration 9735 : loss : 0.212920, loss_ce: 0.006414, loss_dice: 0.419426
[09:17:38.352] TRAIN: iteration 9736 : loss : 0.151089, loss_ce: 0.002823, loss_dice: 0.299355
[09:17:38.560] TRAIN: iteration 9737 : loss : 0.172384, loss_ce: 0.003892, loss_dice: 0.340877
[09:17:40.163] TRAIN: iteration 9738 : loss : 0.202599, loss_ce: 0.006610, loss_dice: 0.398588
[09:17:40.377] TRAIN: iteration 9739 : loss : 0.240145, loss_ce: 0.003882, loss_dice: 0.476408
[09:17:40.585] TRAIN: iteration 9740 : loss : 0.153488, loss_ce: 0.005901, loss_dice: 0.301075
[09:17:40.821] TRAIN: iteration 9741 : loss : 0.107605, loss_ce: 0.004910, loss_dice: 0.210300
[09:17:41.028] TRAIN: iteration 9742 : loss : 0.219316, loss_ce: 0.003383, loss_dice: 0.435249
[09:17:41.236] TRAIN: iteration 9743 : loss : 0.175612, loss_ce: 0.010533, loss_dice: 0.340691
[09:17:41.445] TRAIN: iteration 9744 : loss : 0.244606, loss_ce: 0.004808, loss_dice: 0.484405
[09:17:41.652] TRAIN: iteration 9745 : loss : 0.202121, loss_ce: 0.003944, loss_dice: 0.400298
[09:17:43.147] TRAIN: iteration 9746 : loss : 0.069758, loss_ce: 0.002408, loss_dice: 0.137107
[09:17:43.358] TRAIN: iteration 9747 : loss : 0.157478, loss_ce: 0.008506, loss_dice: 0.306450
[09:17:43.565] TRAIN: iteration 9748 : loss : 0.141426, loss_ce: 0.007603, loss_dice: 0.275249
[09:17:43.773] TRAIN: iteration 9749 : loss : 0.136367, loss_ce: 0.005108, loss_dice: 0.267625
[09:17:43.980] TRAIN: iteration 9750 : loss : 0.096022, loss_ce: 0.007729, loss_dice: 0.184316
[09:17:44.188] TRAIN: iteration 9751 : loss : 0.153609, loss_ce: 0.004163, loss_dice: 0.303054
[09:17:44.398] TRAIN: iteration 9752 : loss : 0.146871, loss_ce: 0.003765, loss_dice: 0.289977
[09:17:44.607] TRAIN: iteration 9753 : loss : 0.256325, loss_ce: 0.021464, loss_dice: 0.491187
[09:17:45.104] TRAIN: iteration 9754 : loss : 0.119126, loss_ce: 0.006138, loss_dice: 0.232114
[09:17:45.311] TRAIN: iteration 9755 : loss : 0.165320, loss_ce: 0.004257, loss_dice: 0.326383
[09:17:45.577] TRAIN: iteration 9756 : loss : 0.189928, loss_ce: 0.035469, loss_dice: 0.344386
[09:17:45.786] TRAIN: iteration 9757 : loss : 0.141335, loss_ce: 0.003495, loss_dice: 0.279174
[09:17:45.994] TRAIN: iteration 9758 : loss : 0.145465, loss_ce: 0.003190, loss_dice: 0.287739
[09:17:46.204] TRAIN: iteration 9759 : loss : 0.063733, loss_ce: 0.005269, loss_dice: 0.122196
[09:17:46.413] TRAIN: iteration 9760 : loss : 0.158840, loss_ce: 0.003807, loss_dice: 0.313873
[09:17:46.413] NaN or Inf found in input tensor.
[09:17:46.628] TRAIN: iteration 9761 : loss : 0.251625, loss_ce: 0.003079, loss_dice: 0.500172
[09:17:48.176] TRAIN: iteration 9762 : loss : 0.202561, loss_ce: 0.008213, loss_dice: 0.396908
[09:17:48.385] TRAIN: iteration 9763 : loss : 0.063924, loss_ce: 0.002331, loss_dice: 0.125517
[09:17:48.593] TRAIN: iteration 9764 : loss : 0.243653, loss_ce: 0.003482, loss_dice: 0.483825
[09:17:48.800] TRAIN: iteration 9765 : loss : 0.057393, loss_ce: 0.003661, loss_dice: 0.111125
[09:17:49.007] TRAIN: iteration 9766 : loss : 0.232701, loss_ce: 0.004130, loss_dice: 0.461272
[09:17:49.215] TRAIN: iteration 9767 : loss : 0.250473, loss_ce: 0.001989, loss_dice: 0.498957
[09:17:49.426] TRAIN: iteration 9768 : loss : 0.251428, loss_ce: 0.002737, loss_dice: 0.500119
[09:17:49.635] TRAIN: iteration 9769 : loss : 0.035711, loss_ce: 0.002159, loss_dice: 0.069262
[09:17:50.276] TRAIN: iteration 9770 : loss : 0.066282, loss_ce: 0.002461, loss_dice: 0.130104
[09:17:50.485] TRAIN: iteration 9771 : loss : 0.134087, loss_ce: 0.007726, loss_dice: 0.260447
[09:17:50.701] TRAIN: iteration 9772 : loss : 0.194621, loss_ce: 0.003056, loss_dice: 0.386185
[09:17:50.909] TRAIN: iteration 9773 : loss : 0.064716, loss_ce: 0.006957, loss_dice: 0.122476
[09:17:51.118] TRAIN: iteration 9774 : loss : 0.253707, loss_ce: 0.007311, loss_dice: 0.500104
[09:17:51.324] TRAIN: iteration 9775 : loss : 0.084532, loss_ce: 0.002301, loss_dice: 0.166763
[09:17:51.532] TRAIN: iteration 9776 : loss : 0.076577, loss_ce: 0.001929, loss_dice: 0.151226
[09:17:51.742] TRAIN: iteration 9777 : loss : 0.251153, loss_ce: 0.002211, loss_dice: 0.500095
[09:17:51.949] TRAIN: iteration 9778 : loss : 0.067012, loss_ce: 0.001863, loss_dice: 0.132161
[09:17:52.158] TRAIN: iteration 9779 : loss : 0.083145, loss_ce: 0.002872, loss_dice: 0.163417
[09:17:55.085] TRAIN: iteration 9780 : loss : 0.154584, loss_ce: 0.004334, loss_dice: 0.304834
[09:17:55.324] TRAIN: iteration 9781 : loss : 0.093512, loss_ce: 0.003689, loss_dice: 0.183335
[09:17:55.534] TRAIN: iteration 9782 : loss : 0.102691, loss_ce: 0.003997, loss_dice: 0.201386
[09:17:55.742] TRAIN: iteration 9783 : loss : 0.078558, loss_ce: 0.005964, loss_dice: 0.151152
[09:17:55.952] TRAIN: iteration 9784 : loss : 0.166483, loss_ce: 0.004340, loss_dice: 0.328627
[09:17:56.160] TRAIN: iteration 9785 : loss : 0.094209, loss_ce: 0.002665, loss_dice: 0.185752
[09:17:56.367] TRAIN: iteration 9786 : loss : 0.092823, loss_ce: 0.007389, loss_dice: 0.178257
[09:17:56.574] TRAIN: iteration 9787 : loss : 0.244257, loss_ce: 0.003182, loss_dice: 0.485332
[09:17:56.782] TRAIN: iteration 9788 : loss : 0.030290, loss_ce: 0.002586, loss_dice: 0.057993
[09:17:56.992] TRAIN: iteration 9789 : loss : 0.251194, loss_ce: 0.002725, loss_dice: 0.499664
[09:17:57.209] TRAIN: iteration 9790 : loss : 0.097152, loss_ce: 0.004708, loss_dice: 0.189596
[09:17:57.420] TRAIN: iteration 9791 : loss : 0.080149, loss_ce: 0.003871, loss_dice: 0.156426
[09:17:57.628] TRAIN: iteration 9792 : loss : 0.211122, loss_ce: 0.003493, loss_dice: 0.418751
[09:17:57.836] TRAIN: iteration 9793 : loss : 0.211568, loss_ce: 0.007236, loss_dice: 0.415899
[09:17:58.044] TRAIN: iteration 9794 : loss : 0.149940, loss_ce: 0.020640, loss_dice: 0.279239
[09:17:58.251] TRAIN: iteration 9795 : loss : 0.253772, loss_ce: 0.006979, loss_dice: 0.500565
[09:17:59.077] TRAIN: iteration 9796 : loss : 0.095234, loss_ce: 0.003908, loss_dice: 0.186560
[09:17:59.292] TRAIN: iteration 9797 : loss : 0.185298, loss_ce: 0.013093, loss_dice: 0.357503
[09:17:59.498] TRAIN: iteration 9798 : loss : 0.234500, loss_ce: 0.002698, loss_dice: 0.466303
[09:17:59.706] TRAIN: iteration 9799 : loss : 0.116694, loss_ce: 0.002767, loss_dice: 0.230622
[09:17:59.913] TRAIN: iteration 9800 : loss : 0.138044, loss_ce: 0.003007, loss_dice: 0.273081
[09:18:00.155] TRAIN: iteration 9801 : loss : 0.246290, loss_ce: 0.005995, loss_dice: 0.486584
[09:18:00.362] TRAIN: iteration 9802 : loss : 0.057376, loss_ce: 0.002647, loss_dice: 0.112106
[09:18:00.569] TRAIN: iteration 9803 : loss : 0.214454, loss_ce: 0.012505, loss_dice: 0.416403
[09:18:01.495] TRAIN: iteration 9804 : loss : 0.252269, loss_ce: 0.004212, loss_dice: 0.500325
[09:18:01.702] TRAIN: iteration 9805 : loss : 0.080707, loss_ce: 0.002334, loss_dice: 0.159079
[09:18:01.917] TRAIN: iteration 9806 : loss : 0.121452, loss_ce: 0.007118, loss_dice: 0.235786
[09:18:02.123] TRAIN: iteration 9807 : loss : 0.133079, loss_ce: 0.014666, loss_dice: 0.251492
[09:18:02.332] TRAIN: iteration 9808 : loss : 0.087379, loss_ce: 0.002739, loss_dice: 0.172020
[09:18:02.538] TRAIN: iteration 9809 : loss : 0.117684, loss_ce: 0.004348, loss_dice: 0.231020
[09:18:02.746] TRAIN: iteration 9810 : loss : 0.133689, loss_ce: 0.002720, loss_dice: 0.264657
[09:18:02.956] TRAIN: iteration 9811 : loss : 0.252220, loss_ce: 0.004125, loss_dice: 0.500316
[09:18:04.269] TRAIN: iteration 9812 : loss : 0.101756, loss_ce: 0.004360, loss_dice: 0.199152
[09:18:04.477] TRAIN: iteration 9813 : loss : 0.149229, loss_ce: 0.007195, loss_dice: 0.291263
[09:18:04.696] TRAIN: iteration 9814 : loss : 0.100464, loss_ce: 0.004025, loss_dice: 0.196902
[09:18:04.903] TRAIN: iteration 9815 : loss : 0.133696, loss_ce: 0.007613, loss_dice: 0.259780
[09:18:05.117] TRAIN: iteration 9816 : loss : 0.191143, loss_ce: 0.005559, loss_dice: 0.376727
[09:18:05.325] TRAIN: iteration 9817 : loss : 0.251966, loss_ce: 0.003661, loss_dice: 0.500271
[09:18:05.539] TRAIN: iteration 9818 : loss : 0.251069, loss_ce: 0.002046, loss_dice: 0.500093
[09:18:05.747] TRAIN: iteration 9819 : loss : 0.163103, loss_ce: 0.008605, loss_dice: 0.317602
[09:18:06.380] TRAIN: iteration 9820 : loss : 0.242801, loss_ce: 0.003149, loss_dice: 0.482453
[09:18:06.620] TRAIN: iteration 9821 : loss : 0.116989, loss_ce: 0.003079, loss_dice: 0.230900
[09:18:06.827] TRAIN: iteration 9822 : loss : 0.220962, loss_ce: 0.002786, loss_dice: 0.439137
[09:18:08.944] TRAIN: iteration 9823 : loss : 0.223288, loss_ce: 0.008790, loss_dice: 0.437786
[09:18:09.153] TRAIN: iteration 9824 : loss : 0.251343, loss_ce: 0.002554, loss_dice: 0.500132
[09:18:09.362] TRAIN: iteration 9825 : loss : 0.198240, loss_ce: 0.003340, loss_dice: 0.393141
[09:18:09.570] TRAIN: iteration 9826 : loss : 0.220352, loss_ce: 0.012817, loss_dice: 0.427886
[09:18:09.777] TRAIN: iteration 9827 : loss : 0.183858, loss_ce: 0.011032, loss_dice: 0.356684
[09:18:09.985] TRAIN: iteration 9828 : loss : 0.193296, loss_ce: 0.005189, loss_dice: 0.381403
[09:18:10.193] TRAIN: iteration 9829 : loss : 0.251047, loss_ce: 0.005216, loss_dice: 0.496879
[09:18:10.415] TRAIN: iteration 9830 : loss : 0.160251, loss_ce: 0.006892, loss_dice: 0.313611
[09:18:10.622] TRAIN: iteration 9831 : loss : 0.184407, loss_ce: 0.007525, loss_dice: 0.361288
[09:18:10.830] TRAIN: iteration 9832 : loss : 0.059648, loss_ce: 0.002763, loss_dice: 0.116533
[09:18:11.291] TRAIN: iteration 9833 : loss : 0.149438, loss_ce: 0.005489, loss_dice: 0.293387
[09:18:11.500] TRAIN: iteration 9834 : loss : 0.202564, loss_ce: 0.004945, loss_dice: 0.400183
[09:18:11.709] TRAIN: iteration 9835 : loss : 0.228761, loss_ce: 0.006856, loss_dice: 0.450665
[09:18:11.918] TRAIN: iteration 9836 : loss : 0.252385, loss_ce: 0.004525, loss_dice: 0.500246
[09:18:12.125] TRAIN: iteration 9837 : loss : 0.098410, loss_ce: 0.005095, loss_dice: 0.191725
[09:18:12.333] TRAIN: iteration 9838 : loss : 0.138718, loss_ce: 0.014739, loss_dice: 0.262698
[09:18:12.546] TRAIN: iteration 9839 : loss : 0.067114, loss_ce: 0.002794, loss_dice: 0.131433
[09:18:12.755] TRAIN: iteration 9840 : loss : 0.075055, loss_ce: 0.005054, loss_dice: 0.145055
[09:18:13.002] TRAIN: iteration 9841 : loss : 0.249874, loss_ce: 0.023780, loss_dice: 0.475969
[09:18:13.213] TRAIN: iteration 9842 : loss : 0.242665, loss_ce: 0.006671, loss_dice: 0.478659
[09:18:13.427] TRAIN: iteration 9843 : loss : 0.130061, loss_ce: 0.004432, loss_dice: 0.255690
[09:18:13.633] TRAIN: iteration 9844 : loss : 0.251608, loss_ce: 0.008432, loss_dice: 0.494784
[09:18:13.845] TRAIN: iteration 9845 : loss : 0.252607, loss_ce: 0.004874, loss_dice: 0.500339
[09:18:17.619] TRAIN: iteration 9846 : loss : 0.220928, loss_ce: 0.006542, loss_dice: 0.435314
[09:18:17.826] TRAIN: iteration 9847 : loss : 0.190695, loss_ce: 0.004308, loss_dice: 0.377082
[09:18:18.033] TRAIN: iteration 9848 : loss : 0.128424, loss_ce: 0.005094, loss_dice: 0.251753
[09:18:18.242] TRAIN: iteration 9849 : loss : 0.177050, loss_ce: 0.004466, loss_dice: 0.349634
[09:18:18.449] TRAIN: iteration 9850 : loss : 0.125716, loss_ce: 0.002891, loss_dice: 0.248542
[09:18:18.662] TRAIN: iteration 9851 : loss : 0.225241, loss_ce: 0.004942, loss_dice: 0.445541
[09:18:18.870] TRAIN: iteration 9852 : loss : 0.252191, loss_ce: 0.004087, loss_dice: 0.500295
[09:18:19.078] TRAIN: iteration 9853 : loss : 0.168241, loss_ce: 0.003439, loss_dice: 0.333044
[09:18:20.559] TRAIN: iteration 9854 : loss : 0.052582, loss_ce: 0.003061, loss_dice: 0.102103
[09:18:20.766] TRAIN: iteration 9855 : loss : 0.191805, loss_ce: 0.036998, loss_dice: 0.346613
[09:18:20.976] TRAIN: iteration 9856 : loss : 0.251201, loss_ce: 0.002297, loss_dice: 0.500104
[09:18:21.191] TRAIN: iteration 9857 : loss : 0.223314, loss_ce: 0.007666, loss_dice: 0.438962
[09:18:21.399] TRAIN: iteration 9858 : loss : 0.181609, loss_ce: 0.022364, loss_dice: 0.340853
[09:18:21.608] TRAIN: iteration 9859 : loss : 0.251413, loss_ce: 0.005519, loss_dice: 0.497307
[09:18:21.816] TRAIN: iteration 9860 : loss : 0.093636, loss_ce: 0.002483, loss_dice: 0.184789
[09:18:22.058] TRAIN: iteration 9861 : loss : 0.144946, loss_ce: 0.002896, loss_dice: 0.286995
[09:18:22.266] TRAIN: iteration 9862 : loss : 0.205734, loss_ce: 0.003588, loss_dice: 0.407880
[09:18:22.474] TRAIN: iteration 9863 : loss : 0.074434, loss_ce: 0.003792, loss_dice: 0.145076
[09:18:22.683] TRAIN: iteration 9864 : loss : 0.252065, loss_ce: 0.003840, loss_dice: 0.500290
[09:18:22.890] TRAIN: iteration 9865 : loss : 0.031399, loss_ce: 0.001641, loss_dice: 0.061157
[09:18:23.097] TRAIN: iteration 9866 : loss : 0.251196, loss_ce: 0.002254, loss_dice: 0.500139
[09:18:23.306] TRAIN: iteration 9867 : loss : 0.207724, loss_ce: 0.003627, loss_dice: 0.411820
[09:18:23.514] TRAIN: iteration 9868 : loss : 0.125275, loss_ce: 0.002272, loss_dice: 0.248278
[09:18:23.724] TRAIN: iteration 9869 : loss : 0.200195, loss_ce: 0.016892, loss_dice: 0.383498
[09:18:24.754] TRAIN: iteration 9870 : loss : 0.222727, loss_ce: 0.003544, loss_dice: 0.441910
[09:18:24.960] TRAIN: iteration 9871 : loss : 0.171110, loss_ce: 0.007939, loss_dice: 0.334281
[09:18:25.169] TRAIN: iteration 9872 : loss : 0.087736, loss_ce: 0.003319, loss_dice: 0.172154
[09:18:25.378] TRAIN: iteration 9873 : loss : 0.123885, loss_ce: 0.005361, loss_dice: 0.242410
[09:18:25.585] TRAIN: iteration 9874 : loss : 0.128881, loss_ce: 0.004367, loss_dice: 0.253395
[09:18:25.792] TRAIN: iteration 9875 : loss : 0.207129, loss_ce: 0.007573, loss_dice: 0.406684
[09:18:26.000] TRAIN: iteration 9876 : loss : 0.094380, loss_ce: 0.004423, loss_dice: 0.184338
[09:18:26.206] TRAIN: iteration 9877 : loss : 0.169405, loss_ce: 0.015744, loss_dice: 0.323066
[09:18:28.870] TRAIN: iteration 9878 : loss : 0.199926, loss_ce: 0.004484, loss_dice: 0.395368
[09:18:29.083] TRAIN: iteration 9879 : loss : 0.218932, loss_ce: 0.008671, loss_dice: 0.429193
[09:18:29.290] TRAIN: iteration 9880 : loss : 0.237415, loss_ce: 0.004483, loss_dice: 0.470347
[09:18:29.534] TRAIN: iteration 9881 : loss : 0.137169, loss_ce: 0.004861, loss_dice: 0.269477
[09:18:29.740] TRAIN: iteration 9882 : loss : 0.183867, loss_ce: 0.007221, loss_dice: 0.360512
[09:18:29.948] TRAIN: iteration 9883 : loss : 0.143200, loss_ce: 0.008829, loss_dice: 0.277572
[09:18:30.182] TRAIN: iteration 9884 : loss : 0.063768, loss_ce: 0.003401, loss_dice: 0.124134
[09:18:30.391] TRAIN: iteration 9885 : loss : 0.096174, loss_ce: 0.007674, loss_dice: 0.184673
[09:18:32.623] TRAIN: iteration 9886 : loss : 0.113087, loss_ce: 0.011604, loss_dice: 0.214571
[09:18:32.833] TRAIN: iteration 9887 : loss : 0.168937, loss_ce: 0.005589, loss_dice: 0.332285
[09:18:33.039] TRAIN: iteration 9888 : loss : 0.221849, loss_ce: 0.008136, loss_dice: 0.435562
[09:18:33.248] TRAIN: iteration 9889 : loss : 0.212805, loss_ce: 0.005806, loss_dice: 0.419804
[09:18:33.456] TRAIN: iteration 9890 : loss : 0.146808, loss_ce: 0.004767, loss_dice: 0.288849
[09:18:33.670] TRAIN: iteration 9891 : loss : 0.147955, loss_ce: 0.010623, loss_dice: 0.285287
[09:18:33.879] TRAIN: iteration 9892 : loss : 0.173665, loss_ce: 0.022650, loss_dice: 0.324680
[09:18:34.088] TRAIN: iteration 9893 : loss : 0.138559, loss_ce: 0.006306, loss_dice: 0.270811
[09:18:35.792] TRAIN: iteration 9894 : loss : 0.245911, loss_ce: 0.005304, loss_dice: 0.486519
[09:18:36.038] TRAIN: iteration 9895 : loss : 0.203841, loss_ce: 0.006241, loss_dice: 0.401440
[09:18:36.246] TRAIN: iteration 9896 : loss : 0.103288, loss_ce: 0.005557, loss_dice: 0.201018
[09:18:36.452] TRAIN: iteration 9897 : loss : 0.171519, loss_ce: 0.006442, loss_dice: 0.336595
[09:18:36.660] TRAIN: iteration 9898 : loss : 0.127941, loss_ce: 0.004314, loss_dice: 0.251568
[09:18:36.869] TRAIN: iteration 9899 : loss : 0.098130, loss_ce: 0.005005, loss_dice: 0.191254
[09:18:37.082] TRAIN: iteration 9900 : loss : 0.231324, loss_ce: 0.007185, loss_dice: 0.455464
[09:18:37.320] TRAIN: iteration 9901 : loss : 0.069439, loss_ce: 0.003546, loss_dice: 0.135332
[09:18:39.135] TRAIN: iteration 9902 : loss : 0.089405, loss_ce: 0.005242, loss_dice: 0.173567
[09:18:39.342] TRAIN: iteration 9903 : loss : 0.080634, loss_ce: 0.002734, loss_dice: 0.158535
[09:18:39.549] TRAIN: iteration 9904 : loss : 0.067564, loss_ce: 0.003480, loss_dice: 0.131648
[09:18:39.762] TRAIN: iteration 9905 : loss : 0.251697, loss_ce: 0.003221, loss_dice: 0.500172
[09:18:39.970] TRAIN: iteration 9906 : loss : 0.251426, loss_ce: 0.002721, loss_dice: 0.500130
[09:18:40.179] TRAIN: iteration 9907 : loss : 0.199330, loss_ce: 0.003851, loss_dice: 0.394809
[09:18:40.388] TRAIN: iteration 9908 : loss : 0.125133, loss_ce: 0.015757, loss_dice: 0.234510
[09:18:40.597] TRAIN: iteration 9909 : loss : 0.226296, loss_ce: 0.011901, loss_dice: 0.440692
[09:18:41.917] TRAIN: iteration 9910 : loss : 0.118338, loss_ce: 0.004470, loss_dice: 0.232206
[09:18:42.128] TRAIN: iteration 9911 : loss : 0.243196, loss_ce: 0.003381, loss_dice: 0.483010
[09:18:42.337] TRAIN: iteration 9912 : loss : 0.240314, loss_ce: 0.003489, loss_dice: 0.477139
[09:18:42.545] TRAIN: iteration 9913 : loss : 0.103902, loss_ce: 0.007531, loss_dice: 0.200274
[09:18:42.752] TRAIN: iteration 9914 : loss : 0.063664, loss_ce: 0.002696, loss_dice: 0.124632
[09:18:42.960] TRAIN: iteration 9915 : loss : 0.090927, loss_ce: 0.004658, loss_dice: 0.177195
[09:18:43.168] TRAIN: iteration 9916 : loss : 0.251715, loss_ce: 0.004668, loss_dice: 0.498761
[09:18:43.375] TRAIN: iteration 9917 : loss : 0.121124, loss_ce: 0.003914, loss_dice: 0.238334
[09:18:45.717] TRAIN: iteration 9918 : loss : 0.122031, loss_ce: 0.004636, loss_dice: 0.239426
[09:18:45.925] TRAIN: iteration 9919 : loss : 0.218723, loss_ce: 0.005552, loss_dice: 0.431893
[09:18:46.133] TRAIN: iteration 9920 : loss : 0.252183, loss_ce: 0.004657, loss_dice: 0.499709
[09:18:46.372] TRAIN: iteration 9921 : loss : 0.249002, loss_ce: 0.006116, loss_dice: 0.491889
[09:18:46.578] TRAIN: iteration 9922 : loss : 0.064848, loss_ce: 0.002398, loss_dice: 0.127299
[09:18:46.786] TRAIN: iteration 9923 : loss : 0.252019, loss_ce: 0.003778, loss_dice: 0.500259
[09:18:46.993] TRAIN: iteration 9924 : loss : 0.251857, loss_ce: 0.003513, loss_dice: 0.500201
[09:18:47.202] TRAIN: iteration 9925 : loss : 0.138036, loss_ce: 0.017367, loss_dice: 0.258705
[09:18:48.472] TRAIN: iteration 9926 : loss : 0.242727, loss_ce: 0.007747, loss_dice: 0.477706
[09:18:48.702] TRAIN: iteration 9927 : loss : 0.180678, loss_ce: 0.003539, loss_dice: 0.357816
[09:18:48.909] TRAIN: iteration 9928 : loss : 0.155366, loss_ce: 0.007389, loss_dice: 0.303343
[09:18:49.120] TRAIN: iteration 9929 : loss : 0.109255, loss_ce: 0.007033, loss_dice: 0.211476
[09:18:49.327] TRAIN: iteration 9930 : loss : 0.189628, loss_ce: 0.005073, loss_dice: 0.374183
[09:18:49.533] TRAIN: iteration 9931 : loss : 0.127713, loss_ce: 0.004804, loss_dice: 0.250622
[09:18:49.740] TRAIN: iteration 9932 : loss : 0.170850, loss_ce: 0.006625, loss_dice: 0.335075
[09:18:49.951] TRAIN: iteration 9933 : loss : 0.086830, loss_ce: 0.005339, loss_dice: 0.168322
[09:18:51.998] TRAIN: iteration 9934 : loss : 0.240290, loss_ce: 0.003833, loss_dice: 0.476747
[09:18:52.209] TRAIN: iteration 9935 : loss : 0.254787, loss_ce: 0.009686, loss_dice: 0.499889
[09:18:52.418] TRAIN: iteration 9936 : loss : 0.075425, loss_ce: 0.007095, loss_dice: 0.143755
[09:18:52.629] TRAIN: iteration 9937 : loss : 0.252209, loss_ce: 0.004163, loss_dice: 0.500256
[09:18:52.839] TRAIN: iteration 9938 : loss : 0.130187, loss_ce: 0.004928, loss_dice: 0.255446
[09:18:53.048] TRAIN: iteration 9939 : loss : 0.245230, loss_ce: 0.004003, loss_dice: 0.486458
[09:18:53.255] TRAIN: iteration 9940 : loss : 0.238190, loss_ce: 0.006437, loss_dice: 0.469943
[09:18:53.504] TRAIN: iteration 9941 : loss : 0.222205, loss_ce: 0.003173, loss_dice: 0.441237
[09:18:53.717] TRAIN: iteration 9942 : loss : 0.251977, loss_ce: 0.005278, loss_dice: 0.498675
[09:18:53.924] TRAIN: iteration 9943 : loss : 0.194254, loss_ce: 0.003280, loss_dice: 0.385227
[09:18:54.147] TRAIN: iteration 9944 : loss : 0.252695, loss_ce: 0.005045, loss_dice: 0.500344
[09:18:54.356] TRAIN: iteration 9945 : loss : 0.181156, loss_ce: 0.005655, loss_dice: 0.356658
[09:18:54.563] TRAIN: iteration 9946 : loss : 0.133689, loss_ce: 0.007499, loss_dice: 0.259879
[09:18:54.771] TRAIN: iteration 9947 : loss : 0.134525, loss_ce: 0.003297, loss_dice: 0.265753
[09:18:54.980] TRAIN: iteration 9948 : loss : 0.179914, loss_ce: 0.004269, loss_dice: 0.355558
[09:18:55.188] TRAIN: iteration 9949 : loss : 0.134665, loss_ce: 0.005142, loss_dice: 0.264189
[09:18:55.399] TRAIN: iteration 9950 : loss : 0.250901, loss_ce: 0.001761, loss_dice: 0.500041
[09:18:56.925] TRAIN: iteration 9951 : loss : 0.251707, loss_ce: 0.003428, loss_dice: 0.499987
[09:18:57.134] TRAIN: iteration 9952 : loss : 0.251240, loss_ce: 0.002347, loss_dice: 0.500133
[09:18:57.341] TRAIN: iteration 9953 : loss : 0.134765, loss_ce: 0.005081, loss_dice: 0.264450
[09:18:57.552] TRAIN: iteration 9954 : loss : 0.196624, loss_ce: 0.022696, loss_dice: 0.370551
[09:18:57.761] TRAIN: iteration 9955 : loss : 0.228771, loss_ce: 0.026776, loss_dice: 0.430766
[09:18:57.970] TRAIN: iteration 9956 : loss : 0.103931, loss_ce: 0.005724, loss_dice: 0.202139
[09:18:58.185] TRAIN: iteration 9957 : loss : 0.251513, loss_ce: 0.002860, loss_dice: 0.500166
[09:18:58.392] TRAIN: iteration 9958 : loss : 0.225268, loss_ce: 0.002933, loss_dice: 0.447603
[09:18:59.235] TRAIN: iteration 9959 : loss : 0.233822, loss_ce: 0.012551, loss_dice: 0.455093
[09:18:59.443] TRAIN: iteration 9960 : loss : 0.160358, loss_ce: 0.007179, loss_dice: 0.313538
[09:18:59.677] TRAIN: iteration 9961 : loss : 0.181599, loss_ce: 0.010939, loss_dice: 0.352259
[09:18:59.886] TRAIN: iteration 9962 : loss : 0.125405, loss_ce: 0.007808, loss_dice: 0.243002
[09:19:00.095] TRAIN: iteration 9963 : loss : 0.143133, loss_ce: 0.005180, loss_dice: 0.281087
[09:19:00.303] TRAIN: iteration 9964 : loss : 0.062319, loss_ce: 0.006635, loss_dice: 0.118003
[09:19:00.628] TRAIN: iteration 9965 : loss : 0.119916, loss_ce: 0.005370, loss_dice: 0.234462
[09:19:00.836] TRAIN: iteration 9966 : loss : 0.252012, loss_ce: 0.003818, loss_dice: 0.500206
[09:19:02.413] TRAIN: iteration 9967 : loss : 0.150374, loss_ce: 0.005350, loss_dice: 0.295399
[09:19:02.622] TRAIN: iteration 9968 : loss : 0.088737, loss_ce: 0.005756, loss_dice: 0.171718
[09:19:02.832] TRAIN: iteration 9969 : loss : 0.167792, loss_ce: 0.004058, loss_dice: 0.331527
[09:19:03.041] TRAIN: iteration 9970 : loss : 0.056430, loss_ce: 0.007084, loss_dice: 0.105775
[09:19:03.249] TRAIN: iteration 9971 : loss : 0.232553, loss_ce: 0.006594, loss_dice: 0.458513
[09:19:03.457] TRAIN: iteration 9972 : loss : 0.166235, loss_ce: 0.003468, loss_dice: 0.329001
[09:19:06.197] TRAIN: iteration 9973 : loss : 0.059918, loss_ce: 0.006768, loss_dice: 0.113067
[09:19:06.412] TRAIN: iteration 9974 : loss : 0.241709, loss_ce: 0.006714, loss_dice: 0.476704
[09:19:07.471] TRAIN: iteration 9975 : loss : 0.247601, loss_ce: 0.005407, loss_dice: 0.489794
[09:19:07.679] TRAIN: iteration 9976 : loss : 0.205336, loss_ce: 0.006511, loss_dice: 0.404161
[09:19:07.894] TRAIN: iteration 9977 : loss : 0.170926, loss_ce: 0.005519, loss_dice: 0.336333
[09:19:08.101] TRAIN: iteration 9978 : loss : 0.105275, loss_ce: 0.003170, loss_dice: 0.207380
[09:19:08.309] TRAIN: iteration 9979 : loss : 0.129562, loss_ce: 0.009745, loss_dice: 0.249380
[09:19:08.538] TRAIN: iteration 9980 : loss : 0.252661, loss_ce: 0.004980, loss_dice: 0.500341
[09:19:08.780] TRAIN: iteration 9981 : loss : 0.100071, loss_ce: 0.004133, loss_dice: 0.196010
[09:19:08.988] TRAIN: iteration 9982 : loss : 0.247221, loss_ce: 0.008845, loss_dice: 0.485596
[09:19:11.988] TRAIN: iteration 9983 : loss : 0.110477, loss_ce: 0.003366, loss_dice: 0.217589
[09:19:12.195] TRAIN: iteration 9984 : loss : 0.193489, loss_ce: 0.004042, loss_dice: 0.382936
[09:19:12.409] TRAIN: iteration 9985 : loss : 0.118773, loss_ce: 0.002949, loss_dice: 0.234598
[09:19:12.616] TRAIN: iteration 9986 : loss : 0.251947, loss_ce: 0.003636, loss_dice: 0.500258
[09:19:12.822] TRAIN: iteration 9987 : loss : 0.160890, loss_ce: 0.021093, loss_dice: 0.300688
[09:19:13.031] TRAIN: iteration 9988 : loss : 0.137369, loss_ce: 0.003580, loss_dice: 0.271157
[09:19:13.238] TRAIN: iteration 9989 : loss : 0.062777, loss_ce: 0.002413, loss_dice: 0.123141
[09:19:13.453] TRAIN: iteration 9990 : loss : 0.095814, loss_ce: 0.009735, loss_dice: 0.181894
[09:19:16.326] TRAIN: iteration 9991 : loss : 0.100684, loss_ce: 0.009779, loss_dice: 0.191589
[09:19:16.532] TRAIN: iteration 9992 : loss : 0.140932, loss_ce: 0.002186, loss_dice: 0.279678
[09:19:16.741] TRAIN: iteration 9993 : loss : 0.177488, loss_ce: 0.003643, loss_dice: 0.351333
[09:19:16.947] TRAIN: iteration 9994 : loss : 0.120049, loss_ce: 0.006573, loss_dice: 0.233524
[09:19:17.155] TRAIN: iteration 9995 : loss : 0.133504, loss_ce: 0.002730, loss_dice: 0.264278
[09:19:17.361] TRAIN: iteration 9996 : loss : 0.107407, loss_ce: 0.004726, loss_dice: 0.210089
[09:19:17.568] TRAIN: iteration 9997 : loss : 0.219782, loss_ce: 0.003834, loss_dice: 0.435731
[09:19:17.781] TRAIN: iteration 9998 : loss : 0.102715, loss_ce: 0.002682, loss_dice: 0.202747
[09:19:19.045] TRAIN: iteration 9999 : loss : 0.247688, loss_ce: 0.002824, loss_dice: 0.492551
[09:19:19.254] TRAIN: iteration 10000 : loss : 0.070049, loss_ce: 0.002396, loss_dice: 0.137702
[09:19:19.491] TRAIN: iteration 10001 : loss : 0.251239, loss_ce: 0.002363, loss_dice: 0.500115
[09:19:19.701] TRAIN: iteration 10002 : loss : 0.247314, loss_ce: 0.003447, loss_dice: 0.491180
[09:19:19.911] TRAIN: iteration 10003 : loss : 0.139146, loss_ce: 0.003497, loss_dice: 0.274796
[09:19:20.119] TRAIN: iteration 10004 : loss : 0.231222, loss_ce: 0.002997, loss_dice: 0.459447
[09:19:20.326] TRAIN: iteration 10005 : loss : 0.251356, loss_ce: 0.002559, loss_dice: 0.500152
[09:19:20.536] TRAIN: iteration 10006 : loss : 0.040697, loss_ce: 0.002363, loss_dice: 0.079031
[09:19:24.594] TRAIN: iteration 10007 : loss : 0.150534, loss_ce: 0.005250, loss_dice: 0.295818
[09:19:24.801] TRAIN: iteration 10008 : loss : 0.088846, loss_ce: 0.008340, loss_dice: 0.169351
[09:19:25.011] TRAIN: iteration 10009 : loss : 0.158363, loss_ce: 0.008679, loss_dice: 0.308046
[09:19:25.218] TRAIN: iteration 10010 : loss : 0.224881, loss_ce: 0.006497, loss_dice: 0.443265
[09:19:25.427] TRAIN: iteration 10011 : loss : 0.077454, loss_ce: 0.003943, loss_dice: 0.150965
[09:19:25.635] TRAIN: iteration 10012 : loss : 0.087307, loss_ce: 0.007027, loss_dice: 0.167586
[09:19:25.842] TRAIN: iteration 10013 : loss : 0.099238, loss_ce: 0.003468, loss_dice: 0.195008
[09:19:26.122] TRAIN: iteration 10014 : loss : 0.053763, loss_ce: 0.004656, loss_dice: 0.102871
[09:19:29.482] TRAIN: iteration 10015 : loss : 0.071650, loss_ce: 0.007722, loss_dice: 0.135579
[09:19:29.689] TRAIN: iteration 10016 : loss : 0.252987, loss_ce: 0.005567, loss_dice: 0.500407
[09:19:29.899] TRAIN: iteration 10017 : loss : 0.109206, loss_ce: 0.004978, loss_dice: 0.213434
[09:19:30.115] TRAIN: iteration 10018 : loss : 0.159713, loss_ce: 0.005792, loss_dice: 0.313633
[09:19:30.330] TRAIN: iteration 10019 : loss : 0.138496, loss_ce: 0.005633, loss_dice: 0.271360
[09:19:30.537] TRAIN: iteration 10020 : loss : 0.247436, loss_ce: 0.004747, loss_dice: 0.490124
[09:19:30.781] TRAIN: iteration 10021 : loss : 0.050047, loss_ce: 0.003928, loss_dice: 0.096166
[09:19:30.990] TRAIN: iteration 10022 : loss : 0.250262, loss_ce: 0.006027, loss_dice: 0.494497
[09:19:32.857] TRAIN: iteration 10023 : loss : 0.213858, loss_ce: 0.004257, loss_dice: 0.423459
[09:19:33.065] TRAIN: iteration 10024 : loss : 0.136917, loss_ce: 0.016898, loss_dice: 0.256936
[09:19:33.272] TRAIN: iteration 10025 : loss : 0.047603, loss_ce: 0.005310, loss_dice: 0.089897
[09:19:33.484] TRAIN: iteration 10026 : loss : 0.230974, loss_ce: 0.005340, loss_dice: 0.456608
[09:19:33.692] TRAIN: iteration 10027 : loss : 0.250911, loss_ce: 0.006082, loss_dice: 0.495740
[09:19:33.900] TRAIN: iteration 10028 : loss : 0.252009, loss_ce: 0.003803, loss_dice: 0.500216
[09:19:34.108] TRAIN: iteration 10029 : loss : 0.130122, loss_ce: 0.004039, loss_dice: 0.256206
[09:19:34.321] TRAIN: iteration 10030 : loss : 0.164473, loss_ce: 0.003592, loss_dice: 0.325354
[09:19:35.943] TRAIN: iteration 10031 : loss : 0.252216, loss_ce: 0.004171, loss_dice: 0.500260
[09:19:36.150] TRAIN: iteration 10032 : loss : 0.144500, loss_ce: 0.006449, loss_dice: 0.282551
[09:19:36.359] TRAIN: iteration 10033 : loss : 0.245660, loss_ce: 0.012790, loss_dice: 0.478529
[09:19:36.568] TRAIN: iteration 10034 : loss : 0.101248, loss_ce: 0.013912, loss_dice: 0.188585
[09:19:36.811] TRAIN: iteration 10035 : loss : 0.077706, loss_ce: 0.003055, loss_dice: 0.152357
[09:19:37.020] TRAIN: iteration 10036 : loss : 0.251657, loss_ce: 0.003111, loss_dice: 0.500203
[09:19:37.227] TRAIN: iteration 10037 : loss : 0.040248, loss_ce: 0.002689, loss_dice: 0.077807
[09:19:37.435] TRAIN: iteration 10038 : loss : 0.088516, loss_ce: 0.004050, loss_dice: 0.172983
[09:19:38.336] TRAIN: iteration 10039 : loss : 0.138386, loss_ce: 0.003745, loss_dice: 0.273027
[09:19:38.543] TRAIN: iteration 10040 : loss : 0.066768, loss_ce: 0.004035, loss_dice: 0.129501
[09:19:38.778] TRAIN: iteration 10041 : loss : 0.151344, loss_ce: 0.004966, loss_dice: 0.297722
[09:19:38.985] TRAIN: iteration 10042 : loss : 0.097383, loss_ce: 0.006754, loss_dice: 0.188013
[09:19:40.099] TRAIN: iteration 10043 : loss : 0.178221, loss_ce: 0.003742, loss_dice: 0.352699
[09:19:40.306] TRAIN: iteration 10044 : loss : 0.100144, loss_ce: 0.005589, loss_dice: 0.194699
[09:19:40.513] TRAIN: iteration 10045 : loss : 0.140565, loss_ce: 0.004198, loss_dice: 0.276932
[09:19:40.719] TRAIN: iteration 10046 : loss : 0.065093, loss_ce: 0.006489, loss_dice: 0.123696
[09:19:42.440] TRAIN: iteration 10047 : loss : 0.157885, loss_ce: 0.004976, loss_dice: 0.310794
[09:19:42.648] TRAIN: iteration 10048 : loss : 0.130111, loss_ce: 0.003478, loss_dice: 0.256745
[09:19:42.855] TRAIN: iteration 10049 : loss : 0.252929, loss_ce: 0.005476, loss_dice: 0.500383
[09:19:43.064] TRAIN: iteration 10050 : loss : 0.044783, loss_ce: 0.007848, loss_dice: 0.081717
[09:19:47.344] TRAIN: iteration 10051 : loss : 0.120285, loss_ce: 0.010749, loss_dice: 0.229820
[09:19:47.551] TRAIN: iteration 10052 : loss : 0.107655, loss_ce: 0.011699, loss_dice: 0.203610
[09:19:47.758] TRAIN: iteration 10053 : loss : 0.224856, loss_ce: 0.005005, loss_dice: 0.444707
[09:19:47.970] TRAIN: iteration 10054 : loss : 0.254292, loss_ce: 0.008625, loss_dice: 0.499960
[09:19:48.182] TRAIN: iteration 10055 : loss : 0.060423, loss_ce: 0.007871, loss_dice: 0.112974
[09:19:48.389] TRAIN: iteration 10056 : loss : 0.254463, loss_ce: 0.008284, loss_dice: 0.500642
[09:19:48.602] TRAIN: iteration 10057 : loss : 0.076016, loss_ce: 0.006680, loss_dice: 0.145353
[09:19:48.813] TRAIN: iteration 10058 : loss : 0.205749, loss_ce: 0.013486, loss_dice: 0.398012
[09:19:49.449] TRAIN: iteration 10059 : loss : 0.230608, loss_ce: 0.005487, loss_dice: 0.455729
[09:19:49.656] TRAIN: iteration 10060 : loss : 0.171959, loss_ce: 0.018231, loss_dice: 0.325686
[09:19:49.894] TRAIN: iteration 10061 : loss : 0.246129, loss_ce: 0.010238, loss_dice: 0.482019
[09:19:50.101] TRAIN: iteration 10062 : loss : 0.195997, loss_ce: 0.006979, loss_dice: 0.385015
[09:19:50.420] TRAIN: iteration 10063 : loss : 0.234321, loss_ce: 0.009299, loss_dice: 0.459343
[09:19:50.626] TRAIN: iteration 10064 : loss : 0.252302, loss_ce: 0.004360, loss_dice: 0.500244
[09:19:50.833] TRAIN: iteration 10065 : loss : 0.075809, loss_ce: 0.008238, loss_dice: 0.143380
[09:19:51.040] TRAIN: iteration 10066 : loss : 0.104304, loss_ce: 0.004988, loss_dice: 0.203621
[09:19:54.385] TRAIN: iteration 10067 : loss : 0.251755, loss_ce: 0.003308, loss_dice: 0.500201
[09:19:54.591] TRAIN: iteration 10068 : loss : 0.253211, loss_ce: 0.005981, loss_dice: 0.500441
[09:19:54.798] TRAIN: iteration 10069 : loss : 0.184125, loss_ce: 0.009362, loss_dice: 0.358889
[09:19:55.012] TRAIN: iteration 10070 : loss : 0.252641, loss_ce: 0.004972, loss_dice: 0.500309
[09:19:55.218] TRAIN: iteration 10071 : loss : 0.133692, loss_ce: 0.004433, loss_dice: 0.262952
[09:19:55.425] TRAIN: iteration 10072 : loss : 0.179957, loss_ce: 0.003555, loss_dice: 0.356359
[09:19:55.631] TRAIN: iteration 10073 : loss : 0.174510, loss_ce: 0.008593, loss_dice: 0.340427
[09:19:55.840] TRAIN: iteration 10074 : loss : 0.216657, loss_ce: 0.006140, loss_dice: 0.427173
[09:19:59.590] TRAIN: iteration 10075 : loss : 0.099796, loss_ce: 0.012649, loss_dice: 0.186943
[09:19:59.796] TRAIN: iteration 10076 : loss : 0.074378, loss_ce: 0.008752, loss_dice: 0.140004
[09:20:00.003] TRAIN: iteration 10077 : loss : 0.182913, loss_ce: 0.008109, loss_dice: 0.357716
[09:20:00.210] TRAIN: iteration 10078 : loss : 0.132109, loss_ce: 0.004006, loss_dice: 0.260212
[09:20:01.888] TRAIN: iteration 10079 : loss : 0.079441, loss_ce: 0.004186, loss_dice: 0.154697
[09:20:02.945] TRAIN: iteration 10080 : loss : 0.188736, loss_ce: 0.014301, loss_dice: 0.363172
[09:20:03.264] TRAIN: iteration 10081 : loss : 0.108700, loss_ce: 0.003755, loss_dice: 0.213646
[09:20:03.473] TRAIN: iteration 10082 : loss : 0.251887, loss_ce: 0.003553, loss_dice: 0.500221
[09:20:03.679] TRAIN: iteration 10083 : loss : 0.094605, loss_ce: 0.002531, loss_dice: 0.186679
[09:20:03.887] TRAIN: iteration 10084 : loss : 0.139239, loss_ce: 0.004634, loss_dice: 0.273845
[09:20:04.404] TRAIN: iteration 10085 : loss : 0.104926, loss_ce: 0.014233, loss_dice: 0.195619
[09:20:04.612] TRAIN: iteration 10086 : loss : 0.148217, loss_ce: 0.003167, loss_dice: 0.293267
[09:20:05.669] TRAIN: iteration 10087 : loss : 0.108732, loss_ce: 0.004347, loss_dice: 0.213117
[09:20:09.658] TRAIN: iteration 10088 : loss : 0.252047, loss_ce: 0.003843, loss_dice: 0.500250
[09:20:09.867] TRAIN: iteration 10089 : loss : 0.251651, loss_ce: 0.003099, loss_dice: 0.500204
[09:20:10.077] TRAIN: iteration 10090 : loss : 0.124020, loss_ce: 0.013398, loss_dice: 0.234643
[09:20:10.283] TRAIN: iteration 10091 : loss : 0.073963, loss_ce: 0.004725, loss_dice: 0.143201
[09:20:10.498] TRAIN: iteration 10092 : loss : 0.043804, loss_ce: 0.002774, loss_dice: 0.084833
[09:20:10.707] TRAIN: iteration 10093 : loss : 0.087242, loss_ce: 0.013541, loss_dice: 0.160942
[09:20:10.917] TRAIN: iteration 10094 : loss : 0.080434, loss_ce: 0.001873, loss_dice: 0.158994
[09:20:11.125] TRAIN: iteration 10095 : loss : 0.250175, loss_ce: 0.004343, loss_dice: 0.496007
[09:20:12.688] TRAIN: iteration 10096 : loss : 0.241068, loss_ce: 0.011876, loss_dice: 0.470259
[09:20:12.895] TRAIN: iteration 10097 : loss : 0.089955, loss_ce: 0.006612, loss_dice: 0.173297
[09:20:13.103] TRAIN: iteration 10098 : loss : 0.102999, loss_ce: 0.004455, loss_dice: 0.201542
[09:20:13.312] TRAIN: iteration 10099 : loss : 0.169365, loss_ce: 0.010887, loss_dice: 0.327842
[09:20:13.518] TRAIN: iteration 10100 : loss : 0.099108, loss_ce: 0.005224, loss_dice: 0.192993
[09:20:14.939] TRAIN: iteration 10101 : loss : 0.080762, loss_ce: 0.003650, loss_dice: 0.157873
[09:20:15.146] TRAIN: iteration 10102 : loss : 0.248289, loss_ce: 0.003593, loss_dice: 0.492984
[09:20:15.361] TRAIN: iteration 10103 : loss : 0.250469, loss_ce: 0.000916, loss_dice: 0.500022
[09:20:17.732] TRAIN: iteration 10104 : loss : 0.249154, loss_ce: 0.005447, loss_dice: 0.492861
[09:20:17.939] TRAIN: iteration 10105 : loss : 0.070976, loss_ce: 0.003351, loss_dice: 0.138600
[09:20:18.145] TRAIN: iteration 10106 : loss : 0.065633, loss_ce: 0.003226, loss_dice: 0.128041
[09:20:18.351] TRAIN: iteration 10107 : loss : 0.252372, loss_ce: 0.004409, loss_dice: 0.500335
[09:20:18.557] TRAIN: iteration 10108 : loss : 0.052955, loss_ce: 0.002771, loss_dice: 0.103140
[09:20:18.765] TRAIN: iteration 10109 : loss : 0.249388, loss_ce: 0.005613, loss_dice: 0.493163
[09:20:19.250] TRAIN: iteration 10110 : loss : 0.253021, loss_ce: 0.005606, loss_dice: 0.500435
[09:20:19.458] TRAIN: iteration 10111 : loss : 0.076077, loss_ce: 0.003933, loss_dice: 0.148221
[09:20:23.917] TRAIN: iteration 10112 : loss : 0.129388, loss_ce: 0.006898, loss_dice: 0.251879
[09:20:26.012] TRAIN: iteration 10113 : loss : 0.101180, loss_ce: 0.007684, loss_dice: 0.194675
[09:20:26.220] TRAIN: iteration 10114 : loss : 0.104747, loss_ce: 0.005599, loss_dice: 0.203896
[09:20:26.430] TRAIN: iteration 10115 : loss : 0.047703, loss_ce: 0.002383, loss_dice: 0.093022
[09:20:26.638] TRAIN: iteration 10116 : loss : 0.199637, loss_ce: 0.002306, loss_dice: 0.396968
[09:20:26.845] TRAIN: iteration 10117 : loss : 0.076398, loss_ce: 0.006021, loss_dice: 0.146775
[09:20:27.053] TRAIN: iteration 10118 : loss : 0.110067, loss_ce: 0.006576, loss_dice: 0.213557
[09:20:27.263] TRAIN: iteration 10119 : loss : 0.251557, loss_ce: 0.003032, loss_dice: 0.500081
[09:20:28.570] TRAIN: iteration 10120 : loss : 0.046188, loss_ce: 0.003174, loss_dice: 0.089201
[09:20:28.804] TRAIN: iteration 10121 : loss : 0.228857, loss_ce: 0.006977, loss_dice: 0.450736
[09:20:29.012] TRAIN: iteration 10122 : loss : 0.085352, loss_ce: 0.004662, loss_dice: 0.166043
[09:20:29.218] TRAIN: iteration 10123 : loss : 0.096296, loss_ce: 0.005643, loss_dice: 0.186949
[09:20:29.424] TRAIN: iteration 10124 : loss : 0.218529, loss_ce: 0.008458, loss_dice: 0.428600
[09:20:29.631] TRAIN: iteration 10125 : loss : 0.183033, loss_ce: 0.007813, loss_dice: 0.358253
[09:20:29.849] TRAIN: iteration 10126 : loss : 0.211739, loss_ce: 0.004661, loss_dice: 0.418818
[09:20:30.056] TRAIN: iteration 10127 : loss : 0.072598, loss_ce: 0.002531, loss_dice: 0.142664
[09:20:33.920] TRAIN: iteration 10128 : loss : 0.095748, loss_ce: 0.004075, loss_dice: 0.187421
[09:20:34.131] TRAIN: iteration 10129 : loss : 0.250328, loss_ce: 0.002441, loss_dice: 0.498215
[09:20:34.344] TRAIN: iteration 10130 : loss : 0.047153, loss_ce: 0.002292, loss_dice: 0.092014
[09:20:34.552] TRAIN: iteration 10131 : loss : 0.198212, loss_ce: 0.003366, loss_dice: 0.393058
[09:20:34.761] TRAIN: iteration 10132 : loss : 0.210936, loss_ce: 0.003528, loss_dice: 0.418345
[09:20:34.972] TRAIN: iteration 10133 : loss : 0.147908, loss_ce: 0.017461, loss_dice: 0.278354
[09:20:35.179] TRAIN: iteration 10134 : loss : 0.055529, loss_ce: 0.003948, loss_dice: 0.107109
[09:20:35.392] TRAIN: iteration 10135 : loss : 0.068124, loss_ce: 0.002019, loss_dice: 0.134230
[09:20:37.654] TRAIN: iteration 10136 : loss : 0.133352, loss_ce: 0.005965, loss_dice: 0.260739
[09:20:40.331] TRAIN: iteration 10137 : loss : 0.170333, loss_ce: 0.006437, loss_dice: 0.334229
[09:20:40.539] TRAIN: iteration 10138 : loss : 0.122390, loss_ce: 0.002644, loss_dice: 0.242136
[09:20:40.753] TRAIN: iteration 10139 : loss : 0.251385, loss_ce: 0.002669, loss_dice: 0.500100
[09:20:40.961] TRAIN: iteration 10140 : loss : 0.172892, loss_ce: 0.003465, loss_dice: 0.342319
[09:20:41.196] TRAIN: iteration 10141 : loss : 0.251100, loss_ce: 0.002127, loss_dice: 0.500073
[09:20:41.403] TRAIN: iteration 10142 : loss : 0.125457, loss_ce: 0.002497, loss_dice: 0.248417
[09:20:41.611] TRAIN: iteration 10143 : loss : 0.250858, loss_ce: 0.001661, loss_dice: 0.500055
[09:20:42.732] TRAIN: iteration 10144 : loss : 0.215139, loss_ce: 0.007538, loss_dice: 0.422740
[09:20:45.273] TRAIN: iteration 10145 : loss : 0.251307, loss_ce: 0.003238, loss_dice: 0.499376
[09:20:45.486] TRAIN: iteration 10146 : loss : 0.099718, loss_ce: 0.003574, loss_dice: 0.195861
[09:20:45.693] TRAIN: iteration 10147 : loss : 0.083074, loss_ce: 0.003389, loss_dice: 0.162758
[09:20:45.903] TRAIN: iteration 10148 : loss : 0.089027, loss_ce: 0.008403, loss_dice: 0.169652
[09:20:46.111] TRAIN: iteration 10149 : loss : 0.251016, loss_ce: 0.002255, loss_dice: 0.499777
[09:20:46.319] TRAIN: iteration 10150 : loss : 0.060970, loss_ce: 0.002112, loss_dice: 0.119827
[09:20:46.526] TRAIN: iteration 10151 : loss : 0.250008, loss_ce: 0.002735, loss_dice: 0.497282
[09:20:46.736] TRAIN: iteration 10152 : loss : 0.250880, loss_ce: 0.003045, loss_dice: 0.498716
[09:20:50.829] TRAIN: iteration 10153 : loss : 0.060733, loss_ce: 0.003431, loss_dice: 0.118035
[09:20:51.041] TRAIN: iteration 10154 : loss : 0.088104, loss_ce: 0.004862, loss_dice: 0.171346
[09:20:51.248] TRAIN: iteration 10155 : loss : 0.236336, loss_ce: 0.003330, loss_dice: 0.469342
[09:20:51.454] TRAIN: iteration 10156 : loss : 0.190396, loss_ce: 0.007632, loss_dice: 0.373161
[09:20:51.663] TRAIN: iteration 10157 : loss : 0.085418, loss_ce: 0.005469, loss_dice: 0.165367
[09:20:51.870] TRAIN: iteration 10158 : loss : 0.047961, loss_ce: 0.003874, loss_dice: 0.092047
[09:20:52.078] TRAIN: iteration 10159 : loss : 0.144263, loss_ce: 0.008666, loss_dice: 0.279861
[09:20:52.286] TRAIN: iteration 10160 : loss : 0.174142, loss_ce: 0.002914, loss_dice: 0.345369
[09:20:54.733] TRAIN: iteration 10161 : loss : 0.145921, loss_ce: 0.002012, loss_dice: 0.289829
[09:20:54.941] TRAIN: iteration 10162 : loss : 0.113161, loss_ce: 0.003898, loss_dice: 0.222424
[09:20:55.148] TRAIN: iteration 10163 : loss : 0.067299, loss_ce: 0.003134, loss_dice: 0.131465
[09:20:55.355] TRAIN: iteration 10164 : loss : 0.195096, loss_ce: 0.007004, loss_dice: 0.383187
[09:20:55.561] TRAIN: iteration 10165 : loss : 0.090616, loss_ce: 0.004099, loss_dice: 0.177134
[09:20:55.767] TRAIN: iteration 10166 : loss : 0.250477, loss_ce: 0.003695, loss_dice: 0.497259
[09:20:55.974] TRAIN: iteration 10167 : loss : 0.201332, loss_ce: 0.008130, loss_dice: 0.394535
[09:20:57.705] TRAIN: iteration 10168 : loss : 0.097264, loss_ce: 0.007695, loss_dice: 0.186833
[09:20:59.528] TRAIN: iteration 10169 : loss : 0.209463, loss_ce: 0.011608, loss_dice: 0.407319
[09:20:59.735] TRAIN: iteration 10170 : loss : 0.254471, loss_ce: 0.010254, loss_dice: 0.498688
[09:20:59.942] TRAIN: iteration 10171 : loss : 0.252200, loss_ce: 0.004099, loss_dice: 0.500301
[09:21:02.900] TRAIN: iteration 10172 : loss : 0.052530, loss_ce: 0.001961, loss_dice: 0.103099
[09:21:03.111] TRAIN: iteration 10173 : loss : 0.152892, loss_ce: 0.003580, loss_dice: 0.302203
[09:21:03.322] TRAIN: iteration 10174 : loss : 0.252036, loss_ce: 0.003796, loss_dice: 0.500276
[09:21:03.531] TRAIN: iteration 10175 : loss : 0.193503, loss_ce: 0.049841, loss_dice: 0.337165
[09:21:03.739] TRAIN: iteration 10176 : loss : 0.177660, loss_ce: 0.009080, loss_dice: 0.346239
[09:21:03.946] TRAIN: iteration 10177 : loss : 0.171511, loss_ce: 0.007142, loss_dice: 0.335879
[09:21:05.165] TRAIN: iteration 10178 : loss : 0.048718, loss_ce: 0.003665, loss_dice: 0.093770
[09:21:05.371] TRAIN: iteration 10179 : loss : 0.117055, loss_ce: 0.010773, loss_dice: 0.223338
[09:21:09.585] TRAIN: iteration 10180 : loss : 0.127145, loss_ce: 0.005969, loss_dice: 0.248321
[09:21:09.825] TRAIN: iteration 10181 : loss : 0.118893, loss_ce: 0.004759, loss_dice: 0.233027
[09:21:10.037] TRAIN: iteration 10182 : loss : 0.139568, loss_ce: 0.004656, loss_dice: 0.274480
[09:21:10.245] TRAIN: iteration 10183 : loss : 0.230983, loss_ce: 0.009922, loss_dice: 0.452045
[09:21:10.451] TRAIN: iteration 10184 : loss : 0.252197, loss_ce: 0.004116, loss_dice: 0.500278
[09:21:10.658] TRAIN: iteration 10185 : loss : 0.114982, loss_ce: 0.006763, loss_dice: 0.223200
[09:21:11.135] TRAIN: iteration 10186 : loss : 0.252752, loss_ce: 0.005150, loss_dice: 0.500355
[09:21:11.343] TRAIN: iteration 10187 : loss : 0.070369, loss_ce: 0.011604, loss_dice: 0.129133
[09:21:14.612] TRAIN: iteration 10188 : loss : 0.253585, loss_ce: 0.006700, loss_dice: 0.500471
[09:21:14.823] TRAIN: iteration 10189 : loss : 0.027614, loss_ce: 0.004955, loss_dice: 0.050273
[09:21:15.032] TRAIN: iteration 10190 : loss : 0.216503, loss_ce: 0.005612, loss_dice: 0.427394
[09:21:15.241] TRAIN: iteration 10191 : loss : 0.179533, loss_ce: 0.014155, loss_dice: 0.344912
[09:21:15.447] TRAIN: iteration 10192 : loss : 0.151100, loss_ce: 0.004296, loss_dice: 0.297904
[09:21:15.654] TRAIN: iteration 10193 : loss : 0.130226, loss_ce: 0.005568, loss_dice: 0.254885
[09:21:16.369] TRAIN: iteration 10194 : loss : 0.121994, loss_ce: 0.007152, loss_dice: 0.236836
[09:21:17.624] TRAIN: iteration 10195 : loss : 0.202324, loss_ce: 0.004829, loss_dice: 0.399818
[09:21:21.513] TRAIN: iteration 10196 : loss : 0.136171, loss_ce: 0.006701, loss_dice: 0.265641
[09:21:21.719] TRAIN: iteration 10197 : loss : 0.242759, loss_ce: 0.004666, loss_dice: 0.480851
[09:21:21.928] TRAIN: iteration 10198 : loss : 0.166014, loss_ce: 0.006213, loss_dice: 0.325815
[09:21:22.137] TRAIN: iteration 10199 : loss : 0.091162, loss_ce: 0.003794, loss_dice: 0.178531
[09:21:22.344] TRAIN: iteration 10200 : loss : 0.195083, loss_ce: 0.004025, loss_dice: 0.386140
[09:21:22.344] NaN or Inf found in input tensor.
[09:21:22.563] TRAIN: iteration 10201 : loss : 0.215565, loss_ce: 0.041302, loss_dice: 0.389829
[09:21:23.454] TRAIN: iteration 10202 : loss : 0.192940, loss_ce: 0.015927, loss_dice: 0.369952
[09:21:23.662] TRAIN: iteration 10203 : loss : 0.250963, loss_ce: 0.005774, loss_dice: 0.496152
[09:21:25.310] TRAIN: iteration 10204 : loss : 0.126530, loss_ce: 0.006547, loss_dice: 0.246513
[09:21:26.359] TRAIN: iteration 10205 : loss : 0.161134, loss_ce: 0.008665, loss_dice: 0.313603
[09:21:26.566] TRAIN: iteration 10206 : loss : 0.171690, loss_ce: 0.005069, loss_dice: 0.338312
[09:21:26.808] TRAIN: iteration 10207 : loss : 0.100115, loss_ce: 0.002626, loss_dice: 0.197605
[09:21:27.015] TRAIN: iteration 10208 : loss : 0.251615, loss_ce: 0.003080, loss_dice: 0.500150
[09:21:27.555] TRAIN: iteration 10209 : loss : 0.185153, loss_ce: 0.007949, loss_dice: 0.362358
[09:21:27.762] TRAIN: iteration 10210 : loss : 0.252744, loss_ce: 0.005509, loss_dice: 0.499979
[09:21:29.418] TRAIN: iteration 10211 : loss : 0.193456, loss_ce: 0.007238, loss_dice: 0.379675
[09:21:32.320] TRAIN: iteration 10212 : loss : 0.077469, loss_ce: 0.003032, loss_dice: 0.151906
[09:21:32.534] TRAIN: iteration 10213 : loss : 0.251112, loss_ce: 0.002148, loss_dice: 0.500075
[09:21:32.741] TRAIN: iteration 10214 : loss : 0.088706, loss_ce: 0.004080, loss_dice: 0.173332
[09:21:32.948] TRAIN: iteration 10215 : loss : 0.126883, loss_ce: 0.006208, loss_dice: 0.247559
[09:21:33.161] TRAIN: iteration 10216 : loss : 0.148444, loss_ce: 0.019261, loss_dice: 0.277628
[09:21:34.076] TRAIN: iteration 10217 : loss : 0.116868, loss_ce: 0.004131, loss_dice: 0.229605
[09:21:34.283] TRAIN: iteration 10218 : loss : 0.036808, loss_ce: 0.003437, loss_dice: 0.070179
[09:21:40.008] TRAIN: iteration 10219 : loss : 0.245323, loss_ce: 0.004330, loss_dice: 0.486316
[09:21:41.033] TRAIN: iteration 10220 : loss : 0.104658, loss_ce: 0.004540, loss_dice: 0.204776
[09:21:41.266] TRAIN: iteration 10221 : loss : 0.161600, loss_ce: 0.004345, loss_dice: 0.318855
[09:21:41.475] TRAIN: iteration 10222 : loss : 0.197520, loss_ce: 0.008670, loss_dice: 0.386371
[09:21:41.681] TRAIN: iteration 10223 : loss : 0.117319, loss_ce: 0.003628, loss_dice: 0.231009
[09:21:41.893] TRAIN: iteration 10224 : loss : 0.078518, loss_ce: 0.004663, loss_dice: 0.152372
[09:21:42.116] TRAIN: iteration 10225 : loss : 0.104613, loss_ce: 0.006610, loss_dice: 0.202616
[09:21:42.324] TRAIN: iteration 10226 : loss : 0.082637, loss_ce: 0.009115, loss_dice: 0.156160
[09:21:46.974] TRAIN: iteration 10227 : loss : 0.089895, loss_ce: 0.005997, loss_dice: 0.173792
[09:21:47.779] TRAIN: iteration 10228 : loss : 0.100218, loss_ce: 0.003952, loss_dice: 0.196483
[09:21:47.986] TRAIN: iteration 10229 : loss : 0.250437, loss_ce: 0.005841, loss_dice: 0.495034
[09:21:48.193] TRAIN: iteration 10230 : loss : 0.086559, loss_ce: 0.005973, loss_dice: 0.167145
[09:21:48.400] TRAIN: iteration 10231 : loss : 0.143891, loss_ce: 0.007928, loss_dice: 0.279855
[09:21:48.608] TRAIN: iteration 10232 : loss : 0.138450, loss_ce: 0.006566, loss_dice: 0.270334
[09:21:48.814] TRAIN: iteration 10233 : loss : 0.070403, loss_ce: 0.007562, loss_dice: 0.133244
[09:21:49.021] TRAIN: iteration 10234 : loss : 0.246957, loss_ce: 0.004445, loss_dice: 0.489469
[09:21:52.551] TRAIN: iteration 10235 : loss : 0.246074, loss_ce: 0.020354, loss_dice: 0.471793
[09:21:56.379] TRAIN: iteration 10236 : loss : 0.049609, loss_ce: 0.003016, loss_dice: 0.096203
[09:21:56.585] TRAIN: iteration 10237 : loss : 0.113571, loss_ce: 0.006390, loss_dice: 0.220752
[09:21:56.792] TRAIN: iteration 10238 : loss : 0.062564, loss_ce: 0.002941, loss_dice: 0.122186
[09:21:56.998] TRAIN: iteration 10239 : loss : 0.165823, loss_ce: 0.008825, loss_dice: 0.322820
[09:21:57.206] TRAIN: iteration 10240 : loss : 0.251633, loss_ce: 0.003073, loss_dice: 0.500193
[09:21:57.441] TRAIN: iteration 10241 : loss : 0.134851, loss_ce: 0.002935, loss_dice: 0.266767
[09:21:57.649] TRAIN: iteration 10242 : loss : 0.248566, loss_ce: 0.003123, loss_dice: 0.494008
[09:21:58.844] TRAIN: iteration 10243 : loss : 0.062643, loss_ce: 0.003868, loss_dice: 0.121419
[09:22:04.746] TRAIN: iteration 10244 : loss : 0.092078, loss_ce: 0.006921, loss_dice: 0.177235
[09:22:04.953] TRAIN: iteration 10245 : loss : 0.110039, loss_ce: 0.002232, loss_dice: 0.217847
[09:22:05.160] TRAIN: iteration 10246 : loss : 0.191273, loss_ce: 0.003942, loss_dice: 0.378605
[09:22:05.367] TRAIN: iteration 10247 : loss : 0.206428, loss_ce: 0.023795, loss_dice: 0.389062
[09:22:05.574] TRAIN: iteration 10248 : loss : 0.093554, loss_ce: 0.012150, loss_dice: 0.174959
[09:22:05.782] TRAIN: iteration 10249 : loss : 0.153744, loss_ce: 0.007182, loss_dice: 0.300306
[09:22:05.991] TRAIN: iteration 10250 : loss : 0.251018, loss_ce: 0.001926, loss_dice: 0.500110
[09:22:06.922] TRAIN: iteration 10251 : loss : 0.053518, loss_ce: 0.005617, loss_dice: 0.101420
[09:22:11.087] TRAIN: iteration 10252 : loss : 0.131893, loss_ce: 0.003054, loss_dice: 0.260733
[09:22:11.296] TRAIN: iteration 10253 : loss : 0.138552, loss_ce: 0.002686, loss_dice: 0.274419
[09:22:11.504] TRAIN: iteration 10254 : loss : 0.086937, loss_ce: 0.003624, loss_dice: 0.170250
[09:22:11.714] TRAIN: iteration 10255 : loss : 0.094850, loss_ce: 0.001308, loss_dice: 0.188392
[09:22:11.921] TRAIN: iteration 10256 : loss : 0.179477, loss_ce: 0.005571, loss_dice: 0.353384
[09:22:12.129] TRAIN: iteration 10257 : loss : 0.079639, loss_ce: 0.002687, loss_dice: 0.156590
[09:22:12.338] TRAIN: iteration 10258 : loss : 0.244379, loss_ce: 0.001062, loss_dice: 0.487695
[09:22:16.039] TRAIN: iteration 10259 : loss : 0.228408, loss_ce: 0.004145, loss_dice: 0.452671
[09:22:18.142] TRAIN: iteration 10260 : loss : 0.091062, loss_ce: 0.001646, loss_dice: 0.180478
[09:22:18.376] TRAIN: iteration 10261 : loss : 0.033899, loss_ce: 0.003536, loss_dice: 0.064263
[09:22:18.588] TRAIN: iteration 10262 : loss : 0.250405, loss_ce: 0.005043, loss_dice: 0.495766
[09:22:18.794] TRAIN: iteration 10263 : loss : 0.197890, loss_ce: 0.026007, loss_dice: 0.369773
[09:22:19.003] TRAIN: iteration 10264 : loss : 0.034702, loss_ce: 0.000631, loss_dice: 0.068773
[09:22:21.241] TRAIN: iteration 10265 : loss : 0.239534, loss_ce: 0.007640, loss_dice: 0.471428
[09:22:21.448] TRAIN: iteration 10266 : loss : 0.041886, loss_ce: 0.000914, loss_dice: 0.082859
[09:22:24.175] TRAIN: iteration 10267 : loss : 0.250900, loss_ce: 0.002221, loss_dice: 0.499579
[09:22:28.038] TRAIN: iteration 10268 : loss : 0.250136, loss_ce: 0.000278, loss_dice: 0.499995
[09:22:28.250] TRAIN: iteration 10269 : loss : 0.201861, loss_ce: 0.007113, loss_dice: 0.396608
[09:22:28.457] TRAIN: iteration 10270 : loss : 0.249940, loss_ce: 0.003126, loss_dice: 0.496753
[09:22:28.664] TRAIN: iteration 10271 : loss : 0.236209, loss_ce: 0.018975, loss_dice: 0.453442
[09:22:28.870] TRAIN: iteration 10272 : loss : 0.204916, loss_ce: 0.007657, loss_dice: 0.402176
[09:22:29.077] TRAIN: iteration 10273 : loss : 0.195715, loss_ce: 0.001899, loss_dice: 0.389531
[09:22:29.283] TRAIN: iteration 10274 : loss : 0.248823, loss_ce: 0.002198, loss_dice: 0.495448
[09:22:31.331] TRAIN: iteration 10275 : loss : 0.092274, loss_ce: 0.004617, loss_dice: 0.179932
[09:22:36.773] TRAIN: iteration 10276 : loss : 0.219592, loss_ce: 0.003934, loss_dice: 0.435251
[09:22:36.981] TRAIN: iteration 10277 : loss : 0.239249, loss_ce: 0.006402, loss_dice: 0.472097
[09:22:37.189] TRAIN: iteration 10278 : loss : 0.158373, loss_ce: 0.006205, loss_dice: 0.310541
[09:22:37.397] TRAIN: iteration 10279 : loss : 0.077330, loss_ce: 0.003932, loss_dice: 0.150729
[09:22:37.603] TRAIN: iteration 10280 : loss : 0.226103, loss_ce: 0.015962, loss_dice: 0.436245
[09:22:37.838] TRAIN: iteration 10281 : loss : 0.262700, loss_ce: 0.026369, loss_dice: 0.499031
[09:22:38.050] TRAIN: iteration 10282 : loss : 0.173633, loss_ce: 0.002642, loss_dice: 0.344624
[09:22:40.459] TRAIN: iteration 10283 : loss : 0.234054, loss_ce: 0.002006, loss_dice: 0.466102
[09:22:44.618] TRAIN: iteration 10284 : loss : 0.135625, loss_ce: 0.003779, loss_dice: 0.267472
[09:22:44.825] TRAIN: iteration 10285 : loss : 0.094276, loss_ce: 0.014541, loss_dice: 0.174011
[09:22:45.033] TRAIN: iteration 10286 : loss : 0.075388, loss_ce: 0.003290, loss_dice: 0.147485
[09:22:45.244] TRAIN: iteration 10287 : loss : 0.073269, loss_ce: 0.004126, loss_dice: 0.142412
[09:22:45.451] TRAIN: iteration 10288 : loss : 0.055870, loss_ce: 0.004725, loss_dice: 0.107015
[09:22:46.977] TRAIN: iteration 10289 : loss : 0.250740, loss_ce: 0.004049, loss_dice: 0.497430
[09:22:47.186] TRAIN: iteration 10290 : loss : 0.065139, loss_ce: 0.002728, loss_dice: 0.127551
[09:22:48.839] TRAIN: iteration 10291 : loss : 0.076936, loss_ce: 0.005243, loss_dice: 0.148630
[09:22:53.075] TRAIN: iteration 10292 : loss : 0.085413, loss_ce: 0.006519, loss_dice: 0.164306
[09:22:53.284] TRAIN: iteration 10293 : loss : 0.105281, loss_ce: 0.004354, loss_dice: 0.206208
[09:22:53.491] TRAIN: iteration 10294 : loss : 0.095966, loss_ce: 0.005778, loss_dice: 0.186153
[09:22:53.698] TRAIN: iteration 10295 : loss : 0.057082, loss_ce: 0.003468, loss_dice: 0.110696
[09:22:53.906] TRAIN: iteration 10296 : loss : 0.103617, loss_ce: 0.009410, loss_dice: 0.197823
[09:22:55.404] TRAIN: iteration 10297 : loss : 0.230123, loss_ce: 0.006342, loss_dice: 0.453903
[09:22:55.610] TRAIN: iteration 10298 : loss : 0.233443, loss_ce: 0.005339, loss_dice: 0.461547
[09:22:56.291] TRAIN: iteration 10299 : loss : 0.257195, loss_ce: 0.013782, loss_dice: 0.500609
[09:23:02.476] TRAIN: iteration 10300 : loss : 0.136507, loss_ce: 0.006264, loss_dice: 0.266751
[09:23:02.692] TRAIN: iteration 10301 : loss : 0.182287, loss_ce: 0.009232, loss_dice: 0.355341
[09:23:02.900] TRAIN: iteration 10302 : loss : 0.147654, loss_ce: 0.004118, loss_dice: 0.291191
[09:23:03.107] TRAIN: iteration 10303 : loss : 0.213650, loss_ce: 0.008901, loss_dice: 0.418399
[09:23:03.314] TRAIN: iteration 10304 : loss : 0.173342, loss_ce: 0.007454, loss_dice: 0.339230
[09:23:03.522] TRAIN: iteration 10305 : loss : 0.096531, loss_ce: 0.005118, loss_dice: 0.187944
[09:23:03.733] TRAIN: iteration 10306 : loss : 0.252187, loss_ce: 0.004134, loss_dice: 0.500241
[09:23:05.233] TRAIN: iteration 10307 : loss : 0.209146, loss_ce: 0.007394, loss_dice: 0.410897
[09:23:09.973] TRAIN: iteration 10308 : loss : 0.129315, loss_ce: 0.005545, loss_dice: 0.253084
[09:23:10.179] TRAIN: iteration 10309 : loss : 0.053653, loss_ce: 0.004306, loss_dice: 0.103000
[09:23:10.386] TRAIN: iteration 10310 : loss : 0.106781, loss_ce: 0.004539, loss_dice: 0.209024
[09:23:10.592] TRAIN: iteration 10311 : loss : 0.104143, loss_ce: 0.005062, loss_dice: 0.203224
[09:23:10.800] TRAIN: iteration 10312 : loss : 0.041660, loss_ce: 0.003334, loss_dice: 0.079985
[09:23:12.327] TRAIN: iteration 10313 : loss : 0.187387, loss_ce: 0.004624, loss_dice: 0.370149
[09:23:12.539] TRAIN: iteration 10314 : loss : 0.074624, loss_ce: 0.002653, loss_dice: 0.146594
[09:23:12.748] TRAIN: iteration 10315 : loss : 0.210929, loss_ce: 0.002783, loss_dice: 0.419075
[09:23:19.431] TRAIN: iteration 10316 : loss : 0.113784, loss_ce: 0.006225, loss_dice: 0.221343
[09:23:19.640] TRAIN: iteration 10317 : loss : 0.198081, loss_ce: 0.002252, loss_dice: 0.393910
[09:23:19.847] TRAIN: iteration 10318 : loss : 0.064125, loss_ce: 0.003273, loss_dice: 0.124976
[09:23:20.053] TRAIN: iteration 10319 : loss : 0.104695, loss_ce: 0.004982, loss_dice: 0.204408
[09:23:20.262] TRAIN: iteration 10320 : loss : 0.250922, loss_ce: 0.001749, loss_dice: 0.500094
[09:23:20.263] NaN or Inf found in input tensor.
[09:23:22.594] TRAIN: iteration 10321 : loss : 0.128048, loss_ce: 0.003870, loss_dice: 0.252227
[09:23:22.801] TRAIN: iteration 10322 : loss : 0.102539, loss_ce: 0.006277, loss_dice: 0.198802
[09:23:23.010] TRAIN: iteration 10323 : loss : 0.250773, loss_ce: 0.001477, loss_dice: 0.500069
[09:23:28.719] TRAIN: iteration 10324 : loss : 0.093317, loss_ce: 0.005819, loss_dice: 0.180815
[09:23:28.929] TRAIN: iteration 10325 : loss : 0.226594, loss_ce: 0.002808, loss_dice: 0.450379
[09:23:29.137] TRAIN: iteration 10326 : loss : 0.081048, loss_ce: 0.002374, loss_dice: 0.159721
[09:23:29.343] TRAIN: iteration 10327 : loss : 0.112968, loss_ce: 0.006037, loss_dice: 0.219899
[09:23:29.550] TRAIN: iteration 10328 : loss : 0.080951, loss_ce: 0.002132, loss_dice: 0.159770
[09:23:31.519] TRAIN: iteration 10329 : loss : 0.251047, loss_ce: 0.002007, loss_dice: 0.500088
[09:23:31.726] TRAIN: iteration 10330 : loss : 0.057703, loss_ce: 0.002318, loss_dice: 0.113087
[09:23:31.936] TRAIN: iteration 10331 : loss : 0.190696, loss_ce: 0.014339, loss_dice: 0.367053
[09:23:33.596] TRAIN: iteration 10332 : loss : 0.176024, loss_ce: 0.043439, loss_dice: 0.308609
[09:23:33.803] TRAIN: iteration 10333 : loss : 0.042952, loss_ce: 0.002482, loss_dice: 0.083421
[09:23:34.020] TRAIN: iteration 10334 : loss : 0.247466, loss_ce: 0.011583, loss_dice: 0.483348
[09:23:34.228] TRAIN: iteration 10335 : loss : 0.201056, loss_ce: 0.005214, loss_dice: 0.396897
[09:23:34.434] TRAIN: iteration 10336 : loss : 0.131107, loss_ce: 0.003027, loss_dice: 0.259186
[09:23:39.179] TRAIN: iteration 10337 : loss : 0.185230, loss_ce: 0.006295, loss_dice: 0.364166
[09:23:39.385] TRAIN: iteration 10338 : loss : 0.167587, loss_ce: 0.006382, loss_dice: 0.328791
[09:23:39.592] TRAIN: iteration 10339 : loss : 0.232995, loss_ce: 0.004083, loss_dice: 0.461907
[09:23:43.747] TRAIN: iteration 10340 : loss : 0.157105, loss_ce: 0.003087, loss_dice: 0.311123
[09:23:43.985] TRAIN: iteration 10341 : loss : 0.252191, loss_ce: 0.004106, loss_dice: 0.500277
[09:23:44.191] TRAIN: iteration 10342 : loss : 0.087095, loss_ce: 0.007161, loss_dice: 0.167029
[09:23:44.400] TRAIN: iteration 10343 : loss : 0.163387, loss_ce: 0.014382, loss_dice: 0.312391
[09:23:44.606] TRAIN: iteration 10344 : loss : 0.218422, loss_ce: 0.003849, loss_dice: 0.432996
[09:23:50.389] TRAIN: iteration 10345 : loss : 0.033424, loss_ce: 0.003487, loss_dice: 0.063361
[09:23:50.595] TRAIN: iteration 10346 : loss : 0.253270, loss_ce: 0.006084, loss_dice: 0.500456
[09:23:50.802] TRAIN: iteration 10347 : loss : 0.145575, loss_ce: 0.006845, loss_dice: 0.284304
[09:23:52.592] TRAIN: iteration 10348 : loss : 0.043242, loss_ce: 0.003879, loss_dice: 0.082605
[09:23:52.799] TRAIN: iteration 10349 : loss : 0.164318, loss_ce: 0.007074, loss_dice: 0.321562
[09:23:53.011] TRAIN: iteration 10350 : loss : 0.111131, loss_ce: 0.009838, loss_dice: 0.212423
[09:23:53.221] TRAIN: iteration 10351 : loss : 0.107922, loss_ce: 0.004658, loss_dice: 0.211186
[09:23:53.427] TRAIN: iteration 10352 : loss : 0.147703, loss_ce: 0.006656, loss_dice: 0.288750
[09:23:59.054] TRAIN: iteration 10353 : loss : 0.167343, loss_ce: 0.006509, loss_dice: 0.328178
[09:23:59.261] TRAIN: iteration 10354 : loss : 0.252070, loss_ce: 0.003930, loss_dice: 0.500209
[09:23:59.469] TRAIN: iteration 10355 : loss : 0.112768, loss_ce: 0.003946, loss_dice: 0.221591
[09:24:02.474] TRAIN: iteration 10356 : loss : 0.135841, loss_ce: 0.005137, loss_dice: 0.266545
[09:24:02.680] TRAIN: iteration 10357 : loss : 0.095996, loss_ce: 0.005557, loss_dice: 0.186434
[09:24:02.886] TRAIN: iteration 10358 : loss : 0.226073, loss_ce: 0.005659, loss_dice: 0.446487
[09:24:03.093] TRAIN: iteration 10359 : loss : 0.088585, loss_ce: 0.003141, loss_dice: 0.174028
[09:24:03.300] TRAIN: iteration 10360 : loss : 0.052562, loss_ce: 0.002353, loss_dice: 0.102772
[09:24:08.053] TRAIN: iteration 10361 : loss : 0.103893, loss_ce: 0.005492, loss_dice: 0.202294
[09:24:08.261] TRAIN: iteration 10362 : loss : 0.084448, loss_ce: 0.007043, loss_dice: 0.161854
[09:24:08.469] TRAIN: iteration 10363 : loss : 0.038627, loss_ce: 0.002597, loss_dice: 0.074656
[09:24:12.920] TRAIN: iteration 10364 : loss : 0.159153, loss_ce: 0.024729, loss_dice: 0.293577
[09:24:13.128] TRAIN: iteration 10365 : loss : 0.143816, loss_ce: 0.002905, loss_dice: 0.284726
[09:24:13.337] TRAIN: iteration 10366 : loss : 0.060760, loss_ce: 0.002145, loss_dice: 0.119374
[09:24:13.545] TRAIN: iteration 10367 : loss : 0.091272, loss_ce: 0.002569, loss_dice: 0.179974
[09:24:13.753] TRAIN: iteration 10368 : loss : 0.251345, loss_ce: 0.002545, loss_dice: 0.500146
[09:24:17.712] TRAIN: iteration 10369 : loss : 0.035804, loss_ce: 0.001078, loss_dice: 0.070531
[09:24:17.918] TRAIN: iteration 10370 : loss : 0.091773, loss_ce: 0.003661, loss_dice: 0.179885
[09:24:18.125] TRAIN: iteration 10371 : loss : 0.077307, loss_ce: 0.004143, loss_dice: 0.150470
[09:24:24.834] TRAIN: iteration 10372 : loss : 0.251024, loss_ce: 0.001951, loss_dice: 0.500098
[09:24:25.041] TRAIN: iteration 10373 : loss : 0.036429, loss_ce: 0.001422, loss_dice: 0.071436
[09:24:25.250] TRAIN: iteration 10374 : loss : 0.250595, loss_ce: 0.001145, loss_dice: 0.500045
[09:24:25.458] TRAIN: iteration 10375 : loss : 0.250165, loss_ce: 0.000330, loss_dice: 0.499999
[09:24:25.665] TRAIN: iteration 10376 : loss : 0.027922, loss_ce: 0.001802, loss_dice: 0.054042
[09:24:28.356] TRAIN: iteration 10377 : loss : 0.126804, loss_ce: 0.001734, loss_dice: 0.251873
[09:24:28.565] TRAIN: iteration 10378 : loss : 0.040993, loss_ce: 0.000870, loss_dice: 0.081117
[09:24:28.772] TRAIN: iteration 10379 : loss : 0.131719, loss_ce: 0.013822, loss_dice: 0.249616
[09:24:34.743] TRAIN: iteration 10380 : loss : 0.251128, loss_ce: 0.002199, loss_dice: 0.500057
[09:24:34.978] TRAIN: iteration 10381 : loss : 0.074015, loss_ce: 0.002998, loss_dice: 0.145033
[09:24:35.188] TRAIN: iteration 10382 : loss : 0.024214, loss_ce: 0.001694, loss_dice: 0.046734
[09:24:35.424] TRAIN: iteration 10383 : loss : 0.165900, loss_ce: 0.010286, loss_dice: 0.321514
[09:24:35.631] TRAIN: iteration 10384 : loss : 0.122293, loss_ce: 0.003055, loss_dice: 0.241530
[09:24:38.170] TRAIN: iteration 10385 : loss : 0.191115, loss_ce: 0.001326, loss_dice: 0.380903
[09:24:38.382] TRAIN: iteration 10386 : loss : 0.184001, loss_ce: 0.018043, loss_dice: 0.349959
[09:24:38.588] TRAIN: iteration 10387 : loss : 0.136662, loss_ce: 0.000980, loss_dice: 0.272344
[09:24:45.371] TRAIN: iteration 10388 : loss : 0.250921, loss_ce: 0.001743, loss_dice: 0.500100
[09:24:45.579] TRAIN: iteration 10389 : loss : 0.250454, loss_ce: 0.000876, loss_dice: 0.500032
[09:24:45.788] TRAIN: iteration 10390 : loss : 0.105757, loss_ce: 0.004960, loss_dice: 0.206553
[09:24:45.999] TRAIN: iteration 10391 : loss : 0.191170, loss_ce: 0.007670, loss_dice: 0.374670
[09:24:46.206] TRAIN: iteration 10392 : loss : 0.044621, loss_ce: 0.001662, loss_dice: 0.087581
[09:24:47.804] TRAIN: iteration 10393 : loss : 0.046244, loss_ce: 0.003579, loss_dice: 0.088909
[09:24:48.013] TRAIN: iteration 10394 : loss : 0.251331, loss_ce: 0.002495, loss_dice: 0.500166
[09:24:48.220] TRAIN: iteration 10395 : loss : 0.128888, loss_ce: 0.008496, loss_dice: 0.249281
[09:24:55.348] TRAIN: iteration 10396 : loss : 0.133196, loss_ce: 0.005000, loss_dice: 0.261392
[09:24:55.558] TRAIN: iteration 10397 : loss : 0.032006, loss_ce: 0.001727, loss_dice: 0.062286
[09:24:55.766] TRAIN: iteration 10398 : loss : 0.251675, loss_ce: 0.003120, loss_dice: 0.500230
[09:24:55.973] TRAIN: iteration 10399 : loss : 0.088743, loss_ce: 0.009116, loss_dice: 0.168371
[09:24:56.180] TRAIN: iteration 10400 : loss : 0.201490, loss_ce: 0.005946, loss_dice: 0.397034
[09:24:57.101] TRAIN: iteration 10401 : loss : 0.251733, loss_ce: 0.003221, loss_dice: 0.500246
[09:24:57.307] TRAIN: iteration 10402 : loss : 0.244609, loss_ce: 0.005190, loss_dice: 0.484028
[09:24:57.515] TRAIN: iteration 10403 : loss : 0.120653, loss_ce: 0.005199, loss_dice: 0.236107
[09:25:06.014] TRAIN: iteration 10404 : loss : 0.058314, loss_ce: 0.005045, loss_dice: 0.111582
[09:25:06.221] TRAIN: iteration 10405 : loss : 0.100119, loss_ce: 0.004468, loss_dice: 0.195770
[09:25:06.427] TRAIN: iteration 10406 : loss : 0.083648, loss_ce: 0.003264, loss_dice: 0.164031
[09:25:06.633] TRAIN: iteration 10407 : loss : 0.251613, loss_ce: 0.003012, loss_dice: 0.500214
[09:25:06.843] TRAIN: iteration 10408 : loss : 0.150159, loss_ce: 0.006871, loss_dice: 0.293447
[09:25:07.048] TRAIN: iteration 10409 : loss : 0.132356, loss_ce: 0.004020, loss_dice: 0.260691
[09:25:07.142] TRAIN: iteration 10410 : loss : 0.251585, loss_ce: 0.002948, loss_dice: 0.500223
[09:30:49.919] VALIDATION: iteration 5 : loss : 0.147607, loss_ce: 0.005924, loss_dice: 0.289289
[09:30:50.670] TRAIN: iteration 10411 : loss : 0.235060, loss_ce: 0.004063, loss_dice: 0.466057
[09:30:52.810] TRAIN: iteration 10412 : loss : 0.148059, loss_ce: 0.009519, loss_dice: 0.286598
[09:30:53.024] TRAIN: iteration 10413 : loss : 0.173600, loss_ce: 0.003391, loss_dice: 0.343809
[09:30:53.235] TRAIN: iteration 10414 : loss : 0.250785, loss_ce: 0.001489, loss_dice: 0.500081
[09:30:53.445] TRAIN: iteration 10415 : loss : 0.221948, loss_ce: 0.001438, loss_dice: 0.442457
[09:30:53.652] TRAIN: iteration 10416 : loss : 0.111320, loss_ce: 0.002356, loss_dice: 0.220284
[09:30:53.861] TRAIN: iteration 10417 : loss : 0.075940, loss_ce: 0.002564, loss_dice: 0.149317
[09:30:54.095] TRAIN: iteration 10418 : loss : 0.055254, loss_ce: 0.002576, loss_dice: 0.107932
[09:30:54.304] TRAIN: iteration 10419 : loss : 0.168548, loss_ce: 0.007959, loss_dice: 0.329137
[09:30:54.515] TRAIN: iteration 10420 : loss : 0.084545, loss_ce: 0.004166, loss_dice: 0.164924
[09:30:54.755] TRAIN: iteration 10421 : loss : 0.130437, loss_ce: 0.019262, loss_dice: 0.241612
[09:30:54.971] TRAIN: iteration 10422 : loss : 0.251100, loss_ce: 0.002071, loss_dice: 0.500129
[09:30:55.178] TRAIN: iteration 10423 : loss : 0.246859, loss_ce: 0.001840, loss_dice: 0.491878
[09:30:55.397] TRAIN: iteration 10424 : loss : 0.250836, loss_ce: 0.001604, loss_dice: 0.500068
[09:30:55.605] TRAIN: iteration 10425 : loss : 0.161942, loss_ce: 0.010595, loss_dice: 0.313288
[09:30:55.812] TRAIN: iteration 10426 : loss : 0.194347, loss_ce: 0.013634, loss_dice: 0.375060
[09:30:56.029] TRAIN: iteration 10427 : loss : 0.175003, loss_ce: 0.003724, loss_dice: 0.346282
[09:30:56.243] TRAIN: iteration 10428 : loss : 0.147407, loss_ce: 0.005367, loss_dice: 0.289447
[09:30:56.450] TRAIN: iteration 10429 : loss : 0.085053, loss_ce: 0.003981, loss_dice: 0.166126
[09:30:56.663] TRAIN: iteration 10430 : loss : 0.045425, loss_ce: 0.003306, loss_dice: 0.087543
[09:30:56.880] TRAIN: iteration 10431 : loss : 0.238582, loss_ce: 0.003950, loss_dice: 0.473214
[09:30:57.089] TRAIN: iteration 10432 : loss : 0.161855, loss_ce: 0.005948, loss_dice: 0.317763
[09:30:57.296] TRAIN: iteration 10433 : loss : 0.080616, loss_ce: 0.005163, loss_dice: 0.156069
[09:30:57.505] TRAIN: iteration 10434 : loss : 0.252949, loss_ce: 0.005489, loss_dice: 0.500409
[09:30:57.716] TRAIN: iteration 10435 : loss : 0.102996, loss_ce: 0.005662, loss_dice: 0.200329
[09:30:57.925] TRAIN: iteration 10436 : loss : 0.122307, loss_ce: 0.004462, loss_dice: 0.240152
[09:30:58.206] TRAIN: iteration 10437 : loss : 0.198082, loss_ce: 0.006444, loss_dice: 0.389719
[09:30:58.417] TRAIN: iteration 10438 : loss : 0.189059, loss_ce: 0.005737, loss_dice: 0.372381
[09:30:58.628] TRAIN: iteration 10439 : loss : 0.142576, loss_ce: 0.011546, loss_dice: 0.273607
[09:30:58.836] TRAIN: iteration 10440 : loss : 0.082596, loss_ce: 0.004476, loss_dice: 0.160716
[09:30:58.836] NaN or Inf found in input tensor.
[09:30:59.053] TRAIN: iteration 10441 : loss : 0.153950, loss_ce: 0.005470, loss_dice: 0.302430
[09:30:59.265] TRAIN: iteration 10442 : loss : 0.076085, loss_ce: 0.006640, loss_dice: 0.145530
[09:30:59.475] TRAIN: iteration 10443 : loss : 0.132113, loss_ce: 0.010960, loss_dice: 0.253266
[09:30:59.687] TRAIN: iteration 10444 : loss : 0.146349, loss_ce: 0.012307, loss_dice: 0.280392
[09:30:59.894] TRAIN: iteration 10445 : loss : 0.123094, loss_ce: 0.003772, loss_dice: 0.242416
[09:31:00.102] TRAIN: iteration 10446 : loss : 0.097012, loss_ce: 0.005252, loss_dice: 0.188773
[09:31:00.317] TRAIN: iteration 10447 : loss : 0.227486, loss_ce: 0.005362, loss_dice: 0.449611
[09:31:00.525] TRAIN: iteration 10448 : loss : 0.251502, loss_ce: 0.002842, loss_dice: 0.500162
[09:31:00.733] TRAIN: iteration 10449 : loss : 0.130797, loss_ce: 0.003138, loss_dice: 0.258457
[09:31:00.945] TRAIN: iteration 10450 : loss : 0.081396, loss_ce: 0.002809, loss_dice: 0.159983
[09:31:01.157] TRAIN: iteration 10451 : loss : 0.133028, loss_ce: 0.005824, loss_dice: 0.260232
[09:31:01.363] TRAIN: iteration 10452 : loss : 0.099353, loss_ce: 0.003735, loss_dice: 0.194971
[09:31:01.575] TRAIN: iteration 10453 : loss : 0.181217, loss_ce: 0.015600, loss_dice: 0.346835
[09:31:01.789] TRAIN: iteration 10454 : loss : 0.244010, loss_ce: 0.002554, loss_dice: 0.485467
[09:31:02.018] TRAIN: iteration 10455 : loss : 0.204244, loss_ce: 0.005083, loss_dice: 0.403405
[09:31:02.226] TRAIN: iteration 10456 : loss : 0.217653, loss_ce: 0.006829, loss_dice: 0.428477
[09:31:02.438] TRAIN: iteration 10457 : loss : 0.251720, loss_ce: 0.003715, loss_dice: 0.499724
[09:31:02.653] TRAIN: iteration 10458 : loss : 0.219752, loss_ce: 0.003358, loss_dice: 0.436146
[09:31:02.868] TRAIN: iteration 10459 : loss : 0.206843, loss_ce: 0.003634, loss_dice: 0.410053
[09:31:03.076] TRAIN: iteration 10460 : loss : 0.217335, loss_ce: 0.002852, loss_dice: 0.431819
[09:31:03.313] TRAIN: iteration 10461 : loss : 0.251356, loss_ce: 0.002603, loss_dice: 0.500109
[09:31:03.519] TRAIN: iteration 10462 : loss : 0.165582, loss_ce: 0.005278, loss_dice: 0.325887
[09:31:03.726] TRAIN: iteration 10463 : loss : 0.150842, loss_ce: 0.008230, loss_dice: 0.293454
[09:31:03.934] TRAIN: iteration 10464 : loss : 0.065213, loss_ce: 0.002947, loss_dice: 0.127478
[09:31:04.143] TRAIN: iteration 10465 : loss : 0.074934, loss_ce: 0.004569, loss_dice: 0.145299
[09:31:04.350] TRAIN: iteration 10466 : loss : 0.200462, loss_ce: 0.004117, loss_dice: 0.396807
[09:31:04.557] TRAIN: iteration 10467 : loss : 0.251872, loss_ce: 0.003570, loss_dice: 0.500173
[09:31:04.764] TRAIN: iteration 10468 : loss : 0.088540, loss_ce: 0.005230, loss_dice: 0.171850
[09:31:04.978] TRAIN: iteration 10469 : loss : 0.176219, loss_ce: 0.004603, loss_dice: 0.347835
[09:31:05.188] TRAIN: iteration 10470 : loss : 0.120509, loss_ce: 0.004938, loss_dice: 0.236081
[09:31:05.400] TRAIN: iteration 10471 : loss : 0.101129, loss_ce: 0.004800, loss_dice: 0.197457
[09:31:05.606] TRAIN: iteration 10472 : loss : 0.239204, loss_ce: 0.004133, loss_dice: 0.474275
[09:31:05.815] TRAIN: iteration 10473 : loss : 0.251915, loss_ce: 0.003650, loss_dice: 0.500179
[09:31:06.032] TRAIN: iteration 10474 : loss : 0.127650, loss_ce: 0.004339, loss_dice: 0.250962
[09:31:06.244] TRAIN: iteration 10475 : loss : 0.220068, loss_ce: 0.007491, loss_dice: 0.432645
[09:31:06.451] TRAIN: iteration 10476 : loss : 0.252311, loss_ce: 0.004313, loss_dice: 0.500309
[09:31:06.660] TRAIN: iteration 10477 : loss : 0.149282, loss_ce: 0.004814, loss_dice: 0.293750
[09:31:06.868] TRAIN: iteration 10478 : loss : 0.184237, loss_ce: 0.041559, loss_dice: 0.326915
[09:31:07.075] TRAIN: iteration 10479 : loss : 0.164618, loss_ce: 0.004834, loss_dice: 0.324402
[09:31:07.282] TRAIN: iteration 10480 : loss : 0.214129, loss_ce: 0.011850, loss_dice: 0.416408
[09:31:07.520] TRAIN: iteration 10481 : loss : 0.174853, loss_ce: 0.006558, loss_dice: 0.343149
[09:31:07.728] TRAIN: iteration 10482 : loss : 0.149743, loss_ce: 0.006443, loss_dice: 0.293043
[09:31:07.939] TRAIN: iteration 10483 : loss : 0.200469, loss_ce: 0.003601, loss_dice: 0.397338
[09:31:08.147] TRAIN: iteration 10484 : loss : 0.118968, loss_ce: 0.004106, loss_dice: 0.233831
[09:31:08.358] TRAIN: iteration 10485 : loss : 0.127690, loss_ce: 0.004701, loss_dice: 0.250678
[09:31:08.567] TRAIN: iteration 10486 : loss : 0.256872, loss_ce: 0.017724, loss_dice: 0.496020
[09:31:08.781] TRAIN: iteration 10487 : loss : 0.159040, loss_ce: 0.004903, loss_dice: 0.313177
[09:31:08.988] TRAIN: iteration 10488 : loss : 0.045692, loss_ce: 0.003356, loss_dice: 0.088027
[09:31:09.199] TRAIN: iteration 10489 : loss : 0.185199, loss_ce: 0.004965, loss_dice: 0.365433
[09:31:09.412] TRAIN: iteration 10490 : loss : 0.078818, loss_ce: 0.006844, loss_dice: 0.150792
[09:31:09.623] TRAIN: iteration 10491 : loss : 0.043683, loss_ce: 0.002117, loss_dice: 0.085250
[09:31:09.833] TRAIN: iteration 10492 : loss : 0.141837, loss_ce: 0.009509, loss_dice: 0.274164
[09:31:10.042] TRAIN: iteration 10493 : loss : 0.098375, loss_ce: 0.010774, loss_dice: 0.185975
[09:31:10.249] TRAIN: iteration 10494 : loss : 0.165183, loss_ce: 0.003377, loss_dice: 0.326988
[09:31:10.456] TRAIN: iteration 10495 : loss : 0.087272, loss_ce: 0.006121, loss_dice: 0.168422
[09:31:10.663] TRAIN: iteration 10496 : loss : 0.108888, loss_ce: 0.004590, loss_dice: 0.213186
[09:31:10.874] TRAIN: iteration 10497 : loss : 0.053781, loss_ce: 0.002958, loss_dice: 0.104605
[09:31:11.086] TRAIN: iteration 10498 : loss : 0.154768, loss_ce: 0.004391, loss_dice: 0.305145
[09:31:11.293] TRAIN: iteration 10499 : loss : 0.249552, loss_ce: 0.005024, loss_dice: 0.494079
[09:31:11.502] TRAIN: iteration 10500 : loss : 0.166052, loss_ce: 0.005307, loss_dice: 0.326797
[09:31:11.742] TRAIN: iteration 10501 : loss : 0.240223, loss_ce: 0.002719, loss_dice: 0.477727
[09:31:11.950] TRAIN: iteration 10502 : loss : 0.129959, loss_ce: 0.005739, loss_dice: 0.254178
[09:31:12.162] TRAIN: iteration 10503 : loss : 0.081144, loss_ce: 0.006100, loss_dice: 0.156189
[09:31:12.369] TRAIN: iteration 10504 : loss : 0.106191, loss_ce: 0.006541, loss_dice: 0.205841
[09:31:12.579] TRAIN: iteration 10505 : loss : 0.194891, loss_ce: 0.008005, loss_dice: 0.381777
[09:31:12.790] TRAIN: iteration 10506 : loss : 0.178515, loss_ce: 0.003985, loss_dice: 0.353044
[09:31:12.998] TRAIN: iteration 10507 : loss : 0.170682, loss_ce: 0.017525, loss_dice: 0.323838
[09:31:13.209] TRAIN: iteration 10508 : loss : 0.102282, loss_ce: 0.003651, loss_dice: 0.200913
[09:31:13.419] TRAIN: iteration 10509 : loss : 0.117945, loss_ce: 0.006357, loss_dice: 0.229532
[09:31:13.626] TRAIN: iteration 10510 : loss : 0.069793, loss_ce: 0.003952, loss_dice: 0.135633
[09:31:13.834] TRAIN: iteration 10511 : loss : 0.246655, loss_ce: 0.003309, loss_dice: 0.490001
[09:31:14.041] TRAIN: iteration 10512 : loss : 0.066979, loss_ce: 0.004869, loss_dice: 0.129090
[09:31:14.250] TRAIN: iteration 10513 : loss : 0.165208, loss_ce: 0.007330, loss_dice: 0.323086
[09:31:14.458] TRAIN: iteration 10514 : loss : 0.119981, loss_ce: 0.004140, loss_dice: 0.235822
[09:31:14.666] TRAIN: iteration 10515 : loss : 0.037564, loss_ce: 0.002351, loss_dice: 0.072777
[09:31:14.873] TRAIN: iteration 10516 : loss : 0.217187, loss_ce: 0.006057, loss_dice: 0.428318
[09:31:15.083] TRAIN: iteration 10517 : loss : 0.165720, loss_ce: 0.006055, loss_dice: 0.325385
[09:31:15.300] TRAIN: iteration 10518 : loss : 0.233683, loss_ce: 0.006243, loss_dice: 0.461124
[09:31:15.508] TRAIN: iteration 10519 : loss : 0.184340, loss_ce: 0.006115, loss_dice: 0.362565
[09:31:15.718] TRAIN: iteration 10520 : loss : 0.200659, loss_ce: 0.005474, loss_dice: 0.395843
[09:31:15.956] TRAIN: iteration 10521 : loss : 0.252250, loss_ce: 0.004184, loss_dice: 0.500315
[09:31:16.165] TRAIN: iteration 10522 : loss : 0.119450, loss_ce: 0.002443, loss_dice: 0.236456
[09:31:16.373] TRAIN: iteration 10523 : loss : 0.142426, loss_ce: 0.003243, loss_dice: 0.281609
[09:31:16.580] TRAIN: iteration 10524 : loss : 0.081950, loss_ce: 0.003547, loss_dice: 0.160353
[09:31:16.790] TRAIN: iteration 10525 : loss : 0.121280, loss_ce: 0.006033, loss_dice: 0.236528
[09:31:16.998] TRAIN: iteration 10526 : loss : 0.251101, loss_ce: 0.003508, loss_dice: 0.498693
[09:31:17.209] TRAIN: iteration 10527 : loss : 0.079155, loss_ce: 0.003083, loss_dice: 0.155226
[09:31:17.423] TRAIN: iteration 10528 : loss : 0.151669, loss_ce: 0.021415, loss_dice: 0.281922
[09:31:17.632] TRAIN: iteration 10529 : loss : 0.062032, loss_ce: 0.001914, loss_dice: 0.122150
[09:31:17.838] TRAIN: iteration 10530 : loss : 0.137645, loss_ce: 0.004099, loss_dice: 0.271190
[09:31:18.045] TRAIN: iteration 10531 : loss : 0.110346, loss_ce: 0.010934, loss_dice: 0.209758
[09:31:18.262] TRAIN: iteration 10532 : loss : 0.200371, loss_ce: 0.002290, loss_dice: 0.398452
[09:31:18.495] TRAIN: iteration 10533 : loss : 0.192385, loss_ce: 0.006908, loss_dice: 0.377863
[09:31:18.704] TRAIN: iteration 10534 : loss : 0.251395, loss_ce: 0.002625, loss_dice: 0.500165
[09:31:18.915] TRAIN: iteration 10535 : loss : 0.086562, loss_ce: 0.005977, loss_dice: 0.167148
[09:31:19.127] TRAIN: iteration 10536 : loss : 0.157552, loss_ce: 0.019727, loss_dice: 0.295376
[09:31:19.337] TRAIN: iteration 10537 : loss : 0.136045, loss_ce: 0.003804, loss_dice: 0.268285
[09:31:19.546] TRAIN: iteration 10538 : loss : 0.108104, loss_ce: 0.014223, loss_dice: 0.201985
[09:31:19.754] TRAIN: iteration 10539 : loss : 0.124599, loss_ce: 0.006511, loss_dice: 0.242688
[09:31:19.968] TRAIN: iteration 10540 : loss : 0.217288, loss_ce: 0.004468, loss_dice: 0.430108
[09:31:20.207] TRAIN: iteration 10541 : loss : 0.241488, loss_ce: 0.005154, loss_dice: 0.477821
[09:31:20.415] TRAIN: iteration 10542 : loss : 0.188439, loss_ce: 0.008607, loss_dice: 0.368272
[09:31:20.623] TRAIN: iteration 10543 : loss : 0.212291, loss_ce: 0.010197, loss_dice: 0.414386
[09:31:20.832] TRAIN: iteration 10544 : loss : 0.123302, loss_ce: 0.005428, loss_dice: 0.241176
[09:31:21.045] TRAIN: iteration 10545 : loss : 0.133182, loss_ce: 0.004535, loss_dice: 0.261829
[09:31:21.253] TRAIN: iteration 10546 : loss : 0.103333, loss_ce: 0.004782, loss_dice: 0.201885
[09:31:21.460] TRAIN: iteration 10547 : loss : 0.092796, loss_ce: 0.004233, loss_dice: 0.181359
[09:31:21.667] TRAIN: iteration 10548 : loss : 0.054508, loss_ce: 0.005433, loss_dice: 0.103584
[09:31:21.880] TRAIN: iteration 10549 : loss : 0.055079, loss_ce: 0.003249, loss_dice: 0.106909
[09:31:22.095] TRAIN: iteration 10550 : loss : 0.103225, loss_ce: 0.005244, loss_dice: 0.201206
[09:31:22.305] TRAIN: iteration 10551 : loss : 0.052182, loss_ce: 0.002207, loss_dice: 0.102158
[09:31:22.514] TRAIN: iteration 10552 : loss : 0.159882, loss_ce: 0.007763, loss_dice: 0.312001
[09:31:22.723] TRAIN: iteration 10553 : loss : 0.175759, loss_ce: 0.005122, loss_dice: 0.346397
[09:31:22.932] TRAIN: iteration 10554 : loss : 0.042891, loss_ce: 0.002112, loss_dice: 0.083669
[09:31:23.147] TRAIN: iteration 10555 : loss : 0.239764, loss_ce: 0.005648, loss_dice: 0.473880
[09:31:23.361] TRAIN: iteration 10556 : loss : 0.174264, loss_ce: 0.010358, loss_dice: 0.338170
[09:31:23.570] TRAIN: iteration 10557 : loss : 0.128675, loss_ce: 0.004558, loss_dice: 0.252792
[09:31:23.782] TRAIN: iteration 10558 : loss : 0.178604, loss_ce: 0.034606, loss_dice: 0.322603
[09:31:23.993] TRAIN: iteration 10559 : loss : 0.097411, loss_ce: 0.003864, loss_dice: 0.190959
[09:31:24.203] TRAIN: iteration 10560 : loss : 0.223324, loss_ce: 0.003861, loss_dice: 0.442786
[09:31:24.441] TRAIN: iteration 10561 : loss : 0.161309, loss_ce: 0.002808, loss_dice: 0.319809
[09:31:24.650] TRAIN: iteration 10562 : loss : 0.193264, loss_ce: 0.009713, loss_dice: 0.376815
[09:31:24.859] TRAIN: iteration 10563 : loss : 0.121504, loss_ce: 0.008071, loss_dice: 0.234937
[09:31:25.067] TRAIN: iteration 10564 : loss : 0.084686, loss_ce: 0.003543, loss_dice: 0.165830
[09:31:25.276] TRAIN: iteration 10565 : loss : 0.101737, loss_ce: 0.007530, loss_dice: 0.195943
[09:31:25.485] TRAIN: iteration 10566 : loss : 0.067417, loss_ce: 0.004600, loss_dice: 0.130234
[09:31:25.693] TRAIN: iteration 10567 : loss : 0.040567, loss_ce: 0.003097, loss_dice: 0.078036
[09:31:25.901] TRAIN: iteration 10568 : loss : 0.057271, loss_ce: 0.004901, loss_dice: 0.109640
[09:31:26.108] TRAIN: iteration 10569 : loss : 0.037499, loss_ce: 0.002667, loss_dice: 0.072332
[09:31:26.315] TRAIN: iteration 10570 : loss : 0.252901, loss_ce: 0.005403, loss_dice: 0.500399
[09:31:26.524] TRAIN: iteration 10571 : loss : 0.239855, loss_ce: 0.003760, loss_dice: 0.475950
[09:31:26.734] TRAIN: iteration 10572 : loss : 0.219719, loss_ce: 0.008736, loss_dice: 0.430703
[09:31:26.941] TRAIN: iteration 10573 : loss : 0.073391, loss_ce: 0.003615, loss_dice: 0.143167
[09:31:27.148] TRAIN: iteration 10574 : loss : 0.176678, loss_ce: 0.010942, loss_dice: 0.342414
[09:31:27.356] TRAIN: iteration 10575 : loss : 0.245205, loss_ce: 0.002761, loss_dice: 0.487648
[09:31:27.564] TRAIN: iteration 10576 : loss : 0.106465, loss_ce: 0.002856, loss_dice: 0.210075
[09:31:27.774] TRAIN: iteration 10577 : loss : 0.137215, loss_ce: 0.015562, loss_dice: 0.258869
[09:31:27.983] TRAIN: iteration 10578 : loss : 0.133296, loss_ce: 0.005515, loss_dice: 0.261077
[09:31:28.191] TRAIN: iteration 10579 : loss : 0.250759, loss_ce: 0.001479, loss_dice: 0.500038
[09:31:28.398] TRAIN: iteration 10580 : loss : 0.115347, loss_ce: 0.003180, loss_dice: 0.227514
[09:31:28.642] TRAIN: iteration 10581 : loss : 0.250996, loss_ce: 0.001922, loss_dice: 0.500070
[09:31:28.852] TRAIN: iteration 10582 : loss : 0.115318, loss_ce: 0.005600, loss_dice: 0.225035
[09:31:29.063] TRAIN: iteration 10583 : loss : 0.241627, loss_ce: 0.002909, loss_dice: 0.480345
[09:31:29.280] TRAIN: iteration 10584 : loss : 0.080102, loss_ce: 0.004156, loss_dice: 0.156049
[09:31:29.489] TRAIN: iteration 10585 : loss : 0.044223, loss_ce: 0.001722, loss_dice: 0.086723
[09:31:29.704] TRAIN: iteration 10586 : loss : 0.153452, loss_ce: 0.003358, loss_dice: 0.303546
[09:31:29.911] TRAIN: iteration 10587 : loss : 0.213404, loss_ce: 0.004127, loss_dice: 0.422681
[09:31:30.121] TRAIN: iteration 10588 : loss : 0.109518, loss_ce: 0.001817, loss_dice: 0.217220
[09:31:30.331] TRAIN: iteration 10589 : loss : 0.120732, loss_ce: 0.019420, loss_dice: 0.222044
[09:31:30.544] TRAIN: iteration 10590 : loss : 0.196025, loss_ce: 0.003043, loss_dice: 0.389007
[09:31:30.751] TRAIN: iteration 10591 : loss : 0.109282, loss_ce: 0.002542, loss_dice: 0.216023
[09:31:30.959] TRAIN: iteration 10592 : loss : 0.178557, loss_ce: 0.002399, loss_dice: 0.354716
[09:31:31.167] TRAIN: iteration 10593 : loss : 0.057555, loss_ce: 0.002284, loss_dice: 0.112826
[09:31:31.383] TRAIN: iteration 10594 : loss : 0.190072, loss_ce: 0.002893, loss_dice: 0.377252
[09:31:31.592] TRAIN: iteration 10595 : loss : 0.248347, loss_ce: 0.001510, loss_dice: 0.495184
[09:31:32.026] TRAIN: iteration 10596 : loss : 0.197853, loss_ce: 0.004389, loss_dice: 0.391317
[09:31:32.241] TRAIN: iteration 10597 : loss : 0.247928, loss_ce: 0.002122, loss_dice: 0.493735
[09:31:32.448] TRAIN: iteration 10598 : loss : 0.251359, loss_ce: 0.002574, loss_dice: 0.500145
[09:31:32.655] TRAIN: iteration 10599 : loss : 0.131859, loss_ce: 0.001938, loss_dice: 0.261780
[09:31:32.862] TRAIN: iteration 10600 : loss : 0.121243, loss_ce: 0.005901, loss_dice: 0.236584
[09:31:33.107] TRAIN: iteration 10601 : loss : 0.115618, loss_ce: 0.003757, loss_dice: 0.227480
[09:31:33.322] TRAIN: iteration 10602 : loss : 0.115046, loss_ce: 0.013830, loss_dice: 0.216263
[09:31:33.529] TRAIN: iteration 10603 : loss : 0.210866, loss_ce: 0.005222, loss_dice: 0.416510
[09:31:33.737] TRAIN: iteration 10604 : loss : 0.055142, loss_ce: 0.002529, loss_dice: 0.107756
[09:31:33.945] TRAIN: iteration 10605 : loss : 0.235553, loss_ce: 0.006507, loss_dice: 0.464600
[09:31:34.156] TRAIN: iteration 10606 : loss : 0.177910, loss_ce: 0.011299, loss_dice: 0.344520
[09:31:34.366] TRAIN: iteration 10607 : loss : 0.064936, loss_ce: 0.008826, loss_dice: 0.121046
[09:31:34.579] TRAIN: iteration 10608 : loss : 0.138448, loss_ce: 0.006109, loss_dice: 0.270787
[09:31:34.794] TRAIN: iteration 10609 : loss : 0.113537, loss_ce: 0.008741, loss_dice: 0.218333
[09:31:35.002] TRAIN: iteration 10610 : loss : 0.192990, loss_ce: 0.008510, loss_dice: 0.377469
[09:31:35.211] TRAIN: iteration 10611 : loss : 0.156064, loss_ce: 0.007067, loss_dice: 0.305061
[09:31:35.438] TRAIN: iteration 10612 : loss : 0.124097, loss_ce: 0.004210, loss_dice: 0.243984
[09:31:35.652] TRAIN: iteration 10613 : loss : 0.172855, loss_ce: 0.005604, loss_dice: 0.340107
[09:31:35.861] TRAIN: iteration 10614 : loss : 0.185774, loss_ce: 0.005030, loss_dice: 0.366518
[09:31:36.070] TRAIN: iteration 10615 : loss : 0.090081, loss_ce: 0.003738, loss_dice: 0.176425
[09:31:36.279] TRAIN: iteration 10616 : loss : 0.118550, loss_ce: 0.004939, loss_dice: 0.232162
[09:31:36.496] TRAIN: iteration 10617 : loss : 0.182661, loss_ce: 0.009703, loss_dice: 0.355619
[09:31:36.707] TRAIN: iteration 10618 : loss : 0.067386, loss_ce: 0.004901, loss_dice: 0.129870
[09:31:36.917] TRAIN: iteration 10619 : loss : 0.103159, loss_ce: 0.002050, loss_dice: 0.204269
[09:31:37.128] TRAIN: iteration 10620 : loss : 0.079475, loss_ce: 0.004860, loss_dice: 0.154091
[09:31:37.367] TRAIN: iteration 10621 : loss : 0.112045, loss_ce: 0.002815, loss_dice: 0.221274
[09:31:37.575] TRAIN: iteration 10622 : loss : 0.050415, loss_ce: 0.001827, loss_dice: 0.099002
[09:31:37.785] TRAIN: iteration 10623 : loss : 0.251736, loss_ce: 0.003239, loss_dice: 0.500233
[09:31:37.993] TRAIN: iteration 10624 : loss : 0.120606, loss_ce: 0.010932, loss_dice: 0.230280
[09:31:38.202] TRAIN: iteration 10625 : loss : 0.169332, loss_ce: 0.007849, loss_dice: 0.330814
[09:31:38.415] TRAIN: iteration 10626 : loss : 0.051155, loss_ce: 0.004367, loss_dice: 0.097943
[09:31:38.625] TRAIN: iteration 10627 : loss : 0.053664, loss_ce: 0.002424, loss_dice: 0.104904
[09:31:38.833] TRAIN: iteration 10628 : loss : 0.117230, loss_ce: 0.001715, loss_dice: 0.232746
[09:31:39.043] TRAIN: iteration 10629 : loss : 0.251557, loss_ce: 0.002916, loss_dice: 0.500199
[09:31:39.253] TRAIN: iteration 10630 : loss : 0.117773, loss_ce: 0.002332, loss_dice: 0.233214
[09:31:39.463] TRAIN: iteration 10631 : loss : 0.071715, loss_ce: 0.002590, loss_dice: 0.140839
[09:31:39.671] TRAIN: iteration 10632 : loss : 0.140241, loss_ce: 0.003923, loss_dice: 0.276560
[09:31:39.879] TRAIN: iteration 10633 : loss : 0.184398, loss_ce: 0.009157, loss_dice: 0.359639
[09:31:40.088] TRAIN: iteration 10634 : loss : 0.069972, loss_ce: 0.006446, loss_dice: 0.133499
[09:31:40.297] TRAIN: iteration 10635 : loss : 0.090177, loss_ce: 0.005696, loss_dice: 0.174658
[09:31:40.505] TRAIN: iteration 10636 : loss : 0.197901, loss_ce: 0.003158, loss_dice: 0.392644
[09:31:40.714] TRAIN: iteration 10637 : loss : 0.145527, loss_ce: 0.009883, loss_dice: 0.281171
[09:31:40.922] TRAIN: iteration 10638 : loss : 0.246019, loss_ce: 0.004159, loss_dice: 0.487879
[09:31:41.131] TRAIN: iteration 10639 : loss : 0.117575, loss_ce: 0.005775, loss_dice: 0.229376
[09:31:41.342] TRAIN: iteration 10640 : loss : 0.183457, loss_ce: 0.005363, loss_dice: 0.361550
[09:31:41.604] TRAIN: iteration 10641 : loss : 0.143352, loss_ce: 0.006992, loss_dice: 0.279711
[09:31:41.813] TRAIN: iteration 10642 : loss : 0.136455, loss_ce: 0.004855, loss_dice: 0.268055
[09:31:42.023] TRAIN: iteration 10643 : loss : 0.190135, loss_ce: 0.007169, loss_dice: 0.373100
[09:31:42.230] TRAIN: iteration 10644 : loss : 0.076390, loss_ce: 0.004706, loss_dice: 0.148074
[09:31:42.437] TRAIN: iteration 10645 : loss : 0.077275, loss_ce: 0.007893, loss_dice: 0.146657
[09:31:42.652] TRAIN: iteration 10646 : loss : 0.253172, loss_ce: 0.005890, loss_dice: 0.500455
[09:31:42.860] TRAIN: iteration 10647 : loss : 0.252914, loss_ce: 0.005595, loss_dice: 0.500233
[09:31:43.072] TRAIN: iteration 10648 : loss : 0.109652, loss_ce: 0.016054, loss_dice: 0.203249
[09:31:43.279] TRAIN: iteration 10649 : loss : 0.250269, loss_ce: 0.003895, loss_dice: 0.496643
[09:31:43.486] TRAIN: iteration 10650 : loss : 0.088497, loss_ce: 0.005389, loss_dice: 0.171604
[09:31:43.694] TRAIN: iteration 10651 : loss : 0.069659, loss_ce: 0.007511, loss_dice: 0.131807
[09:31:43.909] TRAIN: iteration 10652 : loss : 0.246646, loss_ce: 0.002839, loss_dice: 0.490454
[09:31:44.119] TRAIN: iteration 10653 : loss : 0.127013, loss_ce: 0.006352, loss_dice: 0.247674
[09:31:44.326] TRAIN: iteration 10654 : loss : 0.023160, loss_ce: 0.002619, loss_dice: 0.043700
[09:31:44.552] TRAIN: iteration 10655 : loss : 0.135656, loss_ce: 0.011723, loss_dice: 0.259588
[09:31:44.760] TRAIN: iteration 10656 : loss : 0.136635, loss_ce: 0.007367, loss_dice: 0.265904
[09:31:44.968] TRAIN: iteration 10657 : loss : 0.038714, loss_ce: 0.004882, loss_dice: 0.072547
[09:31:45.395] TRAIN: iteration 10658 : loss : 0.136836, loss_ce: 0.006239, loss_dice: 0.267433
[09:31:45.605] TRAIN: iteration 10659 : loss : 0.094212, loss_ce: 0.006133, loss_dice: 0.182291
[09:31:45.813] TRAIN: iteration 10660 : loss : 0.237742, loss_ce: 0.006589, loss_dice: 0.468895
[09:31:46.055] TRAIN: iteration 10661 : loss : 0.130242, loss_ce: 0.005639, loss_dice: 0.254844
[09:31:46.263] TRAIN: iteration 10662 : loss : 0.093192, loss_ce: 0.005934, loss_dice: 0.180450
[09:31:46.470] TRAIN: iteration 10663 : loss : 0.252437, loss_ce: 0.004535, loss_dice: 0.500339
[09:31:46.676] TRAIN: iteration 10664 : loss : 0.066468, loss_ce: 0.005001, loss_dice: 0.127935
[09:31:46.884] TRAIN: iteration 10665 : loss : 0.180004, loss_ce: 0.008800, loss_dice: 0.351208
[09:31:47.092] TRAIN: iteration 10666 : loss : 0.137501, loss_ce: 0.012672, loss_dice: 0.262330
[09:31:47.299] TRAIN: iteration 10667 : loss : 0.181059, loss_ce: 0.005022, loss_dice: 0.357095
[09:31:47.507] TRAIN: iteration 10668 : loss : 0.253829, loss_ce: 0.009073, loss_dice: 0.498584
[09:31:47.714] TRAIN: iteration 10669 : loss : 0.250377, loss_ce: 0.003801, loss_dice: 0.496953
[09:31:47.922] TRAIN: iteration 10670 : loss : 0.165383, loss_ce: 0.004306, loss_dice: 0.326461
[09:31:48.129] TRAIN: iteration 10671 : loss : 0.082099, loss_ce: 0.004248, loss_dice: 0.159950
[09:31:48.337] TRAIN: iteration 10672 : loss : 0.253441, loss_ce: 0.010335, loss_dice: 0.496548
[09:31:48.545] TRAIN: iteration 10673 : loss : 0.149547, loss_ce: 0.003478, loss_dice: 0.295615
[09:31:48.754] TRAIN: iteration 10674 : loss : 0.064005, loss_ce: 0.008769, loss_dice: 0.119241
[09:31:48.969] TRAIN: iteration 10675 : loss : 0.178146, loss_ce: 0.006704, loss_dice: 0.349589
[09:31:49.177] TRAIN: iteration 10676 : loss : 0.135353, loss_ce: 0.003897, loss_dice: 0.266810
[09:31:49.385] TRAIN: iteration 10677 : loss : 0.036397, loss_ce: 0.003415, loss_dice: 0.069380
[09:31:49.593] TRAIN: iteration 10678 : loss : 0.090778, loss_ce: 0.008669, loss_dice: 0.172887
[09:31:50.370] TRAIN: iteration 10679 : loss : 0.069291, loss_ce: 0.003155, loss_dice: 0.135427
[09:31:50.587] TRAIN: iteration 10680 : loss : 0.103528, loss_ce: 0.003135, loss_dice: 0.203920
[09:31:50.825] TRAIN: iteration 10681 : loss : 0.249414, loss_ce: 0.004856, loss_dice: 0.493972
[09:31:51.034] TRAIN: iteration 10682 : loss : 0.079759, loss_ce: 0.003656, loss_dice: 0.155862
[09:31:51.244] TRAIN: iteration 10683 : loss : 0.235819, loss_ce: 0.002577, loss_dice: 0.469061
[09:31:51.452] TRAIN: iteration 10684 : loss : 0.141340, loss_ce: 0.003524, loss_dice: 0.279157
[09:31:51.662] TRAIN: iteration 10685 : loss : 0.137747, loss_ce: 0.003015, loss_dice: 0.272478
[09:31:51.871] TRAIN: iteration 10686 : loss : 0.082480, loss_ce: 0.002913, loss_dice: 0.162047
[09:31:52.097] TRAIN: iteration 10687 : loss : 0.047445, loss_ce: 0.001403, loss_dice: 0.093487
[09:31:52.304] TRAIN: iteration 10688 : loss : 0.121139, loss_ce: 0.002438, loss_dice: 0.239840
[09:31:52.511] TRAIN: iteration 10689 : loss : 0.165831, loss_ce: 0.020742, loss_dice: 0.310921
[09:31:52.719] TRAIN: iteration 10690 : loss : 0.236493, loss_ce: 0.003734, loss_dice: 0.469253
[09:31:52.927] TRAIN: iteration 10691 : loss : 0.232317, loss_ce: 0.006690, loss_dice: 0.457944
[09:31:53.134] TRAIN: iteration 10692 : loss : 0.116199, loss_ce: 0.005363, loss_dice: 0.227035
[09:31:53.342] TRAIN: iteration 10693 : loss : 0.195678, loss_ce: 0.008092, loss_dice: 0.383263
[09:31:53.549] TRAIN: iteration 10694 : loss : 0.177476, loss_ce: 0.003981, loss_dice: 0.350972
[09:31:53.756] TRAIN: iteration 10695 : loss : 0.251456, loss_ce: 0.002742, loss_dice: 0.500170
[09:31:53.963] TRAIN: iteration 10696 : loss : 0.220482, loss_ce: 0.005379, loss_dice: 0.435585
[09:31:54.171] TRAIN: iteration 10697 : loss : 0.243106, loss_ce: 0.002349, loss_dice: 0.483863
[09:31:54.517] TRAIN: iteration 10698 : loss : 0.066002, loss_ce: 0.002662, loss_dice: 0.129342
[09:31:54.724] TRAIN: iteration 10699 : loss : 0.115793, loss_ce: 0.004051, loss_dice: 0.227534
[09:31:54.932] TRAIN: iteration 10700 : loss : 0.161313, loss_ce: 0.008381, loss_dice: 0.314244
[09:31:55.182] TRAIN: iteration 10701 : loss : 0.085887, loss_ce: 0.004112, loss_dice: 0.167662
[09:31:55.393] TRAIN: iteration 10702 : loss : 0.249315, loss_ce: 0.004253, loss_dice: 0.494377
[09:31:55.600] TRAIN: iteration 10703 : loss : 0.251870, loss_ce: 0.003509, loss_dice: 0.500231
[09:31:55.808] TRAIN: iteration 10704 : loss : 0.083103, loss_ce: 0.004345, loss_dice: 0.161860
[09:31:56.018] TRAIN: iteration 10705 : loss : 0.150726, loss_ce: 0.006261, loss_dice: 0.295192
[09:31:56.496] TRAIN: iteration 10706 : loss : 0.187175, loss_ce: 0.004155, loss_dice: 0.370194
[09:31:56.707] TRAIN: iteration 10707 : loss : 0.196222, loss_ce: 0.005287, loss_dice: 0.387157
[09:31:56.916] TRAIN: iteration 10708 : loss : 0.107404, loss_ce: 0.004280, loss_dice: 0.210527
[09:31:57.124] TRAIN: iteration 10709 : loss : 0.107824, loss_ce: 0.006188, loss_dice: 0.209459
[09:31:57.334] TRAIN: iteration 10710 : loss : 0.110764, loss_ce: 0.009792, loss_dice: 0.211736
[09:31:57.544] TRAIN: iteration 10711 : loss : 0.101987, loss_ce: 0.003354, loss_dice: 0.200621
[09:31:57.755] TRAIN: iteration 10712 : loss : 0.201569, loss_ce: 0.009553, loss_dice: 0.393586
[09:31:57.966] TRAIN: iteration 10713 : loss : 0.176606, loss_ce: 0.004005, loss_dice: 0.349207
[09:31:58.173] TRAIN: iteration 10714 : loss : 0.100824, loss_ce: 0.006022, loss_dice: 0.195625
[09:31:58.380] TRAIN: iteration 10715 : loss : 0.181037, loss_ce: 0.006415, loss_dice: 0.355660
[09:31:58.592] TRAIN: iteration 10716 : loss : 0.095303, loss_ce: 0.005023, loss_dice: 0.185583
[09:31:58.798] TRAIN: iteration 10717 : loss : 0.252192, loss_ce: 0.004104, loss_dice: 0.500279
[09:31:59.006] TRAIN: iteration 10718 : loss : 0.114074, loss_ce: 0.016956, loss_dice: 0.211193
[09:31:59.217] TRAIN: iteration 10719 : loss : 0.252147, loss_ce: 0.004015, loss_dice: 0.500278
[09:31:59.495] TRAIN: iteration 10720 : loss : 0.151440, loss_ce: 0.007566, loss_dice: 0.295315
[09:31:59.739] TRAIN: iteration 10721 : loss : 0.212218, loss_ce: 0.008433, loss_dice: 0.416003
[09:31:59.946] TRAIN: iteration 10722 : loss : 0.155766, loss_ce: 0.033428, loss_dice: 0.278105
[09:32:00.154] TRAIN: iteration 10723 : loss : 0.199891, loss_ce: 0.006557, loss_dice: 0.393225
[09:32:00.364] TRAIN: iteration 10724 : loss : 0.120759, loss_ce: 0.006792, loss_dice: 0.234726
[09:32:00.572] TRAIN: iteration 10725 : loss : 0.138192, loss_ce: 0.003680, loss_dice: 0.272704
[09:32:00.783] TRAIN: iteration 10726 : loss : 0.251093, loss_ce: 0.005657, loss_dice: 0.496528
[09:32:00.992] TRAIN: iteration 10727 : loss : 0.136894, loss_ce: 0.005406, loss_dice: 0.268383
[09:32:01.199] TRAIN: iteration 10728 : loss : 0.253328, loss_ce: 0.006185, loss_dice: 0.500471
[09:32:01.407] TRAIN: iteration 10729 : loss : 0.255551, loss_ce: 0.011250, loss_dice: 0.499853
[09:32:01.616] TRAIN: iteration 10730 : loss : 0.252171, loss_ce: 0.004097, loss_dice: 0.500246
[09:32:01.824] TRAIN: iteration 10731 : loss : 0.095133, loss_ce: 0.003220, loss_dice: 0.187046
[09:32:02.033] TRAIN: iteration 10732 : loss : 0.089459, loss_ce: 0.004471, loss_dice: 0.174447
[09:32:02.240] TRAIN: iteration 10733 : loss : 0.240800, loss_ce: 0.003803, loss_dice: 0.477796
[09:32:02.455] TRAIN: iteration 10734 : loss : 0.213965, loss_ce: 0.003615, loss_dice: 0.424315
[09:32:02.809] TRAIN: iteration 10735 : loss : 0.252589, loss_ce: 0.004817, loss_dice: 0.500360
[09:32:03.016] TRAIN: iteration 10736 : loss : 0.222558, loss_ce: 0.002447, loss_dice: 0.442668
[09:32:03.231] TRAIN: iteration 10737 : loss : 0.225890, loss_ce: 0.044548, loss_dice: 0.407232
[09:32:03.445] TRAIN: iteration 10738 : loss : 0.139623, loss_ce: 0.004015, loss_dice: 0.275232
[09:32:03.660] TRAIN: iteration 10739 : loss : 0.251586, loss_ce: 0.002996, loss_dice: 0.500175
[09:32:03.879] TRAIN: iteration 10740 : loss : 0.165377, loss_ce: 0.003858, loss_dice: 0.326896
[09:32:04.116] TRAIN: iteration 10741 : loss : 0.130509, loss_ce: 0.002766, loss_dice: 0.258252
[09:32:04.326] TRAIN: iteration 10742 : loss : 0.230834, loss_ce: 0.011357, loss_dice: 0.450310
[09:32:04.537] TRAIN: iteration 10743 : loss : 0.250918, loss_ce: 0.001764, loss_dice: 0.500072
[09:32:04.744] TRAIN: iteration 10744 : loss : 0.088541, loss_ce: 0.004586, loss_dice: 0.172496
[09:32:04.955] TRAIN: iteration 10745 : loss : 0.246735, loss_ce: 0.010871, loss_dice: 0.482599
[09:32:05.169] TRAIN: iteration 10746 : loss : 0.123429, loss_ce: 0.008205, loss_dice: 0.238653
[09:32:05.376] TRAIN: iteration 10747 : loss : 0.127460, loss_ce: 0.003140, loss_dice: 0.251779
[09:32:05.584] TRAIN: iteration 10748 : loss : 0.251833, loss_ce: 0.003438, loss_dice: 0.500228
[09:32:05.795] TRAIN: iteration 10749 : loss : 0.099503, loss_ce: 0.009608, loss_dice: 0.189398
[09:32:06.010] TRAIN: iteration 10750 : loss : 0.196019, loss_ce: 0.006879, loss_dice: 0.385159
[09:32:06.225] TRAIN: iteration 10751 : loss : 0.070999, loss_ce: 0.004271, loss_dice: 0.137728
[09:32:06.432] TRAIN: iteration 10752 : loss : 0.053764, loss_ce: 0.001741, loss_dice: 0.105788
[09:32:06.645] TRAIN: iteration 10753 : loss : 0.094779, loss_ce: 0.009822, loss_dice: 0.179735
[09:32:06.855] TRAIN: iteration 10754 : loss : 0.071819, loss_ce: 0.003463, loss_dice: 0.140175
[09:32:07.071] TRAIN: iteration 10755 : loss : 0.153958, loss_ce: 0.004481, loss_dice: 0.303435
[09:32:07.284] TRAIN: iteration 10756 : loss : 0.252669, loss_ce: 0.005590, loss_dice: 0.499747
[09:32:07.492] TRAIN: iteration 10757 : loss : 0.207278, loss_ce: 0.006211, loss_dice: 0.408345
[09:32:07.705] TRAIN: iteration 10758 : loss : 0.087496, loss_ce: 0.003871, loss_dice: 0.171122
[09:32:07.918] TRAIN: iteration 10759 : loss : 0.061306, loss_ce: 0.004919, loss_dice: 0.117693
[09:32:08.125] TRAIN: iteration 10760 : loss : 0.067621, loss_ce: 0.006233, loss_dice: 0.129009
[09:32:08.368] TRAIN: iteration 10761 : loss : 0.085305, loss_ce: 0.005254, loss_dice: 0.165357
[09:32:08.579] TRAIN: iteration 10762 : loss : 0.145184, loss_ce: 0.007864, loss_dice: 0.282504
[09:32:08.786] TRAIN: iteration 10763 : loss : 0.206320, loss_ce: 0.007718, loss_dice: 0.404923
[09:32:08.993] TRAIN: iteration 10764 : loss : 0.252667, loss_ce: 0.004986, loss_dice: 0.500348
[09:32:09.202] TRAIN: iteration 10765 : loss : 0.142873, loss_ce: 0.003822, loss_dice: 0.281924
[09:32:09.409] TRAIN: iteration 10766 : loss : 0.132224, loss_ce: 0.011670, loss_dice: 0.252779
[09:32:09.618] TRAIN: iteration 10767 : loss : 0.090818, loss_ce: 0.006618, loss_dice: 0.175019
[09:32:09.826] TRAIN: iteration 10768 : loss : 0.055185, loss_ce: 0.004170, loss_dice: 0.106199
[09:32:10.035] TRAIN: iteration 10769 : loss : 0.252331, loss_ce: 0.008985, loss_dice: 0.495676
[09:32:10.244] TRAIN: iteration 10770 : loss : 0.184047, loss_ce: 0.005271, loss_dice: 0.362824
[09:32:10.480] TRAIN: iteration 10771 : loss : 0.058541, loss_ce: 0.004318, loss_dice: 0.112764
[09:32:10.688] TRAIN: iteration 10772 : loss : 0.250325, loss_ce: 0.009376, loss_dice: 0.491274
[09:32:10.895] TRAIN: iteration 10773 : loss : 0.167127, loss_ce: 0.004681, loss_dice: 0.329572
[09:32:11.109] TRAIN: iteration 10774 : loss : 0.116719, loss_ce: 0.015149, loss_dice: 0.218288
[09:32:11.325] TRAIN: iteration 10775 : loss : 0.252734, loss_ce: 0.005120, loss_dice: 0.500348
[09:32:11.534] TRAIN: iteration 10776 : loss : 0.136672, loss_ce: 0.003678, loss_dice: 0.269666
[09:32:11.742] TRAIN: iteration 10777 : loss : 0.106156, loss_ce: 0.002343, loss_dice: 0.209968
[09:32:11.959] TRAIN: iteration 10778 : loss : 0.075220, loss_ce: 0.004170, loss_dice: 0.146269
[09:32:12.167] TRAIN: iteration 10779 : loss : 0.105187, loss_ce: 0.012112, loss_dice: 0.198261
[09:32:12.376] TRAIN: iteration 10780 : loss : 0.057876, loss_ce: 0.002019, loss_dice: 0.113733
[09:32:12.613] TRAIN: iteration 10781 : loss : 0.089984, loss_ce: 0.002566, loss_dice: 0.177402
[09:32:12.820] TRAIN: iteration 10782 : loss : 0.129836, loss_ce: 0.004421, loss_dice: 0.255252
[09:32:13.037] TRAIN: iteration 10783 : loss : 0.214113, loss_ce: 0.003283, loss_dice: 0.424943
[09:32:13.248] TRAIN: iteration 10784 : loss : 0.151957, loss_ce: 0.025218, loss_dice: 0.278696
[09:32:13.464] TRAIN: iteration 10785 : loss : 0.190169, loss_ce: 0.011055, loss_dice: 0.369282
[09:32:13.671] TRAIN: iteration 10786 : loss : 0.125352, loss_ce: 0.016081, loss_dice: 0.234622
[09:32:13.879] TRAIN: iteration 10787 : loss : 0.040937, loss_ce: 0.002218, loss_dice: 0.079655
[09:32:14.090] TRAIN: iteration 10788 : loss : 0.080121, loss_ce: 0.007103, loss_dice: 0.153139
[09:32:14.299] TRAIN: iteration 10789 : loss : 0.211648, loss_ce: 0.006374, loss_dice: 0.416923
[09:32:14.508] TRAIN: iteration 10790 : loss : 0.093341, loss_ce: 0.001904, loss_dice: 0.184779
[09:32:14.715] TRAIN: iteration 10791 : loss : 0.070389, loss_ce: 0.001580, loss_dice: 0.139198
[09:32:14.922] TRAIN: iteration 10792 : loss : 0.147987, loss_ce: 0.008551, loss_dice: 0.287423
[09:32:15.130] TRAIN: iteration 10793 : loss : 0.040095, loss_ce: 0.002602, loss_dice: 0.077589
[09:32:15.342] TRAIN: iteration 10794 : loss : 0.250921, loss_ce: 0.004950, loss_dice: 0.496891
[09:32:15.549] TRAIN: iteration 10795 : loss : 0.180271, loss_ce: 0.007199, loss_dice: 0.353343
[09:32:15.756] TRAIN: iteration 10796 : loss : 0.113550, loss_ce: 0.004795, loss_dice: 0.222306
[09:32:15.963] TRAIN: iteration 10797 : loss : 0.071582, loss_ce: 0.004522, loss_dice: 0.138643
[09:32:16.171] TRAIN: iteration 10798 : loss : 0.121883, loss_ce: 0.007268, loss_dice: 0.236498
[09:32:16.379] TRAIN: iteration 10799 : loss : 0.081727, loss_ce: 0.008966, loss_dice: 0.154488
[09:32:16.586] TRAIN: iteration 10800 : loss : 0.124683, loss_ce: 0.010896, loss_dice: 0.238469
[09:32:16.837] TRAIN: iteration 10801 : loss : 0.251334, loss_ce: 0.018370, loss_dice: 0.484298
[09:32:17.045] TRAIN: iteration 10802 : loss : 0.065791, loss_ce: 0.005028, loss_dice: 0.126554
[09:32:17.254] TRAIN: iteration 10803 : loss : 0.253038, loss_ce: 0.007282, loss_dice: 0.498794
[09:32:17.462] TRAIN: iteration 10804 : loss : 0.185739, loss_ce: 0.007228, loss_dice: 0.364249
[09:32:17.678] TRAIN: iteration 10805 : loss : 0.140865, loss_ce: 0.004708, loss_dice: 0.277023
[09:32:17.889] TRAIN: iteration 10806 : loss : 0.120967, loss_ce: 0.010061, loss_dice: 0.231873
[09:32:18.101] TRAIN: iteration 10807 : loss : 0.061013, loss_ce: 0.006210, loss_dice: 0.115815
[09:32:18.905] TRAIN: iteration 10808 : loss : 0.066758, loss_ce: 0.011382, loss_dice: 0.122134
[09:32:19.113] TRAIN: iteration 10809 : loss : 0.110331, loss_ce: 0.004478, loss_dice: 0.216184
[09:32:19.325] TRAIN: iteration 10810 : loss : 0.101095, loss_ce: 0.010971, loss_dice: 0.191220
[09:32:19.575] TRAIN: iteration 10811 : loss : 0.152944, loss_ce: 0.005954, loss_dice: 0.299934
[09:32:19.785] TRAIN: iteration 10812 : loss : 0.084213, loss_ce: 0.004593, loss_dice: 0.163832
[09:32:20.000] TRAIN: iteration 10813 : loss : 0.089877, loss_ce: 0.011651, loss_dice: 0.168104
[09:32:20.282] TRAIN: iteration 10814 : loss : 0.086891, loss_ce: 0.010756, loss_dice: 0.163026
[09:32:20.489] TRAIN: iteration 10815 : loss : 0.107003, loss_ce: 0.005878, loss_dice: 0.208128
[09:32:20.696] TRAIN: iteration 10816 : loss : 0.231905, loss_ce: 0.008060, loss_dice: 0.455749
[09:32:20.903] TRAIN: iteration 10817 : loss : 0.104096, loss_ce: 0.003151, loss_dice: 0.205042
[09:32:21.114] TRAIN: iteration 10818 : loss : 0.108309, loss_ce: 0.003253, loss_dice: 0.213365
[09:32:21.322] TRAIN: iteration 10819 : loss : 0.105856, loss_ce: 0.003556, loss_dice: 0.208155
[09:32:21.530] TRAIN: iteration 10820 : loss : 0.249515, loss_ce: 0.005883, loss_dice: 0.493146
[09:32:21.768] TRAIN: iteration 10821 : loss : 0.117251, loss_ce: 0.008850, loss_dice: 0.225652
[09:32:21.975] TRAIN: iteration 10822 : loss : 0.053386, loss_ce: 0.004135, loss_dice: 0.102637
[09:32:22.182] TRAIN: iteration 10823 : loss : 0.088215, loss_ce: 0.003736, loss_dice: 0.172694
[09:32:22.391] TRAIN: iteration 10824 : loss : 0.036578, loss_ce: 0.002356, loss_dice: 0.070799
[09:32:22.599] TRAIN: iteration 10825 : loss : 0.046106, loss_ce: 0.002523, loss_dice: 0.089688
[09:32:22.812] TRAIN: iteration 10826 : loss : 0.109942, loss_ce: 0.008796, loss_dice: 0.211089
[09:32:23.023] TRAIN: iteration 10827 : loss : 0.162738, loss_ce: 0.004697, loss_dice: 0.320779
[09:32:23.230] TRAIN: iteration 10828 : loss : 0.156748, loss_ce: 0.008125, loss_dice: 0.305371
[09:32:23.437] TRAIN: iteration 10829 : loss : 0.090099, loss_ce: 0.003679, loss_dice: 0.176519
[09:32:23.661] TRAIN: iteration 10830 : loss : 0.093326, loss_ce: 0.011067, loss_dice: 0.175585
[09:32:23.869] TRAIN: iteration 10831 : loss : 0.247612, loss_ce: 0.041389, loss_dice: 0.453834
[09:32:24.076] TRAIN: iteration 10832 : loss : 0.127359, loss_ce: 0.003917, loss_dice: 0.250802
[09:32:24.284] TRAIN: iteration 10833 : loss : 0.244832, loss_ce: 0.005442, loss_dice: 0.484222
[09:32:24.492] TRAIN: iteration 10834 : loss : 0.158654, loss_ce: 0.002957, loss_dice: 0.314351
[09:32:24.700] TRAIN: iteration 10835 : loss : 0.170923, loss_ce: 0.005587, loss_dice: 0.336259
[09:32:24.907] TRAIN: iteration 10836 : loss : 0.053992, loss_ce: 0.003695, loss_dice: 0.104288
[09:32:25.256] TRAIN: iteration 10837 : loss : 0.100850, loss_ce: 0.007363, loss_dice: 0.194337
[09:32:25.464] TRAIN: iteration 10838 : loss : 0.210504, loss_ce: 0.003571, loss_dice: 0.417437
[09:32:25.676] TRAIN: iteration 10839 : loss : 0.192746, loss_ce: 0.006903, loss_dice: 0.378589
[09:32:25.883] TRAIN: iteration 10840 : loss : 0.194927, loss_ce: 0.008139, loss_dice: 0.381715
[09:32:26.130] TRAIN: iteration 10841 : loss : 0.102559, loss_ce: 0.006496, loss_dice: 0.198622
[09:32:26.346] TRAIN: iteration 10842 : loss : 0.252316, loss_ce: 0.004360, loss_dice: 0.500273
[09:32:26.553] TRAIN: iteration 10843 : loss : 0.238856, loss_ce: 0.004847, loss_dice: 0.472864
[09:32:26.760] TRAIN: iteration 10844 : loss : 0.178268, loss_ce: 0.004950, loss_dice: 0.351587
[09:32:26.971] TRAIN: iteration 10845 : loss : 0.113630, loss_ce: 0.005726, loss_dice: 0.221535
[09:32:27.179] TRAIN: iteration 10846 : loss : 0.180525, loss_ce: 0.003815, loss_dice: 0.357235
[09:32:27.389] TRAIN: iteration 10847 : loss : 0.144554, loss_ce: 0.007209, loss_dice: 0.281899
[09:32:27.596] TRAIN: iteration 10848 : loss : 0.251676, loss_ce: 0.003885, loss_dice: 0.499468
[09:32:27.803] TRAIN: iteration 10849 : loss : 0.125344, loss_ce: 0.004331, loss_dice: 0.246356
[09:32:31.137] TRAIN: iteration 10850 : loss : 0.183612, loss_ce: 0.006443, loss_dice: 0.360782
[09:32:31.350] TRAIN: iteration 10851 : loss : 0.047437, loss_ce: 0.003992, loss_dice: 0.090882
[09:32:31.561] TRAIN: iteration 10852 : loss : 0.041250, loss_ce: 0.006480, loss_dice: 0.076019
[09:32:31.777] TRAIN: iteration 10853 : loss : 0.088988, loss_ce: 0.004839, loss_dice: 0.173137
[09:32:31.991] TRAIN: iteration 10854 : loss : 0.251759, loss_ce: 0.003332, loss_dice: 0.500186
[09:32:32.278] TRAIN: iteration 10855 : loss : 0.067812, loss_ce: 0.004208, loss_dice: 0.131415
[09:32:32.486] TRAIN: iteration 10856 : loss : 0.238209, loss_ce: 0.003294, loss_dice: 0.473123
[09:32:32.696] TRAIN: iteration 10857 : loss : 0.150701, loss_ce: 0.005661, loss_dice: 0.295740
[09:32:33.153] TRAIN: iteration 10858 : loss : 0.246485, loss_ce: 0.003171, loss_dice: 0.489800
[09:32:33.360] TRAIN: iteration 10859 : loss : 0.062267, loss_ce: 0.003865, loss_dice: 0.120669
[09:32:33.567] TRAIN: iteration 10860 : loss : 0.093294, loss_ce: 0.007093, loss_dice: 0.179496
[09:32:33.801] TRAIN: iteration 10861 : loss : 0.245392, loss_ce: 0.003447, loss_dice: 0.487336
[09:32:34.017] TRAIN: iteration 10862 : loss : 0.105504, loss_ce: 0.011981, loss_dice: 0.199027
[09:32:34.227] TRAIN: iteration 10863 : loss : 0.148664, loss_ce: 0.004445, loss_dice: 0.292882
[09:32:34.435] TRAIN: iteration 10864 : loss : 0.075414, loss_ce: 0.002972, loss_dice: 0.147857
[09:32:34.686] TRAIN: iteration 10865 : loss : 0.169967, loss_ce: 0.009457, loss_dice: 0.330478
[09:32:34.892] TRAIN: iteration 10866 : loss : 0.045623, loss_ce: 0.005192, loss_dice: 0.086055
[09:32:35.104] TRAIN: iteration 10867 : loss : 0.117604, loss_ce: 0.004939, loss_dice: 0.230268
[09:32:35.318] TRAIN: iteration 10868 : loss : 0.215842, loss_ce: 0.005968, loss_dice: 0.425716
[09:32:35.525] TRAIN: iteration 10869 : loss : 0.136836, loss_ce: 0.003572, loss_dice: 0.270099
[09:32:35.803] TRAIN: iteration 10870 : loss : 0.251002, loss_ce: 0.001949, loss_dice: 0.500054
[09:32:36.019] TRAIN: iteration 10871 : loss : 0.144706, loss_ce: 0.003302, loss_dice: 0.286110
[09:32:36.227] TRAIN: iteration 10872 : loss : 0.167726, loss_ce: 0.003981, loss_dice: 0.331471
[09:32:36.434] TRAIN: iteration 10873 : loss : 0.104230, loss_ce: 0.009771, loss_dice: 0.198689
[09:32:36.641] TRAIN: iteration 10874 : loss : 0.250347, loss_ce: 0.000687, loss_dice: 0.500007
[09:32:36.851] TRAIN: iteration 10875 : loss : 0.091219, loss_ce: 0.008682, loss_dice: 0.173755
[09:32:37.058] TRAIN: iteration 10876 : loss : 0.143672, loss_ce: 0.005234, loss_dice: 0.282111
[09:32:37.264] TRAIN: iteration 10877 : loss : 0.187807, loss_ce: 0.002621, loss_dice: 0.372994
[09:32:37.471] TRAIN: iteration 10878 : loss : 0.169877, loss_ce: 0.003680, loss_dice: 0.336074
[09:32:37.680] TRAIN: iteration 10879 : loss : 0.250455, loss_ce: 0.000873, loss_dice: 0.500036
[09:32:37.918] TRAIN: iteration 10880 : loss : 0.250556, loss_ce: 0.001066, loss_dice: 0.500047
[09:32:38.152] TRAIN: iteration 10881 : loss : 0.177743, loss_ce: 0.008221, loss_dice: 0.347266
[09:32:38.367] TRAIN: iteration 10882 : loss : 0.233632, loss_ce: 0.015817, loss_dice: 0.451448
[09:32:38.580] TRAIN: iteration 10883 : loss : 0.070957, loss_ce: 0.002879, loss_dice: 0.139036
[09:32:38.789] TRAIN: iteration 10884 : loss : 0.250683, loss_ce: 0.001313, loss_dice: 0.500053
[09:32:39.003] TRAIN: iteration 10885 : loss : 0.141832, loss_ce: 0.011628, loss_dice: 0.272037
[09:32:39.211] TRAIN: iteration 10886 : loss : 0.130425, loss_ce: 0.009691, loss_dice: 0.251159
[09:32:39.421] TRAIN: iteration 10887 : loss : 0.141659, loss_ce: 0.004573, loss_dice: 0.278746
[09:32:39.629] TRAIN: iteration 10888 : loss : 0.222602, loss_ce: 0.003641, loss_dice: 0.441563
[09:32:39.837] TRAIN: iteration 10889 : loss : 0.253234, loss_ce: 0.006027, loss_dice: 0.500440
[09:32:40.046] TRAIN: iteration 10890 : loss : 0.126360, loss_ce: 0.005777, loss_dice: 0.246944
[09:32:40.265] TRAIN: iteration 10891 : loss : 0.251674, loss_ce: 0.003169, loss_dice: 0.500178
[09:32:40.475] TRAIN: iteration 10892 : loss : 0.253555, loss_ce: 0.006599, loss_dice: 0.500510
[09:32:40.681] TRAIN: iteration 10893 : loss : 0.180312, loss_ce: 0.014286, loss_dice: 0.346338
[09:32:40.897] TRAIN: iteration 10894 : loss : 0.103437, loss_ce: 0.013240, loss_dice: 0.193634
[09:32:41.110] TRAIN: iteration 10895 : loss : 0.068105, loss_ce: 0.009607, loss_dice: 0.126603
[09:32:41.317] TRAIN: iteration 10896 : loss : 0.070470, loss_ce: 0.006230, loss_dice: 0.134711
[09:32:41.531] TRAIN: iteration 10897 : loss : 0.173705, loss_ce: 0.004564, loss_dice: 0.342846
[09:32:41.739] TRAIN: iteration 10898 : loss : 0.249287, loss_ce: 0.007369, loss_dice: 0.491205
[09:32:41.951] TRAIN: iteration 10899 : loss : 0.229289, loss_ce: 0.004280, loss_dice: 0.454297
[09:32:42.158] TRAIN: iteration 10900 : loss : 0.072066, loss_ce: 0.006310, loss_dice: 0.137821
[09:32:42.399] TRAIN: iteration 10901 : loss : 0.168775, loss_ce: 0.018323, loss_dice: 0.319227
[09:32:42.612] TRAIN: iteration 10902 : loss : 0.148153, loss_ce: 0.009152, loss_dice: 0.287153
[09:32:42.821] TRAIN: iteration 10903 : loss : 0.196297, loss_ce: 0.009828, loss_dice: 0.382767
[09:32:43.029] TRAIN: iteration 10904 : loss : 0.243086, loss_ce: 0.005123, loss_dice: 0.481049
[09:32:43.238] TRAIN: iteration 10905 : loss : 0.174045, loss_ce: 0.006554, loss_dice: 0.341535
[09:32:43.451] TRAIN: iteration 10906 : loss : 0.129536, loss_ce: 0.005325, loss_dice: 0.253747
[09:32:43.696] TRAIN: iteration 10907 : loss : 0.112903, loss_ce: 0.003463, loss_dice: 0.222343
[09:32:43.905] TRAIN: iteration 10908 : loss : 0.080243, loss_ce: 0.003578, loss_dice: 0.156908
[09:32:44.114] TRAIN: iteration 10909 : loss : 0.122989, loss_ce: 0.003665, loss_dice: 0.242313
[09:32:44.326] TRAIN: iteration 10910 : loss : 0.174342, loss_ce: 0.006240, loss_dice: 0.342445
[09:32:44.533] TRAIN: iteration 10911 : loss : 0.111472, loss_ce: 0.004130, loss_dice: 0.218815
[09:32:44.746] TRAIN: iteration 10912 : loss : 0.131782, loss_ce: 0.003917, loss_dice: 0.259648
[09:32:44.956] TRAIN: iteration 10913 : loss : 0.203466, loss_ce: 0.005548, loss_dice: 0.401383
[09:32:45.163] TRAIN: iteration 10914 : loss : 0.023258, loss_ce: 0.001108, loss_dice: 0.045409
[09:32:45.370] TRAIN: iteration 10915 : loss : 0.097266, loss_ce: 0.004175, loss_dice: 0.190357
[09:32:46.042] TRAIN: iteration 10916 : loss : 0.082807, loss_ce: 0.001530, loss_dice: 0.164085
[09:32:46.253] TRAIN: iteration 10917 : loss : 0.251476, loss_ce: 0.002756, loss_dice: 0.500196
[09:32:46.463] TRAIN: iteration 10918 : loss : 0.092747, loss_ce: 0.009413, loss_dice: 0.176081
[09:32:46.677] TRAIN: iteration 10919 : loss : 0.085708, loss_ce: 0.003515, loss_dice: 0.167900
[09:32:46.887] TRAIN: iteration 10920 : loss : 0.104099, loss_ce: 0.001278, loss_dice: 0.206920
[09:32:47.127] TRAIN: iteration 10921 : loss : 0.044432, loss_ce: 0.001482, loss_dice: 0.087382
[09:32:47.334] TRAIN: iteration 10922 : loss : 0.185819, loss_ce: 0.024848, loss_dice: 0.346791
[09:32:47.542] TRAIN: iteration 10923 : loss : 0.216463, loss_ce: 0.003148, loss_dice: 0.429778
[09:32:47.749] TRAIN: iteration 10924 : loss : 0.141550, loss_ce: 0.011139, loss_dice: 0.271961
[09:32:47.956] TRAIN: iteration 10925 : loss : 0.214728, loss_ce: 0.002708, loss_dice: 0.426749
[09:32:48.169] TRAIN: iteration 10926 : loss : 0.250330, loss_ce: 0.003658, loss_dice: 0.497001
[09:32:48.380] TRAIN: iteration 10927 : loss : 0.041134, loss_ce: 0.001631, loss_dice: 0.080636
[09:32:48.588] TRAIN: iteration 10928 : loss : 0.085209, loss_ce: 0.004563, loss_dice: 0.165855
[09:32:48.799] TRAIN: iteration 10929 : loss : 0.114745, loss_ce: 0.005454, loss_dice: 0.224037
[09:32:49.006] TRAIN: iteration 10930 : loss : 0.132222, loss_ce: 0.009194, loss_dice: 0.255250
[09:32:49.215] TRAIN: iteration 10931 : loss : 0.186368, loss_ce: 0.004427, loss_dice: 0.368310
[09:32:49.422] TRAIN: iteration 10932 : loss : 0.067847, loss_ce: 0.004074, loss_dice: 0.131619
[09:32:49.630] TRAIN: iteration 10933 : loss : 0.242782, loss_ce: 0.002700, loss_dice: 0.482864
[09:32:49.839] TRAIN: iteration 10934 : loss : 0.251228, loss_ce: 0.002354, loss_dice: 0.500102
[09:32:50.046] TRAIN: iteration 10935 : loss : 0.245789, loss_ce: 0.004223, loss_dice: 0.487356
[09:32:50.255] TRAIN: iteration 10936 : loss : 0.251509, loss_ce: 0.002889, loss_dice: 0.500129
[09:32:50.518] TRAIN: iteration 10937 : loss : 0.234876, loss_ce: 0.008525, loss_dice: 0.461227
[09:32:50.727] TRAIN: iteration 10938 : loss : 0.105811, loss_ce: 0.002754, loss_dice: 0.208867
[09:32:50.934] TRAIN: iteration 10939 : loss : 0.065735, loss_ce: 0.007568, loss_dice: 0.123902
[09:32:51.145] TRAIN: iteration 10940 : loss : 0.081496, loss_ce: 0.006728, loss_dice: 0.156264
[09:32:51.383] TRAIN: iteration 10941 : loss : 0.115335, loss_ce: 0.006416, loss_dice: 0.224255
[09:32:51.594] TRAIN: iteration 10942 : loss : 0.070667, loss_ce: 0.002730, loss_dice: 0.138605
[09:32:51.805] TRAIN: iteration 10943 : loss : 0.248459, loss_ce: 0.003954, loss_dice: 0.492963
[09:32:52.012] TRAIN: iteration 10944 : loss : 0.092636, loss_ce: 0.005863, loss_dice: 0.179409
[09:32:52.219] TRAIN: iteration 10945 : loss : 0.067489, loss_ce: 0.002710, loss_dice: 0.132268
[09:32:52.869] TRAIN: iteration 10946 : loss : 0.160239, loss_ce: 0.003261, loss_dice: 0.317217
[09:32:53.081] TRAIN: iteration 10947 : loss : 0.251569, loss_ce: 0.003005, loss_dice: 0.500134
[09:32:53.288] TRAIN: iteration 10948 : loss : 0.086642, loss_ce: 0.009792, loss_dice: 0.163493
[09:32:53.499] TRAIN: iteration 10949 : loss : 0.053436, loss_ce: 0.003572, loss_dice: 0.103300
[09:32:53.709] TRAIN: iteration 10950 : loss : 0.251256, loss_ce: 0.002409, loss_dice: 0.500103
[09:32:53.917] TRAIN: iteration 10951 : loss : 0.245851, loss_ce: 0.003344, loss_dice: 0.488358
[09:32:54.127] TRAIN: iteration 10952 : loss : 0.172907, loss_ce: 0.002509, loss_dice: 0.343306
[09:32:54.340] TRAIN: iteration 10953 : loss : 0.039934, loss_ce: 0.003267, loss_dice: 0.076602
[09:32:54.547] TRAIN: iteration 10954 : loss : 0.120363, loss_ce: 0.003432, loss_dice: 0.237293
[09:32:54.755] TRAIN: iteration 10955 : loss : 0.225741, loss_ce: 0.006989, loss_dice: 0.444493
[09:32:54.964] TRAIN: iteration 10956 : loss : 0.113118, loss_ce: 0.006826, loss_dice: 0.219410
[09:32:55.171] TRAIN: iteration 10957 : loss : 0.098100, loss_ce: 0.002742, loss_dice: 0.193458
[09:32:55.403] TRAIN: iteration 10958 : loss : 0.120175, loss_ce: 0.004012, loss_dice: 0.236338
[09:32:55.610] TRAIN: iteration 10959 : loss : 0.242046, loss_ce: 0.001592, loss_dice: 0.482501
[09:32:55.821] TRAIN: iteration 10960 : loss : 0.084761, loss_ce: 0.006113, loss_dice: 0.163409
[09:32:55.822] NaN or Inf found in input tensor.
[09:32:56.041] TRAIN: iteration 10961 : loss : 0.253040, loss_ce: 0.007214, loss_dice: 0.498867
[09:32:56.249] TRAIN: iteration 10962 : loss : 0.051919, loss_ce: 0.002471, loss_dice: 0.101366
[09:32:56.457] TRAIN: iteration 10963 : loss : 0.127713, loss_ce: 0.007073, loss_dice: 0.248353
[09:32:56.666] TRAIN: iteration 10964 : loss : 0.221341, loss_ce: 0.003476, loss_dice: 0.439205
[09:32:56.873] TRAIN: iteration 10965 : loss : 0.059730, loss_ce: 0.003101, loss_dice: 0.116359
[09:32:57.080] TRAIN: iteration 10966 : loss : 0.157616, loss_ce: 0.004117, loss_dice: 0.311115
[09:32:57.289] TRAIN: iteration 10967 : loss : 0.073474, loss_ce: 0.002668, loss_dice: 0.144279
[09:32:57.502] TRAIN: iteration 10968 : loss : 0.115420, loss_ce: 0.002743, loss_dice: 0.228098
[09:32:57.709] TRAIN: iteration 10969 : loss : 0.251709, loss_ce: 0.003410, loss_dice: 0.500008
[09:32:58.572] TRAIN: iteration 10970 : loss : 0.027676, loss_ce: 0.001446, loss_dice: 0.053907
[09:32:58.778] TRAIN: iteration 10971 : loss : 0.140881, loss_ce: 0.002672, loss_dice: 0.279090
[09:32:58.985] TRAIN: iteration 10972 : loss : 0.077108, loss_ce: 0.004460, loss_dice: 0.149756
[09:32:59.194] TRAIN: iteration 10973 : loss : 0.154828, loss_ce: 0.002385, loss_dice: 0.307271
[09:32:59.402] TRAIN: iteration 10974 : loss : 0.166296, loss_ce: 0.007176, loss_dice: 0.325417
[09:33:00.714] TRAIN: iteration 10975 : loss : 0.201041, loss_ce: 0.003005, loss_dice: 0.399078
[09:33:00.921] TRAIN: iteration 10976 : loss : 0.127272, loss_ce: 0.020384, loss_dice: 0.234159
[09:33:01.129] TRAIN: iteration 10977 : loss : 0.256779, loss_ce: 0.017576, loss_dice: 0.495981
[09:33:01.338] TRAIN: iteration 10978 : loss : 0.234695, loss_ce: 0.004273, loss_dice: 0.465116
[09:33:01.548] TRAIN: iteration 10979 : loss : 0.046099, loss_ce: 0.005541, loss_dice: 0.086658
[09:33:01.762] TRAIN: iteration 10980 : loss : 0.154870, loss_ce: 0.003187, loss_dice: 0.306553
[09:33:01.998] TRAIN: iteration 10981 : loss : 0.150464, loss_ce: 0.025364, loss_dice: 0.275564
[09:33:02.206] TRAIN: iteration 10982 : loss : 0.251550, loss_ce: 0.010179, loss_dice: 0.492921
[09:33:02.414] TRAIN: iteration 10983 : loss : 0.107108, loss_ce: 0.006249, loss_dice: 0.207967
[09:33:02.622] TRAIN: iteration 10984 : loss : 0.034448, loss_ce: 0.003697, loss_dice: 0.065200
[09:33:02.833] TRAIN: iteration 10985 : loss : 0.120367, loss_ce: 0.005645, loss_dice: 0.235090
[09:33:03.040] TRAIN: iteration 10986 : loss : 0.107071, loss_ce: 0.004167, loss_dice: 0.209975
[09:33:03.251] TRAIN: iteration 10987 : loss : 0.244651, loss_ce: 0.004245, loss_dice: 0.485057
[09:33:03.461] TRAIN: iteration 10988 : loss : 0.094943, loss_ce: 0.003279, loss_dice: 0.186606
[09:33:03.671] TRAIN: iteration 10989 : loss : 0.244372, loss_ce: 0.007871, loss_dice: 0.480874
[09:33:03.879] TRAIN: iteration 10990 : loss : 0.251484, loss_ce: 0.002787, loss_dice: 0.500180
[09:33:04.093] TRAIN: iteration 10991 : loss : 0.169521, loss_ce: 0.007113, loss_dice: 0.331929
[09:33:04.302] TRAIN: iteration 10992 : loss : 0.251473, loss_ce: 0.002782, loss_dice: 0.500164
[09:33:04.510] TRAIN: iteration 10993 : loss : 0.230069, loss_ce: 0.004861, loss_dice: 0.455277
[09:33:04.718] TRAIN: iteration 10994 : loss : 0.088390, loss_ce: 0.001907, loss_dice: 0.174873
[09:33:04.928] TRAIN: iteration 10995 : loss : 0.156728, loss_ce: 0.016036, loss_dice: 0.297420
[09:33:05.136] TRAIN: iteration 10996 : loss : 0.068056, loss_ce: 0.003890, loss_dice: 0.132222
[09:33:05.347] TRAIN: iteration 10997 : loss : 0.250685, loss_ce: 0.002310, loss_dice: 0.499061
[09:33:05.556] TRAIN: iteration 10998 : loss : 0.134013, loss_ce: 0.005265, loss_dice: 0.262762
[09:33:05.767] TRAIN: iteration 10999 : loss : 0.171279, loss_ce: 0.004623, loss_dice: 0.337935
[09:33:05.977] TRAIN: iteration 11000 : loss : 0.225984, loss_ce: 0.014043, loss_dice: 0.437926
[09:33:06.216] TRAIN: iteration 11001 : loss : 0.239710, loss_ce: 0.005645, loss_dice: 0.473775
[09:33:06.424] TRAIN: iteration 11002 : loss : 0.216015, loss_ce: 0.005640, loss_dice: 0.426390
[09:33:06.632] TRAIN: iteration 11003 : loss : 0.062913, loss_ce: 0.003374, loss_dice: 0.122453
[09:33:06.842] TRAIN: iteration 11004 : loss : 0.251357, loss_ce: 0.002564, loss_dice: 0.500149
[09:33:07.055] TRAIN: iteration 11005 : loss : 0.243649, loss_ce: 0.002620, loss_dice: 0.484679
[09:33:07.263] TRAIN: iteration 11006 : loss : 0.180174, loss_ce: 0.005270, loss_dice: 0.355077
[09:33:07.470] TRAIN: iteration 11007 : loss : 0.097393, loss_ce: 0.002768, loss_dice: 0.192017
[09:33:07.679] TRAIN: iteration 11008 : loss : 0.100845, loss_ce: 0.003373, loss_dice: 0.198317
[09:33:07.894] TRAIN: iteration 11009 : loss : 0.146300, loss_ce: 0.011938, loss_dice: 0.280661
[09:33:08.103] TRAIN: iteration 11010 : loss : 0.218558, loss_ce: 0.014168, loss_dice: 0.422947
[09:33:08.312] TRAIN: iteration 11011 : loss : 0.251442, loss_ce: 0.002726, loss_dice: 0.500159
[09:33:08.530] TRAIN: iteration 11012 : loss : 0.043954, loss_ce: 0.002811, loss_dice: 0.085097
[09:33:08.749] TRAIN: iteration 11013 : loss : 0.038970, loss_ce: 0.002716, loss_dice: 0.075224
[09:33:08.957] TRAIN: iteration 11014 : loss : 0.240199, loss_ce: 0.006743, loss_dice: 0.473654
[09:33:09.165] TRAIN: iteration 11015 : loss : 0.242065, loss_ce: 0.003658, loss_dice: 0.480473
[09:33:09.379] TRAIN: iteration 11016 : loss : 0.153546, loss_ce: 0.004903, loss_dice: 0.302189
[09:33:09.591] TRAIN: iteration 11017 : loss : 0.222590, loss_ce: 0.005354, loss_dice: 0.439826
[09:33:09.801] TRAIN: iteration 11018 : loss : 0.252241, loss_ce: 0.008700, loss_dice: 0.495782
[09:33:10.007] TRAIN: iteration 11019 : loss : 0.251349, loss_ce: 0.004813, loss_dice: 0.497885
[09:33:10.214] TRAIN: iteration 11020 : loss : 0.254054, loss_ce: 0.007960, loss_dice: 0.500148
[09:33:10.460] TRAIN: iteration 11021 : loss : 0.080277, loss_ce: 0.004063, loss_dice: 0.156491
[09:33:10.669] TRAIN: iteration 11022 : loss : 0.078749, loss_ce: 0.014843, loss_dice: 0.142656
[09:33:10.877] TRAIN: iteration 11023 : loss : 0.148744, loss_ce: 0.013724, loss_dice: 0.283763
[09:33:11.086] TRAIN: iteration 11024 : loss : 0.069070, loss_ce: 0.015196, loss_dice: 0.122944
[09:33:11.297] TRAIN: iteration 11025 : loss : 0.073709, loss_ce: 0.003495, loss_dice: 0.143924
[09:33:11.507] TRAIN: iteration 11026 : loss : 0.136883, loss_ce: 0.005833, loss_dice: 0.267932
[09:33:11.721] TRAIN: iteration 11027 : loss : 0.148655, loss_ce: 0.007780, loss_dice: 0.289530
[09:33:11.929] TRAIN: iteration 11028 : loss : 0.127345, loss_ce: 0.006829, loss_dice: 0.247860
[09:33:12.140] TRAIN: iteration 11029 : loss : 0.191964, loss_ce: 0.005880, loss_dice: 0.378048
[09:33:12.346] TRAIN: iteration 11030 : loss : 0.066480, loss_ce: 0.007139, loss_dice: 0.125822
[09:33:12.555] TRAIN: iteration 11031 : loss : 0.226764, loss_ce: 0.005119, loss_dice: 0.448409
[09:33:12.762] TRAIN: iteration 11032 : loss : 0.129940, loss_ce: 0.003182, loss_dice: 0.256697
[09:33:12.970] TRAIN: iteration 11033 : loss : 0.240977, loss_ce: 0.003785, loss_dice: 0.478169
[09:33:13.178] TRAIN: iteration 11034 : loss : 0.161418, loss_ce: 0.004971, loss_dice: 0.317864
[09:33:13.391] TRAIN: iteration 11035 : loss : 0.233212, loss_ce: 0.007464, loss_dice: 0.458959
[09:33:13.598] TRAIN: iteration 11036 : loss : 0.095888, loss_ce: 0.005667, loss_dice: 0.186109
[09:33:13.806] TRAIN: iteration 11037 : loss : 0.130680, loss_ce: 0.003168, loss_dice: 0.258193
[09:33:14.013] TRAIN: iteration 11038 : loss : 0.252438, loss_ce: 0.004545, loss_dice: 0.500330
[09:33:14.221] TRAIN: iteration 11039 : loss : 0.194058, loss_ce: 0.005912, loss_dice: 0.382204
[09:33:14.429] TRAIN: iteration 11040 : loss : 0.171441, loss_ce: 0.036728, loss_dice: 0.306154
[09:33:14.667] TRAIN: iteration 11041 : loss : 0.091393, loss_ce: 0.003666, loss_dice: 0.179120
[09:33:14.874] TRAIN: iteration 11042 : loss : 0.061331, loss_ce: 0.004990, loss_dice: 0.117671
[09:33:15.082] TRAIN: iteration 11043 : loss : 0.141660, loss_ce: 0.003203, loss_dice: 0.280116
[09:33:15.293] TRAIN: iteration 11044 : loss : 0.248535, loss_ce: 0.002404, loss_dice: 0.494666
[09:33:15.505] TRAIN: iteration 11045 : loss : 0.251465, loss_ce: 0.002753, loss_dice: 0.500178
[09:33:15.715] TRAIN: iteration 11046 : loss : 0.181793, loss_ce: 0.032539, loss_dice: 0.331048
[09:33:15.924] TRAIN: iteration 11047 : loss : 0.251352, loss_ce: 0.002542, loss_dice: 0.500163
[09:33:16.179] TRAIN: iteration 11048 : loss : 0.208356, loss_ce: 0.004052, loss_dice: 0.412660
[09:33:16.387] TRAIN: iteration 11049 : loss : 0.093937, loss_ce: 0.002952, loss_dice: 0.184921
[09:33:16.595] TRAIN: iteration 11050 : loss : 0.144525, loss_ce: 0.007813, loss_dice: 0.281238
[09:33:16.805] TRAIN: iteration 11051 : loss : 0.054842, loss_ce: 0.003985, loss_dice: 0.105700
[09:33:17.014] TRAIN: iteration 11052 : loss : 0.104180, loss_ce: 0.004779, loss_dice: 0.203581
[09:33:17.224] TRAIN: iteration 11053 : loss : 0.192492, loss_ce: 0.005404, loss_dice: 0.379581
[09:33:17.435] TRAIN: iteration 11054 : loss : 0.091764, loss_ce: 0.006675, loss_dice: 0.176853
[09:33:17.641] TRAIN: iteration 11055 : loss : 0.056480, loss_ce: 0.003014, loss_dice: 0.109947
[09:33:17.852] TRAIN: iteration 11056 : loss : 0.232967, loss_ce: 0.002953, loss_dice: 0.462982
[09:33:18.066] TRAIN: iteration 11057 : loss : 0.191008, loss_ce: 0.004270, loss_dice: 0.377747
[09:33:18.274] TRAIN: iteration 11058 : loss : 0.251644, loss_ce: 0.003122, loss_dice: 0.500166
[09:33:18.481] TRAIN: iteration 11059 : loss : 0.025771, loss_ce: 0.002996, loss_dice: 0.048546
[09:33:18.688] TRAIN: iteration 11060 : loss : 0.067931, loss_ce: 0.003348, loss_dice: 0.132514
[09:33:18.948] TRAIN: iteration 11061 : loss : 0.111599, loss_ce: 0.005240, loss_dice: 0.217959
[09:33:19.157] TRAIN: iteration 11062 : loss : 0.120089, loss_ce: 0.007207, loss_dice: 0.232971
[09:33:19.365] TRAIN: iteration 11063 : loss : 0.094981, loss_ce: 0.004797, loss_dice: 0.185165
[09:33:19.576] TRAIN: iteration 11064 : loss : 0.214078, loss_ce: 0.010114, loss_dice: 0.418041
[09:33:19.790] TRAIN: iteration 11065 : loss : 0.085796, loss_ce: 0.005618, loss_dice: 0.165975
[09:33:20.004] TRAIN: iteration 11066 : loss : 0.115027, loss_ce: 0.004655, loss_dice: 0.225399
[09:33:20.212] TRAIN: iteration 11067 : loss : 0.251716, loss_ce: 0.003254, loss_dice: 0.500178
[09:33:20.420] TRAIN: iteration 11068 : loss : 0.203138, loss_ce: 0.006884, loss_dice: 0.399393
[09:33:20.629] TRAIN: iteration 11069 : loss : 0.252768, loss_ce: 0.005194, loss_dice: 0.500342
[09:33:20.843] TRAIN: iteration 11070 : loss : 0.072719, loss_ce: 0.004065, loss_dice: 0.141374
[09:33:21.051] TRAIN: iteration 11071 : loss : 0.075348, loss_ce: 0.008229, loss_dice: 0.142467
[09:33:21.257] TRAIN: iteration 11072 : loss : 0.252328, loss_ce: 0.004411, loss_dice: 0.500245
[09:33:21.480] TRAIN: iteration 11073 : loss : 0.232540, loss_ce: 0.005986, loss_dice: 0.459095
[09:33:21.692] TRAIN: iteration 11074 : loss : 0.148002, loss_ce: 0.004703, loss_dice: 0.291301
[09:33:21.898] TRAIN: iteration 11075 : loss : 0.252212, loss_ce: 0.004155, loss_dice: 0.500270
[09:33:22.109] TRAIN: iteration 11076 : loss : 0.253697, loss_ce: 0.006865, loss_dice: 0.500529
[09:33:22.315] TRAIN: iteration 11077 : loss : 0.252024, loss_ce: 0.003828, loss_dice: 0.500221
[09:33:22.522] TRAIN: iteration 11078 : loss : 0.169570, loss_ce: 0.003576, loss_dice: 0.335564
[09:33:22.728] TRAIN: iteration 11079 : loss : 0.141937, loss_ce: 0.006568, loss_dice: 0.277306
[09:33:22.936] TRAIN: iteration 11080 : loss : 0.178054, loss_ce: 0.016378, loss_dice: 0.339730
[09:33:23.175] TRAIN: iteration 11081 : loss : 0.244399, loss_ce: 0.005527, loss_dice: 0.483270
[09:33:23.385] TRAIN: iteration 11082 : loss : 0.248930, loss_ce: 0.004274, loss_dice: 0.493587
[09:33:23.596] TRAIN: iteration 11083 : loss : 0.176021, loss_ce: 0.006021, loss_dice: 0.346021
[09:33:24.120] TRAIN: iteration 11084 : loss : 0.172648, loss_ce: 0.006492, loss_dice: 0.338803
[09:33:24.332] TRAIN: iteration 11085 : loss : 0.182622, loss_ce: 0.015549, loss_dice: 0.349694
[09:33:24.539] TRAIN: iteration 11086 : loss : 0.086648, loss_ce: 0.004959, loss_dice: 0.168337
[09:33:24.747] TRAIN: iteration 11087 : loss : 0.092611, loss_ce: 0.008649, loss_dice: 0.176574
[09:33:24.956] TRAIN: iteration 11088 : loss : 0.042279, loss_ce: 0.008584, loss_dice: 0.075975
[09:33:25.164] TRAIN: iteration 11089 : loss : 0.252612, loss_ce: 0.004896, loss_dice: 0.500328
[09:33:25.371] TRAIN: iteration 11090 : loss : 0.198370, loss_ce: 0.008407, loss_dice: 0.388334
[09:33:25.578] TRAIN: iteration 11091 : loss : 0.055598, loss_ce: 0.004172, loss_dice: 0.107023
[09:33:25.785] TRAIN: iteration 11092 : loss : 0.154067, loss_ce: 0.006237, loss_dice: 0.301897
[09:33:25.996] TRAIN: iteration 11093 : loss : 0.163395, loss_ce: 0.005410, loss_dice: 0.321379
[09:33:26.206] TRAIN: iteration 11094 : loss : 0.232515, loss_ce: 0.005159, loss_dice: 0.459870
[09:33:26.412] TRAIN: iteration 11095 : loss : 0.101503, loss_ce: 0.004577, loss_dice: 0.198429
[09:33:26.619] TRAIN: iteration 11096 : loss : 0.170656, loss_ce: 0.005269, loss_dice: 0.336043
[09:33:26.825] TRAIN: iteration 11097 : loss : 0.040560, loss_ce: 0.003994, loss_dice: 0.077127
[09:33:27.038] TRAIN: iteration 11098 : loss : 0.076132, loss_ce: 0.004684, loss_dice: 0.147580
[09:33:27.246] TRAIN: iteration 11099 : loss : 0.252096, loss_ce: 0.003954, loss_dice: 0.500238
[09:33:27.453] TRAIN: iteration 11100 : loss : 0.240940, loss_ce: 0.004412, loss_dice: 0.477468
[09:33:27.690] TRAIN: iteration 11101 : loss : 0.125542, loss_ce: 0.003244, loss_dice: 0.247840
[09:33:27.898] TRAIN: iteration 11102 : loss : 0.071942, loss_ce: 0.004606, loss_dice: 0.139277
[09:33:28.107] TRAIN: iteration 11103 : loss : 0.102274, loss_ce: 0.004592, loss_dice: 0.199955
[09:33:28.315] TRAIN: iteration 11104 : loss : 0.189702, loss_ce: 0.013276, loss_dice: 0.366129
[09:33:28.526] TRAIN: iteration 11105 : loss : 0.055861, loss_ce: 0.003089, loss_dice: 0.108633
[09:33:28.732] TRAIN: iteration 11106 : loss : 0.250895, loss_ce: 0.001740, loss_dice: 0.500050
[09:33:28.940] TRAIN: iteration 11107 : loss : 0.101432, loss_ce: 0.005609, loss_dice: 0.197255
[09:33:29.150] TRAIN: iteration 11108 : loss : 0.247393, loss_ce: 0.002579, loss_dice: 0.492206
[09:33:29.358] TRAIN: iteration 11109 : loss : 0.246453, loss_ce: 0.003852, loss_dice: 0.489053
[09:33:29.567] TRAIN: iteration 11110 : loss : 0.250824, loss_ce: 0.001608, loss_dice: 0.500040
[09:33:30.344] TRAIN: iteration 11111 : loss : 0.111412, loss_ce: 0.002585, loss_dice: 0.220240
[09:33:30.551] TRAIN: iteration 11112 : loss : 0.250997, loss_ce: 0.001925, loss_dice: 0.500068
[09:33:30.759] TRAIN: iteration 11113 : loss : 0.251144, loss_ce: 0.002189, loss_dice: 0.500100
[09:33:30.966] TRAIN: iteration 11114 : loss : 0.136348, loss_ce: 0.002888, loss_dice: 0.269808
[09:33:31.177] TRAIN: iteration 11115 : loss : 0.178831, loss_ce: 0.010706, loss_dice: 0.346955
[09:33:31.390] TRAIN: iteration 11116 : loss : 0.089501, loss_ce: 0.004786, loss_dice: 0.174215
[09:33:31.597] TRAIN: iteration 11117 : loss : 0.084357, loss_ce: 0.007555, loss_dice: 0.161158
[09:33:31.805] TRAIN: iteration 11118 : loss : 0.075985, loss_ce: 0.005319, loss_dice: 0.146650
[09:33:32.019] TRAIN: iteration 11119 : loss : 0.071612, loss_ce: 0.003304, loss_dice: 0.139921
[09:33:32.226] TRAIN: iteration 11120 : loss : 0.237180, loss_ce: 0.008669, loss_dice: 0.465691
[09:33:32.466] TRAIN: iteration 11121 : loss : 0.042151, loss_ce: 0.004232, loss_dice: 0.080070
[09:33:32.674] TRAIN: iteration 11122 : loss : 0.122371, loss_ce: 0.007808, loss_dice: 0.236934
[09:33:32.889] TRAIN: iteration 11123 : loss : 0.078319, loss_ce: 0.006615, loss_dice: 0.150022
[09:33:33.099] TRAIN: iteration 11124 : loss : 0.050821, loss_ce: 0.004877, loss_dice: 0.096766
[09:33:33.312] TRAIN: iteration 11125 : loss : 0.177835, loss_ce: 0.010655, loss_dice: 0.345015
[09:33:33.519] TRAIN: iteration 11126 : loss : 0.209507, loss_ce: 0.005176, loss_dice: 0.413838
[09:33:33.726] TRAIN: iteration 11127 : loss : 0.058194, loss_ce: 0.003704, loss_dice: 0.112684
[09:33:33.936] TRAIN: iteration 11128 : loss : 0.240243, loss_ce: 0.006887, loss_dice: 0.473600
[09:33:34.161] TRAIN: iteration 11129 : loss : 0.153603, loss_ce: 0.008404, loss_dice: 0.298802
[09:33:34.368] TRAIN: iteration 11130 : loss : 0.253043, loss_ce: 0.005689, loss_dice: 0.500396
[09:33:34.575] TRAIN: iteration 11131 : loss : 0.231873, loss_ce: 0.008663, loss_dice: 0.455083
[09:33:35.672] TRAIN: iteration 11132 : loss : 0.253388, loss_ce: 0.006326, loss_dice: 0.500451
[09:33:35.886] TRAIN: iteration 11133 : loss : 0.051561, loss_ce: 0.005619, loss_dice: 0.097503
[09:33:36.094] TRAIN: iteration 11134 : loss : 0.249522, loss_ce: 0.004852, loss_dice: 0.494192
[09:33:36.303] TRAIN: iteration 11135 : loss : 0.112453, loss_ce: 0.012260, loss_dice: 0.212645
[09:33:36.512] TRAIN: iteration 11136 : loss : 0.252389, loss_ce: 0.004480, loss_dice: 0.500298
[09:33:36.721] TRAIN: iteration 11137 : loss : 0.252101, loss_ce: 0.003958, loss_dice: 0.500245
[09:33:36.930] TRAIN: iteration 11138 : loss : 0.175602, loss_ce: 0.007951, loss_dice: 0.343253
[09:33:37.140] TRAIN: iteration 11139 : loss : 0.172403, loss_ce: 0.014600, loss_dice: 0.330206
[09:33:37.410] TRAIN: iteration 11140 : loss : 0.083083, loss_ce: 0.006056, loss_dice: 0.160109
[09:33:37.645] TRAIN: iteration 11141 : loss : 0.066451, loss_ce: 0.004727, loss_dice: 0.128175
[09:33:37.853] TRAIN: iteration 11142 : loss : 0.115903, loss_ce: 0.006379, loss_dice: 0.225428
[09:33:38.068] TRAIN: iteration 11143 : loss : 0.062393, loss_ce: 0.003930, loss_dice: 0.120855
[09:33:38.278] TRAIN: iteration 11144 : loss : 0.086491, loss_ce: 0.006654, loss_dice: 0.166329
[09:33:38.492] TRAIN: iteration 11145 : loss : 0.196216, loss_ce: 0.006016, loss_dice: 0.386416
[09:33:38.701] TRAIN: iteration 11146 : loss : 0.195708, loss_ce: 0.008840, loss_dice: 0.382576
[09:33:38.908] TRAIN: iteration 11147 : loss : 0.115098, loss_ce: 0.005344, loss_dice: 0.224852
[09:33:39.123] TRAIN: iteration 11148 : loss : 0.085839, loss_ce: 0.003636, loss_dice: 0.168041
[09:33:39.331] TRAIN: iteration 11149 : loss : 0.047166, loss_ce: 0.003290, loss_dice: 0.091041
[09:33:39.538] TRAIN: iteration 11150 : loss : 0.100077, loss_ce: 0.007940, loss_dice: 0.192215
[09:33:39.752] TRAIN: iteration 11151 : loss : 0.094417, loss_ce: 0.004077, loss_dice: 0.184757
[09:33:39.959] TRAIN: iteration 11152 : loss : 0.068953, loss_ce: 0.002585, loss_dice: 0.135321
[09:33:40.166] TRAIN: iteration 11153 : loss : 0.230944, loss_ce: 0.012058, loss_dice: 0.449829
[09:33:40.373] TRAIN: iteration 11154 : loss : 0.214760, loss_ce: 0.005858, loss_dice: 0.423661
[09:33:40.582] TRAIN: iteration 11155 : loss : 0.175109, loss_ce: 0.007426, loss_dice: 0.342793
[09:33:40.792] TRAIN: iteration 11156 : loss : 0.251828, loss_ce: 0.003459, loss_dice: 0.500196
[09:33:41.002] TRAIN: iteration 11157 : loss : 0.137772, loss_ce: 0.003213, loss_dice: 0.272332
[09:33:41.212] TRAIN: iteration 11158 : loss : 0.104508, loss_ce: 0.004434, loss_dice: 0.204582
[09:33:41.424] TRAIN: iteration 11159 : loss : 0.227822, loss_ce: 0.008900, loss_dice: 0.446744
[09:33:42.276] TRAIN: iteration 11160 : loss : 0.129504, loss_ce: 0.006232, loss_dice: 0.252777
[09:33:42.511] TRAIN: iteration 11161 : loss : 0.167629, loss_ce: 0.005472, loss_dice: 0.329786
[09:33:42.720] TRAIN: iteration 11162 : loss : 0.223624, loss_ce: 0.004505, loss_dice: 0.442744
[09:33:42.928] TRAIN: iteration 11163 : loss : 0.148132, loss_ce: 0.002795, loss_dice: 0.293468
[09:33:43.136] TRAIN: iteration 11164 : loss : 0.230086, loss_ce: 0.007164, loss_dice: 0.453009
[09:33:43.344] TRAIN: iteration 11165 : loss : 0.045238, loss_ce: 0.002175, loss_dice: 0.088301
[09:33:43.551] TRAIN: iteration 11166 : loss : 0.072427, loss_ce: 0.004028, loss_dice: 0.140827
[09:33:43.758] TRAIN: iteration 11167 : loss : 0.148723, loss_ce: 0.004502, loss_dice: 0.292944
[09:33:44.507] TRAIN: iteration 11168 : loss : 0.156208, loss_ce: 0.008116, loss_dice: 0.304300
[09:33:44.720] TRAIN: iteration 11169 : loss : 0.251754, loss_ce: 0.003330, loss_dice: 0.500178
[09:33:44.931] TRAIN: iteration 11170 : loss : 0.091844, loss_ce: 0.003812, loss_dice: 0.179876
[09:33:45.141] TRAIN: iteration 11171 : loss : 0.041118, loss_ce: 0.002993, loss_dice: 0.079243
[09:33:45.348] TRAIN: iteration 11172 : loss : 0.184988, loss_ce: 0.004250, loss_dice: 0.365725
[09:33:45.555] TRAIN: iteration 11173 : loss : 0.251055, loss_ce: 0.002029, loss_dice: 0.500081
[09:33:45.763] TRAIN: iteration 11174 : loss : 0.107411, loss_ce: 0.002282, loss_dice: 0.212541
[09:33:45.972] TRAIN: iteration 11175 : loss : 0.096689, loss_ce: 0.003281, loss_dice: 0.190098
[09:33:46.889] TRAIN: iteration 11176 : loss : 0.135629, loss_ce: 0.009028, loss_dice: 0.262230
[09:33:47.101] TRAIN: iteration 11177 : loss : 0.115541, loss_ce: 0.006318, loss_dice: 0.224763
[09:33:47.315] TRAIN: iteration 11178 : loss : 0.185762, loss_ce: 0.005104, loss_dice: 0.366421
[09:33:47.522] TRAIN: iteration 11179 : loss : 0.145318, loss_ce: 0.002685, loss_dice: 0.287952
[09:33:47.737] TRAIN: iteration 11180 : loss : 0.140374, loss_ce: 0.001842, loss_dice: 0.278906
[09:33:47.980] TRAIN: iteration 11181 : loss : 0.165909, loss_ce: 0.006230, loss_dice: 0.325589
[09:33:48.191] TRAIN: iteration 11182 : loss : 0.134438, loss_ce: 0.006535, loss_dice: 0.262341
[09:33:48.406] TRAIN: iteration 11183 : loss : 0.153150, loss_ce: 0.003392, loss_dice: 0.302907
[09:33:50.794] TRAIN: iteration 11184 : loss : 0.232796, loss_ce: 0.005864, loss_dice: 0.459729
[09:33:51.003] TRAIN: iteration 11185 : loss : 0.068721, loss_ce: 0.002639, loss_dice: 0.134803
[09:33:51.211] TRAIN: iteration 11186 : loss : 0.228814, loss_ce: 0.006305, loss_dice: 0.451323
[09:33:51.418] TRAIN: iteration 11187 : loss : 0.238215, loss_ce: 0.002838, loss_dice: 0.473591
[09:33:51.625] TRAIN: iteration 11188 : loss : 0.096234, loss_ce: 0.002566, loss_dice: 0.189903
[09:33:51.836] TRAIN: iteration 11189 : loss : 0.096435, loss_ce: 0.004830, loss_dice: 0.188041
[09:33:52.044] TRAIN: iteration 11190 : loss : 0.105945, loss_ce: 0.004677, loss_dice: 0.207212
[09:33:52.252] TRAIN: iteration 11191 : loss : 0.251665, loss_ce: 0.003822, loss_dice: 0.499508
[09:33:54.405] TRAIN: iteration 11192 : loss : 0.252412, loss_ce: 0.004470, loss_dice: 0.500355
[09:33:54.617] TRAIN: iteration 11193 : loss : 0.105019, loss_ce: 0.002385, loss_dice: 0.207653
[09:33:54.824] TRAIN: iteration 11194 : loss : 0.123253, loss_ce: 0.013043, loss_dice: 0.233463
[09:33:55.031] TRAIN: iteration 11195 : loss : 0.250495, loss_ce: 0.004002, loss_dice: 0.496989
[09:33:55.241] TRAIN: iteration 11196 : loss : 0.251870, loss_ce: 0.003464, loss_dice: 0.500276
[09:33:55.451] TRAIN: iteration 11197 : loss : 0.071568, loss_ce: 0.003493, loss_dice: 0.139644
[09:33:55.664] TRAIN: iteration 11198 : loss : 0.074751, loss_ce: 0.002569, loss_dice: 0.146932
[09:33:55.871] TRAIN: iteration 11199 : loss : 0.251104, loss_ce: 0.002072, loss_dice: 0.500135
[09:33:56.084] TRAIN: iteration 11200 : loss : 0.089147, loss_ce: 0.001943, loss_dice: 0.176350
[09:33:56.319] TRAIN: iteration 11201 : loss : 0.251068, loss_ce: 0.002013, loss_dice: 0.500124
[09:33:56.528] TRAIN: iteration 11202 : loss : 0.251682, loss_ce: 0.003134, loss_dice: 0.500229
[09:33:56.737] TRAIN: iteration 11203 : loss : 0.144577, loss_ce: 0.001817, loss_dice: 0.287337
[09:33:56.946] TRAIN: iteration 11204 : loss : 0.249161, loss_ce: 0.000917, loss_dice: 0.497405
[09:33:57.160] TRAIN: iteration 11205 : loss : 0.230048, loss_ce: 0.015528, loss_dice: 0.444568
[09:33:57.394] TRAIN: iteration 11206 : loss : 0.179553, loss_ce: 0.006408, loss_dice: 0.352699
[09:33:57.612] TRAIN: iteration 11207 : loss : 0.042769, loss_ce: 0.001342, loss_dice: 0.084196
[09:33:57.820] TRAIN: iteration 11208 : loss : 0.250379, loss_ce: 0.001361, loss_dice: 0.499397
[09:33:58.028] TRAIN: iteration 11209 : loss : 0.156892, loss_ce: 0.018321, loss_dice: 0.295463
[09:33:58.236] TRAIN: iteration 11210 : loss : 0.103415, loss_ce: 0.003896, loss_dice: 0.202934
[09:33:58.443] TRAIN: iteration 11211 : loss : 0.184731, loss_ce: 0.003255, loss_dice: 0.366207
[09:33:58.650] TRAIN: iteration 11212 : loss : 0.134498, loss_ce: 0.008501, loss_dice: 0.260496
[09:33:58.858] TRAIN: iteration 11213 : loss : 0.155909, loss_ce: 0.016413, loss_dice: 0.295404
[09:33:59.374] TRAIN: iteration 11214 : loss : 0.194090, loss_ce: 0.003281, loss_dice: 0.384900
[09:33:59.582] TRAIN: iteration 11215 : loss : 0.250799, loss_ce: 0.001556, loss_dice: 0.500042
[09:33:59.796] TRAIN: iteration 11216 : loss : 0.132032, loss_ce: 0.005161, loss_dice: 0.258904
[09:34:00.004] TRAIN: iteration 11217 : loss : 0.144007, loss_ce: 0.018610, loss_dice: 0.269404
[09:34:00.211] TRAIN: iteration 11218 : loss : 0.061404, loss_ce: 0.005657, loss_dice: 0.117152
[09:34:00.591] TRAIN: iteration 11219 : loss : 0.241919, loss_ce: 0.004952, loss_dice: 0.478886
[09:34:00.799] TRAIN: iteration 11220 : loss : 0.085938, loss_ce: 0.011999, loss_dice: 0.159876
[09:34:01.035] TRAIN: iteration 11221 : loss : 0.191963, loss_ce: 0.011724, loss_dice: 0.372201
[09:34:01.976] TRAIN: iteration 11222 : loss : 0.090058, loss_ce: 0.006191, loss_dice: 0.173924
[09:34:02.187] TRAIN: iteration 11223 : loss : 0.250218, loss_ce: 0.007796, loss_dice: 0.492641
[09:34:02.400] TRAIN: iteration 11224 : loss : 0.065388, loss_ce: 0.004946, loss_dice: 0.125830
[09:34:02.609] TRAIN: iteration 11225 : loss : 0.078857, loss_ce: 0.005502, loss_dice: 0.152212
[09:34:02.817] TRAIN: iteration 11226 : loss : 0.106795, loss_ce: 0.005558, loss_dice: 0.208033
[09:34:03.024] TRAIN: iteration 11227 : loss : 0.251205, loss_ce: 0.015560, loss_dice: 0.486849
[09:34:03.232] TRAIN: iteration 11228 : loss : 0.203046, loss_ce: 0.006656, loss_dice: 0.399437
[09:34:03.440] TRAIN: iteration 11229 : loss : 0.230319, loss_ce: 0.007344, loss_dice: 0.453294
[09:34:04.348] TRAIN: iteration 11230 : loss : 0.099618, loss_ce: 0.005421, loss_dice: 0.193815
[09:34:04.554] TRAIN: iteration 11231 : loss : 0.100167, loss_ce: 0.016646, loss_dice: 0.183687
[09:34:04.761] TRAIN: iteration 11232 : loss : 0.251547, loss_ce: 0.007455, loss_dice: 0.495640
[09:34:04.968] TRAIN: iteration 11233 : loss : 0.081249, loss_ce: 0.006092, loss_dice: 0.156406
[09:34:05.176] TRAIN: iteration 11234 : loss : 0.069125, loss_ce: 0.007217, loss_dice: 0.131034
[09:34:05.383] TRAIN: iteration 11235 : loss : 0.103941, loss_ce: 0.006860, loss_dice: 0.201021
[09:34:05.590] TRAIN: iteration 11236 : loss : 0.226322, loss_ce: 0.006706, loss_dice: 0.445937
[09:34:05.797] TRAIN: iteration 11237 : loss : 0.215628, loss_ce: 0.013121, loss_dice: 0.418136
[09:34:06.004] TRAIN: iteration 11238 : loss : 0.252975, loss_ce: 0.005601, loss_dice: 0.500349
[09:34:06.213] TRAIN: iteration 11239 : loss : 0.127685, loss_ce: 0.005372, loss_dice: 0.249999
[09:34:06.422] TRAIN: iteration 11240 : loss : 0.252769, loss_ce: 0.005216, loss_dice: 0.500322
[09:34:06.657] TRAIN: iteration 11241 : loss : 0.145531, loss_ce: 0.006254, loss_dice: 0.284808
[09:34:06.865] TRAIN: iteration 11242 : loss : 0.178149, loss_ce: 0.004116, loss_dice: 0.352181
[09:34:07.073] TRAIN: iteration 11243 : loss : 0.203365, loss_ce: 0.005161, loss_dice: 0.401568
[09:34:07.283] TRAIN: iteration 11244 : loss : 0.099411, loss_ce: 0.004867, loss_dice: 0.193956
[09:34:07.511] TRAIN: iteration 11245 : loss : 0.252477, loss_ce: 0.004646, loss_dice: 0.500309
[09:34:07.724] TRAIN: iteration 11246 : loss : 0.152709, loss_ce: 0.005219, loss_dice: 0.300198
[09:34:07.932] TRAIN: iteration 11247 : loss : 0.062871, loss_ce: 0.004518, loss_dice: 0.121225
[09:34:08.148] TRAIN: iteration 11248 : loss : 0.114943, loss_ce: 0.003528, loss_dice: 0.226358
[09:34:08.364] TRAIN: iteration 11249 : loss : 0.048518, loss_ce: 0.002948, loss_dice: 0.094088
[09:34:08.596] TRAIN: iteration 11250 : loss : 0.240103, loss_ce: 0.002078, loss_dice: 0.478129
[09:34:08.805] TRAIN: iteration 11251 : loss : 0.097866, loss_ce: 0.002124, loss_dice: 0.193609
[09:34:09.012] TRAIN: iteration 11252 : loss : 0.172823, loss_ce: 0.005118, loss_dice: 0.340528
[09:34:09.222] TRAIN: iteration 11253 : loss : 0.250455, loss_ce: 0.000888, loss_dice: 0.500023
[09:34:09.435] TRAIN: iteration 11254 : loss : 0.127956, loss_ce: 0.003891, loss_dice: 0.252020
[09:34:09.647] TRAIN: iteration 11255 : loss : 0.198500, loss_ce: 0.005189, loss_dice: 0.391811
[09:34:09.883] TRAIN: iteration 11256 : loss : 0.213785, loss_ce: 0.001726, loss_dice: 0.425844
[09:34:10.095] TRAIN: iteration 11257 : loss : 0.212777, loss_ce: 0.002184, loss_dice: 0.423371
[09:34:10.302] TRAIN: iteration 11258 : loss : 0.102095, loss_ce: 0.004996, loss_dice: 0.199195
[09:34:10.510] TRAIN: iteration 11259 : loss : 0.209581, loss_ce: 0.012063, loss_dice: 0.407099
[09:34:12.027] TRAIN: iteration 11260 : loss : 0.066806, loss_ce: 0.001618, loss_dice: 0.131995
[09:34:12.263] TRAIN: iteration 11261 : loss : 0.250801, loss_ce: 0.001516, loss_dice: 0.500085
[09:34:12.471] TRAIN: iteration 11262 : loss : 0.231474, loss_ce: 0.002484, loss_dice: 0.460463
[09:34:12.682] TRAIN: iteration 11263 : loss : 0.185483, loss_ce: 0.002277, loss_dice: 0.368690
[09:34:12.898] TRAIN: iteration 11264 : loss : 0.111837, loss_ce: 0.004916, loss_dice: 0.218758
[09:34:13.113] TRAIN: iteration 11265 : loss : 0.087778, loss_ce: 0.001701, loss_dice: 0.173854
[09:34:13.326] TRAIN: iteration 11266 : loss : 0.251510, loss_ce: 0.002845, loss_dice: 0.500176
[09:34:13.533] TRAIN: iteration 11267 : loss : 0.127653, loss_ce: 0.005818, loss_dice: 0.249489
[09:34:14.381] TRAIN: iteration 11268 : loss : 0.076110, loss_ce: 0.003072, loss_dice: 0.149149
[09:34:14.609] TRAIN: iteration 11269 : loss : 0.100591, loss_ce: 0.011021, loss_dice: 0.190162
[09:34:14.817] TRAIN: iteration 11270 : loss : 0.097134, loss_ce: 0.002136, loss_dice: 0.192132
[09:34:15.030] TRAIN: iteration 11271 : loss : 0.109110, loss_ce: 0.004228, loss_dice: 0.213991
[09:34:15.240] TRAIN: iteration 11272 : loss : 0.229883, loss_ce: 0.003143, loss_dice: 0.456623
[09:34:15.447] TRAIN: iteration 11273 : loss : 0.225249, loss_ce: 0.003228, loss_dice: 0.447269
[09:34:15.656] TRAIN: iteration 11274 : loss : 0.091877, loss_ce: 0.010010, loss_dice: 0.173745
[09:34:15.865] TRAIN: iteration 11275 : loss : 0.075077, loss_ce: 0.009828, loss_dice: 0.140326
[09:34:17.063] TRAIN: iteration 11276 : loss : 0.162706, loss_ce: 0.003023, loss_dice: 0.322389
[09:34:17.278] TRAIN: iteration 11277 : loss : 0.166211, loss_ce: 0.015182, loss_dice: 0.317241
[09:34:17.494] TRAIN: iteration 11278 : loss : 0.242696, loss_ce: 0.001537, loss_dice: 0.483856
[09:34:17.702] TRAIN: iteration 11279 : loss : 0.244151, loss_ce: 0.005400, loss_dice: 0.482902
[09:34:17.910] TRAIN: iteration 11280 : loss : 0.137328, loss_ce: 0.006378, loss_dice: 0.268277
[09:34:17.911] NaN or Inf found in input tensor.
[09:34:18.129] TRAIN: iteration 11281 : loss : 0.087479, loss_ce: 0.002769, loss_dice: 0.172190
[09:34:18.335] TRAIN: iteration 11282 : loss : 0.150017, loss_ce: 0.007853, loss_dice: 0.292180
[09:34:18.544] TRAIN: iteration 11283 : loss : 0.243273, loss_ce: 0.007667, loss_dice: 0.478879
[09:34:19.517] TRAIN: iteration 11284 : loss : 0.103856, loss_ce: 0.006461, loss_dice: 0.201250
[09:34:19.725] TRAIN: iteration 11285 : loss : 0.132848, loss_ce: 0.015095, loss_dice: 0.250600
[09:34:19.937] TRAIN: iteration 11286 : loss : 0.188557, loss_ce: 0.007697, loss_dice: 0.369418
[09:34:20.150] TRAIN: iteration 11287 : loss : 0.192158, loss_ce: 0.006814, loss_dice: 0.377502
[09:34:20.357] TRAIN: iteration 11288 : loss : 0.071980, loss_ce: 0.005492, loss_dice: 0.138468
[09:34:20.565] TRAIN: iteration 11289 : loss : 0.162698, loss_ce: 0.006199, loss_dice: 0.319196
[09:34:20.781] TRAIN: iteration 11290 : loss : 0.214501, loss_ce: 0.005031, loss_dice: 0.423972
[09:34:20.990] TRAIN: iteration 11291 : loss : 0.199317, loss_ce: 0.005081, loss_dice: 0.393553
[09:34:21.201] TRAIN: iteration 11292 : loss : 0.091587, loss_ce: 0.006121, loss_dice: 0.177052
[09:34:21.412] TRAIN: iteration 11293 : loss : 0.209045, loss_ce: 0.016272, loss_dice: 0.401818
[09:34:21.620] TRAIN: iteration 11294 : loss : 0.251137, loss_ce: 0.002136, loss_dice: 0.500139
[09:34:21.831] TRAIN: iteration 11295 : loss : 0.152802, loss_ce: 0.002588, loss_dice: 0.303015
[09:34:22.041] TRAIN: iteration 11296 : loss : 0.097230, loss_ce: 0.001795, loss_dice: 0.192666
[09:34:22.248] TRAIN: iteration 11297 : loss : 0.207243, loss_ce: 0.006960, loss_dice: 0.407526
[09:34:22.458] TRAIN: iteration 11298 : loss : 0.172953, loss_ce: 0.001879, loss_dice: 0.344027
[09:34:22.667] TRAIN: iteration 11299 : loss : 0.167404, loss_ce: 0.003493, loss_dice: 0.331314
[09:34:22.875] TRAIN: iteration 11300 : loss : 0.250601, loss_ce: 0.001142, loss_dice: 0.500059
[09:34:23.113] TRAIN: iteration 11301 : loss : 0.254517, loss_ce: 0.008834, loss_dice: 0.500200
[09:34:23.323] TRAIN: iteration 11302 : loss : 0.070339, loss_ce: 0.001579, loss_dice: 0.139099
[09:34:23.530] TRAIN: iteration 11303 : loss : 0.250032, loss_ce: 0.004493, loss_dice: 0.495571
[09:34:23.798] TRAIN: iteration 11304 : loss : 0.234775, loss_ce: 0.006452, loss_dice: 0.463098
[09:34:24.007] TRAIN: iteration 11305 : loss : 0.133690, loss_ce: 0.004496, loss_dice: 0.262884
[09:34:24.215] TRAIN: iteration 11306 : loss : 0.178988, loss_ce: 0.001529, loss_dice: 0.356448
[09:34:24.423] TRAIN: iteration 11307 : loss : 0.053226, loss_ce: 0.000699, loss_dice: 0.105753
[09:34:24.631] TRAIN: iteration 11308 : loss : 0.141781, loss_ce: 0.004147, loss_dice: 0.279414
[09:34:24.837] TRAIN: iteration 11309 : loss : 0.227937, loss_ce: 0.001192, loss_dice: 0.454681
[09:34:25.045] TRAIN: iteration 11310 : loss : 0.098737, loss_ce: 0.008008, loss_dice: 0.189466
[09:34:25.262] TRAIN: iteration 11311 : loss : 0.194638, loss_ce: 0.003573, loss_dice: 0.385704
[09:34:25.477] TRAIN: iteration 11312 : loss : 0.236017, loss_ce: 0.006111, loss_dice: 0.465923
[09:34:25.686] TRAIN: iteration 11313 : loss : 0.157794, loss_ce: 0.010830, loss_dice: 0.304758
[09:34:25.894] TRAIN: iteration 11314 : loss : 0.251167, loss_ce: 0.002168, loss_dice: 0.500165
[09:34:27.061] TRAIN: iteration 11315 : loss : 0.148058, loss_ce: 0.012367, loss_dice: 0.283749
[09:34:27.270] TRAIN: iteration 11316 : loss : 0.158224, loss_ce: 0.015120, loss_dice: 0.301329
[09:34:27.477] TRAIN: iteration 11317 : loss : 0.223856, loss_ce: 0.002340, loss_dice: 0.445372
[09:34:27.684] TRAIN: iteration 11318 : loss : 0.100520, loss_ce: 0.007674, loss_dice: 0.193365
[09:34:27.891] TRAIN: iteration 11319 : loss : 0.180765, loss_ce: 0.006440, loss_dice: 0.355091
[09:34:28.099] TRAIN: iteration 11320 : loss : 0.161666, loss_ce: 0.003903, loss_dice: 0.319430
[09:34:28.349] TRAIN: iteration 11321 : loss : 0.176386, loss_ce: 0.003951, loss_dice: 0.348822
[09:34:28.557] TRAIN: iteration 11322 : loss : 0.252229, loss_ce: 0.007578, loss_dice: 0.496881
[09:34:30.112] TRAIN: iteration 11323 : loss : 0.252534, loss_ce: 0.004775, loss_dice: 0.500293
[09:34:30.318] TRAIN: iteration 11324 : loss : 0.138426, loss_ce: 0.004418, loss_dice: 0.272433
[09:34:30.525] TRAIN: iteration 11325 : loss : 0.111400, loss_ce: 0.005343, loss_dice: 0.217457
[09:34:30.739] TRAIN: iteration 11326 : loss : 0.107747, loss_ce: 0.004631, loss_dice: 0.210863
[09:34:30.953] TRAIN: iteration 11327 : loss : 0.238123, loss_ce: 0.006632, loss_dice: 0.469613
[09:34:31.160] TRAIN: iteration 11328 : loss : 0.209457, loss_ce: 0.004181, loss_dice: 0.414733
[09:34:31.368] TRAIN: iteration 11329 : loss : 0.184010, loss_ce: 0.004313, loss_dice: 0.363708
[09:34:31.575] TRAIN: iteration 11330 : loss : 0.133097, loss_ce: 0.004446, loss_dice: 0.261748
[09:34:33.162] TRAIN: iteration 11331 : loss : 0.118932, loss_ce: 0.010224, loss_dice: 0.227639
[09:34:33.370] TRAIN: iteration 11332 : loss : 0.252235, loss_ce: 0.004213, loss_dice: 0.500257
[09:34:33.579] TRAIN: iteration 11333 : loss : 0.121986, loss_ce: 0.006560, loss_dice: 0.237413
[09:34:33.789] TRAIN: iteration 11334 : loss : 0.188870, loss_ce: 0.004292, loss_dice: 0.373449
[09:34:33.996] TRAIN: iteration 11335 : loss : 0.137737, loss_ce: 0.006476, loss_dice: 0.268998
[09:34:34.205] TRAIN: iteration 11336 : loss : 0.086751, loss_ce: 0.004353, loss_dice: 0.169150
[09:34:34.421] TRAIN: iteration 11337 : loss : 0.095543, loss_ce: 0.004465, loss_dice: 0.186622
[09:34:34.634] TRAIN: iteration 11338 : loss : 0.140522, loss_ce: 0.003568, loss_dice: 0.277475
[09:34:35.095] TRAIN: iteration 11339 : loss : 0.251850, loss_ce: 0.003481, loss_dice: 0.500220
[09:34:35.311] TRAIN: iteration 11340 : loss : 0.161476, loss_ce: 0.004116, loss_dice: 0.318837
[09:34:35.546] TRAIN: iteration 11341 : loss : 0.130799, loss_ce: 0.013447, loss_dice: 0.248151
[09:34:35.760] TRAIN: iteration 11342 : loss : 0.051427, loss_ce: 0.004700, loss_dice: 0.098154
[09:34:35.968] TRAIN: iteration 11343 : loss : 0.252259, loss_ce: 0.004222, loss_dice: 0.500295
[09:34:36.176] TRAIN: iteration 11344 : loss : 0.085265, loss_ce: 0.004504, loss_dice: 0.166026
[09:34:36.417] TRAIN: iteration 11345 : loss : 0.223168, loss_ce: 0.004392, loss_dice: 0.441945
[09:34:36.625] TRAIN: iteration 11346 : loss : 0.243606, loss_ce: 0.005197, loss_dice: 0.482014
[09:34:36.839] TRAIN: iteration 11347 : loss : 0.135586, loss_ce: 0.005340, loss_dice: 0.265831
[09:34:37.047] TRAIN: iteration 11348 : loss : 0.127749, loss_ce: 0.006132, loss_dice: 0.249365
[09:34:37.255] TRAIN: iteration 11349 : loss : 0.146176, loss_ce: 0.009037, loss_dice: 0.283315
[09:34:37.467] TRAIN: iteration 11350 : loss : 0.104674, loss_ce: 0.009247, loss_dice: 0.200101
[09:34:38.993] TRAIN: iteration 11351 : loss : 0.153595, loss_ce: 0.004504, loss_dice: 0.302685
[09:34:39.232] TRAIN: iteration 11352 : loss : 0.154271, loss_ce: 0.006780, loss_dice: 0.301761
[09:34:39.440] TRAIN: iteration 11353 : loss : 0.252349, loss_ce: 0.004918, loss_dice: 0.499780
[09:34:39.648] TRAIN: iteration 11354 : loss : 0.129622, loss_ce: 0.005312, loss_dice: 0.253933
[09:34:39.856] TRAIN: iteration 11355 : loss : 0.045887, loss_ce: 0.004388, loss_dice: 0.087387
[09:34:40.069] TRAIN: iteration 11356 : loss : 0.057574, loss_ce: 0.004528, loss_dice: 0.110620
[09:34:40.277] TRAIN: iteration 11357 : loss : 0.067980, loss_ce: 0.003848, loss_dice: 0.132111
[09:34:40.485] TRAIN: iteration 11358 : loss : 0.131938, loss_ce: 0.019953, loss_dice: 0.243923
[09:34:40.740] TRAIN: iteration 11359 : loss : 0.112824, loss_ce: 0.003889, loss_dice: 0.221759
[09:34:40.950] TRAIN: iteration 11360 : loss : 0.250038, loss_ce: 0.004107, loss_dice: 0.495969
[09:34:41.185] TRAIN: iteration 11361 : loss : 0.187721, loss_ce: 0.005623, loss_dice: 0.369820
[09:34:41.395] TRAIN: iteration 11362 : loss : 0.085996, loss_ce: 0.005418, loss_dice: 0.166573
[09:34:41.607] TRAIN: iteration 11363 : loss : 0.220643, loss_ce: 0.005839, loss_dice: 0.435447
[09:34:41.816] TRAIN: iteration 11364 : loss : 0.234332, loss_ce: 0.005042, loss_dice: 0.463622
[09:34:42.023] TRAIN: iteration 11365 : loss : 0.081528, loss_ce: 0.004722, loss_dice: 0.158333
[09:34:42.232] TRAIN: iteration 11366 : loss : 0.143790, loss_ce: 0.024972, loss_dice: 0.262607
[09:34:42.441] TRAIN: iteration 11367 : loss : 0.253405, loss_ce: 0.008481, loss_dice: 0.498328
[09:34:42.650] TRAIN: iteration 11368 : loss : 0.213270, loss_ce: 0.011133, loss_dice: 0.415406
[09:34:42.859] TRAIN: iteration 11369 : loss : 0.173126, loss_ce: 0.003806, loss_dice: 0.342446
[09:34:43.098] TRAIN: iteration 11370 : loss : 0.042151, loss_ce: 0.002989, loss_dice: 0.081314
[09:34:43.306] TRAIN: iteration 11371 : loss : 0.212606, loss_ce: 0.003995, loss_dice: 0.421217
[09:34:43.515] TRAIN: iteration 11372 : loss : 0.048469, loss_ce: 0.004041, loss_dice: 0.092898
[09:34:43.723] TRAIN: iteration 11373 : loss : 0.220909, loss_ce: 0.004855, loss_dice: 0.436962
[09:34:44.290] TRAIN: iteration 11374 : loss : 0.127511, loss_ce: 0.004895, loss_dice: 0.250127
[09:34:44.500] TRAIN: iteration 11375 : loss : 0.150353, loss_ce: 0.013442, loss_dice: 0.287265
[09:34:44.713] TRAIN: iteration 11376 : loss : 0.061437, loss_ce: 0.002682, loss_dice: 0.120192
[09:34:44.920] TRAIN: iteration 11377 : loss : 0.251696, loss_ce: 0.003238, loss_dice: 0.500154
[09:34:45.908] TRAIN: iteration 11378 : loss : 0.243250, loss_ce: 0.004295, loss_dice: 0.482205
[09:34:46.117] TRAIN: iteration 11379 : loss : 0.213518, loss_ce: 0.010993, loss_dice: 0.416044
[09:34:46.328] TRAIN: iteration 11380 : loss : 0.085431, loss_ce: 0.003809, loss_dice: 0.167052
[09:34:46.577] TRAIN: iteration 11381 : loss : 0.102669, loss_ce: 0.003203, loss_dice: 0.202135
[09:34:46.785] TRAIN: iteration 11382 : loss : 0.243790, loss_ce: 0.006283, loss_dice: 0.481298
[09:34:46.993] TRAIN: iteration 11383 : loss : 0.175357, loss_ce: 0.007417, loss_dice: 0.343297
[09:34:47.200] TRAIN: iteration 11384 : loss : 0.251472, loss_ce: 0.003747, loss_dice: 0.499197
[09:34:47.410] TRAIN: iteration 11385 : loss : 0.251843, loss_ce: 0.003459, loss_dice: 0.500227
[09:34:48.094] TRAIN: iteration 11386 : loss : 0.160606, loss_ce: 0.008569, loss_dice: 0.312642
[09:34:48.695] TRAIN: iteration 11387 : loss : 0.196534, loss_ce: 0.006033, loss_dice: 0.387034
[09:34:48.910] TRAIN: iteration 11388 : loss : 0.180582, loss_ce: 0.005550, loss_dice: 0.355615
[09:34:49.117] TRAIN: iteration 11389 : loss : 0.137409, loss_ce: 0.010438, loss_dice: 0.264381
[09:34:49.323] TRAIN: iteration 11390 : loss : 0.142351, loss_ce: 0.007424, loss_dice: 0.277278
[09:34:49.531] TRAIN: iteration 11391 : loss : 0.072560, loss_ce: 0.008174, loss_dice: 0.136946
[09:34:50.130] TRAIN: iteration 11392 : loss : 0.087166, loss_ce: 0.005578, loss_dice: 0.168753
[09:34:50.337] TRAIN: iteration 11393 : loss : 0.100492, loss_ce: 0.003212, loss_dice: 0.197773
[09:34:50.543] TRAIN: iteration 11394 : loss : 0.214177, loss_ce: 0.005717, loss_dice: 0.422637
[09:34:52.336] TRAIN: iteration 11395 : loss : 0.144466, loss_ce: 0.006577, loss_dice: 0.282354
[09:34:52.542] TRAIN: iteration 11396 : loss : 0.252311, loss_ce: 0.004382, loss_dice: 0.500241
[09:34:52.749] TRAIN: iteration 11397 : loss : 0.104967, loss_ce: 0.003566, loss_dice: 0.206368
[09:34:52.959] TRAIN: iteration 11398 : loss : 0.220961, loss_ce: 0.009089, loss_dice: 0.432834
[09:34:53.169] TRAIN: iteration 11399 : loss : 0.137233, loss_ce: 0.006293, loss_dice: 0.268173
[09:34:53.376] TRAIN: iteration 11400 : loss : 0.217740, loss_ce: 0.003754, loss_dice: 0.431725
[09:34:53.377] NaN or Inf found in input tensor.
[09:34:53.592] TRAIN: iteration 11401 : loss : 0.145143, loss_ce: 0.007595, loss_dice: 0.282690
[09:34:53.802] TRAIN: iteration 11402 : loss : 0.099648, loss_ce: 0.005490, loss_dice: 0.193806
[09:34:54.009] TRAIN: iteration 11403 : loss : 0.139331, loss_ce: 0.004251, loss_dice: 0.274411
[09:34:54.216] TRAIN: iteration 11404 : loss : 0.057539, loss_ce: 0.003394, loss_dice: 0.111683
[09:34:54.424] TRAIN: iteration 11405 : loss : 0.195480, loss_ce: 0.008512, loss_dice: 0.382447
[09:34:54.632] TRAIN: iteration 11406 : loss : 0.125037, loss_ce: 0.004482, loss_dice: 0.245592
[09:34:54.839] TRAIN: iteration 11407 : loss : 0.252540, loss_ce: 0.004739, loss_dice: 0.500340
[09:34:55.053] TRAIN: iteration 11408 : loss : 0.071460, loss_ce: 0.007650, loss_dice: 0.135269
[09:34:55.260] TRAIN: iteration 11409 : loss : 0.108425, loss_ce: 0.003591, loss_dice: 0.213260
[09:34:55.467] TRAIN: iteration 11410 : loss : 0.088973, loss_ce: 0.002590, loss_dice: 0.175357
[09:34:58.628] TRAIN: iteration 11411 : loss : 0.073147, loss_ce: 0.003253, loss_dice: 0.143040
[09:34:58.835] TRAIN: iteration 11412 : loss : 0.199073, loss_ce: 0.004460, loss_dice: 0.393687
[09:34:59.042] TRAIN: iteration 11413 : loss : 0.108519, loss_ce: 0.011277, loss_dice: 0.205762
[09:34:59.250] TRAIN: iteration 11414 : loss : 0.109601, loss_ce: 0.006015, loss_dice: 0.213188
[09:34:59.457] TRAIN: iteration 11415 : loss : 0.176224, loss_ce: 0.003248, loss_dice: 0.349200
[09:34:59.664] TRAIN: iteration 11416 : loss : 0.252230, loss_ce: 0.004154, loss_dice: 0.500306
[09:34:59.871] TRAIN: iteration 11417 : loss : 0.195279, loss_ce: 0.002678, loss_dice: 0.387880
[09:35:00.080] TRAIN: iteration 11418 : loss : 0.114942, loss_ce: 0.002506, loss_dice: 0.227378
[09:35:00.287] TRAIN: iteration 11419 : loss : 0.130565, loss_ce: 0.003247, loss_dice: 0.257884
[09:35:00.493] TRAIN: iteration 11420 : loss : 0.132602, loss_ce: 0.022484, loss_dice: 0.242721
[09:35:00.734] TRAIN: iteration 11421 : loss : 0.247702, loss_ce: 0.008875, loss_dice: 0.486529
[09:35:00.947] TRAIN: iteration 11422 : loss : 0.154948, loss_ce: 0.004937, loss_dice: 0.304959
[09:35:01.154] TRAIN: iteration 11423 : loss : 0.159525, loss_ce: 0.005353, loss_dice: 0.313697
[09:35:01.361] TRAIN: iteration 11424 : loss : 0.251906, loss_ce: 0.003555, loss_dice: 0.500257
[09:35:01.576] TRAIN: iteration 11425 : loss : 0.083529, loss_ce: 0.006330, loss_dice: 0.160728
[09:35:01.785] TRAIN: iteration 11426 : loss : 0.114686, loss_ce: 0.005500, loss_dice: 0.223872
[09:35:02.490] TRAIN: iteration 11427 : loss : 0.180410, loss_ce: 0.007448, loss_dice: 0.353372
[09:35:02.697] TRAIN: iteration 11428 : loss : 0.157092, loss_ce: 0.006202, loss_dice: 0.307982
[09:35:02.907] TRAIN: iteration 11429 : loss : 0.233162, loss_ce: 0.005021, loss_dice: 0.461303
[09:35:03.115] TRAIN: iteration 11430 : loss : 0.155340, loss_ce: 0.030093, loss_dice: 0.280587
[09:35:03.330] TRAIN: iteration 11431 : loss : 0.030243, loss_ce: 0.002258, loss_dice: 0.058227
[09:35:03.538] TRAIN: iteration 11432 : loss : 0.141078, loss_ce: 0.002797, loss_dice: 0.279358
[09:35:03.749] TRAIN: iteration 11433 : loss : 0.191461, loss_ce: 0.014351, loss_dice: 0.368572
[09:35:03.959] TRAIN: iteration 11434 : loss : 0.037502, loss_ce: 0.002634, loss_dice: 0.072369
[09:35:04.996] TRAIN: iteration 11435 : loss : 0.251039, loss_ce: 0.001984, loss_dice: 0.500094
[09:35:05.205] TRAIN: iteration 11436 : loss : 0.252115, loss_ce: 0.003944, loss_dice: 0.500287
[09:35:05.412] TRAIN: iteration 11437 : loss : 0.207552, loss_ce: 0.005162, loss_dice: 0.409943
[09:35:05.619] TRAIN: iteration 11438 : loss : 0.052100, loss_ce: 0.004385, loss_dice: 0.099816
[09:35:05.826] TRAIN: iteration 11439 : loss : 0.189608, loss_ce: 0.003376, loss_dice: 0.375839
[09:35:06.035] TRAIN: iteration 11440 : loss : 0.133464, loss_ce: 0.003756, loss_dice: 0.263173
[09:35:06.274] TRAIN: iteration 11441 : loss : 0.076178, loss_ce: 0.005213, loss_dice: 0.147143
[09:35:06.482] TRAIN: iteration 11442 : loss : 0.065152, loss_ce: 0.003933, loss_dice: 0.126370
[09:35:08.323] TRAIN: iteration 11443 : loss : 0.052255, loss_ce: 0.003284, loss_dice: 0.101226
[09:35:08.568] TRAIN: iteration 11444 : loss : 0.252539, loss_ce: 0.004725, loss_dice: 0.500353
[09:35:08.783] TRAIN: iteration 11445 : loss : 0.198626, loss_ce: 0.022157, loss_dice: 0.375096
[09:35:08.998] TRAIN: iteration 11446 : loss : 0.121195, loss_ce: 0.005381, loss_dice: 0.237009
[09:35:09.207] TRAIN: iteration 11447 : loss : 0.126256, loss_ce: 0.002538, loss_dice: 0.249974
[09:35:09.415] TRAIN: iteration 11448 : loss : 0.078496, loss_ce: 0.004574, loss_dice: 0.152417
[09:35:09.622] TRAIN: iteration 11449 : loss : 0.246548, loss_ce: 0.005497, loss_dice: 0.487599
[09:35:09.834] TRAIN: iteration 11450 : loss : 0.172496, loss_ce: 0.001958, loss_dice: 0.343033
[09:35:10.338] TRAIN: iteration 11451 : loss : 0.065805, loss_ce: 0.003867, loss_dice: 0.127743
[09:35:10.550] TRAIN: iteration 11452 : loss : 0.166808, loss_ce: 0.007199, loss_dice: 0.326417
[09:35:10.759] TRAIN: iteration 11453 : loss : 0.099965, loss_ce: 0.002114, loss_dice: 0.197816
[09:35:10.968] TRAIN: iteration 11454 : loss : 0.188725, loss_ce: 0.025623, loss_dice: 0.351827
[09:35:11.176] TRAIN: iteration 11455 : loss : 0.248048, loss_ce: 0.002360, loss_dice: 0.493736
[09:35:11.389] TRAIN: iteration 11456 : loss : 0.120044, loss_ce: 0.002756, loss_dice: 0.237332
[09:35:11.597] TRAIN: iteration 11457 : loss : 0.050971, loss_ce: 0.001648, loss_dice: 0.100294
[09:35:11.805] TRAIN: iteration 11458 : loss : 0.037060, loss_ce: 0.001875, loss_dice: 0.072245
[09:35:12.945] TRAIN: iteration 11459 : loss : 0.246869, loss_ce: 0.002867, loss_dice: 0.490872
[09:35:13.152] TRAIN: iteration 11460 : loss : 0.099956, loss_ce: 0.003925, loss_dice: 0.195988
[09:35:13.391] TRAIN: iteration 11461 : loss : 0.158559, loss_ce: 0.004671, loss_dice: 0.312447
[09:35:13.598] TRAIN: iteration 11462 : loss : 0.239522, loss_ce: 0.003453, loss_dice: 0.475590
[09:35:13.806] TRAIN: iteration 11463 : loss : 0.097031, loss_ce: 0.003626, loss_dice: 0.190437
[09:35:14.018] TRAIN: iteration 11464 : loss : 0.164317, loss_ce: 0.005140, loss_dice: 0.323494
[09:35:14.228] TRAIN: iteration 11465 : loss : 0.159075, loss_ce: 0.011996, loss_dice: 0.306154
[09:35:14.442] TRAIN: iteration 11466 : loss : 0.113952, loss_ce: 0.005545, loss_dice: 0.222359
[09:35:14.650] TRAIN: iteration 11467 : loss : 0.200248, loss_ce: 0.003458, loss_dice: 0.397038
[09:35:14.857] TRAIN: iteration 11468 : loss : 0.041422, loss_ce: 0.003709, loss_dice: 0.079135
[09:35:15.071] TRAIN: iteration 11469 : loss : 0.130351, loss_ce: 0.009625, loss_dice: 0.251076
[09:35:15.279] TRAIN: iteration 11470 : loss : 0.230678, loss_ce: 0.003997, loss_dice: 0.457359
[09:35:15.488] TRAIN: iteration 11471 : loss : 0.055833, loss_ce: 0.006000, loss_dice: 0.105666
[09:35:15.696] TRAIN: iteration 11472 : loss : 0.249579, loss_ce: 0.004750, loss_dice: 0.494409
[09:35:15.903] TRAIN: iteration 11473 : loss : 0.251238, loss_ce: 0.005442, loss_dice: 0.497033
[09:35:16.115] TRAIN: iteration 11474 : loss : 0.164319, loss_ce: 0.004524, loss_dice: 0.324114
[09:35:16.322] TRAIN: iteration 11475 : loss : 0.079733, loss_ce: 0.009202, loss_dice: 0.150263
[09:35:16.529] TRAIN: iteration 11476 : loss : 0.253194, loss_ce: 0.005959, loss_dice: 0.500430
[09:35:16.736] TRAIN: iteration 11477 : loss : 0.101574, loss_ce: 0.004559, loss_dice: 0.198590
[09:35:16.945] TRAIN: iteration 11478 : loss : 0.139775, loss_ce: 0.004151, loss_dice: 0.275399
[09:35:17.152] TRAIN: iteration 11479 : loss : 0.163293, loss_ce: 0.004994, loss_dice: 0.321593
[09:35:17.361] TRAIN: iteration 11480 : loss : 0.125811, loss_ce: 0.004288, loss_dice: 0.247334
[09:35:18.181] TRAIN: iteration 11481 : loss : 0.231683, loss_ce: 0.003264, loss_dice: 0.460103
[09:35:18.388] TRAIN: iteration 11482 : loss : 0.033727, loss_ce: 0.001655, loss_dice: 0.065799
[09:35:18.596] TRAIN: iteration 11483 : loss : 0.220431, loss_ce: 0.003613, loss_dice: 0.437249
[09:35:18.803] TRAIN: iteration 11484 : loss : 0.251663, loss_ce: 0.003123, loss_dice: 0.500202
[09:35:19.010] TRAIN: iteration 11485 : loss : 0.251459, loss_ce: 0.002757, loss_dice: 0.500162
[09:35:19.303] TRAIN: iteration 11486 : loss : 0.250356, loss_ce: 0.002802, loss_dice: 0.497910
[09:35:19.650] TRAIN: iteration 11487 : loss : 0.070099, loss_ce: 0.001377, loss_dice: 0.138821
[09:35:20.144] TRAIN: iteration 11488 : loss : 0.248697, loss_ce: 0.002397, loss_dice: 0.494996
[09:35:21.894] TRAIN: iteration 11489 : loss : 0.188299, loss_ce: 0.003336, loss_dice: 0.373263
[09:35:22.102] TRAIN: iteration 11490 : loss : 0.212328, loss_ce: 0.004137, loss_dice: 0.420518
[09:35:22.312] TRAIN: iteration 11491 : loss : 0.249277, loss_ce: 0.004008, loss_dice: 0.494546
[09:35:22.520] TRAIN: iteration 11492 : loss : 0.139563, loss_ce: 0.001560, loss_dice: 0.277565
[09:35:22.732] TRAIN: iteration 11493 : loss : 0.231858, loss_ce: 0.003498, loss_dice: 0.460218
[09:35:22.945] TRAIN: iteration 11494 : loss : 0.250627, loss_ce: 0.001231, loss_dice: 0.500023
[09:35:23.152] TRAIN: iteration 11495 : loss : 0.174335, loss_ce: 0.022563, loss_dice: 0.326106
[09:35:23.363] TRAIN: iteration 11496 : loss : 0.198540, loss_ce: 0.001454, loss_dice: 0.395627
[09:35:23.825] TRAIN: iteration 11497 : loss : 0.091358, loss_ce: 0.005797, loss_dice: 0.176920
[09:35:24.032] TRAIN: iteration 11498 : loss : 0.142724, loss_ce: 0.003544, loss_dice: 0.281904
[09:35:24.238] TRAIN: iteration 11499 : loss : 0.173542, loss_ce: 0.008896, loss_dice: 0.338188
[09:35:24.445] TRAIN: iteration 11500 : loss : 0.122815, loss_ce: 0.007782, loss_dice: 0.237848
[09:35:24.683] TRAIN: iteration 11501 : loss : 0.110843, loss_ce: 0.003786, loss_dice: 0.217901
[09:35:24.954] TRAIN: iteration 11502 : loss : 0.241540, loss_ce: 0.006161, loss_dice: 0.476918
[09:35:26.391] TRAIN: iteration 11503 : loss : 0.252101, loss_ce: 0.003927, loss_dice: 0.500276
[09:35:26.598] TRAIN: iteration 11504 : loss : 0.075270, loss_ce: 0.005453, loss_dice: 0.145087
[09:35:26.811] TRAIN: iteration 11505 : loss : 0.237109, loss_ce: 0.024546, loss_dice: 0.449673
[09:35:27.020] TRAIN: iteration 11506 : loss : 0.112419, loss_ce: 0.006036, loss_dice: 0.218802
[09:35:27.231] TRAIN: iteration 11507 : loss : 0.252359, loss_ce: 0.004436, loss_dice: 0.500282
[09:35:27.449] TRAIN: iteration 11508 : loss : 0.238302, loss_ce: 0.006055, loss_dice: 0.470550
[09:35:27.656] TRAIN: iteration 11509 : loss : 0.249925, loss_ce: 0.006960, loss_dice: 0.492890
[09:35:27.862] TRAIN: iteration 11510 : loss : 0.238849, loss_ce: 0.012689, loss_dice: 0.465009
[09:35:28.371] TRAIN: iteration 11511 : loss : 0.173267, loss_ce: 0.005690, loss_dice: 0.340845
[09:35:28.579] TRAIN: iteration 11512 : loss : 0.183229, loss_ce: 0.007054, loss_dice: 0.359403
[09:35:29.167] TRAIN: iteration 11513 : loss : 0.053450, loss_ce: 0.004925, loss_dice: 0.101976
[09:35:29.375] TRAIN: iteration 11514 : loss : 0.070833, loss_ce: 0.008406, loss_dice: 0.133261
[09:35:29.582] TRAIN: iteration 11515 : loss : 0.064281, loss_ce: 0.006682, loss_dice: 0.121880
[09:35:29.792] TRAIN: iteration 11516 : loss : 0.179081, loss_ce: 0.003851, loss_dice: 0.354312
[09:35:30.028] TRAIN: iteration 11517 : loss : 0.251955, loss_ce: 0.003686, loss_dice: 0.500224
[09:35:30.242] TRAIN: iteration 11518 : loss : 0.069897, loss_ce: 0.003869, loss_dice: 0.135925
[09:35:30.453] TRAIN: iteration 11519 : loss : 0.250582, loss_ce: 0.005196, loss_dice: 0.495968
[09:35:30.661] TRAIN: iteration 11520 : loss : 0.097811, loss_ce: 0.003564, loss_dice: 0.192058
[09:35:30.910] TRAIN: iteration 11521 : loss : 0.065491, loss_ce: 0.003554, loss_dice: 0.127428
[09:35:31.824] TRAIN: iteration 11522 : loss : 0.252501, loss_ce: 0.004900, loss_dice: 0.500101
[09:35:32.031] TRAIN: iteration 11523 : loss : 0.167695, loss_ce: 0.005124, loss_dice: 0.330267
[09:35:32.241] TRAIN: iteration 11524 : loss : 0.196797, loss_ce: 0.005939, loss_dice: 0.387655
[09:35:32.447] TRAIN: iteration 11525 : loss : 0.094206, loss_ce: 0.002696, loss_dice: 0.185716
[09:35:32.659] TRAIN: iteration 11526 : loss : 0.247394, loss_ce: 0.003190, loss_dice: 0.491598
[09:35:32.868] TRAIN: iteration 11527 : loss : 0.091295, loss_ce: 0.003634, loss_dice: 0.178956
[09:35:33.075] TRAIN: iteration 11528 : loss : 0.063092, loss_ce: 0.003523, loss_dice: 0.122661
[09:35:33.389] TRAIN: iteration 11529 : loss : 0.047861, loss_ce: 0.003640, loss_dice: 0.092082
[09:35:33.770] TRAIN: iteration 11530 : loss : 0.153170, loss_ce: 0.009678, loss_dice: 0.296663
[09:35:33.978] TRAIN: iteration 11531 : loss : 0.091326, loss_ce: 0.010902, loss_dice: 0.171751
[09:35:34.189] TRAIN: iteration 11532 : loss : 0.153986, loss_ce: 0.003764, loss_dice: 0.304208
[09:35:34.396] TRAIN: iteration 11533 : loss : 0.062624, loss_ce: 0.006776, loss_dice: 0.118472
[09:35:34.605] TRAIN: iteration 11534 : loss : 0.070732, loss_ce: 0.002813, loss_dice: 0.138652
[09:35:35.299] TRAIN: iteration 11535 : loss : 0.060571, loss_ce: 0.001288, loss_dice: 0.119855
[09:35:35.507] TRAIN: iteration 11536 : loss : 0.088701, loss_ce: 0.003154, loss_dice: 0.174247
[09:35:35.719] TRAIN: iteration 11537 : loss : 0.183225, loss_ce: 0.003922, loss_dice: 0.362528
[09:35:37.231] TRAIN: iteration 11538 : loss : 0.070839, loss_ce: 0.002649, loss_dice: 0.139028
[09:35:37.438] TRAIN: iteration 11539 : loss : 0.090923, loss_ce: 0.005873, loss_dice: 0.175973
[09:35:37.647] TRAIN: iteration 11540 : loss : 0.048506, loss_ce: 0.002595, loss_dice: 0.094417
[09:35:37.884] TRAIN: iteration 11541 : loss : 0.088575, loss_ce: 0.010091, loss_dice: 0.167059
[09:35:38.093] TRAIN: iteration 11542 : loss : 0.066113, loss_ce: 0.001542, loss_dice: 0.130683
[09:35:39.541] TRAIN: iteration 11543 : loss : 0.251693, loss_ce: 0.003164, loss_dice: 0.500221
[09:35:39.748] TRAIN: iteration 11544 : loss : 0.122491, loss_ce: 0.004418, loss_dice: 0.240564
[09:35:39.955] TRAIN: iteration 11545 : loss : 0.201832, loss_ce: 0.004097, loss_dice: 0.399566
[09:35:40.164] TRAIN: iteration 11546 : loss : 0.186613, loss_ce: 0.006167, loss_dice: 0.367058
[09:35:40.372] TRAIN: iteration 11547 : loss : 0.065349, loss_ce: 0.005464, loss_dice: 0.125235
[09:35:40.579] TRAIN: iteration 11548 : loss : 0.251447, loss_ce: 0.002711, loss_dice: 0.500183
[09:35:40.787] TRAIN: iteration 11549 : loss : 0.066796, loss_ce: 0.003305, loss_dice: 0.130287
[09:35:40.994] TRAIN: iteration 11550 : loss : 0.238868, loss_ce: 0.003688, loss_dice: 0.474047
[09:35:42.224] TRAIN: iteration 11551 : loss : 0.243639, loss_ce: 0.006938, loss_dice: 0.480341
[09:35:42.454] TRAIN: iteration 11552 : loss : 0.119459, loss_ce: 0.002797, loss_dice: 0.236122
[09:35:42.660] TRAIN: iteration 11553 : loss : 0.251201, loss_ce: 0.002312, loss_dice: 0.500090
[09:35:42.867] TRAIN: iteration 11554 : loss : 0.245189, loss_ce: 0.003400, loss_dice: 0.486979
[09:35:43.075] TRAIN: iteration 11555 : loss : 0.048735, loss_ce: 0.002976, loss_dice: 0.094493
[09:35:43.283] TRAIN: iteration 11556 : loss : 0.078779, loss_ce: 0.001596, loss_dice: 0.155962
[09:35:43.490] TRAIN: iteration 11557 : loss : 0.071371, loss_ce: 0.002787, loss_dice: 0.139956
[09:35:43.696] TRAIN: iteration 11558 : loss : 0.075308, loss_ce: 0.002369, loss_dice: 0.148246
[09:35:43.903] TRAIN: iteration 11559 : loss : 0.248054, loss_ce: 0.007127, loss_dice: 0.488981
[09:35:44.582] TRAIN: iteration 11560 : loss : 0.040984, loss_ce: 0.002314, loss_dice: 0.079653
[09:35:44.823] TRAIN: iteration 11561 : loss : 0.178688, loss_ce: 0.004406, loss_dice: 0.352969
[09:35:45.031] TRAIN: iteration 11562 : loss : 0.233876, loss_ce: 0.003873, loss_dice: 0.463880
[09:35:45.242] TRAIN: iteration 11563 : loss : 0.235478, loss_ce: 0.007933, loss_dice: 0.463023
[09:35:45.450] TRAIN: iteration 11564 : loss : 0.109510, loss_ce: 0.005813, loss_dice: 0.213207
[09:35:47.239] TRAIN: iteration 11565 : loss : 0.200038, loss_ce: 0.006948, loss_dice: 0.393128
[09:35:47.447] TRAIN: iteration 11566 : loss : 0.076582, loss_ce: 0.003774, loss_dice: 0.149390
[09:35:47.655] TRAIN: iteration 11567 : loss : 0.099527, loss_ce: 0.002338, loss_dice: 0.196717
[09:35:47.863] TRAIN: iteration 11568 : loss : 0.167321, loss_ce: 0.002530, loss_dice: 0.332112
[09:35:48.071] TRAIN: iteration 11569 : loss : 0.247433, loss_ce: 0.002630, loss_dice: 0.492237
[09:35:48.280] TRAIN: iteration 11570 : loss : 0.251244, loss_ce: 0.002371, loss_dice: 0.500117
[09:35:48.488] TRAIN: iteration 11571 : loss : 0.168121, loss_ce: 0.002890, loss_dice: 0.333353
[09:35:48.697] TRAIN: iteration 11572 : loss : 0.065069, loss_ce: 0.003022, loss_dice: 0.127116
[09:35:49.555] TRAIN: iteration 11573 : loss : 0.114940, loss_ce: 0.005890, loss_dice: 0.223990
[09:35:49.775] TRAIN: iteration 11574 : loss : 0.144095, loss_ce: 0.005650, loss_dice: 0.282541
[09:35:49.982] TRAIN: iteration 11575 : loss : 0.176337, loss_ce: 0.008944, loss_dice: 0.343730
[09:35:50.192] TRAIN: iteration 11576 : loss : 0.106489, loss_ce: 0.002217, loss_dice: 0.210761
[09:35:50.399] TRAIN: iteration 11577 : loss : 0.144110, loss_ce: 0.003794, loss_dice: 0.284427
[09:35:50.727] TRAIN: iteration 11578 : loss : 0.199361, loss_ce: 0.002866, loss_dice: 0.395855
[09:35:50.936] TRAIN: iteration 11579 : loss : 0.180457, loss_ce: 0.003091, loss_dice: 0.357823
[09:35:51.148] TRAIN: iteration 11580 : loss : 0.250337, loss_ce: 0.003189, loss_dice: 0.497485
[09:35:51.148] NaN or Inf found in input tensor.
[09:35:51.807] TRAIN: iteration 11581 : loss : 0.061080, loss_ce: 0.001923, loss_dice: 0.120237
[09:35:52.018] TRAIN: iteration 11582 : loss : 0.080772, loss_ce: 0.002465, loss_dice: 0.159079
[09:35:52.228] TRAIN: iteration 11583 : loss : 0.061176, loss_ce: 0.003653, loss_dice: 0.118700
[09:35:52.436] TRAIN: iteration 11584 : loss : 0.140726, loss_ce: 0.006452, loss_dice: 0.275000
[09:35:52.643] TRAIN: iteration 11585 : loss : 0.172747, loss_ce: 0.004308, loss_dice: 0.341186
[09:35:52.852] TRAIN: iteration 11586 : loss : 0.250410, loss_ce: 0.000809, loss_dice: 0.500011
[09:35:53.415] TRAIN: iteration 11587 : loss : 0.144097, loss_ce: 0.011483, loss_dice: 0.276711
[09:35:53.624] TRAIN: iteration 11588 : loss : 0.090238, loss_ce: 0.007226, loss_dice: 0.173250
[09:35:54.166] TRAIN: iteration 11589 : loss : 0.070343, loss_ce: 0.007762, loss_dice: 0.132923
[09:35:54.375] TRAIN: iteration 11590 : loss : 0.204496, loss_ce: 0.002362, loss_dice: 0.406629
[09:35:54.581] TRAIN: iteration 11591 : loss : 0.250959, loss_ce: 0.001815, loss_dice: 0.500103
[09:35:54.794] TRAIN: iteration 11592 : loss : 0.141904, loss_ce: 0.002539, loss_dice: 0.281269
[09:35:55.001] TRAIN: iteration 11593 : loss : 0.129692, loss_ce: 0.005830, loss_dice: 0.253553
[09:35:55.208] TRAIN: iteration 11594 : loss : 0.211351, loss_ce: 0.003500, loss_dice: 0.419202
[09:35:55.706] TRAIN: iteration 11595 : loss : 0.243172, loss_ce: 0.002292, loss_dice: 0.484051
[09:35:57.688] TRAIN: iteration 11596 : loss : 0.176082, loss_ce: 0.004883, loss_dice: 0.347280
[09:35:57.894] TRAIN: iteration 11597 : loss : 0.090087, loss_ce: 0.006341, loss_dice: 0.173832
[09:35:58.101] TRAIN: iteration 11598 : loss : 0.250840, loss_ce: 0.005363, loss_dice: 0.496317
[09:35:58.307] TRAIN: iteration 11599 : loss : 0.096891, loss_ce: 0.001740, loss_dice: 0.192042
[09:35:58.515] TRAIN: iteration 11600 : loss : 0.113550, loss_ce: 0.013652, loss_dice: 0.213447
[09:35:58.761] TRAIN: iteration 11601 : loss : 0.225536, loss_ce: 0.004011, loss_dice: 0.447060
[09:35:58.971] TRAIN: iteration 11602 : loss : 0.168051, loss_ce: 0.004623, loss_dice: 0.331479
[09:35:59.178] TRAIN: iteration 11603 : loss : 0.029910, loss_ce: 0.001832, loss_dice: 0.057987
[09:36:00.492] TRAIN: iteration 11604 : loss : 0.105746, loss_ce: 0.003325, loss_dice: 0.208167
[09:36:00.699] TRAIN: iteration 11605 : loss : 0.212965, loss_ce: 0.003699, loss_dice: 0.422232
[09:36:01.971] TRAIN: iteration 11606 : loss : 0.062640, loss_ce: 0.006421, loss_dice: 0.118859
[09:36:02.186] TRAIN: iteration 11607 : loss : 0.062901, loss_ce: 0.004226, loss_dice: 0.121577
[09:36:02.394] TRAIN: iteration 11608 : loss : 0.077106, loss_ce: 0.003963, loss_dice: 0.150249
[09:36:02.608] TRAIN: iteration 11609 : loss : 0.055288, loss_ce: 0.004280, loss_dice: 0.106296
[09:36:02.816] TRAIN: iteration 11610 : loss : 0.217919, loss_ce: 0.003277, loss_dice: 0.432561
[09:36:03.023] TRAIN: iteration 11611 : loss : 0.083873, loss_ce: 0.003150, loss_dice: 0.164595
[09:36:03.812] TRAIN: iteration 11612 : loss : 0.078789, loss_ce: 0.004230, loss_dice: 0.153347
[09:36:04.029] TRAIN: iteration 11613 : loss : 0.099800, loss_ce: 0.005259, loss_dice: 0.194340
[09:36:04.236] TRAIN: iteration 11614 : loss : 0.051835, loss_ce: 0.003009, loss_dice: 0.100661
[09:36:04.446] TRAIN: iteration 11615 : loss : 0.212334, loss_ce: 0.003765, loss_dice: 0.420904
[09:36:04.655] TRAIN: iteration 11616 : loss : 0.173660, loss_ce: 0.007733, loss_dice: 0.339587
[09:36:04.861] TRAIN: iteration 11617 : loss : 0.078533, loss_ce: 0.006188, loss_dice: 0.150879
[09:36:05.070] TRAIN: iteration 11618 : loss : 0.066811, loss_ce: 0.005528, loss_dice: 0.128093
[09:36:06.342] TRAIN: iteration 11619 : loss : 0.251720, loss_ce: 0.003241, loss_dice: 0.500200
[09:36:06.550] TRAIN: iteration 11620 : loss : 0.147582, loss_ce: 0.005005, loss_dice: 0.290160
[09:36:07.211] TRAIN: iteration 11621 : loss : 0.100591, loss_ce: 0.006257, loss_dice: 0.194924
[09:36:07.422] TRAIN: iteration 11622 : loss : 0.104453, loss_ce: 0.004429, loss_dice: 0.204476
[09:36:07.631] TRAIN: iteration 11623 : loss : 0.090025, loss_ce: 0.005111, loss_dice: 0.174940
[09:36:07.838] TRAIN: iteration 11624 : loss : 0.067348, loss_ce: 0.008612, loss_dice: 0.126085
[09:36:08.049] TRAIN: iteration 11625 : loss : 0.111187, loss_ce: 0.006772, loss_dice: 0.215602
[09:36:08.257] TRAIN: iteration 11626 : loss : 0.125268, loss_ce: 0.003895, loss_dice: 0.246641
[09:36:08.464] TRAIN: iteration 11627 : loss : 0.183302, loss_ce: 0.002882, loss_dice: 0.363723
[09:36:08.671] TRAIN: iteration 11628 : loss : 0.105401, loss_ce: 0.007864, loss_dice: 0.202937
[09:36:09.070] TRAIN: iteration 11629 : loss : 0.140380, loss_ce: 0.005301, loss_dice: 0.275458
[09:36:09.332] TRAIN: iteration 11630 : loss : 0.065394, loss_ce: 0.009574, loss_dice: 0.121214
[09:36:09.546] TRAIN: iteration 11631 : loss : 0.066803, loss_ce: 0.003734, loss_dice: 0.129873
[09:36:09.755] TRAIN: iteration 11632 : loss : 0.188349, loss_ce: 0.003672, loss_dice: 0.373026
[09:36:09.962] TRAIN: iteration 11633 : loss : 0.252483, loss_ce: 0.004619, loss_dice: 0.500347
[09:36:10.170] TRAIN: iteration 11634 : loss : 0.138151, loss_ce: 0.005368, loss_dice: 0.270935
[09:36:10.379] TRAIN: iteration 11635 : loss : 0.091864, loss_ce: 0.004202, loss_dice: 0.179526
[09:36:10.649] TRAIN: iteration 11636 : loss : 0.182915, loss_ce: 0.002658, loss_dice: 0.363172
[09:36:12.821] TRAIN: iteration 11637 : loss : 0.144099, loss_ce: 0.009800, loss_dice: 0.278399
[09:36:13.028] TRAIN: iteration 11638 : loss : 0.163282, loss_ce: 0.003884, loss_dice: 0.322680
[09:36:15.356] TRAIN: iteration 11639 : loss : 0.167961, loss_ce: 0.008918, loss_dice: 0.327005
[09:36:15.572] TRAIN: iteration 11640 : loss : 0.079483, loss_ce: 0.004909, loss_dice: 0.154057
[09:36:15.806] TRAIN: iteration 11641 : loss : 0.247344, loss_ce: 0.002472, loss_dice: 0.492216
[09:36:16.014] TRAIN: iteration 11642 : loss : 0.188817, loss_ce: 0.003215, loss_dice: 0.374418
[09:36:16.221] TRAIN: iteration 11643 : loss : 0.105753, loss_ce: 0.003485, loss_dice: 0.208021
[09:36:16.428] TRAIN: iteration 11644 : loss : 0.134020, loss_ce: 0.018964, loss_dice: 0.249076
[09:36:17.461] TRAIN: iteration 11645 : loss : 0.109285, loss_ce: 0.006666, loss_dice: 0.211904
[09:36:17.668] TRAIN: iteration 11646 : loss : 0.137102, loss_ce: 0.003202, loss_dice: 0.271002
[09:36:18.271] TRAIN: iteration 11647 : loss : 0.072890, loss_ce: 0.006336, loss_dice: 0.139445
[09:36:18.479] TRAIN: iteration 11648 : loss : 0.253038, loss_ce: 0.005637, loss_dice: 0.500439
[09:36:18.687] TRAIN: iteration 11649 : loss : 0.175990, loss_ce: 0.003031, loss_dice: 0.348950
[09:36:18.894] TRAIN: iteration 11650 : loss : 0.250350, loss_ce: 0.003851, loss_dice: 0.496849
[09:36:19.101] TRAIN: iteration 11651 : loss : 0.095289, loss_ce: 0.007857, loss_dice: 0.182722
[09:36:19.309] TRAIN: iteration 11652 : loss : 0.117331, loss_ce: 0.005073, loss_dice: 0.229589
[09:36:19.590] TRAIN: iteration 11653 : loss : 0.092654, loss_ce: 0.009297, loss_dice: 0.176010
[09:36:19.796] TRAIN: iteration 11654 : loss : 0.112515, loss_ce: 0.004119, loss_dice: 0.220912
[09:36:20.862] TRAIN: iteration 11655 : loss : 0.082597, loss_ce: 0.003358, loss_dice: 0.161835
[09:36:21.074] TRAIN: iteration 11656 : loss : 0.250611, loss_ce: 0.007482, loss_dice: 0.493739
[09:36:21.286] TRAIN: iteration 11657 : loss : 0.251383, loss_ce: 0.002619, loss_dice: 0.500147
[09:36:21.500] TRAIN: iteration 11658 : loss : 0.091068, loss_ce: 0.007068, loss_dice: 0.175068
[09:36:21.709] TRAIN: iteration 11659 : loss : 0.170305, loss_ce: 0.003444, loss_dice: 0.337165
[09:36:21.916] TRAIN: iteration 11660 : loss : 0.248535, loss_ce: 0.002693, loss_dice: 0.494377
[09:36:22.862] TRAIN: iteration 11661 : loss : 0.068153, loss_ce: 0.007883, loss_dice: 0.128423
[09:36:23.070] TRAIN: iteration 11662 : loss : 0.094521, loss_ce: 0.002450, loss_dice: 0.186592
[09:36:24.446] TRAIN: iteration 11663 : loss : 0.234001, loss_ce: 0.003811, loss_dice: 0.464190
[09:36:24.655] TRAIN: iteration 11664 : loss : 0.155669, loss_ce: 0.005689, loss_dice: 0.305649
[09:36:24.863] TRAIN: iteration 11665 : loss : 0.158802, loss_ce: 0.004260, loss_dice: 0.313344
[09:36:25.071] TRAIN: iteration 11666 : loss : 0.250300, loss_ce: 0.001447, loss_dice: 0.499154
[09:36:25.280] TRAIN: iteration 11667 : loss : 0.091315, loss_ce: 0.006287, loss_dice: 0.176342
[09:36:25.489] TRAIN: iteration 11668 : loss : 0.109869, loss_ce: 0.006687, loss_dice: 0.213050
[09:36:25.699] TRAIN: iteration 11669 : loss : 0.251224, loss_ce: 0.002312, loss_dice: 0.500137
[09:36:25.918] TRAIN: iteration 11670 : loss : 0.251186, loss_ce: 0.002259, loss_dice: 0.500113
[09:36:27.401] TRAIN: iteration 11671 : loss : 0.162002, loss_ce: 0.005895, loss_dice: 0.318109
[09:36:27.612] TRAIN: iteration 11672 : loss : 0.165298, loss_ce: 0.002117, loss_dice: 0.328479
[09:36:27.818] TRAIN: iteration 11673 : loss : 0.247356, loss_ce: 0.005295, loss_dice: 0.489416
[09:36:28.024] TRAIN: iteration 11674 : loss : 0.170437, loss_ce: 0.007462, loss_dice: 0.333411
[09:36:28.236] TRAIN: iteration 11675 : loss : 0.175645, loss_ce: 0.005724, loss_dice: 0.345566
[09:36:28.443] TRAIN: iteration 11676 : loss : 0.194792, loss_ce: 0.005985, loss_dice: 0.383599
[09:36:28.650] TRAIN: iteration 11677 : loss : 0.072665, loss_ce: 0.007316, loss_dice: 0.138014
[09:36:28.857] TRAIN: iteration 11678 : loss : 0.073249, loss_ce: 0.007086, loss_dice: 0.139413
[09:36:32.864] TRAIN: iteration 11679 : loss : 0.071533, loss_ce: 0.005043, loss_dice: 0.138023
[09:36:33.072] TRAIN: iteration 11680 : loss : 0.102682, loss_ce: 0.011149, loss_dice: 0.194214
[09:36:33.308] TRAIN: iteration 11681 : loss : 0.079169, loss_ce: 0.006778, loss_dice: 0.151559
[09:36:33.515] TRAIN: iteration 11682 : loss : 0.130517, loss_ce: 0.004660, loss_dice: 0.256374
[09:36:33.721] TRAIN: iteration 11683 : loss : 0.119041, loss_ce: 0.003318, loss_dice: 0.234763
[09:36:33.929] TRAIN: iteration 11684 : loss : 0.044359, loss_ce: 0.003691, loss_dice: 0.085028
[09:36:34.136] TRAIN: iteration 11685 : loss : 0.249785, loss_ce: 0.004622, loss_dice: 0.494948
[09:36:34.344] TRAIN: iteration 11686 : loss : 0.104082, loss_ce: 0.003339, loss_dice: 0.204825
[09:36:36.056] TRAIN: iteration 11687 : loss : 0.079658, loss_ce: 0.007101, loss_dice: 0.152215
[09:36:36.266] TRAIN: iteration 11688 : loss : 0.069580, loss_ce: 0.004420, loss_dice: 0.134739
[09:36:36.472] TRAIN: iteration 11689 : loss : 0.252555, loss_ce: 0.004766, loss_dice: 0.500344
[09:36:36.679] TRAIN: iteration 11690 : loss : 0.251752, loss_ce: 0.003545, loss_dice: 0.499959
[09:36:36.885] TRAIN: iteration 11691 : loss : 0.251705, loss_ce: 0.003246, loss_dice: 0.500164
[09:36:37.191] TRAIN: iteration 11692 : loss : 0.240919, loss_ce: 0.058943, loss_dice: 0.422895
[09:36:37.403] TRAIN: iteration 11693 : loss : 0.251722, loss_ce: 0.003240, loss_dice: 0.500205
[09:36:37.615] TRAIN: iteration 11694 : loss : 0.248254, loss_ce: 0.001876, loss_dice: 0.494632
[09:36:38.503] TRAIN: iteration 11695 : loss : 0.246706, loss_ce: 0.003312, loss_dice: 0.490100
[09:36:38.709] TRAIN: iteration 11696 : loss : 0.201210, loss_ce: 0.002319, loss_dice: 0.400100
[09:36:38.916] TRAIN: iteration 11697 : loss : 0.105352, loss_ce: 0.005887, loss_dice: 0.204817
[09:36:39.124] TRAIN: iteration 11698 : loss : 0.118843, loss_ce: 0.003865, loss_dice: 0.233821
[09:36:39.331] TRAIN: iteration 11699 : loss : 0.174056, loss_ce: 0.002330, loss_dice: 0.345783
[09:36:39.537] TRAIN: iteration 11700 : loss : 0.160182, loss_ce: 0.002014, loss_dice: 0.318350
[09:36:39.773] TRAIN: iteration 11701 : loss : 0.121779, loss_ce: 0.007180, loss_dice: 0.236378
[09:36:39.985] TRAIN: iteration 11702 : loss : 0.232542, loss_ce: 0.022057, loss_dice: 0.443026
[09:36:41.259] TRAIN: iteration 11703 : loss : 0.123839, loss_ce: 0.008903, loss_dice: 0.238774
[09:36:41.466] TRAIN: iteration 11704 : loss : 0.155766, loss_ce: 0.004111, loss_dice: 0.307421
[09:36:41.675] TRAIN: iteration 11705 : loss : 0.112975, loss_ce: 0.006732, loss_dice: 0.219217
[09:36:41.882] TRAIN: iteration 11706 : loss : 0.096648, loss_ce: 0.003900, loss_dice: 0.189396
[09:36:42.090] TRAIN: iteration 11707 : loss : 0.074261, loss_ce: 0.005401, loss_dice: 0.143121
[09:36:42.302] TRAIN: iteration 11708 : loss : 0.117145, loss_ce: 0.004802, loss_dice: 0.229488
[09:36:42.509] TRAIN: iteration 11709 : loss : 0.085460, loss_ce: 0.008896, loss_dice: 0.162025
[09:36:42.717] TRAIN: iteration 11710 : loss : 0.201337, loss_ce: 0.003901, loss_dice: 0.398773
[09:36:43.250] TRAIN: iteration 11711 : loss : 0.111897, loss_ce: 0.002800, loss_dice: 0.220994
[09:36:44.547] TRAIN: iteration 11712 : loss : 0.107785, loss_ce: 0.001978, loss_dice: 0.213592
[09:36:44.752] TRAIN: iteration 11713 : loss : 0.251820, loss_ce: 0.003400, loss_dice: 0.500241
[09:36:44.959] TRAIN: iteration 11714 : loss : 0.122119, loss_ce: 0.010471, loss_dice: 0.233767
[09:36:45.166] TRAIN: iteration 11715 : loss : 0.197965, loss_ce: 0.002518, loss_dice: 0.393412
[09:36:45.373] TRAIN: iteration 11716 : loss : 0.246163, loss_ce: 0.004829, loss_dice: 0.487497
[09:36:45.583] TRAIN: iteration 11717 : loss : 0.250490, loss_ce: 0.000965, loss_dice: 0.500015
[09:36:45.792] TRAIN: iteration 11718 : loss : 0.086794, loss_ce: 0.004210, loss_dice: 0.169378
[09:36:47.398] TRAIN: iteration 11719 : loss : 0.058419, loss_ce: 0.003851, loss_dice: 0.112987
[09:36:48.325] TRAIN: iteration 11720 : loss : 0.108110, loss_ce: 0.004197, loss_dice: 0.212023
[09:36:48.562] TRAIN: iteration 11721 : loss : 0.123999, loss_ce: 0.006322, loss_dice: 0.241677
[09:36:48.768] TRAIN: iteration 11722 : loss : 0.125766, loss_ce: 0.003140, loss_dice: 0.248392
[09:36:48.975] TRAIN: iteration 11723 : loss : 0.194005, loss_ce: 0.001446, loss_dice: 0.386563
[09:36:49.182] TRAIN: iteration 11724 : loss : 0.251085, loss_ce: 0.002043, loss_dice: 0.500128
[09:36:49.389] TRAIN: iteration 11725 : loss : 0.251035, loss_ce: 0.001975, loss_dice: 0.500095
[09:36:49.598] TRAIN: iteration 11726 : loss : 0.218759, loss_ce: 0.005679, loss_dice: 0.431839
[09:36:49.804] TRAIN: iteration 11727 : loss : 0.098205, loss_ce: 0.004399, loss_dice: 0.192012
[09:36:54.164] TRAIN: iteration 11728 : loss : 0.123904, loss_ce: 0.004446, loss_dice: 0.243362
[09:36:54.370] TRAIN: iteration 11729 : loss : 0.084317, loss_ce: 0.002186, loss_dice: 0.166448
[09:36:54.579] TRAIN: iteration 11730 : loss : 0.219339, loss_ce: 0.012566, loss_dice: 0.426112
[09:36:54.793] TRAIN: iteration 11731 : loss : 0.143804, loss_ce: 0.003701, loss_dice: 0.283908
[09:36:54.999] TRAIN: iteration 11732 : loss : 0.144615, loss_ce: 0.018715, loss_dice: 0.270514
[09:36:55.208] TRAIN: iteration 11733 : loss : 0.242105, loss_ce: 0.006888, loss_dice: 0.477321
[09:36:55.417] TRAIN: iteration 11734 : loss : 0.061426, loss_ce: 0.003230, loss_dice: 0.119621
[09:36:55.626] TRAIN: iteration 11735 : loss : 0.184927, loss_ce: 0.007232, loss_dice: 0.362622
[09:36:59.091] TRAIN: iteration 11736 : loss : 0.102877, loss_ce: 0.008874, loss_dice: 0.196879
[09:36:59.299] TRAIN: iteration 11737 : loss : 0.252280, loss_ce: 0.004299, loss_dice: 0.500261
[09:36:59.507] TRAIN: iteration 11738 : loss : 0.143713, loss_ce: 0.006887, loss_dice: 0.280539
[09:36:59.715] TRAIN: iteration 11739 : loss : 0.229811, loss_ce: 0.009475, loss_dice: 0.450147
[09:36:59.922] TRAIN: iteration 11740 : loss : 0.150959, loss_ce: 0.004888, loss_dice: 0.297029
[09:37:00.164] TRAIN: iteration 11741 : loss : 0.153115, loss_ce: 0.012739, loss_dice: 0.293491
[09:37:00.374] TRAIN: iteration 11742 : loss : 0.251948, loss_ce: 0.003707, loss_dice: 0.500188
[09:37:00.582] TRAIN: iteration 11743 : loss : 0.252459, loss_ce: 0.004648, loss_dice: 0.500271
[09:37:02.809] TRAIN: iteration 11744 : loss : 0.050523, loss_ce: 0.004988, loss_dice: 0.096058
[09:37:03.020] TRAIN: iteration 11745 : loss : 0.189795, loss_ce: 0.005494, loss_dice: 0.374096
[09:37:03.234] TRAIN: iteration 11746 : loss : 0.226476, loss_ce: 0.007165, loss_dice: 0.445787
[09:37:03.444] TRAIN: iteration 11747 : loss : 0.043958, loss_ce: 0.004819, loss_dice: 0.083097
[09:37:03.650] TRAIN: iteration 11748 : loss : 0.125576, loss_ce: 0.005800, loss_dice: 0.245353
[09:37:03.858] TRAIN: iteration 11749 : loss : 0.154069, loss_ce: 0.023009, loss_dice: 0.285128
[09:37:04.068] TRAIN: iteration 11750 : loss : 0.226808, loss_ce: 0.008329, loss_dice: 0.445288
[09:37:04.280] TRAIN: iteration 11751 : loss : 0.098349, loss_ce: 0.005591, loss_dice: 0.191108
[09:37:05.147] TRAIN: iteration 11752 : loss : 0.129458, loss_ce: 0.019050, loss_dice: 0.239865
[09:37:05.353] TRAIN: iteration 11753 : loss : 0.181050, loss_ce: 0.015135, loss_dice: 0.346965
[09:37:05.561] TRAIN: iteration 11754 : loss : 0.173882, loss_ce: 0.005278, loss_dice: 0.342487
[09:37:05.772] TRAIN: iteration 11755 : loss : 0.189882, loss_ce: 0.004481, loss_dice: 0.375283
[09:37:05.978] TRAIN: iteration 11756 : loss : 0.097483, loss_ce: 0.005588, loss_dice: 0.189379
[09:37:06.185] TRAIN: iteration 11757 : loss : 0.101955, loss_ce: 0.005823, loss_dice: 0.198087
[09:37:06.396] TRAIN: iteration 11758 : loss : 0.096770, loss_ce: 0.006580, loss_dice: 0.186961
[09:37:06.603] TRAIN: iteration 11759 : loss : 0.219043, loss_ce: 0.006233, loss_dice: 0.431852
[09:37:07.397] TRAIN: iteration 11760 : loss : 0.199230, loss_ce: 0.003349, loss_dice: 0.395111
[09:37:07.638] TRAIN: iteration 11761 : loss : 0.250835, loss_ce: 0.003058, loss_dice: 0.498611
[09:37:07.845] TRAIN: iteration 11762 : loss : 0.222460, loss_ce: 0.004448, loss_dice: 0.440472
[09:37:08.053] TRAIN: iteration 11763 : loss : 0.164318, loss_ce: 0.002669, loss_dice: 0.325966
[09:37:09.078] TRAIN: iteration 11764 : loss : 0.108083, loss_ce: 0.010403, loss_dice: 0.205763
[09:37:09.285] TRAIN: iteration 11765 : loss : 0.113983, loss_ce: 0.002715, loss_dice: 0.225250
[09:37:10.025] TRAIN: iteration 11766 : loss : 0.096605, loss_ce: 0.002243, loss_dice: 0.190967
[09:37:10.233] TRAIN: iteration 11767 : loss : 0.078036, loss_ce: 0.002741, loss_dice: 0.153332
[09:37:13.164] TRAIN: iteration 11768 : loss : 0.230876, loss_ce: 0.001952, loss_dice: 0.459799
[09:37:13.379] TRAIN: iteration 11769 : loss : 0.119044, loss_ce: 0.004599, loss_dice: 0.233488
[09:37:13.586] TRAIN: iteration 11770 : loss : 0.158606, loss_ce: 0.007877, loss_dice: 0.309335
[09:37:13.795] TRAIN: iteration 11771 : loss : 0.250548, loss_ce: 0.001061, loss_dice: 0.500035
[09:37:14.005] TRAIN: iteration 11772 : loss : 0.091063, loss_ce: 0.001992, loss_dice: 0.180134
[09:37:14.212] TRAIN: iteration 11773 : loss : 0.157025, loss_ce: 0.002705, loss_dice: 0.311344
[09:37:18.105] TRAIN: iteration 11774 : loss : 0.102870, loss_ce: 0.003896, loss_dice: 0.201845
[09:37:18.314] TRAIN: iteration 11775 : loss : 0.168361, loss_ce: 0.005283, loss_dice: 0.331439
[09:37:18.522] TRAIN: iteration 11776 : loss : 0.251540, loss_ce: 0.002884, loss_dice: 0.500195
[09:37:18.728] TRAIN: iteration 11777 : loss : 0.198492, loss_ce: 0.009018, loss_dice: 0.387967
[09:37:18.934] TRAIN: iteration 11778 : loss : 0.095463, loss_ce: 0.002372, loss_dice: 0.188553
[09:37:19.141] TRAIN: iteration 11779 : loss : 0.131076, loss_ce: 0.004113, loss_dice: 0.258039
[09:37:19.349] TRAIN: iteration 11780 : loss : 0.232578, loss_ce: 0.008566, loss_dice: 0.456591
[09:37:19.588] TRAIN: iteration 11781 : loss : 0.216869, loss_ce: 0.009301, loss_dice: 0.424437
[09:37:22.965] TRAIN: iteration 11782 : loss : 0.048085, loss_ce: 0.002564, loss_dice: 0.093605
[09:37:23.174] TRAIN: iteration 11783 : loss : 0.251933, loss_ce: 0.007254, loss_dice: 0.496611
[09:37:23.381] TRAIN: iteration 11784 : loss : 0.251528, loss_ce: 0.002892, loss_dice: 0.500165
[09:37:23.589] TRAIN: iteration 11785 : loss : 0.092028, loss_ce: 0.002220, loss_dice: 0.181836
[09:37:23.797] TRAIN: iteration 11786 : loss : 0.250362, loss_ce: 0.004476, loss_dice: 0.496249
[09:37:24.005] TRAIN: iteration 11787 : loss : 0.092528, loss_ce: 0.005451, loss_dice: 0.179605
[09:37:24.214] TRAIN: iteration 11788 : loss : 0.064741, loss_ce: 0.008597, loss_dice: 0.120884
[09:37:24.422] TRAIN: iteration 11789 : loss : 0.083633, loss_ce: 0.004397, loss_dice: 0.162869
[09:37:27.583] TRAIN: iteration 11790 : loss : 0.082051, loss_ce: 0.009944, loss_dice: 0.154159
[09:37:27.789] TRAIN: iteration 11791 : loss : 0.065063, loss_ce: 0.008070, loss_dice: 0.122055
[09:37:27.996] TRAIN: iteration 11792 : loss : 0.252021, loss_ce: 0.003789, loss_dice: 0.500254
[09:37:28.203] TRAIN: iteration 11793 : loss : 0.176084, loss_ce: 0.014314, loss_dice: 0.337854
[09:37:28.410] TRAIN: iteration 11794 : loss : 0.223416, loss_ce: 0.004862, loss_dice: 0.441971
[09:37:28.617] TRAIN: iteration 11795 : loss : 0.249564, loss_ce: 0.006904, loss_dice: 0.492224
[09:37:28.824] TRAIN: iteration 11796 : loss : 0.129856, loss_ce: 0.004679, loss_dice: 0.255033
[09:37:29.032] TRAIN: iteration 11797 : loss : 0.178493, loss_ce: 0.012052, loss_dice: 0.344934
[09:37:30.735] TRAIN: iteration 11798 : loss : 0.148073, loss_ce: 0.004804, loss_dice: 0.291342
[09:37:30.941] TRAIN: iteration 11799 : loss : 0.138644, loss_ce: 0.015244, loss_dice: 0.262044
[09:37:31.149] TRAIN: iteration 11800 : loss : 0.129530, loss_ce: 0.004952, loss_dice: 0.254108
[09:37:31.386] TRAIN: iteration 11801 : loss : 0.252077, loss_ce: 0.003958, loss_dice: 0.500195
[09:37:31.592] TRAIN: iteration 11802 : loss : 0.058530, loss_ce: 0.009037, loss_dice: 0.108024
[09:37:31.799] TRAIN: iteration 11803 : loss : 0.092001, loss_ce: 0.007049, loss_dice: 0.176952
[09:37:32.005] TRAIN: iteration 11804 : loss : 0.074216, loss_ce: 0.006114, loss_dice: 0.142317
[09:37:32.212] TRAIN: iteration 11805 : loss : 0.060083, loss_ce: 0.008136, loss_dice: 0.112030
[09:37:34.307] TRAIN: iteration 11806 : loss : 0.058665, loss_ce: 0.003462, loss_dice: 0.113868
[09:37:34.514] TRAIN: iteration 11807 : loss : 0.080404, loss_ce: 0.005568, loss_dice: 0.155239
[09:37:34.725] TRAIN: iteration 11808 : loss : 0.252709, loss_ce: 0.005075, loss_dice: 0.500343
[09:37:34.932] TRAIN: iteration 11809 : loss : 0.071878, loss_ce: 0.006453, loss_dice: 0.137302
[09:37:35.139] TRAIN: iteration 11810 : loss : 0.211659, loss_ce: 0.008055, loss_dice: 0.415264
[09:37:35.347] TRAIN: iteration 11811 : loss : 0.177415, loss_ce: 0.003999, loss_dice: 0.350832
[09:37:35.554] TRAIN: iteration 11812 : loss : 0.122530, loss_ce: 0.014770, loss_dice: 0.230291
[09:37:35.762] TRAIN: iteration 11813 : loss : 0.077126, loss_ce: 0.004006, loss_dice: 0.150246
[09:37:38.476] TRAIN: iteration 11814 : loss : 0.037297, loss_ce: 0.002426, loss_dice: 0.072168
[09:37:38.683] TRAIN: iteration 11815 : loss : 0.235828, loss_ce: 0.007844, loss_dice: 0.463813
[09:37:38.890] TRAIN: iteration 11816 : loss : 0.221935, loss_ce: 0.004882, loss_dice: 0.438989
[09:37:40.454] TRAIN: iteration 11817 : loss : 0.137678, loss_ce: 0.005656, loss_dice: 0.269701
[09:37:40.660] TRAIN: iteration 11818 : loss : 0.053126, loss_ce: 0.003818, loss_dice: 0.102434
[09:37:40.867] TRAIN: iteration 11819 : loss : 0.126518, loss_ce: 0.005419, loss_dice: 0.247616
[09:37:41.080] TRAIN: iteration 11820 : loss : 0.251970, loss_ce: 0.003735, loss_dice: 0.500204
[09:37:41.319] TRAIN: iteration 11821 : loss : 0.163298, loss_ce: 0.004022, loss_dice: 0.322575
[09:37:42.245] TRAIN: iteration 11822 : loss : 0.146642, loss_ce: 0.008973, loss_dice: 0.284311
[09:37:42.452] TRAIN: iteration 11823 : loss : 0.109347, loss_ce: 0.003096, loss_dice: 0.215599
[09:37:42.659] TRAIN: iteration 11824 : loss : 0.250834, loss_ce: 0.001640, loss_dice: 0.500028
[09:37:45.683] TRAIN: iteration 11825 : loss : 0.161902, loss_ce: 0.004761, loss_dice: 0.319044
[09:37:45.891] TRAIN: iteration 11826 : loss : 0.087261, loss_ce: 0.007379, loss_dice: 0.167142
[09:37:46.099] TRAIN: iteration 11827 : loss : 0.231732, loss_ce: 0.003214, loss_dice: 0.460251
[09:37:46.309] TRAIN: iteration 11828 : loss : 0.251600, loss_ce: 0.003387, loss_dice: 0.499813
[09:37:46.523] TRAIN: iteration 11829 : loss : 0.065356, loss_ce: 0.002072, loss_dice: 0.128640
[09:37:47.755] TRAIN: iteration 11830 : loss : 0.090588, loss_ce: 0.005786, loss_dice: 0.175391
[09:37:47.964] TRAIN: iteration 11831 : loss : 0.093799, loss_ce: 0.011989, loss_dice: 0.175608
[09:37:48.173] TRAIN: iteration 11832 : loss : 0.107232, loss_ce: 0.003992, loss_dice: 0.210472
[09:37:50.689] TRAIN: iteration 11833 : loss : 0.251564, loss_ce: 0.002961, loss_dice: 0.500166
[09:37:50.900] TRAIN: iteration 11834 : loss : 0.156098, loss_ce: 0.005105, loss_dice: 0.307091
[09:37:51.108] TRAIN: iteration 11835 : loss : 0.015419, loss_ce: 0.001707, loss_dice: 0.029131
[09:37:51.315] TRAIN: iteration 11836 : loss : 0.207730, loss_ce: 0.005595, loss_dice: 0.409865
[09:37:51.523] TRAIN: iteration 11837 : loss : 0.074242, loss_ce: 0.004966, loss_dice: 0.143518
[09:37:52.057] TRAIN: iteration 11838 : loss : 0.137343, loss_ce: 0.005036, loss_dice: 0.269650
[09:37:52.265] TRAIN: iteration 11839 : loss : 0.058133, loss_ce: 0.009265, loss_dice: 0.107001
[09:37:52.566] TRAIN: iteration 11840 : loss : 0.087744, loss_ce: 0.004358, loss_dice: 0.171129
[09:37:53.292] TRAIN: iteration 11841 : loss : 0.035594, loss_ce: 0.002124, loss_dice: 0.069064
[09:37:53.499] TRAIN: iteration 11842 : loss : 0.252279, loss_ce: 0.004274, loss_dice: 0.500283
[09:37:53.707] TRAIN: iteration 11843 : loss : 0.200798, loss_ce: 0.010530, loss_dice: 0.391066
[09:37:53.917] TRAIN: iteration 11844 : loss : 0.043955, loss_ce: 0.004441, loss_dice: 0.083469
[09:37:54.126] TRAIN: iteration 11845 : loss : 0.169853, loss_ce: 0.009641, loss_dice: 0.330064
[09:37:57.474] TRAIN: iteration 11846 : loss : 0.225323, loss_ce: 0.005437, loss_dice: 0.445210
[09:37:57.682] TRAIN: iteration 11847 : loss : 0.037233, loss_ce: 0.003625, loss_dice: 0.070841
[09:37:57.894] TRAIN: iteration 11848 : loss : 0.130155, loss_ce: 0.005101, loss_dice: 0.255209
[09:37:58.170] TRAIN: iteration 11849 : loss : 0.061904, loss_ce: 0.005821, loss_dice: 0.117987
[09:37:58.377] TRAIN: iteration 11850 : loss : 0.076473, loss_ce: 0.004492, loss_dice: 0.148454
[09:37:58.586] TRAIN: iteration 11851 : loss : 0.082762, loss_ce: 0.009340, loss_dice: 0.156183
[09:37:58.797] TRAIN: iteration 11852 : loss : 0.091864, loss_ce: 0.002454, loss_dice: 0.181273
[09:37:59.009] TRAIN: iteration 11853 : loss : 0.058462, loss_ce: 0.005234, loss_dice: 0.111690
[09:38:01.918] TRAIN: iteration 11854 : loss : 0.113291, loss_ce: 0.007386, loss_dice: 0.219197
[09:38:02.130] TRAIN: iteration 11855 : loss : 0.092827, loss_ce: 0.003671, loss_dice: 0.181983
[09:38:02.338] TRAIN: iteration 11856 : loss : 0.252783, loss_ce: 0.005212, loss_dice: 0.500354
[09:38:02.547] TRAIN: iteration 11857 : loss : 0.222159, loss_ce: 0.008485, loss_dice: 0.435832
[09:38:02.755] TRAIN: iteration 11858 : loss : 0.252661, loss_ce: 0.004961, loss_dice: 0.500361
[09:38:02.962] TRAIN: iteration 11859 : loss : 0.116828, loss_ce: 0.004258, loss_dice: 0.229399
[09:38:03.170] TRAIN: iteration 11860 : loss : 0.146858, loss_ce: 0.009502, loss_dice: 0.284213
[09:38:03.421] TRAIN: iteration 11861 : loss : 0.137721, loss_ce: 0.006750, loss_dice: 0.268692
[09:38:08.431] TRAIN: iteration 11862 : loss : 0.086612, loss_ce: 0.017874, loss_dice: 0.155350
[09:38:08.639] TRAIN: iteration 11863 : loss : 0.136599, loss_ce: 0.004609, loss_dice: 0.268590
[09:38:08.847] TRAIN: iteration 11864 : loss : 0.103696, loss_ce: 0.007263, loss_dice: 0.200128
[09:38:09.058] TRAIN: iteration 11865 : loss : 0.070629, loss_ce: 0.008487, loss_dice: 0.132771
[09:38:09.265] TRAIN: iteration 11866 : loss : 0.121261, loss_ce: 0.005647, loss_dice: 0.236876
[09:38:09.472] TRAIN: iteration 11867 : loss : 0.103594, loss_ce: 0.014153, loss_dice: 0.193036
[09:38:09.968] TRAIN: iteration 11868 : loss : 0.120420, loss_ce: 0.006454, loss_dice: 0.234385
[09:38:10.176] TRAIN: iteration 11869 : loss : 0.234096, loss_ce: 0.008005, loss_dice: 0.460188
[09:38:15.570] TRAIN: iteration 11870 : loss : 0.113894, loss_ce: 0.006185, loss_dice: 0.221604
[09:38:15.778] TRAIN: iteration 11871 : loss : 0.071771, loss_ce: 0.007242, loss_dice: 0.136301
[09:38:15.985] TRAIN: iteration 11872 : loss : 0.225225, loss_ce: 0.008318, loss_dice: 0.442132
[09:38:16.193] TRAIN: iteration 11873 : loss : 0.047594, loss_ce: 0.002933, loss_dice: 0.092254
[09:38:16.405] TRAIN: iteration 11874 : loss : 0.235963, loss_ce: 0.004047, loss_dice: 0.467878
[09:38:16.613] TRAIN: iteration 11875 : loss : 0.056754, loss_ce: 0.004229, loss_dice: 0.109278
[09:38:16.820] TRAIN: iteration 11876 : loss : 0.134921, loss_ce: 0.013767, loss_dice: 0.256074
[09:38:17.028] TRAIN: iteration 11877 : loss : 0.080539, loss_ce: 0.004364, loss_dice: 0.156713
[09:38:19.945] TRAIN: iteration 11878 : loss : 0.141913, loss_ce: 0.005586, loss_dice: 0.278241
[09:38:20.249] TRAIN: iteration 11879 : loss : 0.119838, loss_ce: 0.003122, loss_dice: 0.236554
[09:38:20.456] TRAIN: iteration 11880 : loss : 0.070148, loss_ce: 0.007179, loss_dice: 0.133118
[09:38:20.698] TRAIN: iteration 11881 : loss : 0.206443, loss_ce: 0.006852, loss_dice: 0.406034
[09:38:20.905] TRAIN: iteration 11882 : loss : 0.095933, loss_ce: 0.004517, loss_dice: 0.187349
[09:38:21.113] TRAIN: iteration 11883 : loss : 0.189833, loss_ce: 0.004635, loss_dice: 0.375031
[09:38:21.321] TRAIN: iteration 11884 : loss : 0.117755, loss_ce: 0.002339, loss_dice: 0.233171
[09:38:21.528] TRAIN: iteration 11885 : loss : 0.117751, loss_ce: 0.023750, loss_dice: 0.211753
[09:38:26.413] TRAIN: iteration 11886 : loss : 0.131881, loss_ce: 0.002023, loss_dice: 0.261738
[09:38:26.622] TRAIN: iteration 11887 : loss : 0.210585, loss_ce: 0.003182, loss_dice: 0.417989
[09:38:26.830] TRAIN: iteration 11888 : loss : 0.250516, loss_ce: 0.001004, loss_dice: 0.500027
[09:38:27.038] TRAIN: iteration 11889 : loss : 0.119603, loss_ce: 0.004966, loss_dice: 0.234239
[09:38:27.246] TRAIN: iteration 11890 : loss : 0.232513, loss_ce: 0.001541, loss_dice: 0.463485
[09:38:27.453] TRAIN: iteration 11891 : loss : 0.074780, loss_ce: 0.005371, loss_dice: 0.144189
[09:38:27.660] TRAIN: iteration 11892 : loss : 0.250659, loss_ce: 0.001296, loss_dice: 0.500023
[09:38:27.868] TRAIN: iteration 11893 : loss : 0.112333, loss_ce: 0.003095, loss_dice: 0.221571
[09:38:34.873] TRAIN: iteration 11894 : loss : 0.219768, loss_ce: 0.003009, loss_dice: 0.436526
[09:38:35.082] TRAIN: iteration 11895 : loss : 0.136177, loss_ce: 0.009605, loss_dice: 0.262749
[09:38:35.290] TRAIN: iteration 11896 : loss : 0.250705, loss_ce: 0.001336, loss_dice: 0.500074
[09:38:35.498] TRAIN: iteration 11897 : loss : 0.203379, loss_ce: 0.004055, loss_dice: 0.402703
[09:38:35.705] TRAIN: iteration 11898 : loss : 0.167479, loss_ce: 0.007053, loss_dice: 0.327905
[09:38:35.914] TRAIN: iteration 11899 : loss : 0.097446, loss_ce: 0.004797, loss_dice: 0.190096
[09:38:36.122] TRAIN: iteration 11900 : loss : 0.253577, loss_ce: 0.008777, loss_dice: 0.498376
[09:38:36.364] TRAIN: iteration 11901 : loss : 0.221996, loss_ce: 0.007494, loss_dice: 0.436498
[09:38:40.478] TRAIN: iteration 11902 : loss : 0.083278, loss_ce: 0.004800, loss_dice: 0.161757
[09:38:40.685] TRAIN: iteration 11903 : loss : 0.228384, loss_ce: 0.004537, loss_dice: 0.452231
[09:38:40.892] TRAIN: iteration 11904 : loss : 0.158689, loss_ce: 0.004847, loss_dice: 0.312530
[09:38:41.102] TRAIN: iteration 11905 : loss : 0.144488, loss_ce: 0.005974, loss_dice: 0.283002
[09:38:41.311] TRAIN: iteration 11906 : loss : 0.102292, loss_ce: 0.003329, loss_dice: 0.201256
[09:38:41.521] TRAIN: iteration 11907 : loss : 0.083959, loss_ce: 0.005556, loss_dice: 0.162363
[09:38:41.728] TRAIN: iteration 11908 : loss : 0.159009, loss_ce: 0.007564, loss_dice: 0.310454
[09:38:41.937] TRAIN: iteration 11909 : loss : 0.131419, loss_ce: 0.005752, loss_dice: 0.257086
[09:38:44.702] TRAIN: iteration 11910 : loss : 0.127678, loss_ce: 0.004146, loss_dice: 0.251210
[09:38:44.910] TRAIN: iteration 11911 : loss : 0.238996, loss_ce: 0.004433, loss_dice: 0.473559
[09:38:45.117] TRAIN: iteration 11912 : loss : 0.120565, loss_ce: 0.003133, loss_dice: 0.237996
[09:38:45.324] TRAIN: iteration 11913 : loss : 0.095698, loss_ce: 0.007876, loss_dice: 0.183521
[09:38:45.532] TRAIN: iteration 11914 : loss : 0.170271, loss_ce: 0.004555, loss_dice: 0.335987
[09:38:45.740] TRAIN: iteration 11915 : loss : 0.234431, loss_ce: 0.007137, loss_dice: 0.461725
[09:38:45.947] TRAIN: iteration 11916 : loss : 0.056969, loss_ce: 0.001373, loss_dice: 0.112565
[09:38:46.154] TRAIN: iteration 11917 : loss : 0.252493, loss_ce: 0.004652, loss_dice: 0.500333
[09:38:51.450] TRAIN: iteration 11918 : loss : 0.151318, loss_ce: 0.003618, loss_dice: 0.299018
[09:38:51.662] TRAIN: iteration 11919 : loss : 0.048068, loss_ce: 0.001248, loss_dice: 0.094888
[09:38:51.869] TRAIN: iteration 11920 : loss : 0.245326, loss_ce: 0.004265, loss_dice: 0.486388
[09:38:52.105] TRAIN: iteration 11921 : loss : 0.146226, loss_ce: 0.006653, loss_dice: 0.285799
[09:38:52.312] TRAIN: iteration 11922 : loss : 0.078435, loss_ce: 0.004236, loss_dice: 0.152635
[09:38:52.520] TRAIN: iteration 11923 : loss : 0.076339, loss_ce: 0.003469, loss_dice: 0.149209
[09:38:52.727] TRAIN: iteration 11924 : loss : 0.251111, loss_ce: 0.002098, loss_dice: 0.500125
[09:38:52.936] TRAIN: iteration 11925 : loss : 0.131302, loss_ce: 0.002506, loss_dice: 0.260097
[09:38:56.829] TRAIN: iteration 11926 : loss : 0.199209, loss_ce: 0.022752, loss_dice: 0.375665
[09:38:57.041] TRAIN: iteration 11927 : loss : 0.080274, loss_ce: 0.003022, loss_dice: 0.157527
[09:38:57.248] TRAIN: iteration 11928 : loss : 0.247180, loss_ce: 0.003034, loss_dice: 0.491327
[09:38:57.457] TRAIN: iteration 11929 : loss : 0.073060, loss_ce: 0.011582, loss_dice: 0.134538
[09:38:57.664] TRAIN: iteration 11930 : loss : 0.171019, loss_ce: 0.008439, loss_dice: 0.333599
[09:38:57.872] TRAIN: iteration 11931 : loss : 0.087787, loss_ce: 0.004763, loss_dice: 0.170811
[09:38:58.116] TRAIN: iteration 11932 : loss : 0.089181, loss_ce: 0.007705, loss_dice: 0.170658
[09:38:58.326] TRAIN: iteration 11933 : loss : 0.179416, loss_ce: 0.003837, loss_dice: 0.354996
[09:39:02.267] TRAIN: iteration 11934 : loss : 0.097783, loss_ce: 0.003289, loss_dice: 0.192278
[09:39:02.475] TRAIN: iteration 11935 : loss : 0.054758, loss_ce: 0.002690, loss_dice: 0.106826
[09:39:02.682] TRAIN: iteration 11936 : loss : 0.136173, loss_ce: 0.020916, loss_dice: 0.251429
[09:39:02.889] TRAIN: iteration 11937 : loss : 0.162693, loss_ce: 0.004413, loss_dice: 0.320973
[09:39:03.097] TRAIN: iteration 11938 : loss : 0.227245, loss_ce: 0.004532, loss_dice: 0.449958
[09:39:03.308] TRAIN: iteration 11939 : loss : 0.126332, loss_ce: 0.003074, loss_dice: 0.249589
[09:39:03.515] TRAIN: iteration 11940 : loss : 0.142253, loss_ce: 0.005010, loss_dice: 0.279496
[09:39:03.754] TRAIN: iteration 11941 : loss : 0.057466, loss_ce: 0.001641, loss_dice: 0.113290
[09:39:09.081] TRAIN: iteration 11942 : loss : 0.076161, loss_ce: 0.002028, loss_dice: 0.150295
[09:39:09.420] TRAIN: iteration 11943 : loss : 0.050004, loss_ce: 0.003088, loss_dice: 0.096920
[09:39:09.627] TRAIN: iteration 11944 : loss : 0.139843, loss_ce: 0.003850, loss_dice: 0.275837
[09:39:09.840] TRAIN: iteration 11945 : loss : 0.094196, loss_ce: 0.001564, loss_dice: 0.186827
[09:39:10.048] TRAIN: iteration 11946 : loss : 0.067245, loss_ce: 0.001697, loss_dice: 0.132793
[09:39:10.259] TRAIN: iteration 11947 : loss : 0.240189, loss_ce: 0.005145, loss_dice: 0.475234
[09:39:10.466] TRAIN: iteration 11948 : loss : 0.073446, loss_ce: 0.005516, loss_dice: 0.141376
[09:39:10.673] TRAIN: iteration 11949 : loss : 0.143070, loss_ce: 0.028971, loss_dice: 0.257170
[09:39:14.995] TRAIN: iteration 11950 : loss : 0.146575, loss_ce: 0.007816, loss_dice: 0.285335
[09:39:15.204] TRAIN: iteration 11951 : loss : 0.186235, loss_ce: 0.003961, loss_dice: 0.368510
[09:39:15.411] TRAIN: iteration 11952 : loss : 0.217628, loss_ce: 0.008495, loss_dice: 0.426760
[09:39:15.621] TRAIN: iteration 11953 : loss : 0.250420, loss_ce: 0.000830, loss_dice: 0.500010
[09:39:15.950] TRAIN: iteration 11954 : loss : 0.139768, loss_ce: 0.008900, loss_dice: 0.270636
[09:39:16.162] TRAIN: iteration 11955 : loss : 0.057487, loss_ce: 0.005614, loss_dice: 0.109361
[09:39:16.373] TRAIN: iteration 11956 : loss : 0.251872, loss_ce: 0.003504, loss_dice: 0.500241
[09:39:16.581] TRAIN: iteration 11957 : loss : 0.060461, loss_ce: 0.007888, loss_dice: 0.113035
[09:39:21.878] TRAIN: iteration 11958 : loss : 0.181990, loss_ce: 0.004852, loss_dice: 0.359129
[09:39:22.087] TRAIN: iteration 11959 : loss : 0.134682, loss_ce: 0.003153, loss_dice: 0.266211
[09:39:22.300] TRAIN: iteration 11960 : loss : 0.219345, loss_ce: 0.006013, loss_dice: 0.432676
[09:39:22.541] TRAIN: iteration 11961 : loss : 0.142738, loss_ce: 0.021351, loss_dice: 0.264125
[09:39:22.749] TRAIN: iteration 11962 : loss : 0.100637, loss_ce: 0.005035, loss_dice: 0.196239
[09:39:22.960] TRAIN: iteration 11963 : loss : 0.173146, loss_ce: 0.008966, loss_dice: 0.337327
[09:39:23.466] TRAIN: iteration 11964 : loss : 0.058022, loss_ce: 0.012325, loss_dice: 0.103719
[09:39:23.674] TRAIN: iteration 11965 : loss : 0.181778, loss_ce: 0.009859, loss_dice: 0.353698
[09:39:29.358] TRAIN: iteration 11966 : loss : 0.057658, loss_ce: 0.003857, loss_dice: 0.111460
[09:39:29.567] TRAIN: iteration 11967 : loss : 0.055474, loss_ce: 0.006798, loss_dice: 0.104151
[09:39:29.776] TRAIN: iteration 11968 : loss : 0.105082, loss_ce: 0.008126, loss_dice: 0.202038
[09:39:29.983] TRAIN: iteration 11969 : loss : 0.249090, loss_ce: 0.002604, loss_dice: 0.495575
[09:39:30.190] TRAIN: iteration 11970 : loss : 0.135458, loss_ce: 0.005327, loss_dice: 0.265590
[09:39:30.399] TRAIN: iteration 11971 : loss : 0.097406, loss_ce: 0.002844, loss_dice: 0.191967
[09:39:30.607] TRAIN: iteration 11972 : loss : 0.156060, loss_ce: 0.003955, loss_dice: 0.308164
[09:39:30.814] TRAIN: iteration 11973 : loss : 0.225326, loss_ce: 0.049328, loss_dice: 0.401325
[09:39:37.313] TRAIN: iteration 11974 : loss : 0.114913, loss_ce: 0.005525, loss_dice: 0.224302
[09:39:37.526] TRAIN: iteration 11975 : loss : 0.146478, loss_ce: 0.004785, loss_dice: 0.288171
[09:39:37.734] TRAIN: iteration 11976 : loss : 0.091782, loss_ce: 0.004487, loss_dice: 0.179078
[09:39:37.944] TRAIN: iteration 11977 : loss : 0.251226, loss_ce: 0.002300, loss_dice: 0.500152
[09:39:38.155] TRAIN: iteration 11978 : loss : 0.207941, loss_ce: 0.006740, loss_dice: 0.409141
[09:39:38.365] TRAIN: iteration 11979 : loss : 0.168066, loss_ce: 0.003097, loss_dice: 0.333035
[09:39:38.577] TRAIN: iteration 11980 : loss : 0.215060, loss_ce: 0.002091, loss_dice: 0.428029
[09:39:38.814] TRAIN: iteration 11981 : loss : 0.131712, loss_ce: 0.008963, loss_dice: 0.254461
[09:39:46.487] TRAIN: iteration 11982 : loss : 0.251160, loss_ce: 0.002209, loss_dice: 0.500111
[09:39:46.694] TRAIN: iteration 11983 : loss : 0.091835, loss_ce: 0.005022, loss_dice: 0.178647
[09:39:46.901] TRAIN: iteration 11984 : loss : 0.172689, loss_ce: 0.012917, loss_dice: 0.332460
[09:39:47.109] TRAIN: iteration 11985 : loss : 0.076311, loss_ce: 0.004167, loss_dice: 0.148455
[09:39:47.315] TRAIN: iteration 11986 : loss : 0.049541, loss_ce: 0.003178, loss_dice: 0.095905
[09:39:47.523] TRAIN: iteration 11987 : loss : 0.124615, loss_ce: 0.003002, loss_dice: 0.246227
[09:39:47.730] TRAIN: iteration 11988 : loss : 0.044289, loss_ce: 0.003813, loss_dice: 0.084765
[09:39:47.939] TRAIN: iteration 11989 : loss : 0.184961, loss_ce: 0.008118, loss_dice: 0.361804
[09:39:54.130] TRAIN: iteration 11990 : loss : 0.077400, loss_ce: 0.007872, loss_dice: 0.146929
[09:39:54.338] TRAIN: iteration 11991 : loss : 0.248478, loss_ce: 0.003435, loss_dice: 0.493522
[09:39:54.545] TRAIN: iteration 11992 : loss : 0.110568, loss_ce: 0.004700, loss_dice: 0.216436
[09:39:54.754] TRAIN: iteration 11993 : loss : 0.138749, loss_ce: 0.005762, loss_dice: 0.271737
[09:39:54.962] TRAIN: iteration 11994 : loss : 0.050538, loss_ce: 0.003165, loss_dice: 0.097911
[09:39:55.172] TRAIN: iteration 11995 : loss : 0.219885, loss_ce: 0.007021, loss_dice: 0.432749
[09:39:55.379] TRAIN: iteration 11996 : loss : 0.100681, loss_ce: 0.011433, loss_dice: 0.189929
[09:39:55.589] TRAIN: iteration 11997 : loss : 0.154010, loss_ce: 0.007267, loss_dice: 0.300752
[09:40:02.877] TRAIN: iteration 11998 : loss : 0.106937, loss_ce: 0.008687, loss_dice: 0.205186
[09:40:03.086] TRAIN: iteration 11999 : loss : 0.149534, loss_ce: 0.002977, loss_dice: 0.296091
[09:40:03.293] TRAIN: iteration 12000 : loss : 0.195866, loss_ce: 0.004922, loss_dice: 0.386810
[09:40:03.529] TRAIN: iteration 12001 : loss : 0.199495, loss_ce: 0.010909, loss_dice: 0.388080
[09:40:03.736] TRAIN: iteration 12002 : loss : 0.249100, loss_ce: 0.002783, loss_dice: 0.495417
[09:40:03.943] TRAIN: iteration 12003 : loss : 0.112377, loss_ce: 0.002730, loss_dice: 0.222025
[09:40:04.151] TRAIN: iteration 12004 : loss : 0.231840, loss_ce: 0.007808, loss_dice: 0.455872
[09:40:04.359] TRAIN: iteration 12005 : loss : 0.190610, loss_ce: 0.005982, loss_dice: 0.375237
[09:40:10.700] TRAIN: iteration 12006 : loss : 0.158163, loss_ce: 0.003799, loss_dice: 0.312527
[09:40:10.913] TRAIN: iteration 12007 : loss : 0.152785, loss_ce: 0.003664, loss_dice: 0.301906
[09:40:11.120] TRAIN: iteration 12008 : loss : 0.108123, loss_ce: 0.004133, loss_dice: 0.212112
[09:40:11.327] TRAIN: iteration 12009 : loss : 0.052099, loss_ce: 0.001765, loss_dice: 0.102433
[09:40:11.534] TRAIN: iteration 12010 : loss : 0.084934, loss_ce: 0.005692, loss_dice: 0.164176
[09:40:11.742] TRAIN: iteration 12011 : loss : 0.063897, loss_ce: 0.004313, loss_dice: 0.123481
[09:40:11.949] TRAIN: iteration 12012 : loss : 0.130309, loss_ce: 0.009099, loss_dice: 0.251518
[09:40:12.157] TRAIN: iteration 12013 : loss : 0.098106, loss_ce: 0.007373, loss_dice: 0.188838
[09:40:19.135] TRAIN: iteration 12014 : loss : 0.220097, loss_ce: 0.003457, loss_dice: 0.436737
[09:40:19.348] TRAIN: iteration 12015 : loss : 0.046913, loss_ce: 0.002347, loss_dice: 0.091479
[09:40:19.557] TRAIN: iteration 12016 : loss : 0.236758, loss_ce: 0.020993, loss_dice: 0.452523
[09:40:19.766] TRAIN: iteration 12017 : loss : 0.076469, loss_ce: 0.005532, loss_dice: 0.147405
[09:40:19.973] TRAIN: iteration 12018 : loss : 0.088468, loss_ce: 0.005263, loss_dice: 0.171674
[09:40:20.182] TRAIN: iteration 12019 : loss : 0.136610, loss_ce: 0.009627, loss_dice: 0.263593
[09:40:20.390] TRAIN: iteration 12020 : loss : 0.110738, loss_ce: 0.006166, loss_dice: 0.215310
[09:40:20.627] TRAIN: iteration 12021 : loss : 0.252469, loss_ce: 0.004656, loss_dice: 0.500282
[09:40:28.258] TRAIN: iteration 12022 : loss : 0.072006, loss_ce: 0.004668, loss_dice: 0.139344
[09:40:28.466] TRAIN: iteration 12023 : loss : 0.044237, loss_ce: 0.004967, loss_dice: 0.083507
[09:40:28.673] TRAIN: iteration 12024 : loss : 0.053054, loss_ce: 0.006148, loss_dice: 0.099960
[09:40:28.880] TRAIN: iteration 12025 : loss : 0.252733, loss_ce: 0.005121, loss_dice: 0.500345
[09:40:29.088] TRAIN: iteration 12026 : loss : 0.252317, loss_ce: 0.004394, loss_dice: 0.500239
[09:40:29.297] TRAIN: iteration 12027 : loss : 0.252810, loss_ce: 0.005297, loss_dice: 0.500323
[09:40:29.504] TRAIN: iteration 12028 : loss : 0.122446, loss_ce: 0.009107, loss_dice: 0.235784
[09:40:29.711] TRAIN: iteration 12029 : loss : 0.124263, loss_ce: 0.014010, loss_dice: 0.234517
[09:40:37.266] TRAIN: iteration 12030 : loss : 0.186439, loss_ce: 0.006044, loss_dice: 0.366835
[09:40:37.473] TRAIN: iteration 12031 : loss : 0.181339, loss_ce: 0.007780, loss_dice: 0.354899
[09:40:37.681] TRAIN: iteration 12032 : loss : 0.094594, loss_ce: 0.012535, loss_dice: 0.176652
[09:40:52.615] TRAIN: iteration 12033 : loss : 0.243602, loss_ce: 0.005693, loss_dice: 0.481511
[09:40:52.823] TRAIN: iteration 12034 : loss : 0.218738, loss_ce: 0.010461, loss_dice: 0.427016
[09:40:53.031] TRAIN: iteration 12035 : loss : 0.254944, loss_ce: 0.009215, loss_dice: 0.500673
[09:40:53.239] TRAIN: iteration 12036 : loss : 0.120392, loss_ce: 0.008225, loss_dice: 0.232559
[09:40:53.449] TRAIN: iteration 12037 : loss : 0.089172, loss_ce: 0.006236, loss_dice: 0.172109
[09:40:53.656] TRAIN: iteration 12038 : loss : 0.070382, loss_ce: 0.007808, loss_dice: 0.132955
[09:40:53.864] TRAIN: iteration 12039 : loss : 0.080432, loss_ce: 0.007788, loss_dice: 0.153076
[09:40:54.073] TRAIN: iteration 12040 : loss : 0.054547, loss_ce: 0.006346, loss_dice: 0.102749
[09:41:01.883] TRAIN: iteration 12041 : loss : 0.246231, loss_ce: 0.006695, loss_dice: 0.485766
[09:41:02.090] TRAIN: iteration 12042 : loss : 0.252122, loss_ce: 0.003968, loss_dice: 0.500276
[09:41:02.298] TRAIN: iteration 12043 : loss : 0.252966, loss_ce: 0.005534, loss_dice: 0.500399
[09:41:02.505] TRAIN: iteration 12044 : loss : 0.132411, loss_ce: 0.016569, loss_dice: 0.248253
[09:41:02.714] TRAIN: iteration 12045 : loss : 0.146777, loss_ce: 0.011315, loss_dice: 0.282239
[09:41:02.922] TRAIN: iteration 12046 : loss : 0.096335, loss_ce: 0.012466, loss_dice: 0.180204
[09:41:03.132] TRAIN: iteration 12047 : loss : 0.051359, loss_ce: 0.001879, loss_dice: 0.100839
[09:41:03.341] TRAIN: iteration 12048 : loss : 0.053182, loss_ce: 0.007436, loss_dice: 0.098928
[09:41:13.303] TRAIN: iteration 12049 : loss : 0.052804, loss_ce: 0.005116, loss_dice: 0.100491
[09:41:13.516] TRAIN: iteration 12050 : loss : 0.251704, loss_ce: 0.003263, loss_dice: 0.500146
[09:41:13.725] TRAIN: iteration 12051 : loss : 0.103971, loss_ce: 0.008379, loss_dice: 0.199563
[09:41:13.932] TRAIN: iteration 12052 : loss : 0.249033, loss_ce: 0.006179, loss_dice: 0.491887
[09:41:14.141] TRAIN: iteration 12053 : loss : 0.143669, loss_ce: 0.008457, loss_dice: 0.278880
[09:41:14.348] TRAIN: iteration 12054 : loss : 0.119493, loss_ce: 0.004999, loss_dice: 0.233987
[09:41:14.555] TRAIN: iteration 12055 : loss : 0.229034, loss_ce: 0.003502, loss_dice: 0.454565
[09:41:14.762] TRAIN: iteration 12056 : loss : 0.188273, loss_ce: 0.005163, loss_dice: 0.371382
[09:41:21.981] TRAIN: iteration 12057 : loss : 0.067795, loss_ce: 0.004012, loss_dice: 0.131577
[09:41:22.188] TRAIN: iteration 12058 : loss : 0.049844, loss_ce: 0.006039, loss_dice: 0.093648
[09:41:22.395] TRAIN: iteration 12059 : loss : 0.040037, loss_ce: 0.003658, loss_dice: 0.076416
[09:41:22.602] TRAIN: iteration 12060 : loss : 0.063338, loss_ce: 0.003429, loss_dice: 0.123248
[09:41:22.840] TRAIN: iteration 12061 : loss : 0.063582, loss_ce: 0.004221, loss_dice: 0.122943
[09:41:23.047] TRAIN: iteration 12062 : loss : 0.220335, loss_ce: 0.006100, loss_dice: 0.434570
[09:41:23.254] TRAIN: iteration 12063 : loss : 0.043314, loss_ce: 0.001998, loss_dice: 0.084630
[09:41:23.461] TRAIN: iteration 12064 : loss : 0.211778, loss_ce: 0.002975, loss_dice: 0.420581
[09:41:31.008] TRAIN: iteration 12065 : loss : 0.111812, loss_ce: 0.006638, loss_dice: 0.216987
[09:41:31.220] TRAIN: iteration 12066 : loss : 0.095812, loss_ce: 0.003628, loss_dice: 0.187996
[09:41:31.426] TRAIN: iteration 12067 : loss : 0.195619, loss_ce: 0.019908, loss_dice: 0.371329
[09:41:31.635] TRAIN: iteration 12068 : loss : 0.169708, loss_ce: 0.002679, loss_dice: 0.336737
[09:41:31.842] TRAIN: iteration 12069 : loss : 0.214725, loss_ce: 0.004003, loss_dice: 0.425447
[09:41:32.049] TRAIN: iteration 12070 : loss : 0.189661, loss_ce: 0.002285, loss_dice: 0.377037
[09:41:32.256] TRAIN: iteration 12071 : loss : 0.129055, loss_ce: 0.004131, loss_dice: 0.253980
[09:41:32.463] TRAIN: iteration 12072 : loss : 0.094011, loss_ce: 0.002037, loss_dice: 0.185986
[09:41:38.378] TRAIN: iteration 12073 : loss : 0.093533, loss_ce: 0.002588, loss_dice: 0.184478
[09:41:38.589] TRAIN: iteration 12074 : loss : 0.054451, loss_ce: 0.004191, loss_dice: 0.104710
[09:41:38.797] TRAIN: iteration 12075 : loss : 0.064848, loss_ce: 0.003952, loss_dice: 0.125743
[09:41:39.063] TRAIN: iteration 12076 : loss : 0.120736, loss_ce: 0.013941, loss_dice: 0.227530
[09:41:39.273] TRAIN: iteration 12077 : loss : 0.097514, loss_ce: 0.006255, loss_dice: 0.188773
[09:41:39.484] TRAIN: iteration 12078 : loss : 0.176337, loss_ce: 0.003384, loss_dice: 0.349291
[09:41:39.696] TRAIN: iteration 12079 : loss : 0.250725, loss_ce: 0.003411, loss_dice: 0.498040
[09:41:39.903] TRAIN: iteration 12080 : loss : 0.109507, loss_ce: 0.002202, loss_dice: 0.216811
[09:41:47.901] TRAIN: iteration 12081 : loss : 0.072875, loss_ce: 0.005103, loss_dice: 0.140647
[09:41:48.109] TRAIN: iteration 12082 : loss : 0.197406, loss_ce: 0.004285, loss_dice: 0.390528
[09:41:48.318] TRAIN: iteration 12083 : loss : 0.175285, loss_ce: 0.010209, loss_dice: 0.340361
[09:41:48.525] TRAIN: iteration 12084 : loss : 0.175081, loss_ce: 0.006800, loss_dice: 0.343361
[09:41:48.731] TRAIN: iteration 12085 : loss : 0.107002, loss_ce: 0.010013, loss_dice: 0.203992
[09:41:48.938] TRAIN: iteration 12086 : loss : 0.209965, loss_ce: 0.008378, loss_dice: 0.411553
[09:41:49.146] TRAIN: iteration 12087 : loss : 0.045191, loss_ce: 0.003059, loss_dice: 0.087323
[09:41:49.353] TRAIN: iteration 12088 : loss : 0.119981, loss_ce: 0.005801, loss_dice: 0.234161
[09:41:57.941] TRAIN: iteration 12089 : loss : 0.130868, loss_ce: 0.004260, loss_dice: 0.257477
[09:41:58.147] TRAIN: iteration 12090 : loss : 0.122817, loss_ce: 0.007085, loss_dice: 0.238550
[09:41:58.353] TRAIN: iteration 12091 : loss : 0.222603, loss_ce: 0.005601, loss_dice: 0.439605
[09:41:58.565] TRAIN: iteration 12092 : loss : 0.148940, loss_ce: 0.016271, loss_dice: 0.281608
[09:41:58.773] TRAIN: iteration 12093 : loss : 0.100364, loss_ce: 0.004832, loss_dice: 0.195897
[09:41:58.982] TRAIN: iteration 12094 : loss : 0.252093, loss_ce: 0.003954, loss_dice: 0.500231
[09:41:59.190] TRAIN: iteration 12095 : loss : 0.035544, loss_ce: 0.002048, loss_dice: 0.069040
[09:41:59.397] TRAIN: iteration 12096 : loss : 0.159288, loss_ce: 0.004892, loss_dice: 0.313683
[09:42:07.966] TRAIN: iteration 12097 : loss : 0.225054, loss_ce: 0.013349, loss_dice: 0.436758
[09:42:08.173] TRAIN: iteration 12098 : loss : 0.075982, loss_ce: 0.003545, loss_dice: 0.148419
[09:42:08.380] TRAIN: iteration 12099 : loss : 0.252111, loss_ce: 0.003960, loss_dice: 0.500261
[09:42:08.586] TRAIN: iteration 12100 : loss : 0.156559, loss_ce: 0.004704, loss_dice: 0.308414
[09:42:08.830] TRAIN: iteration 12101 : loss : 0.184265, loss_ce: 0.020777, loss_dice: 0.347754
[09:42:09.037] TRAIN: iteration 12102 : loss : 0.147109, loss_ce: 0.003061, loss_dice: 0.291156
[09:42:09.244] TRAIN: iteration 12103 : loss : 0.098464, loss_ce: 0.005959, loss_dice: 0.190969
[09:42:09.450] TRAIN: iteration 12104 : loss : 0.051001, loss_ce: 0.004693, loss_dice: 0.097308
[09:42:19.605] TRAIN: iteration 12105 : loss : 0.116074, loss_ce: 0.008800, loss_dice: 0.223348
[09:42:19.812] TRAIN: iteration 12106 : loss : 0.157152, loss_ce: 0.003925, loss_dice: 0.310379
[09:42:20.019] TRAIN: iteration 12107 : loss : 0.071811, loss_ce: 0.002251, loss_dice: 0.141372
[09:42:20.225] TRAIN: iteration 12108 : loss : 0.189569, loss_ce: 0.002911, loss_dice: 0.376227
[09:42:20.438] TRAIN: iteration 12109 : loss : 0.159913, loss_ce: 0.003390, loss_dice: 0.316435
[09:42:20.644] TRAIN: iteration 12110 : loss : 0.180504, loss_ce: 0.002541, loss_dice: 0.358466
[09:42:20.851] TRAIN: iteration 12111 : loss : 0.138925, loss_ce: 0.006394, loss_dice: 0.271456
[09:42:21.060] TRAIN: iteration 12112 : loss : 0.121624, loss_ce: 0.005533, loss_dice: 0.237716
[09:42:29.713] TRAIN: iteration 12113 : loss : 0.242955, loss_ce: 0.002726, loss_dice: 0.483184
[09:42:30.052] TRAIN: iteration 12114 : loss : 0.251840, loss_ce: 0.003471, loss_dice: 0.500209
[09:42:30.260] TRAIN: iteration 12115 : loss : 0.127757, loss_ce: 0.004139, loss_dice: 0.251375
[09:42:30.466] TRAIN: iteration 12116 : loss : 0.049439, loss_ce: 0.003054, loss_dice: 0.095824
[09:42:30.674] TRAIN: iteration 12117 : loss : 0.252022, loss_ce: 0.003791, loss_dice: 0.500253
[09:42:30.880] TRAIN: iteration 12118 : loss : 0.088853, loss_ce: 0.001492, loss_dice: 0.176214
[09:42:31.089] TRAIN: iteration 12119 : loss : 0.251435, loss_ce: 0.002703, loss_dice: 0.500167
[09:42:31.296] TRAIN: iteration 12120 : loss : 0.062360, loss_ce: 0.003807, loss_dice: 0.120914
[09:42:39.159] TRAIN: iteration 12121 : loss : 0.217688, loss_ce: 0.005047, loss_dice: 0.430328
[09:42:39.366] TRAIN: iteration 12122 : loss : 0.068879, loss_ce: 0.002638, loss_dice: 0.135120
[09:42:39.575] TRAIN: iteration 12123 : loss : 0.218415, loss_ce: 0.003603, loss_dice: 0.433227
[09:42:39.784] TRAIN: iteration 12124 : loss : 0.197457, loss_ce: 0.005004, loss_dice: 0.389910
[09:42:39.992] TRAIN: iteration 12125 : loss : 0.084791, loss_ce: 0.001708, loss_dice: 0.167874
[09:42:40.200] TRAIN: iteration 12126 : loss : 0.250465, loss_ce: 0.006997, loss_dice: 0.493933
[09:42:40.412] TRAIN: iteration 12127 : loss : 0.146030, loss_ce: 0.006091, loss_dice: 0.285969
[09:42:40.619] TRAIN: iteration 12128 : loss : 0.093468, loss_ce: 0.002591, loss_dice: 0.184345
[09:42:50.298] TRAIN: iteration 12129 : loss : 0.215445, loss_ce: 0.003877, loss_dice: 0.427013
[09:42:50.507] TRAIN: iteration 12130 : loss : 0.062747, loss_ce: 0.003294, loss_dice: 0.122201
[09:42:50.715] TRAIN: iteration 12131 : loss : 0.250129, loss_ce: 0.002890, loss_dice: 0.497369
[09:42:50.922] TRAIN: iteration 12132 : loss : 0.252048, loss_ce: 0.003807, loss_dice: 0.500288
[09:42:51.129] TRAIN: iteration 12133 : loss : 0.142490, loss_ce: 0.007734, loss_dice: 0.277246
[09:42:51.337] TRAIN: iteration 12134 : loss : 0.135976, loss_ce: 0.010831, loss_dice: 0.261121
[09:42:51.544] TRAIN: iteration 12135 : loss : 0.239781, loss_ce: 0.007476, loss_dice: 0.472086
[09:42:51.751] TRAIN: iteration 12136 : loss : 0.083252, loss_ce: 0.002971, loss_dice: 0.163533
[09:42:59.329] TRAIN: iteration 12137 : loss : 0.134386, loss_ce: 0.006141, loss_dice: 0.262631
[09:42:59.536] TRAIN: iteration 12138 : loss : 0.253185, loss_ce: 0.007143, loss_dice: 0.499227
[09:42:59.743] TRAIN: iteration 12139 : loss : 0.211574, loss_ce: 0.007978, loss_dice: 0.415170
[09:42:59.950] TRAIN: iteration 12140 : loss : 0.169464, loss_ce: 0.002608, loss_dice: 0.336319
[09:43:00.182] TRAIN: iteration 12141 : loss : 0.189005, loss_ce: 0.005095, loss_dice: 0.372914
[09:43:00.389] TRAIN: iteration 12142 : loss : 0.250826, loss_ce: 0.001564, loss_dice: 0.500089
[09:43:00.596] TRAIN: iteration 12143 : loss : 0.093950, loss_ce: 0.002410, loss_dice: 0.185490
[09:43:00.803] TRAIN: iteration 12144 : loss : 0.112978, loss_ce: 0.006774, loss_dice: 0.219182
[09:43:02.473] TRAIN: iteration 12145 : loss : 0.146886, loss_ce: 0.005325, loss_dice: 0.288448
[09:48:39.754] VALIDATION: iteration 6 : loss : 0.140898, loss_ce: 0.005529, loss_dice: 0.276266
[09:48:40.478] TRAIN: iteration 12146 : loss : 0.103611, loss_ce: 0.004804, loss_dice: 0.202418
[09:48:40.919] TRAIN: iteration 12147 : loss : 0.250983, loss_ce: 0.001855, loss_dice: 0.500112
[09:48:41.127] TRAIN: iteration 12148 : loss : 0.100190, loss_ce: 0.004307, loss_dice: 0.196072
[09:48:41.831] TRAIN: iteration 12149 : loss : 0.095662, loss_ce: 0.001987, loss_dice: 0.189338
[09:48:42.045] TRAIN: iteration 12150 : loss : 0.090668, loss_ce: 0.006087, loss_dice: 0.175250
[09:48:42.253] TRAIN: iteration 12151 : loss : 0.238594, loss_ce: 0.009768, loss_dice: 0.467420
[09:48:42.461] TRAIN: iteration 12152 : loss : 0.176621, loss_ce: 0.004330, loss_dice: 0.348912
[09:48:42.672] TRAIN: iteration 12153 : loss : 0.250725, loss_ce: 0.001389, loss_dice: 0.500061
[09:48:42.882] TRAIN: iteration 12154 : loss : 0.250797, loss_ce: 0.001520, loss_dice: 0.500073
[09:48:43.098] TRAIN: iteration 12155 : loss : 0.048835, loss_ce: 0.002630, loss_dice: 0.095040
[09:48:43.892] TRAIN: iteration 12156 : loss : 0.241016, loss_ce: 0.000974, loss_dice: 0.481059
[09:48:44.099] TRAIN: iteration 12157 : loss : 0.177111, loss_ce: 0.011369, loss_dice: 0.342853
[09:48:44.313] TRAIN: iteration 12158 : loss : 0.147462, loss_ce: 0.005123, loss_dice: 0.289802
[09:48:44.524] TRAIN: iteration 12159 : loss : 0.078052, loss_ce: 0.001939, loss_dice: 0.154165
[09:48:44.739] TRAIN: iteration 12160 : loss : 0.059888, loss_ce: 0.001666, loss_dice: 0.118109
[09:48:44.999] TRAIN: iteration 12161 : loss : 0.075062, loss_ce: 0.003421, loss_dice: 0.146702
[09:48:45.207] TRAIN: iteration 12162 : loss : 0.100992, loss_ce: 0.008822, loss_dice: 0.193162
[09:48:45.415] TRAIN: iteration 12163 : loss : 0.118890, loss_ce: 0.003150, loss_dice: 0.234631
[09:48:45.624] TRAIN: iteration 12164 : loss : 0.065295, loss_ce: 0.001410, loss_dice: 0.129180
[09:48:45.833] TRAIN: iteration 12165 : loss : 0.198113, loss_ce: 0.002911, loss_dice: 0.393315
[09:48:46.041] TRAIN: iteration 12166 : loss : 0.065076, loss_ce: 0.005060, loss_dice: 0.125092
[09:48:46.248] TRAIN: iteration 12167 : loss : 0.156347, loss_ce: 0.002751, loss_dice: 0.309943
[09:48:46.459] TRAIN: iteration 12168 : loss : 0.117208, loss_ce: 0.007119, loss_dice: 0.227297
[09:48:46.675] TRAIN: iteration 12169 : loss : 0.082126, loss_ce: 0.003447, loss_dice: 0.160806
[09:48:46.889] TRAIN: iteration 12170 : loss : 0.251652, loss_ce: 0.003076, loss_dice: 0.500227
[09:48:47.102] TRAIN: iteration 12171 : loss : 0.053060, loss_ce: 0.002568, loss_dice: 0.103552
[09:48:47.311] TRAIN: iteration 12172 : loss : 0.143764, loss_ce: 0.005180, loss_dice: 0.282348
[09:48:47.519] TRAIN: iteration 12173 : loss : 0.066135, loss_ce: 0.003078, loss_dice: 0.129192
[09:48:47.727] TRAIN: iteration 12174 : loss : 0.105796, loss_ce: 0.004833, loss_dice: 0.206759
[09:48:47.937] TRAIN: iteration 12175 : loss : 0.109999, loss_ce: 0.002874, loss_dice: 0.217124
[09:48:48.145] TRAIN: iteration 12176 : loss : 0.076902, loss_ce: 0.005435, loss_dice: 0.148369
[09:48:48.354] TRAIN: iteration 12177 : loss : 0.197277, loss_ce: 0.004819, loss_dice: 0.389734
[09:48:48.590] TRAIN: iteration 12178 : loss : 0.048554, loss_ce: 0.001242, loss_dice: 0.095866
[09:48:48.802] TRAIN: iteration 12179 : loss : 0.076512, loss_ce: 0.001715, loss_dice: 0.151309
[09:48:49.014] TRAIN: iteration 12180 : loss : 0.107445, loss_ce: 0.002363, loss_dice: 0.212528
[09:48:49.261] TRAIN: iteration 12181 : loss : 0.083861, loss_ce: 0.012500, loss_dice: 0.155222
[09:48:49.471] TRAIN: iteration 12182 : loss : 0.199565, loss_ce: 0.004685, loss_dice: 0.394446
[09:48:49.680] TRAIN: iteration 12183 : loss : 0.160768, loss_ce: 0.002710, loss_dice: 0.318827
[09:48:49.888] TRAIN: iteration 12184 : loss : 0.246802, loss_ce: 0.006997, loss_dice: 0.486607
[09:48:50.099] TRAIN: iteration 12185 : loss : 0.126653, loss_ce: 0.009720, loss_dice: 0.243586
[09:48:50.309] TRAIN: iteration 12186 : loss : 0.068407, loss_ce: 0.002155, loss_dice: 0.134660
[09:48:50.516] TRAIN: iteration 12187 : loss : 0.128547, loss_ce: 0.006234, loss_dice: 0.250860
[09:48:50.732] TRAIN: iteration 12188 : loss : 0.090420, loss_ce: 0.009266, loss_dice: 0.171573
[09:48:50.943] TRAIN: iteration 12189 : loss : 0.133796, loss_ce: 0.004265, loss_dice: 0.263326
[09:48:51.151] TRAIN: iteration 12190 : loss : 0.209595, loss_ce: 0.005041, loss_dice: 0.414149
[09:48:51.367] TRAIN: iteration 12191 : loss : 0.108655, loss_ce: 0.010798, loss_dice: 0.206512
[09:48:51.575] TRAIN: iteration 12192 : loss : 0.122210, loss_ce: 0.024360, loss_dice: 0.220060
[09:48:51.782] TRAIN: iteration 12193 : loss : 0.122746, loss_ce: 0.007148, loss_dice: 0.238343
[09:48:51.990] TRAIN: iteration 12194 : loss : 0.252322, loss_ce: 0.004326, loss_dice: 0.500318
[09:48:52.197] TRAIN: iteration 12195 : loss : 0.252124, loss_ce: 0.003966, loss_dice: 0.500282
[09:48:52.414] TRAIN: iteration 12196 : loss : 0.031941, loss_ce: 0.005219, loss_dice: 0.058664
[09:48:52.896] TRAIN: iteration 12197 : loss : 0.252436, loss_ce: 0.004541, loss_dice: 0.500331
[09:48:53.104] TRAIN: iteration 12198 : loss : 0.128149, loss_ce: 0.004102, loss_dice: 0.252197
[09:48:53.315] TRAIN: iteration 12199 : loss : 0.244607, loss_ce: 0.005083, loss_dice: 0.484131
[09:48:53.531] TRAIN: iteration 12200 : loss : 0.127898, loss_ce: 0.005590, loss_dice: 0.250205
[09:48:53.771] TRAIN: iteration 12201 : loss : 0.252658, loss_ce: 0.004976, loss_dice: 0.500341
[09:48:53.984] TRAIN: iteration 12202 : loss : 0.159764, loss_ce: 0.008781, loss_dice: 0.310747
[09:48:54.193] TRAIN: iteration 12203 : loss : 0.091599, loss_ce: 0.003977, loss_dice: 0.179221
[09:48:54.399] TRAIN: iteration 12204 : loss : 0.160846, loss_ce: 0.005823, loss_dice: 0.315868
[09:48:54.606] TRAIN: iteration 12205 : loss : 0.109489, loss_ce: 0.010840, loss_dice: 0.208138
[09:48:54.814] TRAIN: iteration 12206 : loss : 0.252943, loss_ce: 0.005504, loss_dice: 0.500381
[09:48:55.023] TRAIN: iteration 12207 : loss : 0.249774, loss_ce: 0.003942, loss_dice: 0.495606
[09:48:55.236] TRAIN: iteration 12208 : loss : 0.253846, loss_ce: 0.009403, loss_dice: 0.498290
[09:48:55.449] TRAIN: iteration 12209 : loss : 0.227976, loss_ce: 0.006350, loss_dice: 0.449602
[09:48:55.664] TRAIN: iteration 12210 : loss : 0.202346, loss_ce: 0.007066, loss_dice: 0.397626
[09:48:55.871] TRAIN: iteration 12211 : loss : 0.251885, loss_ce: 0.003552, loss_dice: 0.500219
[09:48:56.083] TRAIN: iteration 12212 : loss : 0.068680, loss_ce: 0.011138, loss_dice: 0.126222
[09:48:56.294] TRAIN: iteration 12213 : loss : 0.124190, loss_ce: 0.010854, loss_dice: 0.237527
[09:48:56.507] TRAIN: iteration 12214 : loss : 0.111884, loss_ce: 0.007756, loss_dice: 0.216013
[09:48:56.714] TRAIN: iteration 12215 : loss : 0.092457, loss_ce: 0.004676, loss_dice: 0.180238
[09:48:56.944] TRAIN: iteration 12216 : loss : 0.078974, loss_ce: 0.003985, loss_dice: 0.153964
[09:48:57.159] TRAIN: iteration 12217 : loss : 0.083154, loss_ce: 0.009151, loss_dice: 0.157158
[09:48:57.367] TRAIN: iteration 12218 : loss : 0.250626, loss_ce: 0.003698, loss_dice: 0.497554
[09:48:57.583] TRAIN: iteration 12219 : loss : 0.055886, loss_ce: 0.005374, loss_dice: 0.106399
[09:48:57.792] TRAIN: iteration 12220 : loss : 0.087814, loss_ce: 0.004470, loss_dice: 0.171159
[09:48:58.030] TRAIN: iteration 12221 : loss : 0.253027, loss_ce: 0.005774, loss_dice: 0.500281
[09:48:58.239] TRAIN: iteration 12222 : loss : 0.193372, loss_ce: 0.005657, loss_dice: 0.381087
[09:48:58.447] TRAIN: iteration 12223 : loss : 0.075692, loss_ce: 0.003831, loss_dice: 0.147552
[09:48:58.659] TRAIN: iteration 12224 : loss : 0.226309, loss_ce: 0.003990, loss_dice: 0.448627
[09:48:58.867] TRAIN: iteration 12225 : loss : 0.176898, loss_ce: 0.006437, loss_dice: 0.347360
[09:48:59.076] TRAIN: iteration 12226 : loss : 0.185708, loss_ce: 0.004658, loss_dice: 0.366759
[09:48:59.287] TRAIN: iteration 12227 : loss : 0.233504, loss_ce: 0.008244, loss_dice: 0.458764
[09:48:59.498] TRAIN: iteration 12228 : loss : 0.110190, loss_ce: 0.008954, loss_dice: 0.211426
[09:48:59.707] TRAIN: iteration 12229 : loss : 0.085853, loss_ce: 0.005783, loss_dice: 0.165923
[09:48:59.919] TRAIN: iteration 12230 : loss : 0.253053, loss_ce: 0.005665, loss_dice: 0.500441
[09:49:00.137] TRAIN: iteration 12231 : loss : 0.150562, loss_ce: 0.009587, loss_dice: 0.291537
[09:49:00.349] TRAIN: iteration 12232 : loss : 0.165740, loss_ce: 0.002706, loss_dice: 0.328773
[09:49:00.556] TRAIN: iteration 12233 : loss : 0.251440, loss_ce: 0.002726, loss_dice: 0.500154
[09:49:00.764] TRAIN: iteration 12234 : loss : 0.225200, loss_ce: 0.002271, loss_dice: 0.448130
[09:49:00.975] TRAIN: iteration 12235 : loss : 0.045893, loss_ce: 0.005305, loss_dice: 0.086482
[09:49:01.182] TRAIN: iteration 12236 : loss : 0.088305, loss_ce: 0.002551, loss_dice: 0.174059
[09:49:01.389] TRAIN: iteration 12237 : loss : 0.122013, loss_ce: 0.006768, loss_dice: 0.237257
[09:49:01.597] TRAIN: iteration 12238 : loss : 0.251135, loss_ce: 0.002146, loss_dice: 0.500124
[09:49:01.805] TRAIN: iteration 12239 : loss : 0.250937, loss_ce: 0.001785, loss_dice: 0.500089
[09:49:02.018] TRAIN: iteration 12240 : loss : 0.203696, loss_ce: 0.005552, loss_dice: 0.401839
[09:49:02.254] TRAIN: iteration 12241 : loss : 0.117915, loss_ce: 0.005831, loss_dice: 0.229999
[09:49:02.462] TRAIN: iteration 12242 : loss : 0.236504, loss_ce: 0.003529, loss_dice: 0.469479
[09:49:02.669] TRAIN: iteration 12243 : loss : 0.093118, loss_ce: 0.007056, loss_dice: 0.179179
[09:49:02.877] TRAIN: iteration 12244 : loss : 0.159402, loss_ce: 0.005159, loss_dice: 0.313644
[09:49:03.094] TRAIN: iteration 12245 : loss : 0.251251, loss_ce: 0.002367, loss_dice: 0.500135
[09:49:03.308] TRAIN: iteration 12246 : loss : 0.072757, loss_ce: 0.003917, loss_dice: 0.141597
[09:49:03.515] TRAIN: iteration 12247 : loss : 0.242714, loss_ce: 0.007101, loss_dice: 0.478328
[09:49:03.722] TRAIN: iteration 12248 : loss : 0.204964, loss_ce: 0.003867, loss_dice: 0.406060
[09:49:03.929] TRAIN: iteration 12249 : loss : 0.069144, loss_ce: 0.005669, loss_dice: 0.132620
[09:49:04.138] TRAIN: iteration 12250 : loss : 0.251916, loss_ce: 0.003597, loss_dice: 0.500235
[09:49:04.347] TRAIN: iteration 12251 : loss : 0.106928, loss_ce: 0.005366, loss_dice: 0.208490
[09:49:04.553] TRAIN: iteration 12252 : loss : 0.042028, loss_ce: 0.002136, loss_dice: 0.081921
[09:49:04.760] TRAIN: iteration 12253 : loss : 0.174371, loss_ce: 0.010722, loss_dice: 0.338019
[09:49:04.967] TRAIN: iteration 12254 : loss : 0.212201, loss_ce: 0.007053, loss_dice: 0.417348
[09:49:05.175] TRAIN: iteration 12255 : loss : 0.077388, loss_ce: 0.011631, loss_dice: 0.143145
[09:49:05.385] TRAIN: iteration 12256 : loss : 0.160722, loss_ce: 0.007686, loss_dice: 0.313759
[09:49:05.593] TRAIN: iteration 12257 : loss : 0.147791, loss_ce: 0.003381, loss_dice: 0.292202
[09:49:05.802] TRAIN: iteration 12258 : loss : 0.171808, loss_ce: 0.004594, loss_dice: 0.339021
[09:49:06.017] TRAIN: iteration 12259 : loss : 0.221746, loss_ce: 0.008195, loss_dice: 0.435298
[09:49:06.230] TRAIN: iteration 12260 : loss : 0.087727, loss_ce: 0.011943, loss_dice: 0.163512
[09:49:06.468] TRAIN: iteration 12261 : loss : 0.151362, loss_ce: 0.005986, loss_dice: 0.296739
[09:49:06.676] TRAIN: iteration 12262 : loss : 0.067033, loss_ce: 0.004589, loss_dice: 0.129476
[09:49:06.884] TRAIN: iteration 12263 : loss : 0.167980, loss_ce: 0.015571, loss_dice: 0.320390
[09:49:07.095] TRAIN: iteration 12264 : loss : 0.162923, loss_ce: 0.002585, loss_dice: 0.323261
[09:49:07.305] TRAIN: iteration 12265 : loss : 0.250826, loss_ce: 0.001582, loss_dice: 0.500069
[09:49:07.513] TRAIN: iteration 12266 : loss : 0.173025, loss_ce: 0.005789, loss_dice: 0.340261
[09:49:07.722] TRAIN: iteration 12267 : loss : 0.249535, loss_ce: 0.001715, loss_dice: 0.497355
[09:49:07.929] TRAIN: iteration 12268 : loss : 0.196228, loss_ce: 0.012337, loss_dice: 0.380118
[09:49:08.140] TRAIN: iteration 12269 : loss : 0.249090, loss_ce: 0.002609, loss_dice: 0.495571
[09:49:08.355] TRAIN: iteration 12270 : loss : 0.124648, loss_ce: 0.009088, loss_dice: 0.240209
[09:49:08.566] TRAIN: iteration 12271 : loss : 0.222071, loss_ce: 0.004619, loss_dice: 0.439523
[09:49:08.776] TRAIN: iteration 12272 : loss : 0.092802, loss_ce: 0.001954, loss_dice: 0.183650
[09:49:08.996] TRAIN: iteration 12273 : loss : 0.150404, loss_ce: 0.003308, loss_dice: 0.297500
[09:49:09.206] TRAIN: iteration 12274 : loss : 0.148655, loss_ce: 0.004565, loss_dice: 0.292744
[09:49:09.414] TRAIN: iteration 12275 : loss : 0.046047, loss_ce: 0.003223, loss_dice: 0.088870
[09:49:09.620] TRAIN: iteration 12276 : loss : 0.250808, loss_ce: 0.001551, loss_dice: 0.500065
[09:49:09.829] TRAIN: iteration 12277 : loss : 0.094478, loss_ce: 0.005613, loss_dice: 0.183343
[09:49:10.036] TRAIN: iteration 12278 : loss : 0.138720, loss_ce: 0.010402, loss_dice: 0.267038
[09:49:10.243] TRAIN: iteration 12279 : loss : 0.221253, loss_ce: 0.002594, loss_dice: 0.439911
[09:49:10.453] TRAIN: iteration 12280 : loss : 0.112064, loss_ce: 0.006842, loss_dice: 0.217286
[09:49:10.696] TRAIN: iteration 12281 : loss : 0.161230, loss_ce: 0.007209, loss_dice: 0.315251
[09:49:10.907] TRAIN: iteration 12282 : loss : 0.133369, loss_ce: 0.006501, loss_dice: 0.260237
[09:49:11.118] TRAIN: iteration 12283 : loss : 0.080940, loss_ce: 0.006272, loss_dice: 0.155608
[09:49:11.327] TRAIN: iteration 12284 : loss : 0.104350, loss_ce: 0.002689, loss_dice: 0.206012
[09:49:11.536] TRAIN: iteration 12285 : loss : 0.095121, loss_ce: 0.006102, loss_dice: 0.184139
[09:49:11.743] TRAIN: iteration 12286 : loss : 0.054893, loss_ce: 0.002233, loss_dice: 0.107554
[09:49:11.954] TRAIN: iteration 12287 : loss : 0.170285, loss_ce: 0.005826, loss_dice: 0.334743
[09:49:12.162] TRAIN: iteration 12288 : loss : 0.181911, loss_ce: 0.003624, loss_dice: 0.360198
[09:49:12.370] TRAIN: iteration 12289 : loss : 0.249081, loss_ce: 0.002631, loss_dice: 0.495531
[09:49:12.577] TRAIN: iteration 12290 : loss : 0.252574, loss_ce: 0.004807, loss_dice: 0.500342
[09:49:12.787] TRAIN: iteration 12291 : loss : 0.068712, loss_ce: 0.004828, loss_dice: 0.132596
[09:49:12.997] TRAIN: iteration 12292 : loss : 0.053789, loss_ce: 0.003790, loss_dice: 0.103788
[09:49:13.207] TRAIN: iteration 12293 : loss : 0.192792, loss_ce: 0.004115, loss_dice: 0.381469
[09:49:13.416] TRAIN: iteration 12294 : loss : 0.251122, loss_ce: 0.005341, loss_dice: 0.496902
[09:49:13.631] TRAIN: iteration 12295 : loss : 0.084104, loss_ce: 0.005612, loss_dice: 0.162596
[09:49:13.845] TRAIN: iteration 12296 : loss : 0.095981, loss_ce: 0.007080, loss_dice: 0.184882
[09:49:14.058] TRAIN: iteration 12297 : loss : 0.080646, loss_ce: 0.003344, loss_dice: 0.157949
[09:49:14.684] TRAIN: iteration 12298 : loss : 0.079112, loss_ce: 0.003961, loss_dice: 0.154262
[09:49:14.891] TRAIN: iteration 12299 : loss : 0.059201, loss_ce: 0.004795, loss_dice: 0.113607
[09:49:15.099] TRAIN: iteration 12300 : loss : 0.067792, loss_ce: 0.005655, loss_dice: 0.129929
[09:49:15.100] NaN or Inf found in input tensor.
[09:49:15.314] TRAIN: iteration 12301 : loss : 0.080319, loss_ce: 0.003397, loss_dice: 0.157241
[09:49:15.522] TRAIN: iteration 12302 : loss : 0.109940, loss_ce: 0.009331, loss_dice: 0.210549
[09:49:15.730] TRAIN: iteration 12303 : loss : 0.055194, loss_ce: 0.002438, loss_dice: 0.107949
[09:49:15.937] TRAIN: iteration 12304 : loss : 0.104496, loss_ce: 0.003056, loss_dice: 0.205935
[09:49:16.146] TRAIN: iteration 12305 : loss : 0.146952, loss_ce: 0.021021, loss_dice: 0.272882
[09:49:16.354] TRAIN: iteration 12306 : loss : 0.136775, loss_ce: 0.006114, loss_dice: 0.267436
[09:49:16.564] TRAIN: iteration 12307 : loss : 0.114127, loss_ce: 0.013682, loss_dice: 0.214572
[09:49:16.779] TRAIN: iteration 12308 : loss : 0.109961, loss_ce: 0.005344, loss_dice: 0.214579
[09:49:16.994] TRAIN: iteration 12309 : loss : 0.251521, loss_ce: 0.003544, loss_dice: 0.499498
[09:49:17.203] TRAIN: iteration 12310 : loss : 0.236076, loss_ce: 0.003358, loss_dice: 0.468795
[09:49:17.412] TRAIN: iteration 12311 : loss : 0.097533, loss_ce: 0.002483, loss_dice: 0.192582
[09:49:17.635] TRAIN: iteration 12312 : loss : 0.138568, loss_ce: 0.006265, loss_dice: 0.270871
[09:49:17.842] TRAIN: iteration 12313 : loss : 0.254260, loss_ce: 0.009904, loss_dice: 0.498616
[09:49:18.052] TRAIN: iteration 12314 : loss : 0.176128, loss_ce: 0.004298, loss_dice: 0.347957
[09:49:18.268] TRAIN: iteration 12315 : loss : 0.250341, loss_ce: 0.000664, loss_dice: 0.500019
[09:49:18.477] TRAIN: iteration 12316 : loss : 0.168357, loss_ce: 0.011484, loss_dice: 0.325230
[09:49:19.379] TRAIN: iteration 12317 : loss : 0.180442, loss_ce: 0.004103, loss_dice: 0.356781
[09:49:19.589] TRAIN: iteration 12318 : loss : 0.250663, loss_ce: 0.001271, loss_dice: 0.500054
[09:49:19.800] TRAIN: iteration 12319 : loss : 0.110602, loss_ce: 0.003198, loss_dice: 0.218005
[09:49:20.009] TRAIN: iteration 12320 : loss : 0.153490, loss_ce: 0.005865, loss_dice: 0.301114
[09:49:20.249] TRAIN: iteration 12321 : loss : 0.244260, loss_ce: 0.005711, loss_dice: 0.482810
[09:49:20.464] TRAIN: iteration 12322 : loss : 0.232132, loss_ce: 0.024832, loss_dice: 0.439433
[09:49:20.673] TRAIN: iteration 12323 : loss : 0.228130, loss_ce: 0.005636, loss_dice: 0.450624
[09:49:20.885] TRAIN: iteration 12324 : loss : 0.086880, loss_ce: 0.003238, loss_dice: 0.170521
[09:49:21.692] TRAIN: iteration 12325 : loss : 0.254907, loss_ce: 0.009466, loss_dice: 0.500348
[09:49:21.901] TRAIN: iteration 12326 : loss : 0.078847, loss_ce: 0.003538, loss_dice: 0.154156
[09:49:22.112] TRAIN: iteration 12327 : loss : 0.140690, loss_ce: 0.002927, loss_dice: 0.278453
[09:49:22.324] TRAIN: iteration 12328 : loss : 0.085362, loss_ce: 0.001187, loss_dice: 0.169538
[09:49:22.538] TRAIN: iteration 12329 : loss : 0.213673, loss_ce: 0.003465, loss_dice: 0.423881
[09:49:22.752] TRAIN: iteration 12330 : loss : 0.098046, loss_ce: 0.005576, loss_dice: 0.190516
[09:49:22.960] TRAIN: iteration 12331 : loss : 0.152136, loss_ce: 0.006364, loss_dice: 0.297908
[09:49:23.168] TRAIN: iteration 12332 : loss : 0.167561, loss_ce: 0.008939, loss_dice: 0.326183
[09:49:23.376] TRAIN: iteration 12333 : loss : 0.154024, loss_ce: 0.002685, loss_dice: 0.305362
[09:49:23.584] TRAIN: iteration 12334 : loss : 0.016963, loss_ce: 0.001681, loss_dice: 0.032245
[09:49:23.793] TRAIN: iteration 12335 : loss : 0.252194, loss_ce: 0.005195, loss_dice: 0.499193
[09:49:24.001] TRAIN: iteration 12336 : loss : 0.251498, loss_ce: 0.002818, loss_dice: 0.500177
[09:49:24.209] TRAIN: iteration 12337 : loss : 0.140743, loss_ce: 0.004194, loss_dice: 0.277292
[09:49:24.418] TRAIN: iteration 12338 : loss : 0.167235, loss_ce: 0.010876, loss_dice: 0.323594
[09:49:24.625] TRAIN: iteration 12339 : loss : 0.119067, loss_ce: 0.006077, loss_dice: 0.232056
[09:49:24.837] TRAIN: iteration 12340 : loss : 0.077947, loss_ce: 0.003160, loss_dice: 0.152733
[09:49:25.074] TRAIN: iteration 12341 : loss : 0.092065, loss_ce: 0.004504, loss_dice: 0.179627
[09:49:25.285] TRAIN: iteration 12342 : loss : 0.075603, loss_ce: 0.003562, loss_dice: 0.147644
[09:49:25.502] TRAIN: iteration 12343 : loss : 0.142098, loss_ce: 0.010210, loss_dice: 0.273986
[09:49:25.712] TRAIN: iteration 12344 : loss : 0.064127, loss_ce: 0.006852, loss_dice: 0.121402
[09:49:25.924] TRAIN: iteration 12345 : loss : 0.174478, loss_ce: 0.009041, loss_dice: 0.339915
[09:49:26.133] TRAIN: iteration 12346 : loss : 0.128402, loss_ce: 0.005168, loss_dice: 0.251636
[09:49:26.545] TRAIN: iteration 12347 : loss : 0.126178, loss_ce: 0.005443, loss_dice: 0.246914
[09:49:26.753] TRAIN: iteration 12348 : loss : 0.072688, loss_ce: 0.003575, loss_dice: 0.141802
[09:49:26.961] TRAIN: iteration 12349 : loss : 0.110870, loss_ce: 0.004299, loss_dice: 0.217441
[09:49:27.169] TRAIN: iteration 12350 : loss : 0.086962, loss_ce: 0.012695, loss_dice: 0.161229
[09:49:27.379] TRAIN: iteration 12351 : loss : 0.251712, loss_ce: 0.003227, loss_dice: 0.500198
[09:49:27.592] TRAIN: iteration 12352 : loss : 0.230347, loss_ce: 0.004927, loss_dice: 0.455768
[09:49:27.800] TRAIN: iteration 12353 : loss : 0.053363, loss_ce: 0.003618, loss_dice: 0.103108
[09:49:28.009] TRAIN: iteration 12354 : loss : 0.059947, loss_ce: 0.006566, loss_dice: 0.113329
[09:49:28.223] TRAIN: iteration 12355 : loss : 0.252429, loss_ce: 0.009783, loss_dice: 0.495074
[09:49:28.440] TRAIN: iteration 12356 : loss : 0.065959, loss_ce: 0.002033, loss_dice: 0.129886
[09:49:28.655] TRAIN: iteration 12357 : loss : 0.040634, loss_ce: 0.004156, loss_dice: 0.077113
[09:49:28.863] TRAIN: iteration 12358 : loss : 0.083123, loss_ce: 0.004563, loss_dice: 0.161682
[09:49:29.072] TRAIN: iteration 12359 : loss : 0.243523, loss_ce: 0.005529, loss_dice: 0.481517
[09:49:29.288] TRAIN: iteration 12360 : loss : 0.067354, loss_ce: 0.003779, loss_dice: 0.130928
[09:49:29.528] TRAIN: iteration 12361 : loss : 0.070128, loss_ce: 0.002391, loss_dice: 0.137865
[09:49:29.735] TRAIN: iteration 12362 : loss : 0.183286, loss_ce: 0.006162, loss_dice: 0.360410
[09:49:29.943] TRAIN: iteration 12363 : loss : 0.197233, loss_ce: 0.010516, loss_dice: 0.383950
[09:49:30.152] TRAIN: iteration 12364 : loss : 0.238035, loss_ce: 0.003284, loss_dice: 0.472785
[09:49:30.367] TRAIN: iteration 12365 : loss : 0.082797, loss_ce: 0.004599, loss_dice: 0.160994
[09:49:30.575] TRAIN: iteration 12366 : loss : 0.101136, loss_ce: 0.008298, loss_dice: 0.193973
[09:49:30.783] TRAIN: iteration 12367 : loss : 0.099696, loss_ce: 0.018481, loss_dice: 0.180911
[09:49:30.991] TRAIN: iteration 12368 : loss : 0.139044, loss_ce: 0.028892, loss_dice: 0.249196
[09:49:31.201] TRAIN: iteration 12369 : loss : 0.114461, loss_ce: 0.003981, loss_dice: 0.224941
[09:49:31.410] TRAIN: iteration 12370 : loss : 0.111399, loss_ce: 0.004299, loss_dice: 0.218498
[09:49:31.620] TRAIN: iteration 12371 : loss : 0.098205, loss_ce: 0.004896, loss_dice: 0.191513
[09:49:31.829] TRAIN: iteration 12372 : loss : 0.064677, loss_ce: 0.007840, loss_dice: 0.121514
[09:49:32.043] TRAIN: iteration 12373 : loss : 0.094770, loss_ce: 0.007559, loss_dice: 0.181982
[09:49:32.260] TRAIN: iteration 12374 : loss : 0.240667, loss_ce: 0.004752, loss_dice: 0.476582
[09:49:32.468] TRAIN: iteration 12375 : loss : 0.060357, loss_ce: 0.005872, loss_dice: 0.114842
[09:49:32.689] TRAIN: iteration 12376 : loss : 0.237604, loss_ce: 0.005381, loss_dice: 0.469826
[09:49:32.899] TRAIN: iteration 12377 : loss : 0.081732, loss_ce: 0.003474, loss_dice: 0.159990
[09:49:33.107] TRAIN: iteration 12378 : loss : 0.036631, loss_ce: 0.002749, loss_dice: 0.070513
[09:49:33.318] TRAIN: iteration 12379 : loss : 0.199249, loss_ce: 0.007252, loss_dice: 0.391246
[09:49:33.526] TRAIN: iteration 12380 : loss : 0.038673, loss_ce: 0.002305, loss_dice: 0.075041
[09:49:33.760] TRAIN: iteration 12381 : loss : 0.081198, loss_ce: 0.003682, loss_dice: 0.158714
[09:49:33.969] TRAIN: iteration 12382 : loss : 0.229400, loss_ce: 0.003092, loss_dice: 0.455708
[09:49:34.177] TRAIN: iteration 12383 : loss : 0.222181, loss_ce: 0.003303, loss_dice: 0.441060
[09:49:34.387] TRAIN: iteration 12384 : loss : 0.167406, loss_ce: 0.002677, loss_dice: 0.332135
[09:49:34.596] TRAIN: iteration 12385 : loss : 0.138023, loss_ce: 0.002616, loss_dice: 0.273431
[09:49:34.805] TRAIN: iteration 12386 : loss : 0.251008, loss_ce: 0.001930, loss_dice: 0.500085
[09:49:35.026] TRAIN: iteration 12387 : loss : 0.023745, loss_ce: 0.001426, loss_dice: 0.046064
[09:49:35.236] TRAIN: iteration 12388 : loss : 0.235763, loss_ce: 0.001165, loss_dice: 0.470361
[09:49:35.452] TRAIN: iteration 12389 : loss : 0.206424, loss_ce: 0.008425, loss_dice: 0.404422
[09:49:35.662] TRAIN: iteration 12390 : loss : 0.071648, loss_ce: 0.002855, loss_dice: 0.140442
[09:49:35.871] TRAIN: iteration 12391 : loss : 0.250526, loss_ce: 0.001037, loss_dice: 0.500014
[09:49:36.079] TRAIN: iteration 12392 : loss : 0.250836, loss_ce: 0.001596, loss_dice: 0.500076
[09:49:36.287] TRAIN: iteration 12393 : loss : 0.250940, loss_ce: 0.038717, loss_dice: 0.463163
[09:49:36.494] TRAIN: iteration 12394 : loss : 0.245004, loss_ce: 0.005329, loss_dice: 0.484679
[09:49:36.703] TRAIN: iteration 12395 : loss : 0.204220, loss_ce: 0.003876, loss_dice: 0.404564
[09:49:36.912] TRAIN: iteration 12396 : loss : 0.208007, loss_ce: 0.002266, loss_dice: 0.413747
[09:49:37.121] TRAIN: iteration 12397 : loss : 0.075226, loss_ce: 0.006046, loss_dice: 0.144406
[09:49:37.344] TRAIN: iteration 12398 : loss : 0.246567, loss_ce: 0.002152, loss_dice: 0.490981
[09:49:37.559] TRAIN: iteration 12399 : loss : 0.105544, loss_ce: 0.005727, loss_dice: 0.205361
[09:49:37.767] TRAIN: iteration 12400 : loss : 0.251659, loss_ce: 0.003191, loss_dice: 0.500128
[09:49:38.002] TRAIN: iteration 12401 : loss : 0.115743, loss_ce: 0.008975, loss_dice: 0.222510
[09:49:38.213] TRAIN: iteration 12402 : loss : 0.205661, loss_ce: 0.003708, loss_dice: 0.407613
[09:49:38.428] TRAIN: iteration 12403 : loss : 0.097226, loss_ce: 0.004698, loss_dice: 0.189753
[09:49:38.635] TRAIN: iteration 12404 : loss : 0.212447, loss_ce: 0.004302, loss_dice: 0.420592
[09:49:38.843] TRAIN: iteration 12405 : loss : 0.097929, loss_ce: 0.003631, loss_dice: 0.192226
[09:49:39.052] TRAIN: iteration 12406 : loss : 0.252832, loss_ce: 0.006137, loss_dice: 0.499526
[09:49:39.264] TRAIN: iteration 12407 : loss : 0.110954, loss_ce: 0.004577, loss_dice: 0.217331
[09:49:39.471] TRAIN: iteration 12408 : loss : 0.112963, loss_ce: 0.008665, loss_dice: 0.217260
[09:49:39.681] TRAIN: iteration 12409 : loss : 0.110878, loss_ce: 0.004802, loss_dice: 0.216954
[09:49:39.889] TRAIN: iteration 12410 : loss : 0.218488, loss_ce: 0.003890, loss_dice: 0.433087
[09:49:40.097] TRAIN: iteration 12411 : loss : 0.079656, loss_ce: 0.003927, loss_dice: 0.155385
[09:49:40.306] TRAIN: iteration 12412 : loss : 0.251652, loss_ce: 0.003166, loss_dice: 0.500138
[09:49:40.516] TRAIN: iteration 12413 : loss : 0.078665, loss_ce: 0.004961, loss_dice: 0.152368
[09:49:40.723] TRAIN: iteration 12414 : loss : 0.213439, loss_ce: 0.006486, loss_dice: 0.420391
[09:49:40.931] TRAIN: iteration 12415 : loss : 0.102702, loss_ce: 0.003516, loss_dice: 0.201887
[09:49:41.140] TRAIN: iteration 12416 : loss : 0.226679, loss_ce: 0.004804, loss_dice: 0.448554
[09:49:41.350] TRAIN: iteration 12417 : loss : 0.233199, loss_ce: 0.002317, loss_dice: 0.464080
[09:49:41.559] TRAIN: iteration 12418 : loss : 0.049506, loss_ce: 0.003808, loss_dice: 0.095203
[09:49:41.769] TRAIN: iteration 12419 : loss : 0.051467, loss_ce: 0.003992, loss_dice: 0.098942
[09:49:41.976] TRAIN: iteration 12420 : loss : 0.165922, loss_ce: 0.004185, loss_dice: 0.327659
[09:49:42.226] TRAIN: iteration 12421 : loss : 0.066573, loss_ce: 0.003673, loss_dice: 0.129474
[09:49:42.433] TRAIN: iteration 12422 : loss : 0.094611, loss_ce: 0.008630, loss_dice: 0.180593
[09:49:42.640] TRAIN: iteration 12423 : loss : 0.251324, loss_ce: 0.002552, loss_dice: 0.500096
[09:49:42.852] TRAIN: iteration 12424 : loss : 0.251584, loss_ce: 0.002996, loss_dice: 0.500172
[09:49:43.062] TRAIN: iteration 12425 : loss : 0.087272, loss_ce: 0.005195, loss_dice: 0.169350
[09:49:43.273] TRAIN: iteration 12426 : loss : 0.092911, loss_ce: 0.004073, loss_dice: 0.181749
[09:49:43.482] TRAIN: iteration 12427 : loss : 0.167121, loss_ce: 0.004268, loss_dice: 0.329973
[09:49:43.690] TRAIN: iteration 12428 : loss : 0.171500, loss_ce: 0.003793, loss_dice: 0.339206
[09:49:43.898] TRAIN: iteration 12429 : loss : 0.183554, loss_ce: 0.002995, loss_dice: 0.364114
[09:49:44.106] TRAIN: iteration 12430 : loss : 0.115170, loss_ce: 0.004050, loss_dice: 0.226291
[09:49:44.316] TRAIN: iteration 12431 : loss : 0.251390, loss_ce: 0.002643, loss_dice: 0.500137
[09:49:44.525] TRAIN: iteration 12432 : loss : 0.168631, loss_ce: 0.001823, loss_dice: 0.335440
[09:49:44.733] TRAIN: iteration 12433 : loss : 0.139474, loss_ce: 0.002099, loss_dice: 0.276848
[09:49:44.945] TRAIN: iteration 12434 : loss : 0.142024, loss_ce: 0.001935, loss_dice: 0.282113
[09:49:45.154] TRAIN: iteration 12435 : loss : 0.116722, loss_ce: 0.005627, loss_dice: 0.227816
[09:49:45.367] TRAIN: iteration 12436 : loss : 0.077205, loss_ce: 0.003413, loss_dice: 0.150997
[09:49:45.594] TRAIN: iteration 12437 : loss : 0.109562, loss_ce: 0.012864, loss_dice: 0.206260
[09:49:45.803] TRAIN: iteration 12438 : loss : 0.045740, loss_ce: 0.003050, loss_dice: 0.088429
[09:49:46.011] TRAIN: iteration 12439 : loss : 0.250679, loss_ce: 0.001312, loss_dice: 0.500047
[09:49:46.219] TRAIN: iteration 12440 : loss : 0.092753, loss_ce: 0.005409, loss_dice: 0.180096
[09:49:46.460] TRAIN: iteration 12441 : loss : 0.240322, loss_ce: 0.001179, loss_dice: 0.479465
[09:49:46.677] TRAIN: iteration 12442 : loss : 0.075803, loss_ce: 0.003308, loss_dice: 0.148299
[09:49:46.887] TRAIN: iteration 12443 : loss : 0.081015, loss_ce: 0.002152, loss_dice: 0.159877
[09:49:47.095] TRAIN: iteration 12444 : loss : 0.058845, loss_ce: 0.001244, loss_dice: 0.116446
[09:49:47.303] TRAIN: iteration 12445 : loss : 0.199940, loss_ce: 0.011705, loss_dice: 0.388176
[09:49:47.510] TRAIN: iteration 12446 : loss : 0.074370, loss_ce: 0.012555, loss_dice: 0.136184
[09:49:47.718] TRAIN: iteration 12447 : loss : 0.081711, loss_ce: 0.003024, loss_dice: 0.160397
[09:49:47.925] TRAIN: iteration 12448 : loss : 0.051711, loss_ce: 0.001339, loss_dice: 0.102083
[09:49:48.132] TRAIN: iteration 12449 : loss : 0.066636, loss_ce: 0.005021, loss_dice: 0.128251
[09:49:48.341] TRAIN: iteration 12450 : loss : 0.124190, loss_ce: 0.006646, loss_dice: 0.241734
[09:49:48.551] TRAIN: iteration 12451 : loss : 0.152974, loss_ce: 0.012118, loss_dice: 0.293830
[09:49:48.762] TRAIN: iteration 12452 : loss : 0.189360, loss_ce: 0.010580, loss_dice: 0.368139
[09:49:48.969] TRAIN: iteration 12453 : loss : 0.252278, loss_ce: 0.004216, loss_dice: 0.500339
[09:49:49.177] TRAIN: iteration 12454 : loss : 0.123772, loss_ce: 0.003841, loss_dice: 0.243702
[09:49:49.391] TRAIN: iteration 12455 : loss : 0.094941, loss_ce: 0.005114, loss_dice: 0.184769
[09:49:49.599] TRAIN: iteration 12456 : loss : 0.062073, loss_ce: 0.003864, loss_dice: 0.120281
[09:49:49.806] TRAIN: iteration 12457 : loss : 0.153026, loss_ce: 0.004076, loss_dice: 0.301977
[09:49:50.015] TRAIN: iteration 12458 : loss : 0.120553, loss_ce: 0.010285, loss_dice: 0.230822
[09:49:50.231] TRAIN: iteration 12459 : loss : 0.222524, loss_ce: 0.008134, loss_dice: 0.436913
[09:49:50.439] TRAIN: iteration 12460 : loss : 0.254919, loss_ce: 0.010352, loss_dice: 0.499485
[09:49:50.688] TRAIN: iteration 12461 : loss : 0.254272, loss_ce: 0.007901, loss_dice: 0.500643
[09:49:50.897] TRAIN: iteration 12462 : loss : 0.151785, loss_ce: 0.003990, loss_dice: 0.299580
[09:49:51.107] TRAIN: iteration 12463 : loss : 0.092500, loss_ce: 0.007345, loss_dice: 0.177655
[09:49:51.319] TRAIN: iteration 12464 : loss : 0.154457, loss_ce: 0.012110, loss_dice: 0.296804
[09:49:51.535] TRAIN: iteration 12465 : loss : 0.183739, loss_ce: 0.007419, loss_dice: 0.360059
[09:49:51.744] TRAIN: iteration 12466 : loss : 0.251944, loss_ce: 0.003625, loss_dice: 0.500263
[09:49:51.954] TRAIN: iteration 12467 : loss : 0.064785, loss_ce: 0.003597, loss_dice: 0.125974
[09:49:52.165] TRAIN: iteration 12468 : loss : 0.246638, loss_ce: 0.004867, loss_dice: 0.488408
[09:49:52.377] TRAIN: iteration 12469 : loss : 0.147941, loss_ce: 0.005205, loss_dice: 0.290676
[09:49:52.584] TRAIN: iteration 12470 : loss : 0.128948, loss_ce: 0.004081, loss_dice: 0.253816
[09:49:52.798] TRAIN: iteration 12471 : loss : 0.056062, loss_ce: 0.003446, loss_dice: 0.108678
[09:49:53.011] TRAIN: iteration 12472 : loss : 0.097399, loss_ce: 0.011160, loss_dice: 0.183638
[09:49:53.219] TRAIN: iteration 12473 : loss : 0.075779, loss_ce: 0.005835, loss_dice: 0.145724
[09:49:53.432] TRAIN: iteration 12474 : loss : 0.039057, loss_ce: 0.001624, loss_dice: 0.076490
[09:49:53.642] TRAIN: iteration 12475 : loss : 0.134871, loss_ce: 0.028753, loss_dice: 0.240988
[09:49:53.850] TRAIN: iteration 12476 : loss : 0.069906, loss_ce: 0.005459, loss_dice: 0.134354
[09:49:54.062] TRAIN: iteration 12477 : loss : 0.128028, loss_ce: 0.008434, loss_dice: 0.247623
[09:49:54.270] TRAIN: iteration 12478 : loss : 0.206160, loss_ce: 0.003754, loss_dice: 0.408567
[09:49:54.479] TRAIN: iteration 12479 : loss : 0.163435, loss_ce: 0.003448, loss_dice: 0.323421
[09:49:54.688] TRAIN: iteration 12480 : loss : 0.070444, loss_ce: 0.003374, loss_dice: 0.137514
[09:49:56.868] TRAIN: iteration 12481 : loss : 0.093071, loss_ce: 0.006278, loss_dice: 0.179865
[09:49:57.078] TRAIN: iteration 12482 : loss : 0.194160, loss_ce: 0.005854, loss_dice: 0.382466
[09:49:57.289] TRAIN: iteration 12483 : loss : 0.251865, loss_ce: 0.006210, loss_dice: 0.497521
[09:49:57.500] TRAIN: iteration 12484 : loss : 0.227708, loss_ce: 0.005325, loss_dice: 0.450091
[09:49:57.710] TRAIN: iteration 12485 : loss : 0.251668, loss_ce: 0.003120, loss_dice: 0.500216
[09:49:57.928] TRAIN: iteration 12486 : loss : 0.104037, loss_ce: 0.004961, loss_dice: 0.203113
[09:49:58.138] TRAIN: iteration 12487 : loss : 0.220852, loss_ce: 0.019025, loss_dice: 0.422679
[09:49:58.352] TRAIN: iteration 12488 : loss : 0.250775, loss_ce: 0.003054, loss_dice: 0.498496
[09:49:58.562] TRAIN: iteration 12489 : loss : 0.072559, loss_ce: 0.004659, loss_dice: 0.140458
[09:49:58.770] TRAIN: iteration 12490 : loss : 0.072104, loss_ce: 0.003390, loss_dice: 0.140819
[09:49:58.979] TRAIN: iteration 12491 : loss : 0.111086, loss_ce: 0.005334, loss_dice: 0.216839
[09:49:59.186] TRAIN: iteration 12492 : loss : 0.177248, loss_ce: 0.007267, loss_dice: 0.347228
[09:49:59.398] TRAIN: iteration 12493 : loss : 0.096171, loss_ce: 0.016051, loss_dice: 0.176290
[09:49:59.610] TRAIN: iteration 12494 : loss : 0.183246, loss_ce: 0.002001, loss_dice: 0.364491
[09:49:59.819] TRAIN: iteration 12495 : loss : 0.115377, loss_ce: 0.011752, loss_dice: 0.219001
[09:50:00.026] TRAIN: iteration 12496 : loss : 0.127486, loss_ce: 0.006681, loss_dice: 0.248291
[09:50:00.292] TRAIN: iteration 12497 : loss : 0.152397, loss_ce: 0.003632, loss_dice: 0.301162
[09:50:00.501] TRAIN: iteration 12498 : loss : 0.251578, loss_ce: 0.003818, loss_dice: 0.499337
[09:50:00.709] TRAIN: iteration 12499 : loss : 0.131082, loss_ce: 0.002394, loss_dice: 0.259771
[09:50:00.918] TRAIN: iteration 12500 : loss : 0.068016, loss_ce: 0.010182, loss_dice: 0.125849
[09:50:01.156] TRAIN: iteration 12501 : loss : 0.187850, loss_ce: 0.006534, loss_dice: 0.369167
[09:50:01.375] TRAIN: iteration 12502 : loss : 0.080654, loss_ce: 0.001874, loss_dice: 0.159435
[09:50:01.584] TRAIN: iteration 12503 : loss : 0.242510, loss_ce: 0.001716, loss_dice: 0.483304
[09:50:01.795] TRAIN: iteration 12504 : loss : 0.194366, loss_ce: 0.002516, loss_dice: 0.386216
[09:50:02.003] TRAIN: iteration 12505 : loss : 0.115261, loss_ce: 0.002740, loss_dice: 0.227781
[09:50:02.211] TRAIN: iteration 12506 : loss : 0.243309, loss_ce: 0.001587, loss_dice: 0.485032
[09:50:02.418] TRAIN: iteration 12507 : loss : 0.085244, loss_ce: 0.002429, loss_dice: 0.168059
[09:50:02.627] TRAIN: iteration 12508 : loss : 0.119467, loss_ce: 0.006141, loss_dice: 0.232793
[09:50:02.844] TRAIN: iteration 12509 : loss : 0.071607, loss_ce: 0.001274, loss_dice: 0.141940
[09:50:03.054] TRAIN: iteration 12510 : loss : 0.244171, loss_ce: 0.012816, loss_dice: 0.475527
[09:50:03.265] TRAIN: iteration 12511 : loss : 0.069161, loss_ce: 0.001287, loss_dice: 0.137036
[09:50:03.475] TRAIN: iteration 12512 : loss : 0.096032, loss_ce: 0.002169, loss_dice: 0.189895
[09:50:03.684] TRAIN: iteration 12513 : loss : 0.196198, loss_ce: 0.000923, loss_dice: 0.391474
[09:50:03.892] TRAIN: iteration 12514 : loss : 0.250049, loss_ce: 0.001941, loss_dice: 0.498157
[09:50:04.104] TRAIN: iteration 12515 : loss : 0.090388, loss_ce: 0.004024, loss_dice: 0.176752
[09:50:04.312] TRAIN: iteration 12516 : loss : 0.071847, loss_ce: 0.009300, loss_dice: 0.134394
[09:50:04.520] TRAIN: iteration 12517 : loss : 0.070228, loss_ce: 0.004746, loss_dice: 0.135710
[09:50:04.727] TRAIN: iteration 12518 : loss : 0.224800, loss_ce: 0.002559, loss_dice: 0.447042
[09:50:04.938] TRAIN: iteration 12519 : loss : 0.232727, loss_ce: 0.003030, loss_dice: 0.462425
[09:50:05.148] TRAIN: iteration 12520 : loss : 0.092965, loss_ce: 0.006572, loss_dice: 0.179358
[09:50:05.392] TRAIN: iteration 12521 : loss : 0.231081, loss_ce: 0.011416, loss_dice: 0.450746
[09:50:05.604] TRAIN: iteration 12522 : loss : 0.217718, loss_ce: 0.021483, loss_dice: 0.413954
[09:50:05.819] TRAIN: iteration 12523 : loss : 0.252164, loss_ce: 0.004411, loss_dice: 0.499916
[09:50:06.028] TRAIN: iteration 12524 : loss : 0.183392, loss_ce: 0.005704, loss_dice: 0.361080
[09:50:06.254] TRAIN: iteration 12525 : loss : 0.175732, loss_ce: 0.008940, loss_dice: 0.342525
[09:50:06.461] TRAIN: iteration 12526 : loss : 0.101076, loss_ce: 0.004968, loss_dice: 0.197183
[09:50:06.671] TRAIN: iteration 12527 : loss : 0.121008, loss_ce: 0.012537, loss_dice: 0.229478
[09:50:06.886] TRAIN: iteration 12528 : loss : 0.254342, loss_ce: 0.008063, loss_dice: 0.500621
[09:50:07.096] TRAIN: iteration 12529 : loss : 0.047961, loss_ce: 0.002270, loss_dice: 0.093651
[09:50:07.304] TRAIN: iteration 12530 : loss : 0.096846, loss_ce: 0.007696, loss_dice: 0.185996
[09:50:07.518] TRAIN: iteration 12531 : loss : 0.052938, loss_ce: 0.003389, loss_dice: 0.102488
[09:50:07.753] TRAIN: iteration 12532 : loss : 0.079159, loss_ce: 0.004018, loss_dice: 0.154300
[09:50:07.965] TRAIN: iteration 12533 : loss : 0.111798, loss_ce: 0.011464, loss_dice: 0.212132
[09:50:08.175] TRAIN: iteration 12534 : loss : 0.053613, loss_ce: 0.003712, loss_dice: 0.103514
[09:50:08.383] TRAIN: iteration 12535 : loss : 0.116531, loss_ce: 0.006227, loss_dice: 0.226834
[09:50:08.605] TRAIN: iteration 12536 : loss : 0.094658, loss_ce: 0.016867, loss_dice: 0.172449
[09:50:08.818] TRAIN: iteration 12537 : loss : 0.052858, loss_ce: 0.004304, loss_dice: 0.101411
[09:50:09.029] TRAIN: iteration 12538 : loss : 0.206984, loss_ce: 0.004172, loss_dice: 0.409795
[09:50:09.244] TRAIN: iteration 12539 : loss : 0.054427, loss_ce: 0.003940, loss_dice: 0.104913
[09:50:09.451] TRAIN: iteration 12540 : loss : 0.221780, loss_ce: 0.014110, loss_dice: 0.429450
[09:50:09.684] TRAIN: iteration 12541 : loss : 0.070257, loss_ce: 0.003025, loss_dice: 0.137488
[09:50:09.891] TRAIN: iteration 12542 : loss : 0.162382, loss_ce: 0.002405, loss_dice: 0.322360
[09:50:10.099] TRAIN: iteration 12543 : loss : 0.073260, loss_ce: 0.004215, loss_dice: 0.142304
[09:50:10.315] TRAIN: iteration 12544 : loss : 0.251250, loss_ce: 0.002358, loss_dice: 0.500141
[09:50:10.524] TRAIN: iteration 12545 : loss : 0.099704, loss_ce: 0.007196, loss_dice: 0.192212
[09:50:10.742] TRAIN: iteration 12546 : loss : 0.202292, loss_ce: 0.003889, loss_dice: 0.400696
[09:50:10.959] TRAIN: iteration 12547 : loss : 0.054611, loss_ce: 0.005167, loss_dice: 0.104055
[09:50:11.166] TRAIN: iteration 12548 : loss : 0.188398, loss_ce: 0.012736, loss_dice: 0.364061
[09:50:11.375] TRAIN: iteration 12549 : loss : 0.253201, loss_ce: 0.006174, loss_dice: 0.500229
[09:50:11.714] TRAIN: iteration 12550 : loss : 0.206820, loss_ce: 0.006672, loss_dice: 0.406969
[09:50:11.925] TRAIN: iteration 12551 : loss : 0.251130, loss_ce: 0.002124, loss_dice: 0.500137
[09:50:12.133] TRAIN: iteration 12552 : loss : 0.095441, loss_ce: 0.003071, loss_dice: 0.187811
[09:50:12.342] TRAIN: iteration 12553 : loss : 0.172648, loss_ce: 0.002989, loss_dice: 0.342306
[09:50:12.554] TRAIN: iteration 12554 : loss : 0.152953, loss_ce: 0.002256, loss_dice: 0.303649
[09:50:12.762] TRAIN: iteration 12555 : loss : 0.133767, loss_ce: 0.007108, loss_dice: 0.260427
[09:50:12.971] TRAIN: iteration 12556 : loss : 0.250342, loss_ce: 0.004620, loss_dice: 0.496065
[09:50:13.180] TRAIN: iteration 12557 : loss : 0.076157, loss_ce: 0.002649, loss_dice: 0.149665
[09:50:13.390] TRAIN: iteration 12558 : loss : 0.079235, loss_ce: 0.002554, loss_dice: 0.155916
[09:50:13.600] TRAIN: iteration 12559 : loss : 0.248115, loss_ce: 0.002238, loss_dice: 0.493992
[09:50:13.813] TRAIN: iteration 12560 : loss : 0.186718, loss_ce: 0.003402, loss_dice: 0.370034
[09:50:14.054] TRAIN: iteration 12561 : loss : 0.212830, loss_ce: 0.001568, loss_dice: 0.424091
[09:50:14.263] TRAIN: iteration 12562 : loss : 0.106857, loss_ce: 0.002164, loss_dice: 0.211549
[09:50:14.472] TRAIN: iteration 12563 : loss : 0.168285, loss_ce: 0.006491, loss_dice: 0.330079
[09:50:14.688] TRAIN: iteration 12564 : loss : 0.234401, loss_ce: 0.006244, loss_dice: 0.462559
[09:50:14.899] TRAIN: iteration 12565 : loss : 0.181517, loss_ce: 0.023385, loss_dice: 0.339648
[09:50:15.108] TRAIN: iteration 12566 : loss : 0.074748, loss_ce: 0.002350, loss_dice: 0.147146
[09:50:15.319] TRAIN: iteration 12567 : loss : 0.078784, loss_ce: 0.003484, loss_dice: 0.154083
[09:50:15.536] TRAIN: iteration 12568 : loss : 0.240547, loss_ce: 0.012821, loss_dice: 0.468273
[09:50:15.747] TRAIN: iteration 12569 : loss : 0.248994, loss_ce: 0.003930, loss_dice: 0.494057
[09:50:15.959] TRAIN: iteration 12570 : loss : 0.119411, loss_ce: 0.002045, loss_dice: 0.236777
[09:50:16.169] TRAIN: iteration 12571 : loss : 0.159937, loss_ce: 0.006100, loss_dice: 0.313775
[09:50:16.379] TRAIN: iteration 12572 : loss : 0.218746, loss_ce: 0.015229, loss_dice: 0.422262
[09:50:16.642] TRAIN: iteration 12573 : loss : 0.059058, loss_ce: 0.004127, loss_dice: 0.113988
[09:50:16.852] TRAIN: iteration 12574 : loss : 0.173930, loss_ce: 0.001942, loss_dice: 0.345919
[09:50:17.060] TRAIN: iteration 12575 : loss : 0.158089, loss_ce: 0.006082, loss_dice: 0.310096
[09:50:17.270] TRAIN: iteration 12576 : loss : 0.180843, loss_ce: 0.005338, loss_dice: 0.356349
[09:50:17.485] TRAIN: iteration 12577 : loss : 0.251198, loss_ce: 0.002319, loss_dice: 0.500078
[09:50:17.694] TRAIN: iteration 12578 : loss : 0.107787, loss_ce: 0.004089, loss_dice: 0.211484
[09:50:17.902] TRAIN: iteration 12579 : loss : 0.251085, loss_ce: 0.002105, loss_dice: 0.500065
[09:50:18.111] TRAIN: iteration 12580 : loss : 0.125482, loss_ce: 0.005914, loss_dice: 0.245050
[09:50:18.364] TRAIN: iteration 12581 : loss : 0.104522, loss_ce: 0.003433, loss_dice: 0.205610
[09:50:18.576] TRAIN: iteration 12582 : loss : 0.076361, loss_ce: 0.007160, loss_dice: 0.145563
[09:50:18.791] TRAIN: iteration 12583 : loss : 0.175859, loss_ce: 0.020422, loss_dice: 0.331296
[09:50:18.999] TRAIN: iteration 12584 : loss : 0.047973, loss_ce: 0.004133, loss_dice: 0.091812
[09:50:19.209] TRAIN: iteration 12585 : loss : 0.249293, loss_ce: 0.006046, loss_dice: 0.492539
[09:50:20.939] TRAIN: iteration 12586 : loss : 0.153261, loss_ce: 0.009332, loss_dice: 0.297191
[09:50:21.147] TRAIN: iteration 12587 : loss : 0.249583, loss_ce: 0.004510, loss_dice: 0.494656
[09:50:21.358] TRAIN: iteration 12588 : loss : 0.160016, loss_ce: 0.005000, loss_dice: 0.315033
[09:50:21.572] TRAIN: iteration 12589 : loss : 0.128082, loss_ce: 0.009321, loss_dice: 0.246843
[09:50:21.780] TRAIN: iteration 12590 : loss : 0.093131, loss_ce: 0.007566, loss_dice: 0.178696
[09:50:21.989] TRAIN: iteration 12591 : loss : 0.195020, loss_ce: 0.008163, loss_dice: 0.381877
[09:50:22.197] TRAIN: iteration 12592 : loss : 0.171038, loss_ce: 0.006132, loss_dice: 0.335944
[09:50:22.406] TRAIN: iteration 12593 : loss : 0.088177, loss_ce: 0.009845, loss_dice: 0.166508
[09:50:22.614] TRAIN: iteration 12594 : loss : 0.207753, loss_ce: 0.015619, loss_dice: 0.399887
[09:50:22.824] TRAIN: iteration 12595 : loss : 0.101611, loss_ce: 0.004218, loss_dice: 0.199004
[09:50:23.031] TRAIN: iteration 12596 : loss : 0.252291, loss_ce: 0.004342, loss_dice: 0.500239
[09:50:23.239] TRAIN: iteration 12597 : loss : 0.253707, loss_ce: 0.006926, loss_dice: 0.500488
[09:50:23.448] TRAIN: iteration 12598 : loss : 0.118263, loss_ce: 0.005142, loss_dice: 0.231385
[09:50:23.657] TRAIN: iteration 12599 : loss : 0.066797, loss_ce: 0.005367, loss_dice: 0.128228
[09:50:23.865] TRAIN: iteration 12600 : loss : 0.118070, loss_ce: 0.005764, loss_dice: 0.230377
[09:50:24.105] TRAIN: iteration 12601 : loss : 0.163737, loss_ce: 0.006295, loss_dice: 0.321178
[09:50:24.314] TRAIN: iteration 12602 : loss : 0.131468, loss_ce: 0.002987, loss_dice: 0.259949
[09:50:24.521] TRAIN: iteration 12603 : loss : 0.055036, loss_ce: 0.004190, loss_dice: 0.105882
[09:50:24.730] TRAIN: iteration 12604 : loss : 0.251856, loss_ce: 0.003501, loss_dice: 0.500212
[09:50:24.943] TRAIN: iteration 12605 : loss : 0.211976, loss_ce: 0.005135, loss_dice: 0.418817
[09:50:25.151] TRAIN: iteration 12606 : loss : 0.249672, loss_ce: 0.004513, loss_dice: 0.494830
[09:50:25.359] TRAIN: iteration 12607 : loss : 0.129835, loss_ce: 0.003383, loss_dice: 0.256287
[09:50:25.573] TRAIN: iteration 12608 : loss : 0.227525, loss_ce: 0.004482, loss_dice: 0.450567
[09:50:25.787] TRAIN: iteration 12609 : loss : 0.051283, loss_ce: 0.004404, loss_dice: 0.098162
[09:50:25.995] TRAIN: iteration 12610 : loss : 0.248637, loss_ce: 0.003002, loss_dice: 0.494272
[09:50:26.205] TRAIN: iteration 12611 : loss : 0.249262, loss_ce: 0.006401, loss_dice: 0.492124
[09:50:28.092] TRAIN: iteration 12612 : loss : 0.034049, loss_ce: 0.001933, loss_dice: 0.066165
[09:50:28.304] TRAIN: iteration 12613 : loss : 0.186799, loss_ce: 0.001287, loss_dice: 0.372312
[09:50:28.514] TRAIN: iteration 12614 : loss : 0.110064, loss_ce: 0.009535, loss_dice: 0.210594
[09:50:28.721] TRAIN: iteration 12615 : loss : 0.160891, loss_ce: 0.002487, loss_dice: 0.319294
[09:50:28.929] TRAIN: iteration 12616 : loss : 0.076230, loss_ce: 0.002518, loss_dice: 0.149942
[09:50:29.145] TRAIN: iteration 12617 : loss : 0.102510, loss_ce: 0.002786, loss_dice: 0.202234
[09:50:29.354] TRAIN: iteration 12618 : loss : 0.251595, loss_ce: 0.002995, loss_dice: 0.500194
[09:50:29.564] TRAIN: iteration 12619 : loss : 0.193770, loss_ce: 0.008795, loss_dice: 0.378745
[09:50:30.020] TRAIN: iteration 12620 : loss : 0.111568, loss_ce: 0.001966, loss_dice: 0.221171
[09:50:30.252] TRAIN: iteration 12621 : loss : 0.158001, loss_ce: 0.002822, loss_dice: 0.313180
[09:50:30.459] TRAIN: iteration 12622 : loss : 0.213901, loss_ce: 0.002771, loss_dice: 0.425031
[09:50:30.667] TRAIN: iteration 12623 : loss : 0.081484, loss_ce: 0.002143, loss_dice: 0.160824
[09:50:30.876] TRAIN: iteration 12624 : loss : 0.251341, loss_ce: 0.002511, loss_dice: 0.500171
[09:50:31.089] TRAIN: iteration 12625 : loss : 0.091452, loss_ce: 0.005879, loss_dice: 0.177025
[09:50:31.304] TRAIN: iteration 12626 : loss : 0.250991, loss_ce: 0.001873, loss_dice: 0.500109
[09:50:31.514] TRAIN: iteration 12627 : loss : 0.059426, loss_ce: 0.002533, loss_dice: 0.116319
[09:50:31.728] TRAIN: iteration 12628 : loss : 0.250826, loss_ce: 0.003668, loss_dice: 0.497984
[09:50:31.936] TRAIN: iteration 12629 : loss : 0.139801, loss_ce: 0.005176, loss_dice: 0.274426
[09:50:32.152] TRAIN: iteration 12630 : loss : 0.112776, loss_ce: 0.005564, loss_dice: 0.219988
[09:50:32.363] TRAIN: iteration 12631 : loss : 0.171936, loss_ce: 0.007944, loss_dice: 0.335927
[09:50:32.571] TRAIN: iteration 12632 : loss : 0.251651, loss_ce: 0.003084, loss_dice: 0.500217
[09:50:32.795] TRAIN: iteration 12633 : loss : 0.036395, loss_ce: 0.003622, loss_dice: 0.069168
[09:50:33.008] TRAIN: iteration 12634 : loss : 0.073137, loss_ce: 0.003427, loss_dice: 0.142848
[09:50:33.216] TRAIN: iteration 12635 : loss : 0.238085, loss_ce: 0.008429, loss_dice: 0.467741
[09:50:33.424] TRAIN: iteration 12636 : loss : 0.056171, loss_ce: 0.004217, loss_dice: 0.108125
[09:50:33.641] TRAIN: iteration 12637 : loss : 0.103182, loss_ce: 0.003854, loss_dice: 0.202511
[09:50:33.854] TRAIN: iteration 12638 : loss : 0.251979, loss_ce: 0.003682, loss_dice: 0.500276
[09:50:34.061] TRAIN: iteration 12639 : loss : 0.063950, loss_ce: 0.002226, loss_dice: 0.125673
[09:50:34.270] TRAIN: iteration 12640 : loss : 0.053960, loss_ce: 0.005465, loss_dice: 0.102456
[09:50:34.508] TRAIN: iteration 12641 : loss : 0.139286, loss_ce: 0.005364, loss_dice: 0.273209
[09:50:34.724] TRAIN: iteration 12642 : loss : 0.251059, loss_ce: 0.002004, loss_dice: 0.500115
[09:50:34.940] TRAIN: iteration 12643 : loss : 0.251037, loss_ce: 0.002840, loss_dice: 0.499233
[09:50:35.147] TRAIN: iteration 12644 : loss : 0.065149, loss_ce: 0.003350, loss_dice: 0.126949
[09:50:35.356] TRAIN: iteration 12645 : loss : 0.040743, loss_ce: 0.003123, loss_dice: 0.078363
[09:50:35.563] TRAIN: iteration 12646 : loss : 0.193333, loss_ce: 0.002882, loss_dice: 0.383783
[09:50:35.777] TRAIN: iteration 12647 : loss : 0.248243, loss_ce: 0.009566, loss_dice: 0.486920
[09:50:35.985] TRAIN: iteration 12648 : loss : 0.251529, loss_ce: 0.002879, loss_dice: 0.500180
[09:50:36.193] TRAIN: iteration 12649 : loss : 0.053604, loss_ce: 0.004180, loss_dice: 0.103028
[09:50:36.402] TRAIN: iteration 12650 : loss : 0.048107, loss_ce: 0.004601, loss_dice: 0.091612
[09:50:36.609] TRAIN: iteration 12651 : loss : 0.095617, loss_ce: 0.003811, loss_dice: 0.187422
[09:50:37.408] TRAIN: iteration 12652 : loss : 0.180608, loss_ce: 0.005045, loss_dice: 0.356170
[09:50:37.623] TRAIN: iteration 12653 : loss : 0.057833, loss_ce: 0.002407, loss_dice: 0.113258
[09:50:37.839] TRAIN: iteration 12654 : loss : 0.092398, loss_ce: 0.002956, loss_dice: 0.181839
[09:50:38.049] TRAIN: iteration 12655 : loss : 0.116734, loss_ce: 0.004757, loss_dice: 0.228710
[09:50:38.259] TRAIN: iteration 12656 : loss : 0.069233, loss_ce: 0.013170, loss_dice: 0.125297
[09:50:38.469] TRAIN: iteration 12657 : loss : 0.251122, loss_ce: 0.002114, loss_dice: 0.500130
[09:50:38.676] TRAIN: iteration 12658 : loss : 0.134617, loss_ce: 0.001654, loss_dice: 0.267580
[09:50:38.883] TRAIN: iteration 12659 : loss : 0.112864, loss_ce: 0.004781, loss_dice: 0.220948
[09:50:39.192] TRAIN: iteration 12660 : loss : 0.148834, loss_ce: 0.011745, loss_dice: 0.285922
[09:50:39.430] TRAIN: iteration 12661 : loss : 0.059973, loss_ce: 0.004530, loss_dice: 0.115417
[09:50:39.642] TRAIN: iteration 12662 : loss : 0.102413, loss_ce: 0.002238, loss_dice: 0.202589
[09:50:39.850] TRAIN: iteration 12663 : loss : 0.145906, loss_ce: 0.006240, loss_dice: 0.285571
[09:50:40.060] TRAIN: iteration 12664 : loss : 0.163634, loss_ce: 0.006958, loss_dice: 0.320310
[09:50:40.269] TRAIN: iteration 12665 : loss : 0.105159, loss_ce: 0.018948, loss_dice: 0.191370
[09:50:40.478] TRAIN: iteration 12666 : loss : 0.059750, loss_ce: 0.003942, loss_dice: 0.115558
[09:50:40.687] TRAIN: iteration 12667 : loss : 0.127355, loss_ce: 0.002679, loss_dice: 0.252032
[09:50:40.894] TRAIN: iteration 12668 : loss : 0.229095, loss_ce: 0.004929, loss_dice: 0.453261
[09:50:41.102] TRAIN: iteration 12669 : loss : 0.072725, loss_ce: 0.004755, loss_dice: 0.140696
[09:50:41.310] TRAIN: iteration 12670 : loss : 0.251828, loss_ce: 0.003440, loss_dice: 0.500217
[09:50:41.519] TRAIN: iteration 12671 : loss : 0.092347, loss_ce: 0.005861, loss_dice: 0.178832
[09:50:41.739] TRAIN: iteration 12672 : loss : 0.237163, loss_ce: 0.002990, loss_dice: 0.471337
[09:50:41.948] TRAIN: iteration 12673 : loss : 0.145858, loss_ce: 0.013205, loss_dice: 0.278511
[09:50:42.161] TRAIN: iteration 12674 : loss : 0.169489, loss_ce: 0.005121, loss_dice: 0.333858
[09:50:42.370] TRAIN: iteration 12675 : loss : 0.110826, loss_ce: 0.011689, loss_dice: 0.209964
[09:50:42.748] TRAIN: iteration 12676 : loss : 0.074408, loss_ce: 0.006131, loss_dice: 0.142684
[09:50:42.993] TRAIN: iteration 12677 : loss : 0.252095, loss_ce: 0.003967, loss_dice: 0.500222
[09:50:43.207] TRAIN: iteration 12678 : loss : 0.095523, loss_ce: 0.009259, loss_dice: 0.181787
[09:50:43.415] TRAIN: iteration 12679 : loss : 0.252723, loss_ce: 0.005065, loss_dice: 0.500380
[09:50:43.622] TRAIN: iteration 12680 : loss : 0.251883, loss_ce: 0.003948, loss_dice: 0.499818
[09:50:43.869] TRAIN: iteration 12681 : loss : 0.059742, loss_ce: 0.006598, loss_dice: 0.112885
[09:50:44.079] TRAIN: iteration 12682 : loss : 0.251779, loss_ce: 0.003354, loss_dice: 0.500203
[09:50:44.288] TRAIN: iteration 12683 : loss : 0.251058, loss_ce: 0.002050, loss_dice: 0.500067
[09:50:44.497] TRAIN: iteration 12684 : loss : 0.135067, loss_ce: 0.002952, loss_dice: 0.267182
[09:50:44.711] TRAIN: iteration 12685 : loss : 0.135582, loss_ce: 0.002916, loss_dice: 0.268247
[09:50:44.920] TRAIN: iteration 12686 : loss : 0.136152, loss_ce: 0.006314, loss_dice: 0.265990
[09:50:45.132] TRAIN: iteration 12687 : loss : 0.134169, loss_ce: 0.020787, loss_dice: 0.247552
[09:50:45.344] TRAIN: iteration 12688 : loss : 0.252317, loss_ce: 0.005640, loss_dice: 0.498993
[09:50:45.555] TRAIN: iteration 12689 : loss : 0.101073, loss_ce: 0.002699, loss_dice: 0.199447
[09:50:45.907] TRAIN: iteration 12690 : loss : 0.114860, loss_ce: 0.003549, loss_dice: 0.226171
[09:50:46.120] TRAIN: iteration 12691 : loss : 0.060237, loss_ce: 0.001671, loss_dice: 0.118802
[09:50:46.328] TRAIN: iteration 12692 : loss : 0.202500, loss_ce: 0.003634, loss_dice: 0.401366
[09:50:46.545] TRAIN: iteration 12693 : loss : 0.097560, loss_ce: 0.011890, loss_dice: 0.183231
[09:50:46.758] TRAIN: iteration 12694 : loss : 0.141724, loss_ce: 0.003695, loss_dice: 0.279753
[09:50:46.968] TRAIN: iteration 12695 : loss : 0.039981, loss_ce: 0.004629, loss_dice: 0.075333
[09:50:47.179] TRAIN: iteration 12696 : loss : 0.091285, loss_ce: 0.001696, loss_dice: 0.180874
[09:50:48.037] TRAIN: iteration 12697 : loss : 0.245476, loss_ce: 0.005667, loss_dice: 0.485285
[09:50:48.251] TRAIN: iteration 12698 : loss : 0.040028, loss_ce: 0.003633, loss_dice: 0.076424
[09:50:48.459] TRAIN: iteration 12699 : loss : 0.047067, loss_ce: 0.004890, loss_dice: 0.089245
[09:50:48.711] TRAIN: iteration 12700 : loss : 0.083630, loss_ce: 0.002247, loss_dice: 0.165012
[09:50:48.947] TRAIN: iteration 12701 : loss : 0.085389, loss_ce: 0.005326, loss_dice: 0.165451
[09:50:49.162] TRAIN: iteration 12702 : loss : 0.228947, loss_ce: 0.004434, loss_dice: 0.453460
[09:50:49.371] TRAIN: iteration 12703 : loss : 0.165171, loss_ce: 0.007183, loss_dice: 0.323158
[09:50:49.580] TRAIN: iteration 12704 : loss : 0.155263, loss_ce: 0.005629, loss_dice: 0.304898
[09:50:49.791] TRAIN: iteration 12705 : loss : 0.198517, loss_ce: 0.005636, loss_dice: 0.391398
[09:50:50.000] TRAIN: iteration 12706 : loss : 0.118588, loss_ce: 0.003004, loss_dice: 0.234172
[09:50:50.209] TRAIN: iteration 12707 : loss : 0.124149, loss_ce: 0.005327, loss_dice: 0.242971
[09:50:50.605] TRAIN: iteration 12708 : loss : 0.139219, loss_ce: 0.013723, loss_dice: 0.264716
[09:50:50.813] TRAIN: iteration 12709 : loss : 0.090253, loss_ce: 0.005622, loss_dice: 0.174884
[09:50:51.021] TRAIN: iteration 12710 : loss : 0.047049, loss_ce: 0.002122, loss_dice: 0.091976
[09:50:51.228] TRAIN: iteration 12711 : loss : 0.254366, loss_ce: 0.009698, loss_dice: 0.499034
[09:50:51.435] TRAIN: iteration 12712 : loss : 0.110545, loss_ce: 0.007569, loss_dice: 0.213521
[09:50:51.645] TRAIN: iteration 12713 : loss : 0.062101, loss_ce: 0.010619, loss_dice: 0.113584
[09:50:51.856] TRAIN: iteration 12714 : loss : 0.132492, loss_ce: 0.002321, loss_dice: 0.262663
[09:50:52.069] TRAIN: iteration 12715 : loss : 0.070641, loss_ce: 0.007601, loss_dice: 0.133681
[09:50:53.043] TRAIN: iteration 12716 : loss : 0.252281, loss_ce: 0.004344, loss_dice: 0.500218
[09:50:53.252] TRAIN: iteration 12717 : loss : 0.037435, loss_ce: 0.001265, loss_dice: 0.073604
[09:50:53.460] TRAIN: iteration 12718 : loss : 0.250526, loss_ce: 0.002466, loss_dice: 0.498585
[09:50:53.675] TRAIN: iteration 12719 : loss : 0.120324, loss_ce: 0.006728, loss_dice: 0.233920
[09:50:53.889] TRAIN: iteration 12720 : loss : 0.075113, loss_ce: 0.004387, loss_dice: 0.145838
[09:50:54.128] TRAIN: iteration 12721 : loss : 0.058007, loss_ce: 0.003581, loss_dice: 0.112434
[09:50:54.336] TRAIN: iteration 12722 : loss : 0.107943, loss_ce: 0.001163, loss_dice: 0.214722
[09:50:54.544] TRAIN: iteration 12723 : loss : 0.177517, loss_ce: 0.005452, loss_dice: 0.349583
[09:50:54.751] TRAIN: iteration 12724 : loss : 0.072595, loss_ce: 0.005799, loss_dice: 0.139391
[09:50:54.959] TRAIN: iteration 12725 : loss : 0.256303, loss_ce: 0.014045, loss_dice: 0.498561
[09:50:55.168] TRAIN: iteration 12726 : loss : 0.192488, loss_ce: 0.001947, loss_dice: 0.383029
[09:50:55.378] TRAIN: iteration 12727 : loss : 0.248413, loss_ce: 0.002481, loss_dice: 0.494345
[09:50:55.586] TRAIN: iteration 12728 : loss : 0.139804, loss_ce: 0.003477, loss_dice: 0.276132
[09:50:55.792] TRAIN: iteration 12729 : loss : 0.251499, loss_ce: 0.002810, loss_dice: 0.500187
[09:50:56.000] TRAIN: iteration 12730 : loss : 0.128887, loss_ce: 0.012496, loss_dice: 0.245279
[09:50:56.208] TRAIN: iteration 12731 : loss : 0.173106, loss_ce: 0.007104, loss_dice: 0.339109
[09:50:56.417] TRAIN: iteration 12732 : loss : 0.148719, loss_ce: 0.003272, loss_dice: 0.294165
[09:50:56.626] TRAIN: iteration 12733 : loss : 0.074971, loss_ce: 0.009235, loss_dice: 0.140706
[09:50:56.833] TRAIN: iteration 12734 : loss : 0.251729, loss_ce: 0.003238, loss_dice: 0.500220
[09:50:57.043] TRAIN: iteration 12735 : loss : 0.040479, loss_ce: 0.007813, loss_dice: 0.073145
[09:50:57.252] TRAIN: iteration 12736 : loss : 0.181832, loss_ce: 0.007422, loss_dice: 0.356242
[09:50:57.468] TRAIN: iteration 12737 : loss : 0.251989, loss_ce: 0.004173, loss_dice: 0.499806
[09:50:57.675] TRAIN: iteration 12738 : loss : 0.057504, loss_ce: 0.006980, loss_dice: 0.108028
[09:50:57.887] TRAIN: iteration 12739 : loss : 0.253385, loss_ce: 0.006287, loss_dice: 0.500482
[09:50:58.103] TRAIN: iteration 12740 : loss : 0.168917, loss_ce: 0.007006, loss_dice: 0.330829
[09:50:58.342] TRAIN: iteration 12741 : loss : 0.171502, loss_ce: 0.004439, loss_dice: 0.338566
[09:50:58.549] TRAIN: iteration 12742 : loss : 0.235763, loss_ce: 0.004833, loss_dice: 0.466693
[09:50:58.762] TRAIN: iteration 12743 : loss : 0.122268, loss_ce: 0.004792, loss_dice: 0.239744
[09:50:58.978] TRAIN: iteration 12744 : loss : 0.060667, loss_ce: 0.004046, loss_dice: 0.117287
[09:50:59.187] TRAIN: iteration 12745 : loss : 0.121257, loss_ce: 0.006487, loss_dice: 0.236028
[09:50:59.396] TRAIN: iteration 12746 : loss : 0.182738, loss_ce: 0.006414, loss_dice: 0.359061
[09:50:59.612] TRAIN: iteration 12747 : loss : 0.077141, loss_ce: 0.003532, loss_dice: 0.150750
[09:50:59.820] TRAIN: iteration 12748 : loss : 0.115189, loss_ce: 0.003712, loss_dice: 0.226665
[09:51:00.029] TRAIN: iteration 12749 : loss : 0.251038, loss_ce: 0.004899, loss_dice: 0.497178
[09:51:00.236] TRAIN: iteration 12750 : loss : 0.078409, loss_ce: 0.003039, loss_dice: 0.153778
[09:51:00.444] TRAIN: iteration 12751 : loss : 0.200219, loss_ce: 0.008022, loss_dice: 0.392415
[09:51:00.653] TRAIN: iteration 12752 : loss : 0.071494, loss_ce: 0.002677, loss_dice: 0.140311
[09:51:00.867] TRAIN: iteration 12753 : loss : 0.144179, loss_ce: 0.001367, loss_dice: 0.286991
[09:51:01.096] TRAIN: iteration 12754 : loss : 0.172416, loss_ce: 0.002715, loss_dice: 0.342117
[09:51:01.305] TRAIN: iteration 12755 : loss : 0.122212, loss_ce: 0.004190, loss_dice: 0.240233
[09:51:01.514] TRAIN: iteration 12756 : loss : 0.095175, loss_ce: 0.008813, loss_dice: 0.181537
[09:51:01.722] TRAIN: iteration 12757 : loss : 0.184451, loss_ce: 0.005598, loss_dice: 0.363304
[09:51:01.931] TRAIN: iteration 12758 : loss : 0.250373, loss_ce: 0.000727, loss_dice: 0.500018
[09:51:02.139] TRAIN: iteration 12759 : loss : 0.074035, loss_ce: 0.003531, loss_dice: 0.144539
[09:51:02.348] TRAIN: iteration 12760 : loss : 0.116402, loss_ce: 0.004208, loss_dice: 0.228595
[09:51:02.587] TRAIN: iteration 12761 : loss : 0.116100, loss_ce: 0.009871, loss_dice: 0.222329
[09:51:02.807] TRAIN: iteration 12762 : loss : 0.109977, loss_ce: 0.002566, loss_dice: 0.217387
[09:51:03.018] TRAIN: iteration 12763 : loss : 0.100455, loss_ce: 0.003008, loss_dice: 0.197901
[09:51:03.227] TRAIN: iteration 12764 : loss : 0.208612, loss_ce: 0.007078, loss_dice: 0.410146
[09:51:03.436] TRAIN: iteration 12765 : loss : 0.250163, loss_ce: 0.002860, loss_dice: 0.497466
[09:51:03.644] TRAIN: iteration 12766 : loss : 0.043279, loss_ce: 0.005283, loss_dice: 0.081276
[09:51:03.852] TRAIN: iteration 12767 : loss : 0.120900, loss_ce: 0.019579, loss_dice: 0.222221
[09:51:04.062] TRAIN: iteration 12768 : loss : 0.116725, loss_ce: 0.002190, loss_dice: 0.231260
[09:51:04.277] TRAIN: iteration 12769 : loss : 0.249805, loss_ce: 0.003334, loss_dice: 0.496277
[09:51:04.487] TRAIN: iteration 12770 : loss : 0.159305, loss_ce: 0.004282, loss_dice: 0.314327
[09:51:04.703] TRAIN: iteration 12771 : loss : 0.247626, loss_ce: 0.004553, loss_dice: 0.490699
[09:51:04.910] TRAIN: iteration 12772 : loss : 0.165922, loss_ce: 0.003614, loss_dice: 0.328231
[09:51:05.123] TRAIN: iteration 12773 : loss : 0.251618, loss_ce: 0.003093, loss_dice: 0.500144
[09:51:05.331] TRAIN: iteration 12774 : loss : 0.070314, loss_ce: 0.006455, loss_dice: 0.134172
[09:51:05.538] TRAIN: iteration 12775 : loss : 0.143687, loss_ce: 0.003083, loss_dice: 0.284290
[09:51:05.746] TRAIN: iteration 12776 : loss : 0.074942, loss_ce: 0.006489, loss_dice: 0.143395
[09:51:05.953] TRAIN: iteration 12777 : loss : 0.251342, loss_ce: 0.002537, loss_dice: 0.500147
[09:51:06.163] TRAIN: iteration 12778 : loss : 0.188962, loss_ce: 0.014013, loss_dice: 0.363911
[09:51:06.371] TRAIN: iteration 12779 : loss : 0.061850, loss_ce: 0.003591, loss_dice: 0.120108
[09:51:06.582] TRAIN: iteration 12780 : loss : 0.251272, loss_ce: 0.002405, loss_dice: 0.500140
[09:51:06.821] TRAIN: iteration 12781 : loss : 0.042716, loss_ce: 0.003952, loss_dice: 0.081479
[09:51:07.030] TRAIN: iteration 12782 : loss : 0.110548, loss_ce: 0.002508, loss_dice: 0.218589
[09:51:07.240] TRAIN: iteration 12783 : loss : 0.105830, loss_ce: 0.003272, loss_dice: 0.208388
[09:51:07.449] TRAIN: iteration 12784 : loss : 0.201131, loss_ce: 0.010401, loss_dice: 0.391862
[09:51:07.664] TRAIN: iteration 12785 : loss : 0.049782, loss_ce: 0.003205, loss_dice: 0.096359
[09:51:07.872] TRAIN: iteration 12786 : loss : 0.111419, loss_ce: 0.002566, loss_dice: 0.220272
[09:51:08.079] TRAIN: iteration 12787 : loss : 0.122505, loss_ce: 0.002200, loss_dice: 0.242809
[09:51:08.289] TRAIN: iteration 12788 : loss : 0.088208, loss_ce: 0.002108, loss_dice: 0.174309
[09:51:08.498] TRAIN: iteration 12789 : loss : 0.141412, loss_ce: 0.002388, loss_dice: 0.280435
[09:51:08.707] TRAIN: iteration 12790 : loss : 0.086641, loss_ce: 0.001423, loss_dice: 0.171860
[09:51:08.917] TRAIN: iteration 12791 : loss : 0.090979, loss_ce: 0.002684, loss_dice: 0.179275
[09:51:09.125] TRAIN: iteration 12792 : loss : 0.250468, loss_ce: 0.000916, loss_dice: 0.500020
[09:51:09.333] TRAIN: iteration 12793 : loss : 0.092958, loss_ce: 0.004160, loss_dice: 0.181757
[09:51:09.542] TRAIN: iteration 12794 : loss : 0.121137, loss_ce: 0.001255, loss_dice: 0.241019
[09:51:10.151] TRAIN: iteration 12795 : loss : 0.084736, loss_ce: 0.000998, loss_dice: 0.168473
[09:51:10.360] TRAIN: iteration 12796 : loss : 0.207294, loss_ce: 0.007097, loss_dice: 0.407491
[09:51:10.572] TRAIN: iteration 12797 : loss : 0.186612, loss_ce: 0.027288, loss_dice: 0.345936
[09:51:10.781] TRAIN: iteration 12798 : loss : 0.155611, loss_ce: 0.001618, loss_dice: 0.309604
[09:51:10.988] TRAIN: iteration 12799 : loss : 0.251009, loss_ce: 0.004076, loss_dice: 0.497941
[09:51:11.204] TRAIN: iteration 12800 : loss : 0.251167, loss_ce: 0.002189, loss_dice: 0.500144
[09:51:12.364] TRAIN: iteration 12801 : loss : 0.178730, loss_ce: 0.006368, loss_dice: 0.351092
[09:51:12.578] TRAIN: iteration 12802 : loss : 0.105378, loss_ce: 0.001599, loss_dice: 0.209156
[09:51:12.789] TRAIN: iteration 12803 : loss : 0.082901, loss_ce: 0.004967, loss_dice: 0.160835
[09:51:13.000] TRAIN: iteration 12804 : loss : 0.231976, loss_ce: 0.008052, loss_dice: 0.455901
[09:51:13.211] TRAIN: iteration 12805 : loss : 0.081880, loss_ce: 0.002634, loss_dice: 0.161126
[09:51:13.425] TRAIN: iteration 12806 : loss : 0.215614, loss_ce: 0.002754, loss_dice: 0.428474
[09:51:13.635] TRAIN: iteration 12807 : loss : 0.117693, loss_ce: 0.004010, loss_dice: 0.231375
[09:51:13.845] TRAIN: iteration 12808 : loss : 0.175932, loss_ce: 0.004826, loss_dice: 0.347038
[09:51:14.055] TRAIN: iteration 12809 : loss : 0.067004, loss_ce: 0.003271, loss_dice: 0.130737
[09:51:14.262] TRAIN: iteration 12810 : loss : 0.094908, loss_ce: 0.006213, loss_dice: 0.183604
[09:51:14.470] TRAIN: iteration 12811 : loss : 0.072332, loss_ce: 0.006949, loss_dice: 0.137714
[09:51:14.685] TRAIN: iteration 12812 : loss : 0.117172, loss_ce: 0.005678, loss_dice: 0.228666
[09:51:14.893] TRAIN: iteration 12813 : loss : 0.114650, loss_ce: 0.005848, loss_dice: 0.223453
[09:51:15.101] TRAIN: iteration 12814 : loss : 0.130807, loss_ce: 0.004369, loss_dice: 0.257245
[09:51:15.309] TRAIN: iteration 12815 : loss : 0.105397, loss_ce: 0.003555, loss_dice: 0.207240
[09:51:15.517] TRAIN: iteration 12816 : loss : 0.243869, loss_ce: 0.004070, loss_dice: 0.483667
[09:51:15.724] TRAIN: iteration 12817 : loss : 0.251860, loss_ce: 0.003569, loss_dice: 0.500150
[09:51:15.932] TRAIN: iteration 12818 : loss : 0.218291, loss_ce: 0.006582, loss_dice: 0.430001
[09:51:16.140] TRAIN: iteration 12819 : loss : 0.135589, loss_ce: 0.010624, loss_dice: 0.260554
[09:51:16.347] TRAIN: iteration 12820 : loss : 0.158470, loss_ce: 0.004407, loss_dice: 0.312532
[09:51:16.590] TRAIN: iteration 12821 : loss : 0.181563, loss_ce: 0.004502, loss_dice: 0.358625
[09:51:16.798] TRAIN: iteration 12822 : loss : 0.120261, loss_ce: 0.006854, loss_dice: 0.233667
[09:51:17.006] TRAIN: iteration 12823 : loss : 0.094034, loss_ce: 0.006389, loss_dice: 0.181679
[09:51:17.214] TRAIN: iteration 12824 : loss : 0.088107, loss_ce: 0.005711, loss_dice: 0.170503
[09:51:17.422] TRAIN: iteration 12825 : loss : 0.110928, loss_ce: 0.004226, loss_dice: 0.217631
[09:51:17.631] TRAIN: iteration 12826 : loss : 0.209968, loss_ce: 0.004048, loss_dice: 0.415888
[09:51:17.839] TRAIN: iteration 12827 : loss : 0.112667, loss_ce: 0.010355, loss_dice: 0.214979
[09:51:18.047] TRAIN: iteration 12828 : loss : 0.252285, loss_ce: 0.004283, loss_dice: 0.500287
[09:51:18.254] TRAIN: iteration 12829 : loss : 0.139431, loss_ce: 0.004282, loss_dice: 0.274580
[09:51:18.468] TRAIN: iteration 12830 : loss : 0.187024, loss_ce: 0.004188, loss_dice: 0.369859
[09:51:18.677] TRAIN: iteration 12831 : loss : 0.069147, loss_ce: 0.005978, loss_dice: 0.132316
[09:51:18.884] TRAIN: iteration 12832 : loss : 0.123450, loss_ce: 0.007664, loss_dice: 0.239236
[09:51:19.094] TRAIN: iteration 12833 : loss : 0.118396, loss_ce: 0.002814, loss_dice: 0.233979
[09:51:19.301] TRAIN: iteration 12834 : loss : 0.251550, loss_ce: 0.005264, loss_dice: 0.497836
[09:51:19.516] TRAIN: iteration 12835 : loss : 0.183450, loss_ce: 0.002342, loss_dice: 0.364557
[09:51:19.725] TRAIN: iteration 12836 : loss : 0.243921, loss_ce: 0.001587, loss_dice: 0.486255
[09:51:19.932] TRAIN: iteration 12837 : loss : 0.123379, loss_ce: 0.016965, loss_dice: 0.229794
[09:51:20.140] TRAIN: iteration 12838 : loss : 0.101375, loss_ce: 0.010584, loss_dice: 0.192167
[09:51:20.348] TRAIN: iteration 12839 : loss : 0.250523, loss_ce: 0.001029, loss_dice: 0.500017
[09:51:20.555] TRAIN: iteration 12840 : loss : 0.134618, loss_ce: 0.002970, loss_dice: 0.266266
[09:51:21.295] TRAIN: iteration 12841 : loss : 0.250915, loss_ce: 0.001746, loss_dice: 0.500085
[09:51:21.505] TRAIN: iteration 12842 : loss : 0.094762, loss_ce: 0.012922, loss_dice: 0.176601
[09:51:21.716] TRAIN: iteration 12843 : loss : 0.112555, loss_ce: 0.003448, loss_dice: 0.221662
[09:51:21.925] TRAIN: iteration 12844 : loss : 0.156526, loss_ce: 0.034332, loss_dice: 0.278721
[09:51:22.133] TRAIN: iteration 12845 : loss : 0.207553, loss_ce: 0.019312, loss_dice: 0.395794
[09:51:22.352] TRAIN: iteration 12846 : loss : 0.103472, loss_ce: 0.005888, loss_dice: 0.201056
[09:51:22.563] TRAIN: iteration 12847 : loss : 0.146222, loss_ce: 0.004717, loss_dice: 0.287727
[09:51:22.771] TRAIN: iteration 12848 : loss : 0.252320, loss_ce: 0.004540, loss_dice: 0.500100
[09:51:23.120] TRAIN: iteration 12849 : loss : 0.179359, loss_ce: 0.003591, loss_dice: 0.355127
[09:51:23.333] TRAIN: iteration 12850 : loss : 0.248169, loss_ce: 0.002997, loss_dice: 0.493341
[09:51:23.542] TRAIN: iteration 12851 : loss : 0.257756, loss_ce: 0.028121, loss_dice: 0.487392
[09:51:23.750] TRAIN: iteration 12852 : loss : 0.249750, loss_ce: 0.009546, loss_dice: 0.489955
[09:51:23.964] TRAIN: iteration 12853 : loss : 0.071871, loss_ce: 0.006136, loss_dice: 0.137605
[09:51:24.175] TRAIN: iteration 12854 : loss : 0.199971, loss_ce: 0.006656, loss_dice: 0.393285
[09:51:26.784] TRAIN: iteration 12855 : loss : 0.205220, loss_ce: 0.005093, loss_dice: 0.405346
[09:51:26.990] TRAIN: iteration 12856 : loss : 0.242699, loss_ce: 0.004754, loss_dice: 0.480645
[09:51:27.197] TRAIN: iteration 12857 : loss : 0.094552, loss_ce: 0.013430, loss_dice: 0.175674
[09:51:27.405] TRAIN: iteration 12858 : loss : 0.056832, loss_ce: 0.005235, loss_dice: 0.108429
[09:51:27.613] TRAIN: iteration 12859 : loss : 0.154695, loss_ce: 0.006496, loss_dice: 0.302894
[09:51:27.821] TRAIN: iteration 12860 : loss : 0.130821, loss_ce: 0.004702, loss_dice: 0.256940
[09:51:28.063] TRAIN: iteration 12861 : loss : 0.166416, loss_ce: 0.005613, loss_dice: 0.327220
[09:51:28.272] TRAIN: iteration 12862 : loss : 0.135459, loss_ce: 0.004593, loss_dice: 0.266325
[09:51:28.733] TRAIN: iteration 12863 : loss : 0.227726, loss_ce: 0.003029, loss_dice: 0.452423
[09:51:28.941] TRAIN: iteration 12864 : loss : 0.137704, loss_ce: 0.003328, loss_dice: 0.272080
[09:51:29.151] TRAIN: iteration 12865 : loss : 0.199883, loss_ce: 0.004320, loss_dice: 0.395447
[09:51:29.358] TRAIN: iteration 12866 : loss : 0.144857, loss_ce: 0.002251, loss_dice: 0.287463
[09:51:29.566] TRAIN: iteration 12867 : loss : 0.253128, loss_ce: 0.006637, loss_dice: 0.499620
[09:51:29.775] TRAIN: iteration 12868 : loss : 0.250835, loss_ce: 0.001622, loss_dice: 0.500048
[09:51:29.982] TRAIN: iteration 12869 : loss : 0.237471, loss_ce: 0.002817, loss_dice: 0.472125
[09:51:30.189] TRAIN: iteration 12870 : loss : 0.160493, loss_ce: 0.007787, loss_dice: 0.313199
[09:51:30.396] TRAIN: iteration 12871 : loss : 0.148532, loss_ce: 0.004968, loss_dice: 0.292095
[09:51:30.603] TRAIN: iteration 12872 : loss : 0.095422, loss_ce: 0.002928, loss_dice: 0.187916
[09:51:30.813] TRAIN: iteration 12873 : loss : 0.077184, loss_ce: 0.005468, loss_dice: 0.148900
[09:51:31.020] TRAIN: iteration 12874 : loss : 0.157828, loss_ce: 0.003154, loss_dice: 0.312503
[09:51:31.228] TRAIN: iteration 12875 : loss : 0.110767, loss_ce: 0.007369, loss_dice: 0.214166
[09:51:31.435] TRAIN: iteration 12876 : loss : 0.169654, loss_ce: 0.026706, loss_dice: 0.312602
[09:51:31.643] TRAIN: iteration 12877 : loss : 0.205932, loss_ce: 0.012596, loss_dice: 0.399267
[09:51:31.854] TRAIN: iteration 12878 : loss : 0.062685, loss_ce: 0.003115, loss_dice: 0.122254
[09:51:32.061] TRAIN: iteration 12879 : loss : 0.188784, loss_ce: 0.005608, loss_dice: 0.371960
[09:51:32.269] TRAIN: iteration 12880 : loss : 0.251926, loss_ce: 0.003644, loss_dice: 0.500208
[09:51:32.508] TRAIN: iteration 12881 : loss : 0.214023, loss_ce: 0.012960, loss_dice: 0.415086
[09:51:32.714] TRAIN: iteration 12882 : loss : 0.168279, loss_ce: 0.004893, loss_dice: 0.331666
[09:51:32.929] TRAIN: iteration 12883 : loss : 0.229537, loss_ce: 0.006829, loss_dice: 0.452245
[09:51:33.150] TRAIN: iteration 12884 : loss : 0.145247, loss_ce: 0.004004, loss_dice: 0.286490
[09:51:33.360] TRAIN: iteration 12885 : loss : 0.074659, loss_ce: 0.007648, loss_dice: 0.141670
[09:51:33.571] TRAIN: iteration 12886 : loss : 0.059516, loss_ce: 0.005756, loss_dice: 0.113275
[09:51:33.780] TRAIN: iteration 12887 : loss : 0.037305, loss_ce: 0.005306, loss_dice: 0.069304
[09:51:33.987] TRAIN: iteration 12888 : loss : 0.143698, loss_ce: 0.004041, loss_dice: 0.283355
[09:51:34.207] TRAIN: iteration 12889 : loss : 0.238600, loss_ce: 0.004266, loss_dice: 0.472934
[09:51:34.414] TRAIN: iteration 12890 : loss : 0.199942, loss_ce: 0.010374, loss_dice: 0.389509
[09:51:34.623] TRAIN: iteration 12891 : loss : 0.237477, loss_ce: 0.003829, loss_dice: 0.471125
[09:51:34.832] TRAIN: iteration 12892 : loss : 0.070745, loss_ce: 0.004831, loss_dice: 0.136659
[09:51:35.045] TRAIN: iteration 12893 : loss : 0.220650, loss_ce: 0.003930, loss_dice: 0.437370
[09:51:35.278] TRAIN: iteration 12894 : loss : 0.249986, loss_ce: 0.005417, loss_dice: 0.494555
[09:51:35.487] TRAIN: iteration 12895 : loss : 0.066078, loss_ce: 0.004155, loss_dice: 0.128000
[09:51:35.696] TRAIN: iteration 12896 : loss : 0.077636, loss_ce: 0.005407, loss_dice: 0.149864
[09:51:35.913] TRAIN: iteration 12897 : loss : 0.147747, loss_ce: 0.008185, loss_dice: 0.287309
[09:51:36.123] TRAIN: iteration 12898 : loss : 0.152216, loss_ce: 0.007131, loss_dice: 0.297301
[09:51:36.331] TRAIN: iteration 12899 : loss : 0.153489, loss_ce: 0.003438, loss_dice: 0.303540
[09:51:36.540] TRAIN: iteration 12900 : loss : 0.137471, loss_ce: 0.003743, loss_dice: 0.271198
[09:51:36.779] TRAIN: iteration 12901 : loss : 0.243669, loss_ce: 0.004311, loss_dice: 0.483027
[09:51:36.987] TRAIN: iteration 12902 : loss : 0.251828, loss_ce: 0.003934, loss_dice: 0.499722
[09:51:37.196] TRAIN: iteration 12903 : loss : 0.202372, loss_ce: 0.010423, loss_dice: 0.394321
[09:51:37.405] TRAIN: iteration 12904 : loss : 0.251061, loss_ce: 0.002038, loss_dice: 0.500084
[09:51:37.612] TRAIN: iteration 12905 : loss : 0.251553, loss_ce: 0.002911, loss_dice: 0.500194
[09:51:37.930] TRAIN: iteration 12906 : loss : 0.251353, loss_ce: 0.002546, loss_dice: 0.500160
[09:51:38.138] TRAIN: iteration 12907 : loss : 0.234416, loss_ce: 0.007786, loss_dice: 0.461046
[09:51:38.346] TRAIN: iteration 12908 : loss : 0.247929, loss_ce: 0.004064, loss_dice: 0.491795
[09:51:38.555] TRAIN: iteration 12909 : loss : 0.112011, loss_ce: 0.004248, loss_dice: 0.219775
[09:51:38.763] TRAIN: iteration 12910 : loss : 0.059367, loss_ce: 0.003168, loss_dice: 0.115566
[09:51:38.973] TRAIN: iteration 12911 : loss : 0.146945, loss_ce: 0.012056, loss_dice: 0.281834
[09:51:39.183] TRAIN: iteration 12912 : loss : 0.074741, loss_ce: 0.002888, loss_dice: 0.146595
[09:51:39.459] TRAIN: iteration 12913 : loss : 0.059718, loss_ce: 0.001712, loss_dice: 0.117724
[09:51:39.666] TRAIN: iteration 12914 : loss : 0.093312, loss_ce: 0.003087, loss_dice: 0.183537
[09:51:39.874] TRAIN: iteration 12915 : loss : 0.250952, loss_ce: 0.001833, loss_dice: 0.500070
[09:51:40.136] TRAIN: iteration 12916 : loss : 0.079497, loss_ce: 0.001977, loss_dice: 0.157017
[09:51:40.350] TRAIN: iteration 12917 : loss : 0.199075, loss_ce: 0.002868, loss_dice: 0.395282
[09:51:40.557] TRAIN: iteration 12918 : loss : 0.066258, loss_ce: 0.008922, loss_dice: 0.123595
[09:51:40.765] TRAIN: iteration 12919 : loss : 0.088304, loss_ce: 0.001435, loss_dice: 0.175174
[09:51:40.972] TRAIN: iteration 12920 : loss : 0.223311, loss_ce: 0.004921, loss_dice: 0.441701
[09:51:42.270] TRAIN: iteration 12921 : loss : 0.250759, loss_ce: 0.001465, loss_dice: 0.500053
[09:51:42.480] TRAIN: iteration 12922 : loss : 0.047925, loss_ce: 0.002548, loss_dice: 0.093302
[09:51:42.693] TRAIN: iteration 12923 : loss : 0.163619, loss_ce: 0.003495, loss_dice: 0.323744
[09:51:42.900] TRAIN: iteration 12924 : loss : 0.197763, loss_ce: 0.002074, loss_dice: 0.393453
[09:51:43.109] TRAIN: iteration 12925 : loss : 0.103009, loss_ce: 0.008547, loss_dice: 0.197471
[09:51:43.319] TRAIN: iteration 12926 : loss : 0.198796, loss_ce: 0.011295, loss_dice: 0.386297
[09:51:43.530] TRAIN: iteration 12927 : loss : 0.168301, loss_ce: 0.002375, loss_dice: 0.334227
[09:51:43.741] TRAIN: iteration 12928 : loss : 0.250874, loss_ce: 0.001665, loss_dice: 0.500083
[09:51:43.950] TRAIN: iteration 12929 : loss : 0.135168, loss_ce: 0.006134, loss_dice: 0.264202
[09:51:44.160] TRAIN: iteration 12930 : loss : 0.251100, loss_ce: 0.002093, loss_dice: 0.500108
[09:51:44.391] TRAIN: iteration 12931 : loss : 0.091078, loss_ce: 0.007891, loss_dice: 0.174265
[09:51:44.599] TRAIN: iteration 12932 : loss : 0.090486, loss_ce: 0.005798, loss_dice: 0.175174
[09:51:44.807] TRAIN: iteration 12933 : loss : 0.089412, loss_ce: 0.005955, loss_dice: 0.172869
[09:51:45.017] TRAIN: iteration 12934 : loss : 0.060568, loss_ce: 0.003077, loss_dice: 0.118059
[09:51:45.229] TRAIN: iteration 12935 : loss : 0.065048, loss_ce: 0.007199, loss_dice: 0.122898
[09:51:45.438] TRAIN: iteration 12936 : loss : 0.088838, loss_ce: 0.002386, loss_dice: 0.175291
[09:51:45.654] TRAIN: iteration 12937 : loss : 0.196849, loss_ce: 0.005038, loss_dice: 0.388660
[09:51:45.867] TRAIN: iteration 12938 : loss : 0.062223, loss_ce: 0.006258, loss_dice: 0.118188
[09:51:46.075] TRAIN: iteration 12939 : loss : 0.082427, loss_ce: 0.004686, loss_dice: 0.160168
[09:51:46.287] TRAIN: iteration 12940 : loss : 0.206818, loss_ce: 0.007989, loss_dice: 0.405646
[09:51:46.526] TRAIN: iteration 12941 : loss : 0.082881, loss_ce: 0.004523, loss_dice: 0.161240
[09:51:46.736] TRAIN: iteration 12942 : loss : 0.120920, loss_ce: 0.008932, loss_dice: 0.232907
[09:51:46.951] TRAIN: iteration 12943 : loss : 0.251976, loss_ce: 0.003691, loss_dice: 0.500260
[09:51:47.162] TRAIN: iteration 12944 : loss : 0.132804, loss_ce: 0.003639, loss_dice: 0.261969
[09:51:47.372] TRAIN: iteration 12945 : loss : 0.209526, loss_ce: 0.003123, loss_dice: 0.415930
[09:51:47.588] TRAIN: iteration 12946 : loss : 0.145558, loss_ce: 0.004545, loss_dice: 0.286571
[09:51:47.797] TRAIN: iteration 12947 : loss : 0.090206, loss_ce: 0.003257, loss_dice: 0.177156
[09:51:48.005] TRAIN: iteration 12948 : loss : 0.087405, loss_ce: 0.009798, loss_dice: 0.165012
[09:51:48.216] TRAIN: iteration 12949 : loss : 0.145908, loss_ce: 0.006151, loss_dice: 0.285665
[09:51:48.426] TRAIN: iteration 12950 : loss : 0.077986, loss_ce: 0.005254, loss_dice: 0.150718
[09:51:49.084] TRAIN: iteration 12951 : loss : 0.190946, loss_ce: 0.003048, loss_dice: 0.378844
[09:51:49.292] TRAIN: iteration 12952 : loss : 0.079150, loss_ce: 0.005924, loss_dice: 0.152375
[09:51:49.511] TRAIN: iteration 12953 : loss : 0.129828, loss_ce: 0.006510, loss_dice: 0.253146
[09:51:49.718] TRAIN: iteration 12954 : loss : 0.125164, loss_ce: 0.002647, loss_dice: 0.247681
[09:51:49.928] TRAIN: iteration 12955 : loss : 0.146822, loss_ce: 0.003622, loss_dice: 0.290022
[09:51:50.135] TRAIN: iteration 12956 : loss : 0.155346, loss_ce: 0.006702, loss_dice: 0.303990
[09:51:50.342] TRAIN: iteration 12957 : loss : 0.244554, loss_ce: 0.001908, loss_dice: 0.487201
[09:51:50.551] TRAIN: iteration 12958 : loss : 0.219417, loss_ce: 0.002661, loss_dice: 0.436173
[09:51:50.760] TRAIN: iteration 12959 : loss : 0.087631, loss_ce: 0.001670, loss_dice: 0.173592
[09:51:50.967] TRAIN: iteration 12960 : loss : 0.253922, loss_ce: 0.008838, loss_dice: 0.499005
[09:51:50.968] NaN or Inf found in input tensor.
[09:51:51.182] TRAIN: iteration 12961 : loss : 0.118872, loss_ce: 0.002118, loss_dice: 0.235626
[09:51:51.389] TRAIN: iteration 12962 : loss : 0.250619, loss_ce: 0.001191, loss_dice: 0.500046
[09:51:51.597] TRAIN: iteration 12963 : loss : 0.246082, loss_ce: 0.009056, loss_dice: 0.483109
[09:51:51.807] TRAIN: iteration 12964 : loss : 0.249805, loss_ce: 0.002241, loss_dice: 0.497369
[09:51:52.017] TRAIN: iteration 12965 : loss : 0.146898, loss_ce: 0.013326, loss_dice: 0.280469
[09:51:52.225] TRAIN: iteration 12966 : loss : 0.236058, loss_ce: 0.006321, loss_dice: 0.465795
[09:51:52.731] TRAIN: iteration 12967 : loss : 0.250909, loss_ce: 0.001728, loss_dice: 0.500090
[09:51:52.938] TRAIN: iteration 12968 : loss : 0.223661, loss_ce: 0.004119, loss_dice: 0.443203
[09:51:53.146] TRAIN: iteration 12969 : loss : 0.130724, loss_ce: 0.002191, loss_dice: 0.259258
[09:51:54.568] TRAIN: iteration 12970 : loss : 0.075774, loss_ce: 0.005440, loss_dice: 0.146108
[09:51:54.775] TRAIN: iteration 12971 : loss : 0.175029, loss_ce: 0.007988, loss_dice: 0.342070
[09:51:54.991] TRAIN: iteration 12972 : loss : 0.110585, loss_ce: 0.005844, loss_dice: 0.215327
[09:51:55.200] TRAIN: iteration 12973 : loss : 0.131377, loss_ce: 0.003213, loss_dice: 0.259540
[09:51:55.416] TRAIN: iteration 12974 : loss : 0.181571, loss_ce: 0.002892, loss_dice: 0.360250
[09:51:55.623] TRAIN: iteration 12975 : loss : 0.122351, loss_ce: 0.005450, loss_dice: 0.239252
[09:51:55.832] TRAIN: iteration 12976 : loss : 0.160740, loss_ce: 0.002040, loss_dice: 0.319439
[09:51:56.042] TRAIN: iteration 12977 : loss : 0.231660, loss_ce: 0.011046, loss_dice: 0.452273
[09:51:56.295] TRAIN: iteration 12978 : loss : 0.068153, loss_ce: 0.002793, loss_dice: 0.133513
[09:51:56.503] TRAIN: iteration 12979 : loss : 0.249491, loss_ce: 0.004625, loss_dice: 0.494357
[09:51:56.710] TRAIN: iteration 12980 : loss : 0.189857, loss_ce: 0.003596, loss_dice: 0.376119
[09:51:56.945] TRAIN: iteration 12981 : loss : 0.041496, loss_ce: 0.004868, loss_dice: 0.078123
[09:51:57.160] TRAIN: iteration 12982 : loss : 0.083202, loss_ce: 0.002468, loss_dice: 0.163937
[09:51:57.372] TRAIN: iteration 12983 : loss : 0.077164, loss_ce: 0.002088, loss_dice: 0.152239
[09:51:57.580] TRAIN: iteration 12984 : loss : 0.251105, loss_ce: 0.002147, loss_dice: 0.500063
[09:51:57.850] TRAIN: iteration 12985 : loss : 0.071617, loss_ce: 0.005328, loss_dice: 0.137907
[09:51:58.070] TRAIN: iteration 12986 : loss : 0.243967, loss_ce: 0.004364, loss_dice: 0.483570
[09:51:58.508] TRAIN: iteration 12987 : loss : 0.211271, loss_ce: 0.001383, loss_dice: 0.421160
[09:51:58.724] TRAIN: iteration 12988 : loss : 0.250949, loss_ce: 0.001818, loss_dice: 0.500080
[09:51:58.933] TRAIN: iteration 12989 : loss : 0.232898, loss_ce: 0.003653, loss_dice: 0.462142
[09:51:59.140] TRAIN: iteration 12990 : loss : 0.151481, loss_ce: 0.004600, loss_dice: 0.298361
[09:51:59.353] TRAIN: iteration 12991 : loss : 0.223296, loss_ce: 0.005855, loss_dice: 0.440736
[09:51:59.565] TRAIN: iteration 12992 : loss : 0.157596, loss_ce: 0.014500, loss_dice: 0.300692
[09:51:59.779] TRAIN: iteration 12993 : loss : 0.213735, loss_ce: 0.002363, loss_dice: 0.425106
[09:51:59.992] TRAIN: iteration 12994 : loss : 0.119128, loss_ce: 0.005918, loss_dice: 0.232338
[09:52:00.202] TRAIN: iteration 12995 : loss : 0.130430, loss_ce: 0.003148, loss_dice: 0.257711
[09:52:00.417] TRAIN: iteration 12996 : loss : 0.157922, loss_ce: 0.002305, loss_dice: 0.313538
[09:52:00.630] TRAIN: iteration 12997 : loss : 0.171544, loss_ce: 0.008129, loss_dice: 0.334959
[09:52:00.843] TRAIN: iteration 12998 : loss : 0.068560, loss_ce: 0.009143, loss_dice: 0.127978
[09:52:01.050] TRAIN: iteration 12999 : loss : 0.140722, loss_ce: 0.004177, loss_dice: 0.277267
[09:52:01.260] TRAIN: iteration 13000 : loss : 0.087727, loss_ce: 0.005671, loss_dice: 0.169784
[09:52:01.500] TRAIN: iteration 13001 : loss : 0.064352, loss_ce: 0.003638, loss_dice: 0.125065
[09:52:01.707] TRAIN: iteration 13002 : loss : 0.123936, loss_ce: 0.004596, loss_dice: 0.243276
[09:52:01.919] TRAIN: iteration 13003 : loss : 0.099118, loss_ce: 0.004282, loss_dice: 0.193954
[09:52:02.128] TRAIN: iteration 13004 : loss : 0.252303, loss_ce: 0.004374, loss_dice: 0.500232
[09:52:02.336] TRAIN: iteration 13005 : loss : 0.236274, loss_ce: 0.008009, loss_dice: 0.464539
[09:52:02.544] TRAIN: iteration 13006 : loss : 0.153122, loss_ce: 0.009619, loss_dice: 0.296625
[09:52:02.754] TRAIN: iteration 13007 : loss : 0.057574, loss_ce: 0.003345, loss_dice: 0.111802
[09:52:03.167] TRAIN: iteration 13008 : loss : 0.253201, loss_ce: 0.007323, loss_dice: 0.499080
[09:52:03.373] TRAIN: iteration 13009 : loss : 0.085339, loss_ce: 0.004109, loss_dice: 0.166570
[09:52:03.581] TRAIN: iteration 13010 : loss : 0.252210, loss_ce: 0.004158, loss_dice: 0.500261
[09:52:05.857] TRAIN: iteration 13011 : loss : 0.155743, loss_ce: 0.008955, loss_dice: 0.302531
[09:52:06.066] TRAIN: iteration 13012 : loss : 0.038231, loss_ce: 0.003736, loss_dice: 0.072725
[09:52:06.274] TRAIN: iteration 13013 : loss : 0.102411, loss_ce: 0.004113, loss_dice: 0.200708
[09:52:06.482] TRAIN: iteration 13014 : loss : 0.065155, loss_ce: 0.005931, loss_dice: 0.124379
[09:52:06.708] TRAIN: iteration 13015 : loss : 0.247348, loss_ce: 0.006236, loss_dice: 0.488460
[09:52:06.915] TRAIN: iteration 13016 : loss : 0.146106, loss_ce: 0.007604, loss_dice: 0.284608
[09:52:07.122] TRAIN: iteration 13017 : loss : 0.069406, loss_ce: 0.003193, loss_dice: 0.135619
[09:52:07.337] TRAIN: iteration 13018 : loss : 0.066776, loss_ce: 0.010101, loss_dice: 0.123450
[09:52:07.546] TRAIN: iteration 13019 : loss : 0.198649, loss_ce: 0.005174, loss_dice: 0.392123
[09:52:07.755] TRAIN: iteration 13020 : loss : 0.129822, loss_ce: 0.010706, loss_dice: 0.248939
[09:52:07.992] TRAIN: iteration 13021 : loss : 0.094371, loss_ce: 0.005889, loss_dice: 0.182853
[09:52:08.199] TRAIN: iteration 13022 : loss : 0.215258, loss_ce: 0.003854, loss_dice: 0.426662
[09:52:08.408] TRAIN: iteration 13023 : loss : 0.055228, loss_ce: 0.002213, loss_dice: 0.108243
[09:52:08.615] TRAIN: iteration 13024 : loss : 0.181919, loss_ce: 0.009616, loss_dice: 0.354222
[09:52:08.828] TRAIN: iteration 13025 : loss : 0.167139, loss_ce: 0.004213, loss_dice: 0.330065
[09:52:09.035] TRAIN: iteration 13026 : loss : 0.043810, loss_ce: 0.002182, loss_dice: 0.085437
[09:52:09.244] TRAIN: iteration 13027 : loss : 0.138687, loss_ce: 0.003335, loss_dice: 0.274039
[09:52:09.452] TRAIN: iteration 13028 : loss : 0.056734, loss_ce: 0.003238, loss_dice: 0.110230
[09:52:09.660] TRAIN: iteration 13029 : loss : 0.226071, loss_ce: 0.002561, loss_dice: 0.449581
[09:52:09.869] TRAIN: iteration 13030 : loss : 0.050479, loss_ce: 0.002139, loss_dice: 0.098819
[09:52:10.077] TRAIN: iteration 13031 : loss : 0.037742, loss_ce: 0.001906, loss_dice: 0.073579
[09:52:10.286] TRAIN: iteration 13032 : loss : 0.053267, loss_ce: 0.001477, loss_dice: 0.105056
[09:52:10.495] TRAIN: iteration 13033 : loss : 0.130080, loss_ce: 0.009280, loss_dice: 0.250880
[09:52:10.702] TRAIN: iteration 13034 : loss : 0.104201, loss_ce: 0.001874, loss_dice: 0.206529
[09:52:10.912] TRAIN: iteration 13035 : loss : 0.243356, loss_ce: 0.001371, loss_dice: 0.485342
[09:52:11.125] TRAIN: iteration 13036 : loss : 0.179842, loss_ce: 0.003680, loss_dice: 0.356003
[09:52:11.333] TRAIN: iteration 13037 : loss : 0.125178, loss_ce: 0.003053, loss_dice: 0.247304
[09:52:11.541] TRAIN: iteration 13038 : loss : 0.246707, loss_ce: 0.004139, loss_dice: 0.489274
[09:52:11.750] TRAIN: iteration 13039 : loss : 0.126429, loss_ce: 0.004419, loss_dice: 0.248440
[09:52:11.960] TRAIN: iteration 13040 : loss : 0.200510, loss_ce: 0.012682, loss_dice: 0.388338
[09:52:12.203] TRAIN: iteration 13041 : loss : 0.071015, loss_ce: 0.002865, loss_dice: 0.139166
[09:52:12.419] TRAIN: iteration 13042 : loss : 0.087287, loss_ce: 0.001329, loss_dice: 0.173244
[09:52:12.633] TRAIN: iteration 13043 : loss : 0.121204, loss_ce: 0.004903, loss_dice: 0.237505
[09:52:12.844] TRAIN: iteration 13044 : loss : 0.062419, loss_ce: 0.001259, loss_dice: 0.123579
[09:52:13.058] TRAIN: iteration 13045 : loss : 0.242104, loss_ce: 0.002443, loss_dice: 0.481765
[09:52:13.272] TRAIN: iteration 13046 : loss : 0.025583, loss_ce: 0.000958, loss_dice: 0.050209
[09:52:13.486] TRAIN: iteration 13047 : loss : 0.160081, loss_ce: 0.002588, loss_dice: 0.317575
[09:52:14.134] TRAIN: iteration 13048 : loss : 0.184685, loss_ce: 0.007241, loss_dice: 0.362128
[09:52:14.351] TRAIN: iteration 13049 : loss : 0.061787, loss_ce: 0.004564, loss_dice: 0.119009
[09:52:16.966] TRAIN: iteration 13050 : loss : 0.068027, loss_ce: 0.003542, loss_dice: 0.132512
[09:52:17.179] TRAIN: iteration 13051 : loss : 0.027286, loss_ce: 0.003082, loss_dice: 0.051490
[09:52:17.386] TRAIN: iteration 13052 : loss : 0.195531, loss_ce: 0.006570, loss_dice: 0.384492
[09:52:17.594] TRAIN: iteration 13053 : loss : 0.089400, loss_ce: 0.002613, loss_dice: 0.176187
[09:52:17.807] TRAIN: iteration 13054 : loss : 0.250946, loss_ce: 0.001822, loss_dice: 0.500069
[09:52:18.014] TRAIN: iteration 13055 : loss : 0.231515, loss_ce: 0.006156, loss_dice: 0.456874
[09:52:18.222] TRAIN: iteration 13056 : loss : 0.163363, loss_ce: 0.007844, loss_dice: 0.318883
[09:52:18.428] TRAIN: iteration 13057 : loss : 0.122958, loss_ce: 0.007234, loss_dice: 0.238682
[09:52:18.651] TRAIN: iteration 13058 : loss : 0.253521, loss_ce: 0.006590, loss_dice: 0.500452
[09:52:18.859] TRAIN: iteration 13059 : loss : 0.250242, loss_ce: 0.004556, loss_dice: 0.495928
[09:52:19.068] TRAIN: iteration 13060 : loss : 0.090761, loss_ce: 0.009641, loss_dice: 0.171881
[09:52:19.305] TRAIN: iteration 13061 : loss : 0.202444, loss_ce: 0.005013, loss_dice: 0.399874
[09:52:19.513] TRAIN: iteration 13062 : loss : 0.115108, loss_ce: 0.005753, loss_dice: 0.224463
[09:52:19.720] TRAIN: iteration 13063 : loss : 0.089409, loss_ce: 0.005638, loss_dice: 0.173179
[09:52:19.927] TRAIN: iteration 13064 : loss : 0.166153, loss_ce: 0.006290, loss_dice: 0.326017
[09:52:20.140] TRAIN: iteration 13065 : loss : 0.257360, loss_ce: 0.013632, loss_dice: 0.501088
[09:52:21.672] TRAIN: iteration 13066 : loss : 0.166809, loss_ce: 0.009254, loss_dice: 0.324363
[09:52:21.880] TRAIN: iteration 13067 : loss : 0.155379, loss_ce: 0.004632, loss_dice: 0.306126
[09:52:22.088] TRAIN: iteration 13068 : loss : 0.250556, loss_ce: 0.004862, loss_dice: 0.496251
[09:52:22.302] TRAIN: iteration 13069 : loss : 0.156108, loss_ce: 0.007487, loss_dice: 0.304729
[09:52:22.517] TRAIN: iteration 13070 : loss : 0.107520, loss_ce: 0.003756, loss_dice: 0.211284
[09:52:22.727] TRAIN: iteration 13071 : loss : 0.243797, loss_ce: 0.005993, loss_dice: 0.481600
[09:52:22.935] TRAIN: iteration 13072 : loss : 0.086363, loss_ce: 0.002678, loss_dice: 0.170048
[09:52:23.144] TRAIN: iteration 13073 : loss : 0.231183, loss_ce: 0.003786, loss_dice: 0.458580
[09:52:23.352] TRAIN: iteration 13074 : loss : 0.068596, loss_ce: 0.009101, loss_dice: 0.128092
[09:52:23.565] TRAIN: iteration 13075 : loss : 0.227593, loss_ce: 0.003195, loss_dice: 0.451991
[09:52:23.950] TRAIN: iteration 13076 : loss : 0.199613, loss_ce: 0.003446, loss_dice: 0.395781
[09:52:24.158] TRAIN: iteration 13077 : loss : 0.104445, loss_ce: 0.010650, loss_dice: 0.198241
[09:52:24.365] TRAIN: iteration 13078 : loss : 0.250835, loss_ce: 0.001629, loss_dice: 0.500040
[09:52:24.575] TRAIN: iteration 13079 : loss : 0.217921, loss_ce: 0.008902, loss_dice: 0.426940
[09:52:24.790] TRAIN: iteration 13080 : loss : 0.250997, loss_ce: 0.001889, loss_dice: 0.500105
[09:52:25.030] TRAIN: iteration 13081 : loss : 0.250756, loss_ce: 0.001468, loss_dice: 0.500043
[09:52:25.239] TRAIN: iteration 13082 : loss : 0.096842, loss_ce: 0.005369, loss_dice: 0.188315
[09:52:25.448] TRAIN: iteration 13083 : loss : 0.098137, loss_ce: 0.003376, loss_dice: 0.192898
[09:52:25.833] TRAIN: iteration 13084 : loss : 0.064259, loss_ce: 0.003796, loss_dice: 0.124723
[09:52:26.044] TRAIN: iteration 13085 : loss : 0.217819, loss_ce: 0.030406, loss_dice: 0.405231
[09:52:26.252] TRAIN: iteration 13086 : loss : 0.081071, loss_ce: 0.001777, loss_dice: 0.160365
[09:52:26.459] TRAIN: iteration 13087 : loss : 0.162551, loss_ce: 0.001736, loss_dice: 0.323366
[09:52:26.670] TRAIN: iteration 13088 : loss : 0.106898, loss_ce: 0.007246, loss_dice: 0.206550
[09:52:26.876] TRAIN: iteration 13089 : loss : 0.161420, loss_ce: 0.002764, loss_dice: 0.320076
[09:52:27.083] TRAIN: iteration 13090 : loss : 0.114111, loss_ce: 0.002286, loss_dice: 0.225936
[09:52:27.292] TRAIN: iteration 13091 : loss : 0.061667, loss_ce: 0.003580, loss_dice: 0.119753
[09:52:28.215] TRAIN: iteration 13092 : loss : 0.249378, loss_ce: 0.004122, loss_dice: 0.494635
[09:52:28.428] TRAIN: iteration 13093 : loss : 0.161614, loss_ce: 0.004281, loss_dice: 0.318947
[09:52:28.637] TRAIN: iteration 13094 : loss : 0.208712, loss_ce: 0.004146, loss_dice: 0.413277
[09:52:28.851] TRAIN: iteration 13095 : loss : 0.153630, loss_ce: 0.008640, loss_dice: 0.298620
[09:52:29.060] TRAIN: iteration 13096 : loss : 0.248530, loss_ce: 0.006435, loss_dice: 0.490624
[09:52:29.267] TRAIN: iteration 13097 : loss : 0.112529, loss_ce: 0.006195, loss_dice: 0.218863
[09:52:29.475] TRAIN: iteration 13098 : loss : 0.254033, loss_ce: 0.008020, loss_dice: 0.500045
[09:52:29.686] TRAIN: iteration 13099 : loss : 0.145373, loss_ce: 0.005317, loss_dice: 0.285429
[09:52:30.801] TRAIN: iteration 13100 : loss : 0.209450, loss_ce: 0.006188, loss_dice: 0.412711
[09:52:31.041] TRAIN: iteration 13101 : loss : 0.254654, loss_ce: 0.008612, loss_dice: 0.500697
[09:52:31.251] TRAIN: iteration 13102 : loss : 0.094496, loss_ce: 0.024578, loss_dice: 0.164414
[09:52:31.458] TRAIN: iteration 13103 : loss : 0.187289, loss_ce: 0.007808, loss_dice: 0.366770
[09:52:31.664] TRAIN: iteration 13104 : loss : 0.062208, loss_ce: 0.007897, loss_dice: 0.116520
[09:52:31.877] TRAIN: iteration 13105 : loss : 0.113316, loss_ce: 0.006738, loss_dice: 0.219893
[09:52:32.085] TRAIN: iteration 13106 : loss : 0.250901, loss_ce: 0.009419, loss_dice: 0.492383
[09:52:32.293] TRAIN: iteration 13107 : loss : 0.171022, loss_ce: 0.007620, loss_dice: 0.334423
[09:52:32.504] TRAIN: iteration 13108 : loss : 0.246949, loss_ce: 0.007053, loss_dice: 0.486844
[09:52:32.711] TRAIN: iteration 13109 : loss : 0.218853, loss_ce: 0.005996, loss_dice: 0.431711
[09:52:32.920] TRAIN: iteration 13110 : loss : 0.249718, loss_ce: 0.011196, loss_dice: 0.488241
[09:52:33.129] TRAIN: iteration 13111 : loss : 0.093484, loss_ce: 0.010143, loss_dice: 0.176825
[09:52:34.189] TRAIN: iteration 13112 : loss : 0.213818, loss_ce: 0.003729, loss_dice: 0.423906
[09:52:34.402] TRAIN: iteration 13113 : loss : 0.157050, loss_ce: 0.006596, loss_dice: 0.307504
[09:52:34.707] TRAIN: iteration 13114 : loss : 0.105428, loss_ce: 0.007023, loss_dice: 0.203833
[09:52:34.915] TRAIN: iteration 13115 : loss : 0.199589, loss_ce: 0.004136, loss_dice: 0.395042
[09:52:35.421] TRAIN: iteration 13116 : loss : 0.035135, loss_ce: 0.004221, loss_dice: 0.066049
[09:52:35.631] TRAIN: iteration 13117 : loss : 0.252861, loss_ce: 0.005387, loss_dice: 0.500334
[09:52:35.845] TRAIN: iteration 13118 : loss : 0.073905, loss_ce: 0.007138, loss_dice: 0.140672
[09:52:36.060] TRAIN: iteration 13119 : loss : 0.104665, loss_ce: 0.003305, loss_dice: 0.206025
[09:52:36.327] TRAIN: iteration 13120 : loss : 0.129727, loss_ce: 0.004463, loss_dice: 0.254992
[09:52:36.577] TRAIN: iteration 13121 : loss : 0.162803, loss_ce: 0.005458, loss_dice: 0.320149
[09:52:36.791] TRAIN: iteration 13122 : loss : 0.251347, loss_ce: 0.002546, loss_dice: 0.500147
[09:52:37.006] TRAIN: iteration 13123 : loss : 0.117722, loss_ce: 0.002511, loss_dice: 0.232933
[09:52:37.215] TRAIN: iteration 13124 : loss : 0.064868, loss_ce: 0.002237, loss_dice: 0.127500
[09:52:37.422] TRAIN: iteration 13125 : loss : 0.111461, loss_ce: 0.008739, loss_dice: 0.214184
[09:52:37.630] TRAIN: iteration 13126 : loss : 0.115201, loss_ce: 0.009666, loss_dice: 0.220735
[09:52:37.840] TRAIN: iteration 13127 : loss : 0.079133, loss_ce: 0.002613, loss_dice: 0.155653
[09:52:38.230] TRAIN: iteration 13128 : loss : 0.165811, loss_ce: 0.005204, loss_dice: 0.326418
[09:52:38.441] TRAIN: iteration 13129 : loss : 0.114126, loss_ce: 0.008457, loss_dice: 0.219795
[09:52:38.653] TRAIN: iteration 13130 : loss : 0.205137, loss_ce: 0.002627, loss_dice: 0.407647
[09:52:38.860] TRAIN: iteration 13131 : loss : 0.226605, loss_ce: 0.008630, loss_dice: 0.444580
[09:52:39.216] TRAIN: iteration 13132 : loss : 0.182445, loss_ce: 0.002798, loss_dice: 0.362093
[09:52:39.424] TRAIN: iteration 13133 : loss : 0.087741, loss_ce: 0.002030, loss_dice: 0.173452
[09:52:39.630] TRAIN: iteration 13134 : loss : 0.253713, loss_ce: 0.008500, loss_dice: 0.498925
[09:52:39.836] TRAIN: iteration 13135 : loss : 0.059818, loss_ce: 0.002375, loss_dice: 0.117261
[09:52:40.051] TRAIN: iteration 13136 : loss : 0.125574, loss_ce: 0.002350, loss_dice: 0.248798
[09:52:40.259] TRAIN: iteration 13137 : loss : 0.115514, loss_ce: 0.004282, loss_dice: 0.226746
[09:52:40.468] TRAIN: iteration 13138 : loss : 0.060357, loss_ce: 0.003821, loss_dice: 0.116893
[09:52:40.949] TRAIN: iteration 13139 : loss : 0.249251, loss_ce: 0.007459, loss_dice: 0.491042
[09:52:42.266] TRAIN: iteration 13140 : loss : 0.172721, loss_ce: 0.003891, loss_dice: 0.341550
[09:52:42.502] TRAIN: iteration 13141 : loss : 0.056816, loss_ce: 0.004016, loss_dice: 0.109617
[09:52:42.709] TRAIN: iteration 13142 : loss : 0.187197, loss_ce: 0.006657, loss_dice: 0.367738
[09:52:42.927] TRAIN: iteration 13143 : loss : 0.136457, loss_ce: 0.011226, loss_dice: 0.261688
[09:52:43.135] TRAIN: iteration 13144 : loss : 0.251825, loss_ce: 0.003750, loss_dice: 0.499900
[09:52:43.344] TRAIN: iteration 13145 : loss : 0.135732, loss_ce: 0.004663, loss_dice: 0.266802
[09:52:43.552] TRAIN: iteration 13146 : loss : 0.156009, loss_ce: 0.005386, loss_dice: 0.306633
[09:52:43.760] TRAIN: iteration 13147 : loss : 0.053536, loss_ce: 0.004076, loss_dice: 0.102996
[09:52:44.137] TRAIN: iteration 13148 : loss : 0.231631, loss_ce: 0.003673, loss_dice: 0.459590
[09:52:44.349] TRAIN: iteration 13149 : loss : 0.051172, loss_ce: 0.004615, loss_dice: 0.097728
[09:52:44.557] TRAIN: iteration 13150 : loss : 0.232444, loss_ce: 0.005974, loss_dice: 0.458914
[09:52:44.765] TRAIN: iteration 13151 : loss : 0.033122, loss_ce: 0.002059, loss_dice: 0.064184
[09:52:44.972] TRAIN: iteration 13152 : loss : 0.214160, loss_ce: 0.003860, loss_dice: 0.424460
[09:52:45.184] TRAIN: iteration 13153 : loss : 0.225382, loss_ce: 0.004321, loss_dice: 0.446442
[09:52:45.392] TRAIN: iteration 13154 : loss : 0.121386, loss_ce: 0.003960, loss_dice: 0.238813
[09:52:45.598] TRAIN: iteration 13155 : loss : 0.030849, loss_ce: 0.002903, loss_dice: 0.058794
[09:52:46.514] TRAIN: iteration 13156 : loss : 0.247849, loss_ce: 0.004455, loss_dice: 0.491244
[09:52:46.722] TRAIN: iteration 13157 : loss : 0.028268, loss_ce: 0.002482, loss_dice: 0.054054
[09:52:46.929] TRAIN: iteration 13158 : loss : 0.244876, loss_ce: 0.004366, loss_dice: 0.485387
[09:52:47.137] TRAIN: iteration 13159 : loss : 0.196762, loss_ce: 0.003013, loss_dice: 0.390511
[09:52:47.345] TRAIN: iteration 13160 : loss : 0.090346, loss_ce: 0.008545, loss_dice: 0.172146
[09:52:48.242] TRAIN: iteration 13161 : loss : 0.114804, loss_ce: 0.007074, loss_dice: 0.222534
[09:52:48.451] TRAIN: iteration 13162 : loss : 0.130610, loss_ce: 0.009003, loss_dice: 0.252216
[09:52:48.660] TRAIN: iteration 13163 : loss : 0.079450, loss_ce: 0.002819, loss_dice: 0.156082
[09:52:50.025] TRAIN: iteration 13164 : loss : 0.085413, loss_ce: 0.006284, loss_dice: 0.164542
[09:52:50.238] TRAIN: iteration 13165 : loss : 0.097759, loss_ce: 0.002839, loss_dice: 0.192680
[09:52:50.449] TRAIN: iteration 13166 : loss : 0.196054, loss_ce: 0.004070, loss_dice: 0.388037
[09:52:50.657] TRAIN: iteration 13167 : loss : 0.176538, loss_ce: 0.003929, loss_dice: 0.349147
[09:52:50.863] TRAIN: iteration 13168 : loss : 0.135714, loss_ce: 0.002698, loss_dice: 0.268730
[09:52:51.690] TRAIN: iteration 13169 : loss : 0.067636, loss_ce: 0.003401, loss_dice: 0.131871
[09:52:51.906] TRAIN: iteration 13170 : loss : 0.091386, loss_ce: 0.004395, loss_dice: 0.178376
[09:52:52.114] TRAIN: iteration 13171 : loss : 0.071600, loss_ce: 0.005002, loss_dice: 0.138199
[09:52:52.460] TRAIN: iteration 13172 : loss : 0.252612, loss_ce: 0.005609, loss_dice: 0.499616
[09:52:52.669] TRAIN: iteration 13173 : loss : 0.217522, loss_ce: 0.003143, loss_dice: 0.431900
[09:52:52.877] TRAIN: iteration 13174 : loss : 0.131752, loss_ce: 0.007438, loss_dice: 0.256066
[09:52:53.087] TRAIN: iteration 13175 : loss : 0.116349, loss_ce: 0.003679, loss_dice: 0.229018
[09:52:53.298] TRAIN: iteration 13176 : loss : 0.127474, loss_ce: 0.005789, loss_dice: 0.249159
[09:52:53.505] TRAIN: iteration 13177 : loss : 0.096255, loss_ce: 0.004005, loss_dice: 0.188505
[09:52:53.713] TRAIN: iteration 13178 : loss : 0.131498, loss_ce: 0.003118, loss_dice: 0.259878
[09:52:53.921] TRAIN: iteration 13179 : loss : 0.172561, loss_ce: 0.006837, loss_dice: 0.338285
[09:52:55.206] TRAIN: iteration 13180 : loss : 0.053575, loss_ce: 0.002924, loss_dice: 0.104227
[09:52:55.439] TRAIN: iteration 13181 : loss : 0.123828, loss_ce: 0.003862, loss_dice: 0.243795
[09:52:55.645] TRAIN: iteration 13182 : loss : 0.142747, loss_ce: 0.006753, loss_dice: 0.278741
[09:52:55.859] TRAIN: iteration 13183 : loss : 0.183658, loss_ce: 0.003671, loss_dice: 0.363645
[09:52:56.071] TRAIN: iteration 13184 : loss : 0.251146, loss_ce: 0.002459, loss_dice: 0.499832
[09:52:56.278] TRAIN: iteration 13185 : loss : 0.251620, loss_ce: 0.003040, loss_dice: 0.500200
[09:52:56.487] TRAIN: iteration 13186 : loss : 0.122083, loss_ce: 0.004938, loss_dice: 0.239228
[09:52:56.694] TRAIN: iteration 13187 : loss : 0.250681, loss_ce: 0.003510, loss_dice: 0.497852
[09:52:57.676] TRAIN: iteration 13188 : loss : 0.059191, loss_ce: 0.004299, loss_dice: 0.114083
[09:52:57.884] TRAIN: iteration 13189 : loss : 0.158730, loss_ce: 0.006974, loss_dice: 0.310486
[09:52:58.095] TRAIN: iteration 13190 : loss : 0.082362, loss_ce: 0.004953, loss_dice: 0.159771
[09:52:58.309] TRAIN: iteration 13191 : loss : 0.060686, loss_ce: 0.002078, loss_dice: 0.119294
[09:52:58.519] TRAIN: iteration 13192 : loss : 0.119140, loss_ce: 0.002428, loss_dice: 0.235853
[09:52:58.726] TRAIN: iteration 13193 : loss : 0.053478, loss_ce: 0.005446, loss_dice: 0.101511
[09:52:58.937] TRAIN: iteration 13194 : loss : 0.119316, loss_ce: 0.001981, loss_dice: 0.236651
[09:52:59.147] TRAIN: iteration 13195 : loss : 0.133653, loss_ce: 0.005796, loss_dice: 0.261510
[09:52:59.361] TRAIN: iteration 13196 : loss : 0.139577, loss_ce: 0.002356, loss_dice: 0.276798
[09:52:59.568] TRAIN: iteration 13197 : loss : 0.114263, loss_ce: 0.004099, loss_dice: 0.224426
[09:52:59.780] TRAIN: iteration 13198 : loss : 0.186118, loss_ce: 0.003680, loss_dice: 0.368556
[09:52:59.988] TRAIN: iteration 13199 : loss : 0.147235, loss_ce: 0.005755, loss_dice: 0.288714
[09:53:00.195] TRAIN: iteration 13200 : loss : 0.097467, loss_ce: 0.006276, loss_dice: 0.188657
[09:53:00.439] TRAIN: iteration 13201 : loss : 0.251126, loss_ce: 0.002115, loss_dice: 0.500137
[09:53:00.831] TRAIN: iteration 13202 : loss : 0.100668, loss_ce: 0.004754, loss_dice: 0.196581
[09:53:01.039] TRAIN: iteration 13203 : loss : 0.049512, loss_ce: 0.003406, loss_dice: 0.095618
[09:53:01.897] TRAIN: iteration 13204 : loss : 0.031976, loss_ce: 0.001443, loss_dice: 0.062509
[09:53:02.105] TRAIN: iteration 13205 : loss : 0.157818, loss_ce: 0.007842, loss_dice: 0.307794
[09:53:02.312] TRAIN: iteration 13206 : loss : 0.184819, loss_ce: 0.006761, loss_dice: 0.362878
[09:53:02.520] TRAIN: iteration 13207 : loss : 0.109667, loss_ce: 0.011675, loss_dice: 0.207659
[09:53:02.728] TRAIN: iteration 13208 : loss : 0.093548, loss_ce: 0.007672, loss_dice: 0.179424
[09:53:02.939] TRAIN: iteration 13209 : loss : 0.232503, loss_ce: 0.007759, loss_dice: 0.457246
[09:53:03.147] TRAIN: iteration 13210 : loss : 0.105559, loss_ce: 0.004042, loss_dice: 0.207077
[09:53:03.354] TRAIN: iteration 13211 : loss : 0.225117, loss_ce: 0.003375, loss_dice: 0.446859
[09:53:04.008] TRAIN: iteration 13212 : loss : 0.066063, loss_ce: 0.006793, loss_dice: 0.125333
[09:53:05.303] TRAIN: iteration 13213 : loss : 0.087081, loss_ce: 0.002086, loss_dice: 0.172077
[09:53:05.511] TRAIN: iteration 13214 : loss : 0.252137, loss_ce: 0.004195, loss_dice: 0.500079
[09:53:05.721] TRAIN: iteration 13215 : loss : 0.251816, loss_ce: 0.003393, loss_dice: 0.500238
[09:53:05.928] TRAIN: iteration 13216 : loss : 0.092933, loss_ce: 0.003363, loss_dice: 0.182503
[09:53:06.136] TRAIN: iteration 13217 : loss : 0.126554, loss_ce: 0.007523, loss_dice: 0.245585
[09:53:06.343] TRAIN: iteration 13218 : loss : 0.195699, loss_ce: 0.005160, loss_dice: 0.386239
[09:53:06.551] TRAIN: iteration 13219 : loss : 0.235051, loss_ce: 0.004516, loss_dice: 0.465585
[09:53:06.760] TRAIN: iteration 13220 : loss : 0.251999, loss_ce: 0.003742, loss_dice: 0.500256
[09:53:06.761] NaN or Inf found in input tensor.
[09:53:06.978] TRAIN: iteration 13221 : loss : 0.252790, loss_ce: 0.005183, loss_dice: 0.500398
[09:53:07.186] TRAIN: iteration 13222 : loss : 0.124930, loss_ce: 0.002776, loss_dice: 0.247085
[09:53:07.400] TRAIN: iteration 13223 : loss : 0.057484, loss_ce: 0.002410, loss_dice: 0.112558
[09:53:07.609] TRAIN: iteration 13224 : loss : 0.089375, loss_ce: 0.002341, loss_dice: 0.176408
[09:53:07.818] TRAIN: iteration 13225 : loss : 0.251390, loss_ce: 0.002616, loss_dice: 0.500164
[09:53:08.028] TRAIN: iteration 13226 : loss : 0.078074, loss_ce: 0.005509, loss_dice: 0.150639
[09:53:08.236] TRAIN: iteration 13227 : loss : 0.096035, loss_ce: 0.004628, loss_dice: 0.187442
[09:53:09.964] TRAIN: iteration 13228 : loss : 0.250628, loss_ce: 0.001221, loss_dice: 0.500036
[09:53:10.171] TRAIN: iteration 13229 : loss : 0.101185, loss_ce: 0.002003, loss_dice: 0.200368
[09:53:10.378] TRAIN: iteration 13230 : loss : 0.085293, loss_ce: 0.003894, loss_dice: 0.166692
[09:53:10.586] TRAIN: iteration 13231 : loss : 0.071325, loss_ce: 0.005826, loss_dice: 0.136823
[09:53:10.793] TRAIN: iteration 13232 : loss : 0.122312, loss_ce: 0.012741, loss_dice: 0.231882
[09:53:11.002] TRAIN: iteration 13233 : loss : 0.251147, loss_ce: 0.002149, loss_dice: 0.500146
[09:53:11.210] TRAIN: iteration 13234 : loss : 0.120701, loss_ce: 0.012096, loss_dice: 0.229306
[09:53:11.431] TRAIN: iteration 13235 : loss : 0.113489, loss_ce: 0.004966, loss_dice: 0.222012
[09:53:12.243] TRAIN: iteration 13236 : loss : 0.094323, loss_ce: 0.002492, loss_dice: 0.186154
[09:53:12.450] TRAIN: iteration 13237 : loss : 0.217912, loss_ce: 0.002446, loss_dice: 0.433378
[09:53:12.657] TRAIN: iteration 13238 : loss : 0.251550, loss_ce: 0.002895, loss_dice: 0.500206
[09:53:12.885] TRAIN: iteration 13239 : loss : 0.192427, loss_ce: 0.002779, loss_dice: 0.382076
[09:53:13.095] TRAIN: iteration 13240 : loss : 0.145158, loss_ce: 0.003873, loss_dice: 0.286443
[09:53:15.205] TRAIN: iteration 13241 : loss : 0.225306, loss_ce: 0.004433, loss_dice: 0.446179
[09:53:15.413] TRAIN: iteration 13242 : loss : 0.211778, loss_ce: 0.003037, loss_dice: 0.420519
[09:53:15.621] TRAIN: iteration 13243 : loss : 0.128570, loss_ce: 0.005355, loss_dice: 0.251786
[09:53:15.836] TRAIN: iteration 13244 : loss : 0.126948, loss_ce: 0.004270, loss_dice: 0.249627
[09:53:16.045] TRAIN: iteration 13245 : loss : 0.246781, loss_ce: 0.006938, loss_dice: 0.486623
[09:53:16.254] TRAIN: iteration 13246 : loss : 0.084474, loss_ce: 0.001691, loss_dice: 0.167257
[09:53:16.462] TRAIN: iteration 13247 : loss : 0.251150, loss_ce: 0.002170, loss_dice: 0.500129
[09:53:16.670] TRAIN: iteration 13248 : loss : 0.238118, loss_ce: 0.004228, loss_dice: 0.472008
[09:53:17.311] TRAIN: iteration 13249 : loss : 0.068732, loss_ce: 0.004317, loss_dice: 0.133148
[09:53:17.518] TRAIN: iteration 13250 : loss : 0.116894, loss_ce: 0.002590, loss_dice: 0.231198
[09:53:17.725] TRAIN: iteration 13251 : loss : 0.052955, loss_ce: 0.002471, loss_dice: 0.103439
[09:53:17.932] TRAIN: iteration 13252 : loss : 0.089536, loss_ce: 0.004611, loss_dice: 0.174460
[09:53:18.140] TRAIN: iteration 13253 : loss : 0.106219, loss_ce: 0.003883, loss_dice: 0.208555
[09:53:18.348] TRAIN: iteration 13254 : loss : 0.042677, loss_ce: 0.001450, loss_dice: 0.083904
[09:53:18.554] TRAIN: iteration 13255 : loss : 0.251478, loss_ce: 0.003748, loss_dice: 0.499208
[09:53:18.765] TRAIN: iteration 13256 : loss : 0.250013, loss_ce: 0.002102, loss_dice: 0.497925
[09:53:20.000] TRAIN: iteration 13257 : loss : 0.224012, loss_ce: 0.001762, loss_dice: 0.446261
[09:53:21.163] TRAIN: iteration 13258 : loss : 0.103346, loss_ce: 0.006319, loss_dice: 0.200373
[09:53:21.370] TRAIN: iteration 13259 : loss : 0.251432, loss_ce: 0.075880, loss_dice: 0.426983
[09:53:21.577] TRAIN: iteration 13260 : loss : 0.051551, loss_ce: 0.001233, loss_dice: 0.101869
[09:53:21.812] TRAIN: iteration 13261 : loss : 0.185413, loss_ce: 0.002065, loss_dice: 0.368761
[09:53:22.020] TRAIN: iteration 13262 : loss : 0.097248, loss_ce: 0.006593, loss_dice: 0.187904
[09:53:22.226] TRAIN: iteration 13263 : loss : 0.099865, loss_ce: 0.006368, loss_dice: 0.193362
[09:53:22.434] TRAIN: iteration 13264 : loss : 0.100501, loss_ce: 0.005404, loss_dice: 0.195599
[09:53:22.640] TRAIN: iteration 13265 : loss : 0.130161, loss_ce: 0.003573, loss_dice: 0.256748
[09:53:24.538] TRAIN: iteration 13266 : loss : 0.148938, loss_ce: 0.002819, loss_dice: 0.295058
[09:53:24.753] TRAIN: iteration 13267 : loss : 0.158413, loss_ce: 0.002871, loss_dice: 0.313956
[09:53:24.962] TRAIN: iteration 13268 : loss : 0.205091, loss_ce: 0.002664, loss_dice: 0.407519
[09:53:25.173] TRAIN: iteration 13269 : loss : 0.218149, loss_ce: 0.011039, loss_dice: 0.425258
[09:53:25.382] TRAIN: iteration 13270 : loss : 0.149441, loss_ce: 0.003029, loss_dice: 0.295853
[09:53:25.595] TRAIN: iteration 13271 : loss : 0.123253, loss_ce: 0.017059, loss_dice: 0.229448
[09:53:25.804] TRAIN: iteration 13272 : loss : 0.153729, loss_ce: 0.005724, loss_dice: 0.301734
[09:53:26.015] TRAIN: iteration 13273 : loss : 0.083541, loss_ce: 0.006465, loss_dice: 0.160617
[09:53:26.618] TRAIN: iteration 13274 : loss : 0.251699, loss_ce: 0.005081, loss_dice: 0.498317
[09:53:26.825] TRAIN: iteration 13275 : loss : 0.067113, loss_ce: 0.003886, loss_dice: 0.130340
[09:53:27.034] TRAIN: iteration 13276 : loss : 0.100420, loss_ce: 0.004843, loss_dice: 0.195996
[09:53:27.248] TRAIN: iteration 13277 : loss : 0.243998, loss_ce: 0.013550, loss_dice: 0.474445
[09:53:27.455] TRAIN: iteration 13278 : loss : 0.207808, loss_ce: 0.004723, loss_dice: 0.410894
[09:53:27.663] TRAIN: iteration 13279 : loss : 0.067334, loss_ce: 0.005161, loss_dice: 0.129507
[09:53:27.877] TRAIN: iteration 13280 : loss : 0.084911, loss_ce: 0.005791, loss_dice: 0.164032
[09:53:28.185] TRAIN: iteration 13281 : loss : 0.090162, loss_ce: 0.003849, loss_dice: 0.176476
[09:53:28.638] TRAIN: iteration 13282 : loss : 0.210731, loss_ce: 0.005647, loss_dice: 0.415815
[09:53:28.845] TRAIN: iteration 13283 : loss : 0.213880, loss_ce: 0.006154, loss_dice: 0.421607
[09:53:29.054] TRAIN: iteration 13284 : loss : 0.095474, loss_ce: 0.004817, loss_dice: 0.186130
[09:53:29.263] TRAIN: iteration 13285 : loss : 0.212187, loss_ce: 0.006145, loss_dice: 0.418228
[09:53:29.476] TRAIN: iteration 13286 : loss : 0.199349, loss_ce: 0.005148, loss_dice: 0.393549
[09:53:29.686] TRAIN: iteration 13287 : loss : 0.067353, loss_ce: 0.004734, loss_dice: 0.129973
[09:53:30.562] TRAIN: iteration 13288 : loss : 0.101754, loss_ce: 0.006462, loss_dice: 0.197046
[09:53:31.261] TRAIN: iteration 13289 : loss : 0.226089, loss_ce: 0.003365, loss_dice: 0.448813
[09:53:31.468] TRAIN: iteration 13290 : loss : 0.133394, loss_ce: 0.003777, loss_dice: 0.263011
[09:53:31.675] TRAIN: iteration 13291 : loss : 0.237102, loss_ce: 0.009712, loss_dice: 0.464491
[09:53:31.890] TRAIN: iteration 13292 : loss : 0.251546, loss_ce: 0.002926, loss_dice: 0.500165
[09:53:32.100] TRAIN: iteration 13293 : loss : 0.252983, loss_ce: 0.018552, loss_dice: 0.487414
[09:53:32.313] TRAIN: iteration 13294 : loss : 0.118943, loss_ce: 0.003244, loss_dice: 0.234643
[09:53:32.521] TRAIN: iteration 13295 : loss : 0.233357, loss_ce: 0.002632, loss_dice: 0.464083
[09:53:33.121] TRAIN: iteration 13296 : loss : 0.180438, loss_ce: 0.004646, loss_dice: 0.356230
[09:53:34.366] TRAIN: iteration 13297 : loss : 0.115741, loss_ce: 0.002461, loss_dice: 0.229022
[09:53:34.573] TRAIN: iteration 13298 : loss : 0.103562, loss_ce: 0.006909, loss_dice: 0.200216
[09:53:34.786] TRAIN: iteration 13299 : loss : 0.196571, loss_ce: 0.022700, loss_dice: 0.370443
[09:53:34.993] TRAIN: iteration 13300 : loss : 0.103147, loss_ce: 0.003034, loss_dice: 0.203260
[09:53:35.362] TRAIN: iteration 13301 : loss : 0.174544, loss_ce: 0.008123, loss_dice: 0.340965
[09:53:35.576] TRAIN: iteration 13302 : loss : 0.198153, loss_ce: 0.003761, loss_dice: 0.392545
[09:53:35.783] TRAIN: iteration 13303 : loss : 0.102026, loss_ce: 0.006284, loss_dice: 0.197768
[09:53:35.991] TRAIN: iteration 13304 : loss : 0.079976, loss_ce: 0.002692, loss_dice: 0.157260
[09:53:36.199] TRAIN: iteration 13305 : loss : 0.125750, loss_ce: 0.007635, loss_dice: 0.243866
[09:53:36.408] TRAIN: iteration 13306 : loss : 0.147443, loss_ce: 0.003913, loss_dice: 0.290973
[09:53:36.618] TRAIN: iteration 13307 : loss : 0.132897, loss_ce: 0.006162, loss_dice: 0.259631
[09:53:36.826] TRAIN: iteration 13308 : loss : 0.117000, loss_ce: 0.007790, loss_dice: 0.226210
[09:53:37.035] TRAIN: iteration 13309 : loss : 0.067236, loss_ce: 0.003877, loss_dice: 0.130596
[09:53:37.243] TRAIN: iteration 13310 : loss : 0.151748, loss_ce: 0.003764, loss_dice: 0.299732
[09:53:37.457] TRAIN: iteration 13311 : loss : 0.252051, loss_ce: 0.003857, loss_dice: 0.500246
[09:53:37.665] TRAIN: iteration 13312 : loss : 0.037631, loss_ce: 0.002761, loss_dice: 0.072502
[09:53:37.901] TRAIN: iteration 13313 : loss : 0.033139, loss_ce: 0.003550, loss_dice: 0.062728
[09:53:39.627] TRAIN: iteration 13314 : loss : 0.197852, loss_ce: 0.004700, loss_dice: 0.391005
[09:53:39.834] TRAIN: iteration 13315 : loss : 0.252672, loss_ce: 0.005008, loss_dice: 0.500337
[09:53:40.043] TRAIN: iteration 13316 : loss : 0.044576, loss_ce: 0.003456, loss_dice: 0.085696
[09:53:40.283] TRAIN: iteration 13317 : loss : 0.119872, loss_ce: 0.011848, loss_dice: 0.227895
[09:53:41.485] TRAIN: iteration 13318 : loss : 0.252616, loss_ce: 0.004891, loss_dice: 0.500341
[09:53:41.775] TRAIN: iteration 13319 : loss : 0.154122, loss_ce: 0.009410, loss_dice: 0.298834
[09:53:41.981] TRAIN: iteration 13320 : loss : 0.195622, loss_ce: 0.005373, loss_dice: 0.385872
[09:53:42.217] TRAIN: iteration 13321 : loss : 0.136977, loss_ce: 0.004164, loss_dice: 0.269791
[09:53:42.423] TRAIN: iteration 13322 : loss : 0.127800, loss_ce: 0.009052, loss_dice: 0.246549
[09:53:42.630] TRAIN: iteration 13323 : loss : 0.249826, loss_ce: 0.009168, loss_dice: 0.490485
[09:53:42.839] TRAIN: iteration 13324 : loss : 0.199199, loss_ce: 0.003793, loss_dice: 0.394605
[09:53:43.051] TRAIN: iteration 13325 : loss : 0.243893, loss_ce: 0.007585, loss_dice: 0.480201
[09:53:43.258] TRAIN: iteration 13326 : loss : 0.160408, loss_ce: 0.006415, loss_dice: 0.314401
[09:53:46.236] TRAIN: iteration 13327 : loss : 0.123368, loss_ce: 0.006442, loss_dice: 0.240295
[09:53:46.443] TRAIN: iteration 13328 : loss : 0.252779, loss_ce: 0.005204, loss_dice: 0.500355
[09:53:46.650] TRAIN: iteration 13329 : loss : 0.165896, loss_ce: 0.004830, loss_dice: 0.326963
[09:53:46.857] TRAIN: iteration 13330 : loss : 0.121214, loss_ce: 0.005873, loss_dice: 0.236555
[09:53:47.065] TRAIN: iteration 13331 : loss : 0.095463, loss_ce: 0.007491, loss_dice: 0.183435
[09:53:47.272] TRAIN: iteration 13332 : loss : 0.225354, loss_ce: 0.005909, loss_dice: 0.444800
[09:53:47.479] TRAIN: iteration 13333 : loss : 0.143663, loss_ce: 0.005795, loss_dice: 0.281532
[09:53:47.686] TRAIN: iteration 13334 : loss : 0.236268, loss_ce: 0.006540, loss_dice: 0.465996
[09:53:48.390] TRAIN: iteration 13335 : loss : 0.252188, loss_ce: 0.004082, loss_dice: 0.500295
[09:53:48.597] TRAIN: iteration 13336 : loss : 0.112216, loss_ce: 0.002517, loss_dice: 0.221916
[09:53:48.805] TRAIN: iteration 13337 : loss : 0.069359, loss_ce: 0.009214, loss_dice: 0.129504
[09:53:49.012] TRAIN: iteration 13338 : loss : 0.153106, loss_ce: 0.004715, loss_dice: 0.301496
[09:53:49.226] TRAIN: iteration 13339 : loss : 0.092849, loss_ce: 0.003431, loss_dice: 0.182266
[09:53:49.438] TRAIN: iteration 13340 : loss : 0.222704, loss_ce: 0.004216, loss_dice: 0.441192
[09:53:49.686] TRAIN: iteration 13341 : loss : 0.147850, loss_ce: 0.008147, loss_dice: 0.287553
[09:53:49.898] TRAIN: iteration 13342 : loss : 0.244380, loss_ce: 0.005949, loss_dice: 0.482811
[09:53:50.443] TRAIN: iteration 13343 : loss : 0.063125, loss_ce: 0.004452, loss_dice: 0.121798
[09:53:50.654] TRAIN: iteration 13344 : loss : 0.150519, loss_ce: 0.004241, loss_dice: 0.296797
[09:53:50.865] TRAIN: iteration 13345 : loss : 0.139443, loss_ce: 0.006815, loss_dice: 0.272071
[09:53:53.356] TRAIN: iteration 13346 : loss : 0.235599, loss_ce: 0.003470, loss_dice: 0.467729
[09:53:53.570] TRAIN: iteration 13347 : loss : 0.231558, loss_ce: 0.003959, loss_dice: 0.459156
[09:53:53.778] TRAIN: iteration 13348 : loss : 0.065514, loss_ce: 0.002560, loss_dice: 0.128468
[09:53:53.985] TRAIN: iteration 13349 : loss : 0.086188, loss_ce: 0.003445, loss_dice: 0.168930
[09:53:54.194] TRAIN: iteration 13350 : loss : 0.142608, loss_ce: 0.028212, loss_dice: 0.257005
[09:53:54.479] TRAIN: iteration 13351 : loss : 0.185462, loss_ce: 0.002947, loss_dice: 0.367976
[09:53:54.688] TRAIN: iteration 13352 : loss : 0.251363, loss_ce: 0.002592, loss_dice: 0.500134
[09:53:54.898] TRAIN: iteration 13353 : loss : 0.069942, loss_ce: 0.004435, loss_dice: 0.135449
[09:53:55.121] TRAIN: iteration 13354 : loss : 0.082968, loss_ce: 0.008346, loss_dice: 0.157590
[09:53:55.330] TRAIN: iteration 13355 : loss : 0.069674, loss_ce: 0.001753, loss_dice: 0.137595
[09:53:55.537] TRAIN: iteration 13356 : loss : 0.186939, loss_ce: 0.006832, loss_dice: 0.367047
[09:53:55.797] TRAIN: iteration 13357 : loss : 0.250962, loss_ce: 0.001852, loss_dice: 0.500071
[09:53:56.004] TRAIN: iteration 13358 : loss : 0.145515, loss_ce: 0.002840, loss_dice: 0.288191
[09:53:58.468] TRAIN: iteration 13359 : loss : 0.104506, loss_ce: 0.006636, loss_dice: 0.202376
[09:53:58.675] TRAIN: iteration 13360 : loss : 0.079759, loss_ce: 0.002229, loss_dice: 0.157289
[09:53:58.928] TRAIN: iteration 13361 : loss : 0.155457, loss_ce: 0.008260, loss_dice: 0.302654
[09:53:59.136] TRAIN: iteration 13362 : loss : 0.083052, loss_ce: 0.002531, loss_dice: 0.163573
[09:53:59.345] TRAIN: iteration 13363 : loss : 0.113303, loss_ce: 0.005695, loss_dice: 0.220910
[09:53:59.557] TRAIN: iteration 13364 : loss : 0.094187, loss_ce: 0.010567, loss_dice: 0.177807
[09:53:59.766] TRAIN: iteration 13365 : loss : 0.071513, loss_ce: 0.007550, loss_dice: 0.135477
[09:53:59.974] TRAIN: iteration 13366 : loss : 0.083711, loss_ce: 0.009770, loss_dice: 0.157652
[09:54:01.052] TRAIN: iteration 13367 : loss : 0.142862, loss_ce: 0.015701, loss_dice: 0.270024
[09:54:01.261] TRAIN: iteration 13368 : loss : 0.108816, loss_ce: 0.008108, loss_dice: 0.209524
[09:54:01.469] TRAIN: iteration 13369 : loss : 0.147830, loss_ce: 0.005565, loss_dice: 0.290096
[09:54:01.678] TRAIN: iteration 13370 : loss : 0.227631, loss_ce: 0.005868, loss_dice: 0.449394
[09:54:01.886] TRAIN: iteration 13371 : loss : 0.047137, loss_ce: 0.005160, loss_dice: 0.089115
[09:54:02.116] TRAIN: iteration 13372 : loss : 0.181290, loss_ce: 0.004781, loss_dice: 0.357800
[09:54:02.326] TRAIN: iteration 13373 : loss : 0.156440, loss_ce: 0.008454, loss_dice: 0.304425
[09:54:02.534] TRAIN: iteration 13374 : loss : 0.080014, loss_ce: 0.002208, loss_dice: 0.157820
[09:54:06.410] TRAIN: iteration 13375 : loss : 0.151878, loss_ce: 0.006647, loss_dice: 0.297109
[09:54:06.617] TRAIN: iteration 13376 : loss : 0.111406, loss_ce: 0.012393, loss_dice: 0.210418
[09:54:06.825] TRAIN: iteration 13377 : loss : 0.046244, loss_ce: 0.003144, loss_dice: 0.089345
[09:54:07.033] TRAIN: iteration 13378 : loss : 0.105308, loss_ce: 0.003642, loss_dice: 0.206973
[09:54:07.241] TRAIN: iteration 13379 : loss : 0.088263, loss_ce: 0.004804, loss_dice: 0.171722
[09:54:07.448] TRAIN: iteration 13380 : loss : 0.223257, loss_ce: 0.003895, loss_dice: 0.442620
[09:54:07.685] TRAIN: iteration 13381 : loss : 0.044381, loss_ce: 0.003821, loss_dice: 0.084941
[09:54:07.894] TRAIN: iteration 13382 : loss : 0.251388, loss_ce: 0.002597, loss_dice: 0.500178
[09:54:09.911] TRAIN: iteration 13383 : loss : 0.058091, loss_ce: 0.002325, loss_dice: 0.113857
[09:54:10.118] TRAIN: iteration 13384 : loss : 0.217277, loss_ce: 0.002480, loss_dice: 0.432075
[09:54:10.326] TRAIN: iteration 13385 : loss : 0.146641, loss_ce: 0.003165, loss_dice: 0.290118
[09:54:10.534] TRAIN: iteration 13386 : loss : 0.103597, loss_ce: 0.004945, loss_dice: 0.202249
[09:54:10.745] TRAIN: iteration 13387 : loss : 0.227954, loss_ce: 0.040370, loss_dice: 0.415537
[09:54:10.952] TRAIN: iteration 13388 : loss : 0.159765, loss_ce: 0.016495, loss_dice: 0.303035
[09:54:11.161] TRAIN: iteration 13389 : loss : 0.167864, loss_ce: 0.002508, loss_dice: 0.333220
[09:54:11.369] TRAIN: iteration 13390 : loss : 0.084579, loss_ce: 0.001488, loss_dice: 0.167671
[09:54:13.900] TRAIN: iteration 13391 : loss : 0.083215, loss_ce: 0.005805, loss_dice: 0.160624
[09:54:14.114] TRAIN: iteration 13392 : loss : 0.164942, loss_ce: 0.011559, loss_dice: 0.318324
[09:54:14.327] TRAIN: iteration 13393 : loss : 0.145375, loss_ce: 0.003876, loss_dice: 0.286875
[09:54:14.540] TRAIN: iteration 13394 : loss : 0.061268, loss_ce: 0.003154, loss_dice: 0.119381
[09:54:14.750] TRAIN: iteration 13395 : loss : 0.158596, loss_ce: 0.010899, loss_dice: 0.306294
[09:54:14.960] TRAIN: iteration 13396 : loss : 0.231228, loss_ce: 0.006178, loss_dice: 0.456279
[09:54:15.171] TRAIN: iteration 13397 : loss : 0.181809, loss_ce: 0.017560, loss_dice: 0.346057
[09:54:15.378] TRAIN: iteration 13398 : loss : 0.247789, loss_ce: 0.009391, loss_dice: 0.486187
[09:54:16.895] TRAIN: iteration 13399 : loss : 0.248873, loss_ce: 0.004760, loss_dice: 0.492985
[09:54:17.104] TRAIN: iteration 13400 : loss : 0.207446, loss_ce: 0.007051, loss_dice: 0.407841
[09:54:17.104] NaN or Inf found in input tensor.
[09:54:17.318] TRAIN: iteration 13401 : loss : 0.210403, loss_ce: 0.003641, loss_dice: 0.417166
[09:54:17.526] TRAIN: iteration 13402 : loss : 0.197377, loss_ce: 0.003304, loss_dice: 0.391449
[09:54:17.735] TRAIN: iteration 13403 : loss : 0.137757, loss_ce: 0.014175, loss_dice: 0.261339
[09:54:17.943] TRAIN: iteration 13404 : loss : 0.251532, loss_ce: 0.002913, loss_dice: 0.500151
[09:54:18.151] TRAIN: iteration 13405 : loss : 0.094153, loss_ce: 0.005416, loss_dice: 0.182891
[09:54:18.358] TRAIN: iteration 13406 : loss : 0.219342, loss_ce: 0.004836, loss_dice: 0.433848
[09:54:18.567] TRAIN: iteration 13407 : loss : 0.037288, loss_ce: 0.002000, loss_dice: 0.072576
[09:54:19.433] TRAIN: iteration 13408 : loss : 0.057318, loss_ce: 0.003655, loss_dice: 0.110981
[09:54:19.640] TRAIN: iteration 13409 : loss : 0.093955, loss_ce: 0.002855, loss_dice: 0.185055
[09:54:19.847] TRAIN: iteration 13410 : loss : 0.070048, loss_ce: 0.003419, loss_dice: 0.136677
[09:54:20.053] TRAIN: iteration 13411 : loss : 0.231542, loss_ce: 0.003440, loss_dice: 0.459645
[09:54:20.260] TRAIN: iteration 13412 : loss : 0.083782, loss_ce: 0.003021, loss_dice: 0.164543
[09:54:20.468] TRAIN: iteration 13413 : loss : 0.249838, loss_ce: 0.003599, loss_dice: 0.496078
[09:54:20.676] TRAIN: iteration 13414 : loss : 0.252982, loss_ce: 0.005556, loss_dice: 0.500408
[09:54:21.073] TRAIN: iteration 13415 : loss : 0.144804, loss_ce: 0.005714, loss_dice: 0.283893
[09:54:22.123] TRAIN: iteration 13416 : loss : 0.251716, loss_ce: 0.003233, loss_dice: 0.500200
[09:54:22.330] TRAIN: iteration 13417 : loss : 0.195241, loss_ce: 0.003753, loss_dice: 0.386729
[09:54:22.547] TRAIN: iteration 13418 : loss : 0.075389, loss_ce: 0.005019, loss_dice: 0.145759
[09:54:22.754] TRAIN: iteration 13419 : loss : 0.226542, loss_ce: 0.003242, loss_dice: 0.449842
[09:54:22.963] TRAIN: iteration 13420 : loss : 0.151434, loss_ce: 0.025953, loss_dice: 0.276915
[09:54:23.207] TRAIN: iteration 13421 : loss : 0.224393, loss_ce: 0.005431, loss_dice: 0.443355
[09:54:23.414] TRAIN: iteration 13422 : loss : 0.157905, loss_ce: 0.004143, loss_dice: 0.311666
[09:54:23.622] TRAIN: iteration 13423 : loss : 0.252620, loss_ce: 0.004896, loss_dice: 0.500344
[09:54:24.124] TRAIN: iteration 13424 : loss : 0.251317, loss_ce: 0.002540, loss_dice: 0.500094
[09:54:24.331] TRAIN: iteration 13425 : loss : 0.079257, loss_ce: 0.002367, loss_dice: 0.156147
[09:54:24.537] TRAIN: iteration 13426 : loss : 0.105779, loss_ce: 0.004391, loss_dice: 0.207168
[09:54:25.978] TRAIN: iteration 13427 : loss : 0.042693, loss_ce: 0.001700, loss_dice: 0.083687
[09:54:26.187] TRAIN: iteration 13428 : loss : 0.096214, loss_ce: 0.013243, loss_dice: 0.179184
[09:54:26.396] TRAIN: iteration 13429 : loss : 0.094552, loss_ce: 0.009886, loss_dice: 0.179217
[09:54:26.745] TRAIN: iteration 13430 : loss : 0.096243, loss_ce: 0.007060, loss_dice: 0.185427
[09:54:26.954] TRAIN: iteration 13431 : loss : 0.168667, loss_ce: 0.002167, loss_dice: 0.335166
[09:54:28.262] TRAIN: iteration 13432 : loss : 0.050421, loss_ce: 0.005500, loss_dice: 0.095341
[09:54:28.470] TRAIN: iteration 13433 : loss : 0.114882, loss_ce: 0.005283, loss_dice: 0.224480
[09:54:28.678] TRAIN: iteration 13434 : loss : 0.087291, loss_ce: 0.007843, loss_dice: 0.166739
[09:54:28.886] TRAIN: iteration 13435 : loss : 0.173468, loss_ce: 0.006684, loss_dice: 0.340251
[09:54:29.093] TRAIN: iteration 13436 : loss : 0.141636, loss_ce: 0.003411, loss_dice: 0.279861
[09:54:30.405] TRAIN: iteration 13437 : loss : 0.209848, loss_ce: 0.006940, loss_dice: 0.412756
[09:54:30.644] TRAIN: iteration 13438 : loss : 0.079585, loss_ce: 0.003755, loss_dice: 0.155415
[09:54:30.853] TRAIN: iteration 13439 : loss : 0.132051, loss_ce: 0.016929, loss_dice: 0.247173
[09:54:31.863] TRAIN: iteration 13440 : loss : 0.111709, loss_ce: 0.004888, loss_dice: 0.218529
[09:54:32.103] TRAIN: iteration 13441 : loss : 0.107435, loss_ce: 0.002651, loss_dice: 0.212219
[09:54:32.310] TRAIN: iteration 13442 : loss : 0.108225, loss_ce: 0.002425, loss_dice: 0.214025
[09:54:32.517] TRAIN: iteration 13443 : loss : 0.074565, loss_ce: 0.002337, loss_dice: 0.146793
[09:54:32.730] TRAIN: iteration 13444 : loss : 0.048333, loss_ce: 0.001506, loss_dice: 0.095161
[09:54:33.505] TRAIN: iteration 13445 : loss : 0.091756, loss_ce: 0.004551, loss_dice: 0.178961
[09:54:33.712] TRAIN: iteration 13446 : loss : 0.171477, loss_ce: 0.005891, loss_dice: 0.337063
[09:54:33.919] TRAIN: iteration 13447 : loss : 0.157020, loss_ce: 0.003243, loss_dice: 0.310797
[09:54:34.127] TRAIN: iteration 13448 : loss : 0.251013, loss_ce: 0.001911, loss_dice: 0.500115
[09:54:34.336] TRAIN: iteration 13449 : loss : 0.091435, loss_ce: 0.004413, loss_dice: 0.178457
[09:54:35.286] TRAIN: iteration 13450 : loss : 0.203187, loss_ce: 0.003942, loss_dice: 0.402432
[09:54:35.493] TRAIN: iteration 13451 : loss : 0.198387, loss_ce: 0.012511, loss_dice: 0.384264
[09:54:35.700] TRAIN: iteration 13452 : loss : 0.252684, loss_ce: 0.004983, loss_dice: 0.500386
[09:54:36.852] TRAIN: iteration 13453 : loss : 0.039584, loss_ce: 0.001769, loss_dice: 0.077399
[09:54:37.059] TRAIN: iteration 13454 : loss : 0.074109, loss_ce: 0.010550, loss_dice: 0.137669
[09:54:37.267] TRAIN: iteration 13455 : loss : 0.187980, loss_ce: 0.015625, loss_dice: 0.360336
[09:54:37.474] TRAIN: iteration 13456 : loss : 0.188541, loss_ce: 0.002944, loss_dice: 0.374139
[09:54:37.681] TRAIN: iteration 13457 : loss : 0.207237, loss_ce: 0.005193, loss_dice: 0.409281
[09:54:38.440] TRAIN: iteration 13458 : loss : 0.236911, loss_ce: 0.015954, loss_dice: 0.457867
[09:54:38.653] TRAIN: iteration 13459 : loss : 0.030280, loss_ce: 0.000959, loss_dice: 0.059600
[09:54:38.862] TRAIN: iteration 13460 : loss : 0.148410, loss_ce: 0.013843, loss_dice: 0.282976
[09:54:39.113] TRAIN: iteration 13461 : loss : 0.147213, loss_ce: 0.016680, loss_dice: 0.277746
[09:54:39.320] TRAIN: iteration 13462 : loss : 0.140963, loss_ce: 0.003267, loss_dice: 0.278659
[09:54:39.528] TRAIN: iteration 13463 : loss : 0.116671, loss_ce: 0.005254, loss_dice: 0.228087
[09:54:39.735] TRAIN: iteration 13464 : loss : 0.125535, loss_ce: 0.004700, loss_dice: 0.246370
[09:54:39.942] TRAIN: iteration 13465 : loss : 0.102536, loss_ce: 0.008306, loss_dice: 0.196767
[09:54:45.157] TRAIN: iteration 13466 : loss : 0.198322, loss_ce: 0.006312, loss_dice: 0.390333
[09:54:45.365] TRAIN: iteration 13467 : loss : 0.108921, loss_ce: 0.005528, loss_dice: 0.212313
[09:54:45.573] TRAIN: iteration 13468 : loss : 0.069898, loss_ce: 0.007802, loss_dice: 0.131995
[09:54:45.783] TRAIN: iteration 13469 : loss : 0.049208, loss_ce: 0.001918, loss_dice: 0.096497
[09:54:45.990] TRAIN: iteration 13470 : loss : 0.238272, loss_ce: 0.005275, loss_dice: 0.471268
[09:54:46.198] TRAIN: iteration 13471 : loss : 0.232577, loss_ce: 0.006718, loss_dice: 0.458436
[09:54:46.406] TRAIN: iteration 13472 : loss : 0.226179, loss_ce: 0.007251, loss_dice: 0.445107
[09:54:46.613] TRAIN: iteration 13473 : loss : 0.123747, loss_ce: 0.004876, loss_dice: 0.242619
[09:54:50.083] TRAIN: iteration 13474 : loss : 0.251429, loss_ce: 0.002707, loss_dice: 0.500152
[09:54:50.294] TRAIN: iteration 13475 : loss : 0.098949, loss_ce: 0.010420, loss_dice: 0.187477
[09:54:50.502] TRAIN: iteration 13476 : loss : 0.049651, loss_ce: 0.004684, loss_dice: 0.094618
[09:54:50.710] TRAIN: iteration 13477 : loss : 0.208707, loss_ce: 0.005985, loss_dice: 0.411429
[09:54:50.922] TRAIN: iteration 13478 : loss : 0.245426, loss_ce: 0.003166, loss_dice: 0.487685
[09:54:51.130] TRAIN: iteration 13479 : loss : 0.251907, loss_ce: 0.008280, loss_dice: 0.495535
[09:54:51.340] TRAIN: iteration 13480 : loss : 0.147355, loss_ce: 0.004725, loss_dice: 0.289984
[09:54:51.574] TRAIN: iteration 13481 : loss : 0.153957, loss_ce: 0.008524, loss_dice: 0.299390
[09:54:54.387] TRAIN: iteration 13482 : loss : 0.064466, loss_ce: 0.003708, loss_dice: 0.125224
[09:54:54.599] TRAIN: iteration 13483 : loss : 0.099688, loss_ce: 0.010386, loss_dice: 0.188990
[09:54:54.805] TRAIN: iteration 13484 : loss : 0.113920, loss_ce: 0.014337, loss_dice: 0.213503
[09:54:55.013] TRAIN: iteration 13485 : loss : 0.163583, loss_ce: 0.005674, loss_dice: 0.321492
[09:54:55.223] TRAIN: iteration 13486 : loss : 0.061940, loss_ce: 0.003582, loss_dice: 0.120298
[09:54:55.432] TRAIN: iteration 13487 : loss : 0.235751, loss_ce: 0.002620, loss_dice: 0.468882
[09:54:55.638] TRAIN: iteration 13488 : loss : 0.241816, loss_ce: 0.002893, loss_dice: 0.480739
[09:54:55.845] TRAIN: iteration 13489 : loss : 0.090426, loss_ce: 0.003117, loss_dice: 0.177735
[09:54:58.753] TRAIN: iteration 13490 : loss : 0.242683, loss_ce: 0.003181, loss_dice: 0.482186
[09:54:58.961] TRAIN: iteration 13491 : loss : 0.113400, loss_ce: 0.004482, loss_dice: 0.222319
[09:54:59.168] TRAIN: iteration 13492 : loss : 0.149422, loss_ce: 0.008481, loss_dice: 0.290363
[09:54:59.375] TRAIN: iteration 13493 : loss : 0.076653, loss_ce: 0.001585, loss_dice: 0.151722
[09:54:59.588] TRAIN: iteration 13494 : loss : 0.126808, loss_ce: 0.005343, loss_dice: 0.248274
[09:54:59.795] TRAIN: iteration 13495 : loss : 0.136622, loss_ce: 0.028597, loss_dice: 0.244648
[09:55:00.002] TRAIN: iteration 13496 : loss : 0.060078, loss_ce: 0.008259, loss_dice: 0.111896
[09:55:00.211] TRAIN: iteration 13497 : loss : 0.107811, loss_ce: 0.003898, loss_dice: 0.211724
[09:55:02.606] TRAIN: iteration 13498 : loss : 0.161080, loss_ce: 0.004354, loss_dice: 0.317807
[09:55:02.818] TRAIN: iteration 13499 : loss : 0.111053, loss_ce: 0.003326, loss_dice: 0.218780
[09:55:03.029] TRAIN: iteration 13500 : loss : 0.098061, loss_ce: 0.002188, loss_dice: 0.193934
[09:55:03.292] TRAIN: iteration 13501 : loss : 0.251174, loss_ce: 0.002243, loss_dice: 0.500105
[09:55:03.501] TRAIN: iteration 13502 : loss : 0.251165, loss_ce: 0.002226, loss_dice: 0.500105
[09:55:03.712] TRAIN: iteration 13503 : loss : 0.186067, loss_ce: 0.004385, loss_dice: 0.367748
[09:55:03.918] TRAIN: iteration 13504 : loss : 0.082343, loss_ce: 0.005581, loss_dice: 0.159105
[09:55:04.127] TRAIN: iteration 13505 : loss : 0.117650, loss_ce: 0.004232, loss_dice: 0.231068
[09:55:04.851] TRAIN: iteration 13506 : loss : 0.246341, loss_ce: 0.002512, loss_dice: 0.490171
[09:55:05.058] TRAIN: iteration 13507 : loss : 0.226446, loss_ce: 0.001958, loss_dice: 0.450934
[09:55:05.266] TRAIN: iteration 13508 : loss : 0.250635, loss_ce: 0.001225, loss_dice: 0.500044
[09:55:05.481] TRAIN: iteration 13509 : loss : 0.125971, loss_ce: 0.008790, loss_dice: 0.243151
[09:55:05.688] TRAIN: iteration 13510 : loss : 0.173988, loss_ce: 0.006967, loss_dice: 0.341010
[09:55:06.144] TRAIN: iteration 13511 : loss : 0.076630, loss_ce: 0.008399, loss_dice: 0.144861
[09:55:06.351] TRAIN: iteration 13512 : loss : 0.051449, loss_ce: 0.005509, loss_dice: 0.097390
[09:55:06.560] TRAIN: iteration 13513 : loss : 0.082174, loss_ce: 0.006549, loss_dice: 0.157800
[09:55:07.870] TRAIN: iteration 13514 : loss : 0.091413, loss_ce: 0.003860, loss_dice: 0.178966
[09:55:09.150] TRAIN: iteration 13515 : loss : 0.167158, loss_ce: 0.012860, loss_dice: 0.321456
[09:55:09.359] TRAIN: iteration 13516 : loss : 0.076228, loss_ce: 0.007502, loss_dice: 0.144954
[09:55:09.570] TRAIN: iteration 13517 : loss : 0.249130, loss_ce: 0.004369, loss_dice: 0.493890
[09:55:09.779] TRAIN: iteration 13518 : loss : 0.251575, loss_ce: 0.003326, loss_dice: 0.499824
[09:55:10.540] TRAIN: iteration 13519 : loss : 0.075957, loss_ce: 0.006946, loss_dice: 0.144968
[09:55:10.748] TRAIN: iteration 13520 : loss : 0.234346, loss_ce: 0.003528, loss_dice: 0.465164
[09:55:10.994] TRAIN: iteration 13521 : loss : 0.137558, loss_ce: 0.002920, loss_dice: 0.272196
[09:55:11.208] TRAIN: iteration 13522 : loss : 0.126168, loss_ce: 0.028818, loss_dice: 0.223519
[09:55:14.047] TRAIN: iteration 13523 : loss : 0.084048, loss_ce: 0.004190, loss_dice: 0.163906
[09:55:14.255] TRAIN: iteration 13524 : loss : 0.073501, loss_ce: 0.005375, loss_dice: 0.141627
[09:55:14.466] TRAIN: iteration 13525 : loss : 0.251714, loss_ce: 0.003238, loss_dice: 0.500191
[09:55:14.673] TRAIN: iteration 13526 : loss : 0.194119, loss_ce: 0.007894, loss_dice: 0.380345
[09:55:15.120] TRAIN: iteration 13527 : loss : 0.099396, loss_ce: 0.007615, loss_dice: 0.191178
[09:55:15.328] TRAIN: iteration 13528 : loss : 0.062719, loss_ce: 0.006848, loss_dice: 0.118591
[09:55:15.537] TRAIN: iteration 13529 : loss : 0.250299, loss_ce: 0.003685, loss_dice: 0.496914
[09:55:15.748] TRAIN: iteration 13530 : loss : 0.249776, loss_ce: 0.012859, loss_dice: 0.486693
[09:55:17.223] TRAIN: iteration 13531 : loss : 0.131080, loss_ce: 0.007112, loss_dice: 0.255049
[09:55:17.431] TRAIN: iteration 13532 : loss : 0.179551, loss_ce: 0.006264, loss_dice: 0.352839
[09:55:17.638] TRAIN: iteration 13533 : loss : 0.094019, loss_ce: 0.003465, loss_dice: 0.184572
[09:55:17.845] TRAIN: iteration 13534 : loss : 0.043841, loss_ce: 0.004188, loss_dice: 0.083495
[09:55:21.811] TRAIN: iteration 13535 : loss : 0.144019, loss_ce: 0.007541, loss_dice: 0.280496
[09:55:22.018] TRAIN: iteration 13536 : loss : 0.117388, loss_ce: 0.005298, loss_dice: 0.229478
[09:55:22.232] TRAIN: iteration 13537 : loss : 0.085329, loss_ce: 0.002809, loss_dice: 0.167849
[09:55:22.442] TRAIN: iteration 13538 : loss : 0.250991, loss_ce: 0.001907, loss_dice: 0.500075
[09:55:22.650] TRAIN: iteration 13539 : loss : 0.214504, loss_ce: 0.006595, loss_dice: 0.422412
[09:55:22.856] TRAIN: iteration 13540 : loss : 0.052666, loss_ce: 0.001613, loss_dice: 0.103718
[09:55:23.075] TRAIN: iteration 13541 : loss : 0.095424, loss_ce: 0.007335, loss_dice: 0.183513
[09:55:23.283] TRAIN: iteration 13542 : loss : 0.251164, loss_ce: 0.002222, loss_dice: 0.500105
[09:55:27.150] TRAIN: iteration 13543 : loss : 0.083838, loss_ce: 0.004054, loss_dice: 0.163622
[09:55:27.361] TRAIN: iteration 13544 : loss : 0.097321, loss_ce: 0.011241, loss_dice: 0.183402
[09:55:27.574] TRAIN: iteration 13545 : loss : 0.251407, loss_ce: 0.002649, loss_dice: 0.500164
[09:55:27.782] TRAIN: iteration 13546 : loss : 0.124102, loss_ce: 0.013732, loss_dice: 0.234472
[09:55:27.990] TRAIN: iteration 13547 : loss : 0.068333, loss_ce: 0.010282, loss_dice: 0.126384
[09:55:28.207] TRAIN: iteration 13548 : loss : 0.100676, loss_ce: 0.017223, loss_dice: 0.184129
[09:55:28.416] TRAIN: iteration 13549 : loss : 0.249400, loss_ce: 0.009633, loss_dice: 0.489167
[09:55:28.623] TRAIN: iteration 13550 : loss : 0.096961, loss_ce: 0.002784, loss_dice: 0.191138
[09:55:31.780] TRAIN: iteration 13551 : loss : 0.252427, loss_ce: 0.004566, loss_dice: 0.500287
[09:55:31.992] TRAIN: iteration 13552 : loss : 0.251459, loss_ce: 0.003929, loss_dice: 0.498989
[09:55:32.200] TRAIN: iteration 13553 : loss : 0.080127, loss_ce: 0.004778, loss_dice: 0.155475
[09:55:32.411] TRAIN: iteration 13554 : loss : 0.174801, loss_ce: 0.004909, loss_dice: 0.344694
[09:55:32.625] TRAIN: iteration 13555 : loss : 0.078085, loss_ce: 0.006782, loss_dice: 0.149388
[09:55:32.861] TRAIN: iteration 13556 : loss : 0.093036, loss_ce: 0.003955, loss_dice: 0.182118
[09:55:33.071] TRAIN: iteration 13557 : loss : 0.098835, loss_ce: 0.007251, loss_dice: 0.190418
[09:55:33.279] TRAIN: iteration 13558 : loss : 0.233948, loss_ce: 0.009520, loss_dice: 0.458377
[09:55:36.648] TRAIN: iteration 13559 : loss : 0.091922, loss_ce: 0.004632, loss_dice: 0.179212
[09:55:36.858] TRAIN: iteration 13560 : loss : 0.251802, loss_ce: 0.003716, loss_dice: 0.499888
[09:55:37.102] TRAIN: iteration 13561 : loss : 0.070161, loss_ce: 0.011773, loss_dice: 0.128548
[09:55:37.308] TRAIN: iteration 13562 : loss : 0.233361, loss_ce: 0.004159, loss_dice: 0.462564
[09:55:37.516] TRAIN: iteration 13563 : loss : 0.095804, loss_ce: 0.005422, loss_dice: 0.186186
[09:55:37.725] TRAIN: iteration 13564 : loss : 0.094711, loss_ce: 0.006291, loss_dice: 0.183132
[09:55:37.932] TRAIN: iteration 13565 : loss : 0.045739, loss_ce: 0.003670, loss_dice: 0.087809
[09:55:38.140] TRAIN: iteration 13566 : loss : 0.063549, loss_ce: 0.002476, loss_dice: 0.124623
[09:55:41.417] TRAIN: iteration 13567 : loss : 0.054075, loss_ce: 0.001946, loss_dice: 0.106203
[09:55:41.622] TRAIN: iteration 13568 : loss : 0.234586, loss_ce: 0.001581, loss_dice: 0.467591
[09:55:41.830] TRAIN: iteration 13569 : loss : 0.064605, loss_ce: 0.002688, loss_dice: 0.126523
[09:55:42.037] TRAIN: iteration 13570 : loss : 0.136243, loss_ce: 0.002203, loss_dice: 0.270282
[09:55:42.244] TRAIN: iteration 13571 : loss : 0.174102, loss_ce: 0.018860, loss_dice: 0.329343
[09:55:42.453] TRAIN: iteration 13572 : loss : 0.237426, loss_ce: 0.008739, loss_dice: 0.466113
[09:55:42.660] TRAIN: iteration 13573 : loss : 0.032222, loss_ce: 0.001800, loss_dice: 0.062644
[09:55:42.868] TRAIN: iteration 13574 : loss : 0.194047, loss_ce: 0.004720, loss_dice: 0.383375
[09:55:46.631] TRAIN: iteration 13575 : loss : 0.124465, loss_ce: 0.002347, loss_dice: 0.246584
[09:55:46.839] TRAIN: iteration 13576 : loss : 0.077363, loss_ce: 0.003519, loss_dice: 0.151207
[09:55:47.047] TRAIN: iteration 13577 : loss : 0.052089, loss_ce: 0.002454, loss_dice: 0.101723
[09:55:47.255] TRAIN: iteration 13578 : loss : 0.079813, loss_ce: 0.005989, loss_dice: 0.153636
[09:55:47.462] TRAIN: iteration 13579 : loss : 0.087842, loss_ce: 0.003092, loss_dice: 0.172591
[09:55:47.672] TRAIN: iteration 13580 : loss : 0.062912, loss_ce: 0.009830, loss_dice: 0.115995
[09:55:47.908] TRAIN: iteration 13581 : loss : 0.047541, loss_ce: 0.004206, loss_dice: 0.090876
[09:55:48.116] TRAIN: iteration 13582 : loss : 0.035595, loss_ce: 0.002212, loss_dice: 0.068979
[09:55:51.031] TRAIN: iteration 13583 : loss : 0.161880, loss_ce: 0.024942, loss_dice: 0.298817
[09:55:51.239] TRAIN: iteration 13584 : loss : 0.044788, loss_ce: 0.002788, loss_dice: 0.086789
[09:55:51.451] TRAIN: iteration 13585 : loss : 0.158415, loss_ce: 0.002897, loss_dice: 0.313933
[09:55:51.660] TRAIN: iteration 13586 : loss : 0.229867, loss_ce: 0.006323, loss_dice: 0.453411
[09:55:51.868] TRAIN: iteration 13587 : loss : 0.167256, loss_ce: 0.005440, loss_dice: 0.329071
[09:55:52.075] TRAIN: iteration 13588 : loss : 0.251508, loss_ce: 0.002835, loss_dice: 0.500180
[09:55:52.283] TRAIN: iteration 13589 : loss : 0.108468, loss_ce: 0.005123, loss_dice: 0.211813
[09:55:52.489] TRAIN: iteration 13590 : loss : 0.251009, loss_ce: 0.001931, loss_dice: 0.500088
[09:55:54.126] TRAIN: iteration 13591 : loss : 0.034210, loss_ce: 0.002078, loss_dice: 0.066342
[09:55:54.333] TRAIN: iteration 13592 : loss : 0.053923, loss_ce: 0.001833, loss_dice: 0.106013
[09:55:54.540] TRAIN: iteration 13593 : loss : 0.061372, loss_ce: 0.002559, loss_dice: 0.120184
[09:55:54.749] TRAIN: iteration 13594 : loss : 0.135630, loss_ce: 0.002837, loss_dice: 0.268422
[09:55:54.955] TRAIN: iteration 13595 : loss : 0.250685, loss_ce: 0.001331, loss_dice: 0.500039
[09:55:55.162] TRAIN: iteration 13596 : loss : 0.102681, loss_ce: 0.005836, loss_dice: 0.199526
[09:55:55.370] TRAIN: iteration 13597 : loss : 0.045455, loss_ce: 0.004466, loss_dice: 0.086445
[09:55:55.577] TRAIN: iteration 13598 : loss : 0.152566, loss_ce: 0.011240, loss_dice: 0.293891
[09:55:55.784] TRAIN: iteration 13599 : loss : 0.095563, loss_ce: 0.003470, loss_dice: 0.187655
[09:55:58.679] TRAIN: iteration 13600 : loss : 0.079898, loss_ce: 0.004515, loss_dice: 0.155280
[09:55:58.915] TRAIN: iteration 13601 : loss : 0.247930, loss_ce: 0.001922, loss_dice: 0.493938
[09:55:59.122] TRAIN: iteration 13602 : loss : 0.242547, loss_ce: 0.002641, loss_dice: 0.482453
[09:56:00.616] TRAIN: iteration 13603 : loss : 0.093857, loss_ce: 0.003182, loss_dice: 0.184532
[09:56:01.978] TRAIN: iteration 13604 : loss : 0.212224, loss_ce: 0.003632, loss_dice: 0.420815
[09:56:02.188] TRAIN: iteration 13605 : loss : 0.049685, loss_ce: 0.004047, loss_dice: 0.095323
[09:56:02.398] TRAIN: iteration 13606 : loss : 0.235985, loss_ce: 0.002777, loss_dice: 0.469194
[09:56:02.609] TRAIN: iteration 13607 : loss : 0.108995, loss_ce: 0.005710, loss_dice: 0.212280
[09:56:05.228] TRAIN: iteration 13608 : loss : 0.083013, loss_ce: 0.005680, loss_dice: 0.160346
[09:56:05.438] TRAIN: iteration 13609 : loss : 0.052993, loss_ce: 0.002228, loss_dice: 0.103757
[09:56:05.657] TRAIN: iteration 13610 : loss : 0.244082, loss_ce: 0.006216, loss_dice: 0.481948
[09:56:05.866] TRAIN: iteration 13611 : loss : 0.161581, loss_ce: 0.004265, loss_dice: 0.318896
[09:56:07.044] TRAIN: iteration 13612 : loss : 0.127352, loss_ce: 0.002834, loss_dice: 0.251871
[09:56:07.251] TRAIN: iteration 13613 : loss : 0.072553, loss_ce: 0.004850, loss_dice: 0.140257
[09:56:07.459] TRAIN: iteration 13614 : loss : 0.252777, loss_ce: 0.008589, loss_dice: 0.496965
[09:56:07.668] TRAIN: iteration 13615 : loss : 0.061995, loss_ce: 0.006822, loss_dice: 0.117169
[09:56:09.261] TRAIN: iteration 13616 : loss : 0.097303, loss_ce: 0.002384, loss_dice: 0.192222
[09:56:09.470] TRAIN: iteration 13617 : loss : 0.251980, loss_ce: 0.004647, loss_dice: 0.499313
[09:56:09.678] TRAIN: iteration 13618 : loss : 0.206093, loss_ce: 0.002714, loss_dice: 0.409471
[09:56:09.887] TRAIN: iteration 13619 : loss : 0.251068, loss_ce: 0.002041, loss_dice: 0.500095
[09:56:14.604] TRAIN: iteration 13620 : loss : 0.030438, loss_ce: 0.003265, loss_dice: 0.057610
[09:56:14.846] TRAIN: iteration 13621 : loss : 0.085108, loss_ce: 0.006288, loss_dice: 0.163927
[09:56:15.061] TRAIN: iteration 13622 : loss : 0.074188, loss_ce: 0.003274, loss_dice: 0.145102
[09:56:15.269] TRAIN: iteration 13623 : loss : 0.143470, loss_ce: 0.003259, loss_dice: 0.283680
[09:56:15.479] TRAIN: iteration 13624 : loss : 0.161179, loss_ce: 0.013607, loss_dice: 0.308752
[09:56:15.687] TRAIN: iteration 13625 : loss : 0.068623, loss_ce: 0.002609, loss_dice: 0.134636
[09:56:15.894] TRAIN: iteration 13626 : loss : 0.164599, loss_ce: 0.003449, loss_dice: 0.325750
[09:56:16.106] TRAIN: iteration 13627 : loss : 0.119478, loss_ce: 0.006784, loss_dice: 0.232173
[09:56:19.546] TRAIN: iteration 13628 : loss : 0.249654, loss_ce: 0.002798, loss_dice: 0.496510
[09:56:19.761] TRAIN: iteration 13629 : loss : 0.062285, loss_ce: 0.005479, loss_dice: 0.119091
[09:56:20.089] TRAIN: iteration 13630 : loss : 0.108665, loss_ce: 0.003771, loss_dice: 0.213558
[09:56:20.295] TRAIN: iteration 13631 : loss : 0.251526, loss_ce: 0.002875, loss_dice: 0.500178
[09:56:20.503] TRAIN: iteration 13632 : loss : 0.210868, loss_ce: 0.003264, loss_dice: 0.418472
[09:56:20.744] TRAIN: iteration 13633 : loss : 0.152810, loss_ce: 0.002937, loss_dice: 0.302684
[09:56:20.953] TRAIN: iteration 13634 : loss : 0.251238, loss_ce: 0.002368, loss_dice: 0.500109
[09:56:21.162] TRAIN: iteration 13635 : loss : 0.041424, loss_ce: 0.006798, loss_dice: 0.076050
[09:56:22.269] TRAIN: iteration 13636 : loss : 0.047797, loss_ce: 0.006847, loss_dice: 0.088747
[09:56:22.475] TRAIN: iteration 13637 : loss : 0.069876, loss_ce: 0.001964, loss_dice: 0.137789
[09:56:23.538] TRAIN: iteration 13638 : loss : 0.137661, loss_ce: 0.005720, loss_dice: 0.269603
[09:56:24.081] TRAIN: iteration 13639 : loss : 0.199360, loss_ce: 0.005049, loss_dice: 0.393670
[09:56:24.289] TRAIN: iteration 13640 : loss : 0.252099, loss_ce: 0.003921, loss_dice: 0.500277
[09:56:24.528] TRAIN: iteration 13641 : loss : 0.107018, loss_ce: 0.002456, loss_dice: 0.211581
[09:56:27.386] TRAIN: iteration 13642 : loss : 0.189455, loss_ce: 0.005304, loss_dice: 0.373607
[09:56:27.597] TRAIN: iteration 13643 : loss : 0.039805, loss_ce: 0.004313, loss_dice: 0.075296
[09:56:28.279] TRAIN: iteration 13644 : loss : 0.044853, loss_ce: 0.003791, loss_dice: 0.085914
[09:56:28.489] TRAIN: iteration 13645 : loss : 0.075858, loss_ce: 0.002356, loss_dice: 0.149360
[09:56:29.199] TRAIN: iteration 13646 : loss : 0.155850, loss_ce: 0.002912, loss_dice: 0.308788
[09:56:29.407] TRAIN: iteration 13647 : loss : 0.241206, loss_ce: 0.004606, loss_dice: 0.477806
[09:56:29.615] TRAIN: iteration 13648 : loss : 0.124107, loss_ce: 0.002812, loss_dice: 0.245402
[09:56:30.277] TRAIN: iteration 13649 : loss : 0.044867, loss_ce: 0.003514, loss_dice: 0.086219
[09:56:32.321] TRAIN: iteration 13650 : loss : 0.054436, loss_ce: 0.002510, loss_dice: 0.106361
[09:56:32.528] TRAIN: iteration 13651 : loss : 0.050374, loss_ce: 0.002805, loss_dice: 0.097942
[09:56:33.477] TRAIN: iteration 13652 : loss : 0.075166, loss_ce: 0.003421, loss_dice: 0.146912
[09:56:33.684] TRAIN: iteration 13653 : loss : 0.053934, loss_ce: 0.004161, loss_dice: 0.103708
[09:56:37.051] TRAIN: iteration 13654 : loss : 0.250836, loss_ce: 0.001600, loss_dice: 0.500072
[09:56:37.262] TRAIN: iteration 13655 : loss : 0.059676, loss_ce: 0.001927, loss_dice: 0.117425
[09:56:37.472] TRAIN: iteration 13656 : loss : 0.250904, loss_ce: 0.001729, loss_dice: 0.500079
[09:56:38.202] TRAIN: iteration 13657 : loss : 0.134112, loss_ce: 0.004055, loss_dice: 0.264170
[09:56:38.409] TRAIN: iteration 13658 : loss : 0.250901, loss_ce: 0.001705, loss_dice: 0.500096
[09:56:38.615] TRAIN: iteration 13659 : loss : 0.252428, loss_ce: 0.004902, loss_dice: 0.499954
[09:56:39.109] TRAIN: iteration 13660 : loss : 0.100355, loss_ce: 0.012673, loss_dice: 0.188038
[09:56:39.347] TRAIN: iteration 13661 : loss : 0.097895, loss_ce: 0.004984, loss_dice: 0.190806
[09:56:43.684] TRAIN: iteration 13662 : loss : 0.116962, loss_ce: 0.003792, loss_dice: 0.230132
[09:56:43.895] TRAIN: iteration 13663 : loss : 0.251146, loss_ce: 0.003741, loss_dice: 0.498551
[09:56:44.102] TRAIN: iteration 13664 : loss : 0.037238, loss_ce: 0.003535, loss_dice: 0.070941
[09:56:46.492] TRAIN: iteration 13665 : loss : 0.172495, loss_ce: 0.007926, loss_dice: 0.337064
[09:56:46.703] TRAIN: iteration 13666 : loss : 0.251779, loss_ce: 0.003310, loss_dice: 0.500247
[09:56:46.910] TRAIN: iteration 13667 : loss : 0.134216, loss_ce: 0.007731, loss_dice: 0.260700
[09:56:47.118] TRAIN: iteration 13668 : loss : 0.057910, loss_ce: 0.003960, loss_dice: 0.111860
[09:56:47.325] TRAIN: iteration 13669 : loss : 0.171162, loss_ce: 0.006566, loss_dice: 0.335758
[09:56:51.506] TRAIN: iteration 13670 : loss : 0.114677, loss_ce: 0.005304, loss_dice: 0.224051
[09:56:51.720] TRAIN: iteration 13671 : loss : 0.117721, loss_ce: 0.020392, loss_dice: 0.215050
[09:56:51.928] TRAIN: iteration 13672 : loss : 0.132116, loss_ce: 0.002902, loss_dice: 0.261331
[09:56:52.149] TRAIN: iteration 13673 : loss : 0.134980, loss_ce: 0.004171, loss_dice: 0.265788
[09:56:52.357] TRAIN: iteration 13674 : loss : 0.068061, loss_ce: 0.005149, loss_dice: 0.130972
[09:56:52.569] TRAIN: iteration 13675 : loss : 0.214909, loss_ce: 0.008641, loss_dice: 0.421178
[09:56:52.782] TRAIN: iteration 13676 : loss : 0.081357, loss_ce: 0.005568, loss_dice: 0.157146
[09:56:54.271] TRAIN: iteration 13677 : loss : 0.054315, loss_ce: 0.003038, loss_dice: 0.105593
[09:56:55.943] TRAIN: iteration 13678 : loss : 0.036067, loss_ce: 0.002917, loss_dice: 0.069217
[09:56:56.151] TRAIN: iteration 13679 : loss : 0.148842, loss_ce: 0.005033, loss_dice: 0.292650
[09:56:56.358] TRAIN: iteration 13680 : loss : 0.085050, loss_ce: 0.002477, loss_dice: 0.167622
[09:56:56.359] NaN or Inf found in input tensor.
[09:56:57.683] TRAIN: iteration 13681 : loss : 0.241688, loss_ce: 0.004445, loss_dice: 0.478931
[09:56:57.891] TRAIN: iteration 13682 : loss : 0.219037, loss_ce: 0.006568, loss_dice: 0.431506
[09:56:58.100] TRAIN: iteration 13683 : loss : 0.068066, loss_ce: 0.005980, loss_dice: 0.130153
[09:56:58.307] TRAIN: iteration 13684 : loss : 0.089892, loss_ce: 0.003418, loss_dice: 0.176366
[09:57:00.843] TRAIN: iteration 13685 : loss : 0.251222, loss_ce: 0.003689, loss_dice: 0.498755
[09:57:01.051] TRAIN: iteration 13686 : loss : 0.237141, loss_ce: 0.003878, loss_dice: 0.470403
[09:57:01.266] TRAIN: iteration 13687 : loss : 0.179776, loss_ce: 0.009748, loss_dice: 0.349803
[09:57:01.472] TRAIN: iteration 13688 : loss : 0.140221, loss_ce: 0.002245, loss_dice: 0.278196
[09:57:03.222] TRAIN: iteration 13689 : loss : 0.233783, loss_ce: 0.022558, loss_dice: 0.445008
[09:57:03.431] TRAIN: iteration 13690 : loss : 0.137704, loss_ce: 0.008680, loss_dice: 0.266727
[09:57:03.640] TRAIN: iteration 13691 : loss : 0.117567, loss_ce: 0.003176, loss_dice: 0.231957
[09:57:03.848] TRAIN: iteration 13692 : loss : 0.226894, loss_ce: 0.002602, loss_dice: 0.451186
[09:57:07.155] TRAIN: iteration 13693 : loss : 0.037194, loss_ce: 0.001658, loss_dice: 0.072731
[09:57:07.362] TRAIN: iteration 13694 : loss : 0.079179, loss_ce: 0.004951, loss_dice: 0.153407
[09:57:07.569] TRAIN: iteration 13695 : loss : 0.182917, loss_ce: 0.015507, loss_dice: 0.350327
[09:57:07.892] TRAIN: iteration 13696 : loss : 0.078279, loss_ce: 0.006583, loss_dice: 0.149976
[09:57:08.099] TRAIN: iteration 13697 : loss : 0.112631, loss_ce: 0.002769, loss_dice: 0.222493
[09:57:08.307] TRAIN: iteration 13698 : loss : 0.128375, loss_ce: 0.003493, loss_dice: 0.253258
[09:57:08.515] TRAIN: iteration 13699 : loss : 0.249446, loss_ce: 0.007058, loss_dice: 0.491834
[09:57:08.725] TRAIN: iteration 13700 : loss : 0.114123, loss_ce: 0.004137, loss_dice: 0.224109
[09:57:12.173] TRAIN: iteration 13701 : loss : 0.087738, loss_ce: 0.006297, loss_dice: 0.169179
[09:57:14.244] TRAIN: iteration 13702 : loss : 0.053515, loss_ce: 0.005890, loss_dice: 0.101140
[09:57:14.458] TRAIN: iteration 13703 : loss : 0.250058, loss_ce: 0.003012, loss_dice: 0.497104
[09:57:14.666] TRAIN: iteration 13704 : loss : 0.099848, loss_ce: 0.007770, loss_dice: 0.191925
[09:57:15.259] TRAIN: iteration 13705 : loss : 0.028563, loss_ce: 0.002198, loss_dice: 0.054929
[09:57:15.467] TRAIN: iteration 13706 : loss : 0.178150, loss_ce: 0.004649, loss_dice: 0.351652
[09:57:15.674] TRAIN: iteration 13707 : loss : 0.116190, loss_ce: 0.003033, loss_dice: 0.229348
[09:57:15.940] TRAIN: iteration 13708 : loss : 0.121515, loss_ce: 0.005646, loss_dice: 0.237383
[09:57:20.222] TRAIN: iteration 13709 : loss : 0.054547, loss_ce: 0.004377, loss_dice: 0.104718
[09:57:20.430] TRAIN: iteration 13710 : loss : 0.247364, loss_ce: 0.010868, loss_dice: 0.483860
[09:57:20.637] TRAIN: iteration 13711 : loss : 0.081083, loss_ce: 0.004305, loss_dice: 0.157861
[09:57:20.846] TRAIN: iteration 13712 : loss : 0.055347, loss_ce: 0.002332, loss_dice: 0.108363
[09:57:21.474] TRAIN: iteration 13713 : loss : 0.233191, loss_ce: 0.005363, loss_dice: 0.461020
[09:57:21.683] TRAIN: iteration 13714 : loss : 0.058191, loss_ce: 0.005620, loss_dice: 0.110763
[09:57:21.890] TRAIN: iteration 13715 : loss : 0.132994, loss_ce: 0.005710, loss_dice: 0.260278
[09:57:22.102] TRAIN: iteration 13716 : loss : 0.238750, loss_ce: 0.004495, loss_dice: 0.473006
[09:57:27.890] TRAIN: iteration 13717 : loss : 0.226185, loss_ce: 0.002843, loss_dice: 0.449528
[09:57:28.097] TRAIN: iteration 13718 : loss : 0.028830, loss_ce: 0.003346, loss_dice: 0.054314
[09:57:28.305] TRAIN: iteration 13719 : loss : 0.085910, loss_ce: 0.002961, loss_dice: 0.168858
[09:57:28.513] TRAIN: iteration 13720 : loss : 0.153930, loss_ce: 0.003778, loss_dice: 0.304083
[09:57:28.751] TRAIN: iteration 13721 : loss : 0.107608, loss_ce: 0.003693, loss_dice: 0.211524
[09:57:28.960] TRAIN: iteration 13722 : loss : 0.250927, loss_ce: 0.001805, loss_dice: 0.500049
[09:57:29.168] TRAIN: iteration 13723 : loss : 0.073355, loss_ce: 0.005401, loss_dice: 0.141310
[09:57:30.767] TRAIN: iteration 13724 : loss : 0.122904, loss_ce: 0.004950, loss_dice: 0.240858
[09:57:36.517] TRAIN: iteration 13725 : loss : 0.111050, loss_ce: 0.003310, loss_dice: 0.218791
[09:57:36.725] TRAIN: iteration 13726 : loss : 0.104091, loss_ce: 0.003972, loss_dice: 0.204209
[09:57:36.932] TRAIN: iteration 13727 : loss : 0.051453, loss_ce: 0.005097, loss_dice: 0.097809
[09:57:37.140] TRAIN: iteration 13728 : loss : 0.085988, loss_ce: 0.002981, loss_dice: 0.168995
[09:57:37.348] TRAIN: iteration 13729 : loss : 0.250885, loss_ce: 0.001698, loss_dice: 0.500072
[09:57:37.555] TRAIN: iteration 13730 : loss : 0.137887, loss_ce: 0.007375, loss_dice: 0.268399
[09:57:37.764] TRAIN: iteration 13731 : loss : 0.242917, loss_ce: 0.002559, loss_dice: 0.483275
[09:57:37.972] TRAIN: iteration 13732 : loss : 0.074996, loss_ce: 0.006764, loss_dice: 0.143229
[09:57:46.264] TRAIN: iteration 13733 : loss : 0.095220, loss_ce: 0.002437, loss_dice: 0.188004
[09:57:46.476] TRAIN: iteration 13734 : loss : 0.239923, loss_ce: 0.003091, loss_dice: 0.476754
[09:57:46.684] TRAIN: iteration 13735 : loss : 0.134318, loss_ce: 0.004968, loss_dice: 0.263668
[09:57:46.890] TRAIN: iteration 13736 : loss : 0.105588, loss_ce: 0.004835, loss_dice: 0.206341
[09:57:47.097] TRAIN: iteration 13737 : loss : 0.128774, loss_ce: 0.003810, loss_dice: 0.253739
[09:57:47.304] TRAIN: iteration 13738 : loss : 0.055256, loss_ce: 0.004060, loss_dice: 0.106452
[09:57:47.512] TRAIN: iteration 13739 : loss : 0.248384, loss_ce: 0.001959, loss_dice: 0.494810
[09:57:47.720] TRAIN: iteration 13740 : loss : 0.099008, loss_ce: 0.001855, loss_dice: 0.196162
[09:57:54.811] TRAIN: iteration 13741 : loss : 0.091653, loss_ce: 0.004741, loss_dice: 0.178566
[09:57:55.020] TRAIN: iteration 13742 : loss : 0.043718, loss_ce: 0.003918, loss_dice: 0.083518
[09:57:55.229] TRAIN: iteration 13743 : loss : 0.164795, loss_ce: 0.006326, loss_dice: 0.323264
[09:57:55.436] TRAIN: iteration 13744 : loss : 0.210051, loss_ce: 0.002649, loss_dice: 0.417453
[09:57:55.644] TRAIN: iteration 13745 : loss : 0.149984, loss_ce: 0.011114, loss_dice: 0.288853
[09:57:55.859] TRAIN: iteration 13746 : loss : 0.080156, loss_ce: 0.005537, loss_dice: 0.154776
[09:57:56.066] TRAIN: iteration 13747 : loss : 0.133268, loss_ce: 0.005012, loss_dice: 0.261524
[09:57:56.273] TRAIN: iteration 13748 : loss : 0.087610, loss_ce: 0.004750, loss_dice: 0.170470
[09:58:02.607] TRAIN: iteration 13749 : loss : 0.041713, loss_ce: 0.004617, loss_dice: 0.078810
[09:58:02.813] TRAIN: iteration 13750 : loss : 0.251350, loss_ce: 0.002567, loss_dice: 0.500133
[09:58:03.023] TRAIN: iteration 13751 : loss : 0.126703, loss_ce: 0.005677, loss_dice: 0.247730
[09:58:03.229] TRAIN: iteration 13752 : loss : 0.172621, loss_ce: 0.013317, loss_dice: 0.331925
[09:58:03.443] TRAIN: iteration 13753 : loss : 0.063576, loss_ce: 0.003973, loss_dice: 0.123179
[09:58:03.670] TRAIN: iteration 13754 : loss : 0.178105, loss_ce: 0.007096, loss_dice: 0.349113
[09:58:03.877] TRAIN: iteration 13755 : loss : 0.251454, loss_ce: 0.006355, loss_dice: 0.496553
[09:58:04.085] TRAIN: iteration 13756 : loss : 0.030855, loss_ce: 0.004504, loss_dice: 0.057206
[09:58:11.312] TRAIN: iteration 13757 : loss : 0.117370, loss_ce: 0.003010, loss_dice: 0.231730
[09:58:11.520] TRAIN: iteration 13758 : loss : 0.034896, loss_ce: 0.002074, loss_dice: 0.067718
[09:58:11.728] TRAIN: iteration 13759 : loss : 0.157409, loss_ce: 0.003863, loss_dice: 0.310955
[09:58:11.936] TRAIN: iteration 13760 : loss : 0.252498, loss_ce: 0.004674, loss_dice: 0.500322
[09:58:12.173] TRAIN: iteration 13761 : loss : 0.178851, loss_ce: 0.008292, loss_dice: 0.349410
[09:58:12.379] TRAIN: iteration 13762 : loss : 0.085162, loss_ce: 0.005014, loss_dice: 0.165310
[09:58:12.586] TRAIN: iteration 13763 : loss : 0.059587, loss_ce: 0.006651, loss_dice: 0.112522
[09:58:12.792] TRAIN: iteration 13764 : loss : 0.155406, loss_ce: 0.006916, loss_dice: 0.303895
[09:58:20.730] TRAIN: iteration 13765 : loss : 0.150957, loss_ce: 0.004041, loss_dice: 0.297873
[09:58:20.936] TRAIN: iteration 13766 : loss : 0.164382, loss_ce: 0.006219, loss_dice: 0.322544
[09:58:21.144] TRAIN: iteration 13767 : loss : 0.072787, loss_ce: 0.003959, loss_dice: 0.141615
[09:58:21.351] TRAIN: iteration 13768 : loss : 0.196228, loss_ce: 0.011001, loss_dice: 0.381454
[09:58:21.557] TRAIN: iteration 13769 : loss : 0.093522, loss_ce: 0.005168, loss_dice: 0.181876
[09:58:21.766] TRAIN: iteration 13770 : loss : 0.150459, loss_ce: 0.005597, loss_dice: 0.295322
[09:58:21.977] TRAIN: iteration 13771 : loss : 0.050010, loss_ce: 0.003978, loss_dice: 0.096042
[09:58:22.185] TRAIN: iteration 13772 : loss : 0.250554, loss_ce: 0.004584, loss_dice: 0.496523
[09:58:28.886] TRAIN: iteration 13773 : loss : 0.140883, loss_ce: 0.004294, loss_dice: 0.277472
[09:58:29.099] TRAIN: iteration 13774 : loss : 0.125700, loss_ce: 0.004187, loss_dice: 0.247214
[09:58:29.307] TRAIN: iteration 13775 : loss : 0.235306, loss_ce: 0.008283, loss_dice: 0.462330
[09:58:29.514] TRAIN: iteration 13776 : loss : 0.220350, loss_ce: 0.003763, loss_dice: 0.436938
[09:58:29.721] TRAIN: iteration 13777 : loss : 0.232072, loss_ce: 0.004369, loss_dice: 0.459774
[09:58:29.930] TRAIN: iteration 13778 : loss : 0.075508, loss_ce: 0.011803, loss_dice: 0.139213
[09:58:30.139] TRAIN: iteration 13779 : loss : 0.081061, loss_ce: 0.011447, loss_dice: 0.150675
[09:58:30.347] TRAIN: iteration 13780 : loss : 0.044609, loss_ce: 0.003715, loss_dice: 0.085504
[09:58:34.227] TRAIN: iteration 13781 : loss : 0.156071, loss_ce: 0.006173, loss_dice: 0.305969
[09:58:34.434] TRAIN: iteration 13782 : loss : 0.252950, loss_ce: 0.005504, loss_dice: 0.500397
[09:58:34.641] TRAIN: iteration 13783 : loss : 0.108531, loss_ce: 0.004767, loss_dice: 0.212296
[09:58:34.848] TRAIN: iteration 13784 : loss : 0.083934, loss_ce: 0.006045, loss_dice: 0.161823
[09:58:35.056] TRAIN: iteration 13785 : loss : 0.077632, loss_ce: 0.003748, loss_dice: 0.151516
[09:58:35.263] TRAIN: iteration 13786 : loss : 0.232225, loss_ce: 0.003567, loss_dice: 0.460883
[09:58:35.471] TRAIN: iteration 13787 : loss : 0.252367, loss_ce: 0.004409, loss_dice: 0.500324
[09:58:35.678] TRAIN: iteration 13788 : loss : 0.102560, loss_ce: 0.009805, loss_dice: 0.195316
[09:58:42.614] TRAIN: iteration 13789 : loss : 0.245753, loss_ce: 0.004337, loss_dice: 0.487170
[09:58:42.915] TRAIN: iteration 13790 : loss : 0.250466, loss_ce: 0.002681, loss_dice: 0.498250
[09:58:43.123] TRAIN: iteration 13791 : loss : 0.095082, loss_ce: 0.004697, loss_dice: 0.185467
[09:58:43.973] TRAIN: iteration 13792 : loss : 0.064957, loss_ce: 0.009780, loss_dice: 0.120134
[09:58:44.180] TRAIN: iteration 13793 : loss : 0.176510, loss_ce: 0.020178, loss_dice: 0.332843
[09:58:44.387] TRAIN: iteration 13794 : loss : 0.167578, loss_ce: 0.003825, loss_dice: 0.331331
[09:58:44.594] TRAIN: iteration 13795 : loss : 0.039322, loss_ce: 0.005098, loss_dice: 0.073545
[09:58:44.801] TRAIN: iteration 13796 : loss : 0.047949, loss_ce: 0.002518, loss_dice: 0.093380
[09:58:51.486] TRAIN: iteration 13797 : loss : 0.150071, loss_ce: 0.009577, loss_dice: 0.290565
[09:58:51.693] TRAIN: iteration 13798 : loss : 0.037693, loss_ce: 0.002946, loss_dice: 0.072439
[09:58:51.900] TRAIN: iteration 13799 : loss : 0.205203, loss_ce: 0.033337, loss_dice: 0.377069
[09:58:52.109] TRAIN: iteration 13800 : loss : 0.066468, loss_ce: 0.006448, loss_dice: 0.126489
[09:58:52.347] TRAIN: iteration 13801 : loss : 0.112564, loss_ce: 0.007387, loss_dice: 0.217741
[09:58:52.555] TRAIN: iteration 13802 : loss : 0.189356, loss_ce: 0.005016, loss_dice: 0.373697
[09:58:52.762] TRAIN: iteration 13803 : loss : 0.031346, loss_ce: 0.004138, loss_dice: 0.058554
[09:58:52.971] TRAIN: iteration 13804 : loss : 0.087574, loss_ce: 0.008548, loss_dice: 0.166601
[09:59:00.181] TRAIN: iteration 13805 : loss : 0.145239, loss_ce: 0.004648, loss_dice: 0.285831
[09:59:00.387] TRAIN: iteration 13806 : loss : 0.174337, loss_ce: 0.003575, loss_dice: 0.345099
[09:59:00.595] TRAIN: iteration 13807 : loss : 0.099048, loss_ce: 0.003392, loss_dice: 0.194704
[09:59:00.803] TRAIN: iteration 13808 : loss : 0.251305, loss_ce: 0.002496, loss_dice: 0.500114
[09:59:01.132] TRAIN: iteration 13809 : loss : 0.143971, loss_ce: 0.008076, loss_dice: 0.279865
[09:59:01.339] TRAIN: iteration 13810 : loss : 0.181236, loss_ce: 0.005753, loss_dice: 0.356719
[09:59:01.552] TRAIN: iteration 13811 : loss : 0.034849, loss_ce: 0.002087, loss_dice: 0.067610
[09:59:01.759] TRAIN: iteration 13812 : loss : 0.090787, loss_ce: 0.006668, loss_dice: 0.174907
[09:59:08.800] TRAIN: iteration 13813 : loss : 0.101334, loss_ce: 0.003025, loss_dice: 0.199642
[09:59:09.007] TRAIN: iteration 13814 : loss : 0.097079, loss_ce: 0.003237, loss_dice: 0.190922
[09:59:09.214] TRAIN: iteration 13815 : loss : 0.250847, loss_ce: 0.001643, loss_dice: 0.500050
[09:59:09.773] TRAIN: iteration 13816 : loss : 0.026479, loss_ce: 0.001959, loss_dice: 0.050999
[09:59:10.458] TRAIN: iteration 13817 : loss : 0.083438, loss_ce: 0.006076, loss_dice: 0.160800
[09:59:10.665] TRAIN: iteration 13818 : loss : 0.117859, loss_ce: 0.006020, loss_dice: 0.229698
[09:59:10.873] TRAIN: iteration 13819 : loss : 0.061255, loss_ce: 0.002914, loss_dice: 0.119595
[09:59:11.083] TRAIN: iteration 13820 : loss : 0.075008, loss_ce: 0.003668, loss_dice: 0.146347
[09:59:18.053] TRAIN: iteration 13821 : loss : 0.188186, loss_ce: 0.005827, loss_dice: 0.370544
[09:59:18.264] TRAIN: iteration 13822 : loss : 0.181364, loss_ce: 0.006958, loss_dice: 0.355769
[09:59:18.472] TRAIN: iteration 13823 : loss : 0.178516, loss_ce: 0.002949, loss_dice: 0.354083
[09:59:21.041] TRAIN: iteration 13824 : loss : 0.102747, loss_ce: 0.007322, loss_dice: 0.198171
[09:59:21.248] TRAIN: iteration 13825 : loss : 0.067779, loss_ce: 0.003460, loss_dice: 0.132097
[09:59:21.455] TRAIN: iteration 13826 : loss : 0.083850, loss_ce: 0.005161, loss_dice: 0.162538
[09:59:21.661] TRAIN: iteration 13827 : loss : 0.156452, loss_ce: 0.002086, loss_dice: 0.310817
[09:59:21.869] TRAIN: iteration 13828 : loss : 0.215691, loss_ce: 0.014040, loss_dice: 0.417343
[09:59:28.134] TRAIN: iteration 13829 : loss : 0.175594, loss_ce: 0.002515, loss_dice: 0.348672
[09:59:28.341] TRAIN: iteration 13830 : loss : 0.026733, loss_ce: 0.002124, loss_dice: 0.051341
[09:59:28.548] TRAIN: iteration 13831 : loss : 0.226417, loss_ce: 0.003335, loss_dice: 0.449500
[09:59:28.755] TRAIN: iteration 13832 : loss : 0.063147, loss_ce: 0.004870, loss_dice: 0.121424
[09:59:28.966] TRAIN: iteration 13833 : loss : 0.153476, loss_ce: 0.004234, loss_dice: 0.302718
[09:59:29.174] TRAIN: iteration 13834 : loss : 0.167259, loss_ce: 0.004188, loss_dice: 0.330329
[09:59:29.381] TRAIN: iteration 13835 : loss : 0.226152, loss_ce: 0.004851, loss_dice: 0.447453
[09:59:29.593] TRAIN: iteration 13836 : loss : 0.185379, loss_ce: 0.003077, loss_dice: 0.367681
[09:59:36.694] TRAIN: iteration 13837 : loss : 0.251506, loss_ce: 0.003391, loss_dice: 0.499621
[09:59:36.900] TRAIN: iteration 13838 : loss : 0.083757, loss_ce: 0.001788, loss_dice: 0.165725
[09:59:37.110] TRAIN: iteration 13839 : loss : 0.126903, loss_ce: 0.003780, loss_dice: 0.250026
[09:59:37.336] TRAIN: iteration 13840 : loss : 0.069814, loss_ce: 0.006048, loss_dice: 0.133580
[09:59:37.574] TRAIN: iteration 13841 : loss : 0.064938, loss_ce: 0.003075, loss_dice: 0.126802
[09:59:37.780] TRAIN: iteration 13842 : loss : 0.149777, loss_ce: 0.004087, loss_dice: 0.295467
[09:59:37.988] TRAIN: iteration 13843 : loss : 0.091797, loss_ce: 0.003747, loss_dice: 0.179847
[09:59:38.195] TRAIN: iteration 13844 : loss : 0.147538, loss_ce: 0.010168, loss_dice: 0.284909
[09:59:46.400] TRAIN: iteration 13845 : loss : 0.124272, loss_ce: 0.002143, loss_dice: 0.246400
[09:59:46.608] TRAIN: iteration 13846 : loss : 0.073134, loss_ce: 0.004430, loss_dice: 0.141838
[09:59:46.814] TRAIN: iteration 13847 : loss : 0.071477, loss_ce: 0.004851, loss_dice: 0.138103
[09:59:47.021] TRAIN: iteration 13848 : loss : 0.251783, loss_ce: 0.003975, loss_dice: 0.499591
[09:59:47.228] TRAIN: iteration 13849 : loss : 0.086344, loss_ce: 0.004438, loss_dice: 0.168251
[09:59:47.438] TRAIN: iteration 13850 : loss : 0.048549, loss_ce: 0.003308, loss_dice: 0.093789
[09:59:47.649] TRAIN: iteration 13851 : loss : 0.064675, loss_ce: 0.004483, loss_dice: 0.124866
[09:59:47.857] TRAIN: iteration 13852 : loss : 0.045908, loss_ce: 0.001715, loss_dice: 0.090101
[09:59:53.981] TRAIN: iteration 13853 : loss : 0.144128, loss_ce: 0.005247, loss_dice: 0.283009
[09:59:54.189] TRAIN: iteration 13854 : loss : 0.228359, loss_ce: 0.005266, loss_dice: 0.451453
[09:59:54.396] TRAIN: iteration 13855 : loss : 0.251054, loss_ce: 0.002014, loss_dice: 0.500095
[09:59:56.769] TRAIN: iteration 13856 : loss : 0.081335, loss_ce: 0.004409, loss_dice: 0.158262
[09:59:56.978] TRAIN: iteration 13857 : loss : 0.043007, loss_ce: 0.002655, loss_dice: 0.083359
[09:59:57.188] TRAIN: iteration 13858 : loss : 0.131983, loss_ce: 0.003489, loss_dice: 0.260478
[09:59:57.396] TRAIN: iteration 13859 : loss : 0.251253, loss_ce: 0.002382, loss_dice: 0.500124
[09:59:57.604] TRAIN: iteration 13860 : loss : 0.142282, loss_ce: 0.002670, loss_dice: 0.281895
[10:00:03.054] TRAIN: iteration 13861 : loss : 0.234069, loss_ce: 0.003864, loss_dice: 0.464275
[10:00:03.262] TRAIN: iteration 13862 : loss : 0.114906, loss_ce: 0.002007, loss_dice: 0.227806
[10:00:03.468] TRAIN: iteration 13863 : loss : 0.056760, loss_ce: 0.004401, loss_dice: 0.109120
[10:00:07.860] TRAIN: iteration 13864 : loss : 0.060587, loss_ce: 0.002524, loss_dice: 0.118649
[10:00:08.066] TRAIN: iteration 13865 : loss : 0.155131, loss_ce: 0.008112, loss_dice: 0.302149
[10:00:08.273] TRAIN: iteration 13866 : loss : 0.080299, loss_ce: 0.005005, loss_dice: 0.155594
[10:00:08.480] TRAIN: iteration 13867 : loss : 0.251232, loss_ce: 0.002299, loss_dice: 0.500165
[10:00:08.686] TRAIN: iteration 13868 : loss : 0.188213, loss_ce: 0.002260, loss_dice: 0.374167
[10:00:11.166] TRAIN: iteration 13869 : loss : 0.050148, loss_ce: 0.003312, loss_dice: 0.096984
[10:00:11.557] TRAIN: iteration 13870 : loss : 0.100707, loss_ce: 0.016752, loss_dice: 0.184661
[10:00:11.764] TRAIN: iteration 13871 : loss : 0.064440, loss_ce: 0.006810, loss_dice: 0.122070
[10:00:18.154] TRAIN: iteration 13872 : loss : 0.051285, loss_ce: 0.003506, loss_dice: 0.099065
[10:00:18.362] TRAIN: iteration 13873 : loss : 0.177733, loss_ce: 0.008300, loss_dice: 0.347167
[10:00:18.569] TRAIN: iteration 13874 : loss : 0.179687, loss_ce: 0.003141, loss_dice: 0.356234
[10:00:18.777] TRAIN: iteration 13875 : loss : 0.164098, loss_ce: 0.003030, loss_dice: 0.325166
[10:00:18.990] TRAIN: iteration 13876 : loss : 0.198459, loss_ce: 0.004082, loss_dice: 0.392835
[10:00:19.198] TRAIN: iteration 13877 : loss : 0.035633, loss_ce: 0.001486, loss_dice: 0.069780
[10:00:20.319] TRAIN: iteration 13878 : loss : 0.218428, loss_ce: 0.003468, loss_dice: 0.433388
[10:00:20.526] TRAIN: iteration 13879 : loss : 0.094381, loss_ce: 0.011022, loss_dice: 0.177741
[10:00:20.851] TRAIN: iteration 13880 : loss : 0.160573, loss_ce: 0.005454, loss_dice: 0.315693
[10:05:45.428] VALIDATION: iteration 7 : loss : 0.138380, loss_ce: 0.005649, loss_dice: 0.271111
[10:05:47.507] TRAIN: iteration 13881 : loss : 0.105300, loss_ce: 0.002615, loss_dice: 0.207984
[10:05:47.718] TRAIN: iteration 13882 : loss : 0.236796, loss_ce: 0.006137, loss_dice: 0.467455
[10:05:47.932] TRAIN: iteration 13883 : loss : 0.191458, loss_ce: 0.004331, loss_dice: 0.378584
[10:05:48.142] TRAIN: iteration 13884 : loss : 0.080057, loss_ce: 0.003766, loss_dice: 0.156348
[10:05:48.351] TRAIN: iteration 13885 : loss : 0.107869, loss_ce: 0.003846, loss_dice: 0.211892
[10:05:48.564] TRAIN: iteration 13886 : loss : 0.113007, loss_ce: 0.003599, loss_dice: 0.222415
[10:05:48.771] TRAIN: iteration 13887 : loss : 0.161192, loss_ce: 0.005218, loss_dice: 0.317167
[10:05:48.979] TRAIN: iteration 13888 : loss : 0.061599, loss_ce: 0.006052, loss_dice: 0.117146
[10:05:49.193] TRAIN: iteration 13889 : loss : 0.074196, loss_ce: 0.015945, loss_dice: 0.132448
[10:05:49.401] TRAIN: iteration 13890 : loss : 0.187780, loss_ce: 0.020890, loss_dice: 0.354671
[10:05:49.610] TRAIN: iteration 13891 : loss : 0.063693, loss_ce: 0.003746, loss_dice: 0.123640
[10:05:49.818] TRAIN: iteration 13892 : loss : 0.251447, loss_ce: 0.002726, loss_dice: 0.500168
[10:05:50.030] TRAIN: iteration 13893 : loss : 0.251449, loss_ce: 0.002698, loss_dice: 0.500199
[10:05:50.239] TRAIN: iteration 13894 : loss : 0.074618, loss_ce: 0.005005, loss_dice: 0.144231
[10:05:50.454] TRAIN: iteration 13895 : loss : 0.254766, loss_ce: 0.008821, loss_dice: 0.500712
[10:05:50.663] TRAIN: iteration 13896 : loss : 0.221514, loss_ce: 0.009045, loss_dice: 0.433982
[10:05:50.874] TRAIN: iteration 13897 : loss : 0.252371, loss_ce: 0.004406, loss_dice: 0.500337
[10:05:51.085] TRAIN: iteration 13898 : loss : 0.095767, loss_ce: 0.017439, loss_dice: 0.174094
[10:05:51.300] TRAIN: iteration 13899 : loss : 0.042372, loss_ce: 0.004064, loss_dice: 0.080680
[10:05:51.506] TRAIN: iteration 13900 : loss : 0.185379, loss_ce: 0.008882, loss_dice: 0.361876
[10:05:51.507] NaN or Inf found in input tensor.
[10:05:51.727] TRAIN: iteration 13901 : loss : 0.171370, loss_ce: 0.002234, loss_dice: 0.340506
[10:05:51.940] TRAIN: iteration 13902 : loss : 0.183060, loss_ce: 0.012915, loss_dice: 0.353204
[10:05:52.153] TRAIN: iteration 13903 : loss : 0.251676, loss_ce: 0.003171, loss_dice: 0.500182
[10:05:52.363] TRAIN: iteration 13904 : loss : 0.148854, loss_ce: 0.002080, loss_dice: 0.295628
[10:05:52.576] TRAIN: iteration 13905 : loss : 0.041956, loss_ce: 0.002234, loss_dice: 0.081678
[10:05:52.788] TRAIN: iteration 13906 : loss : 0.112963, loss_ce: 0.002037, loss_dice: 0.223889
[10:05:52.996] TRAIN: iteration 13907 : loss : 0.140372, loss_ce: 0.003334, loss_dice: 0.277410
[10:05:53.204] TRAIN: iteration 13908 : loss : 0.097392, loss_ce: 0.006302, loss_dice: 0.188481
[10:05:53.412] TRAIN: iteration 13909 : loss : 0.132319, loss_ce: 0.005583, loss_dice: 0.259055
[10:05:53.626] TRAIN: iteration 13910 : loss : 0.249381, loss_ce: 0.003454, loss_dice: 0.495307
[10:05:53.843] TRAIN: iteration 13911 : loss : 0.251072, loss_ce: 0.002311, loss_dice: 0.499833
[10:05:54.053] TRAIN: iteration 13912 : loss : 0.113047, loss_ce: 0.012200, loss_dice: 0.213893
[10:05:54.264] TRAIN: iteration 13913 : loss : 0.117083, loss_ce: 0.003925, loss_dice: 0.230242
[10:05:54.480] TRAIN: iteration 13914 : loss : 0.250664, loss_ce: 0.001282, loss_dice: 0.500046
[10:05:54.686] TRAIN: iteration 13915 : loss : 0.251238, loss_ce: 0.002343, loss_dice: 0.500133
[10:05:54.893] TRAIN: iteration 13916 : loss : 0.149822, loss_ce: 0.008360, loss_dice: 0.291284
[10:05:55.106] TRAIN: iteration 13917 : loss : 0.110962, loss_ce: 0.002618, loss_dice: 0.219307
[10:05:55.312] TRAIN: iteration 13918 : loss : 0.194836, loss_ce: 0.013094, loss_dice: 0.376578
[10:05:55.520] TRAIN: iteration 13919 : loss : 0.141551, loss_ce: 0.017382, loss_dice: 0.265720
[10:05:55.734] TRAIN: iteration 13920 : loss : 0.252216, loss_ce: 0.004138, loss_dice: 0.500294
[10:05:55.972] TRAIN: iteration 13921 : loss : 0.251344, loss_ce: 0.002539, loss_dice: 0.500149
[10:05:56.179] TRAIN: iteration 13922 : loss : 0.250787, loss_ce: 0.001523, loss_dice: 0.500051
[10:05:56.387] TRAIN: iteration 13923 : loss : 0.050369, loss_ce: 0.006779, loss_dice: 0.093958
[10:05:56.594] TRAIN: iteration 13924 : loss : 0.129525, loss_ce: 0.011686, loss_dice: 0.247365
[10:05:56.802] TRAIN: iteration 13925 : loss : 0.134195, loss_ce: 0.007894, loss_dice: 0.260496
[10:05:57.013] TRAIN: iteration 13926 : loss : 0.205866, loss_ce: 0.004734, loss_dice: 0.406997
[10:05:57.375] TRAIN: iteration 13927 : loss : 0.237146, loss_ce: 0.004966, loss_dice: 0.469326
[10:05:57.583] TRAIN: iteration 13928 : loss : 0.086606, loss_ce: 0.002633, loss_dice: 0.170578
[10:05:57.791] TRAIN: iteration 13929 : loss : 0.039085, loss_ce: 0.006303, loss_dice: 0.071866
[10:05:58.005] TRAIN: iteration 13930 : loss : 0.173893, loss_ce: 0.005379, loss_dice: 0.342407
[10:05:58.218] TRAIN: iteration 13931 : loss : 0.142263, loss_ce: 0.004239, loss_dice: 0.280287
[10:05:58.428] TRAIN: iteration 13932 : loss : 0.108524, loss_ce: 0.005227, loss_dice: 0.211821
[10:05:58.642] TRAIN: iteration 13933 : loss : 0.185716, loss_ce: 0.017069, loss_dice: 0.354363
[10:05:58.857] TRAIN: iteration 13934 : loss : 0.250981, loss_ce: 0.008255, loss_dice: 0.493708
[10:05:59.145] TRAIN: iteration 13935 : loss : 0.191621, loss_ce: 0.004148, loss_dice: 0.379095
[10:05:59.356] TRAIN: iteration 13936 : loss : 0.140810, loss_ce: 0.006598, loss_dice: 0.275023
[10:06:00.325] TRAIN: iteration 13937 : loss : 0.240425, loss_ce: 0.007123, loss_dice: 0.473727
[10:06:00.534] TRAIN: iteration 13938 : loss : 0.062483, loss_ce: 0.004006, loss_dice: 0.120961
[10:06:00.740] TRAIN: iteration 13939 : loss : 0.051256, loss_ce: 0.007578, loss_dice: 0.094933
[10:06:00.947] TRAIN: iteration 13940 : loss : 0.049456, loss_ce: 0.006453, loss_dice: 0.092459
[10:06:01.192] TRAIN: iteration 13941 : loss : 0.157680, loss_ce: 0.004563, loss_dice: 0.310797
[10:06:01.404] TRAIN: iteration 13942 : loss : 0.203805, loss_ce: 0.004261, loss_dice: 0.403349
[10:06:01.611] TRAIN: iteration 13943 : loss : 0.064727, loss_ce: 0.004203, loss_dice: 0.125251
[10:06:01.817] TRAIN: iteration 13944 : loss : 0.250242, loss_ce: 0.011714, loss_dice: 0.488771
[10:06:02.025] TRAIN: iteration 13945 : loss : 0.117604, loss_ce: 0.003659, loss_dice: 0.231548
[10:06:02.233] TRAIN: iteration 13946 : loss : 0.248463, loss_ce: 0.004472, loss_dice: 0.492454
[10:06:02.442] TRAIN: iteration 13947 : loss : 0.123668, loss_ce: 0.004261, loss_dice: 0.243075
[10:06:02.649] TRAIN: iteration 13948 : loss : 0.148684, loss_ce: 0.002170, loss_dice: 0.295198
[10:06:02.864] TRAIN: iteration 13949 : loss : 0.250423, loss_ce: 0.000839, loss_dice: 0.500006
[10:06:03.073] TRAIN: iteration 13950 : loss : 0.250402, loss_ce: 0.000790, loss_dice: 0.500014
[10:06:03.282] TRAIN: iteration 13951 : loss : 0.033207, loss_ce: 0.002340, loss_dice: 0.064075
[10:06:03.503] TRAIN: iteration 13952 : loss : 0.207401, loss_ce: 0.001972, loss_dice: 0.412830
[10:06:03.711] TRAIN: iteration 13953 : loss : 0.102238, loss_ce: 0.005514, loss_dice: 0.198962
[10:06:03.920] TRAIN: iteration 13954 : loss : 0.240015, loss_ce: 0.005599, loss_dice: 0.474430
[10:06:04.127] TRAIN: iteration 13955 : loss : 0.105274, loss_ce: 0.003082, loss_dice: 0.207467
[10:06:04.342] TRAIN: iteration 13956 : loss : 0.107428, loss_ce: 0.007245, loss_dice: 0.207611
[10:06:04.549] TRAIN: iteration 13957 : loss : 0.194356, loss_ce: 0.001896, loss_dice: 0.386815
[10:06:04.758] TRAIN: iteration 13958 : loss : 0.241242, loss_ce: 0.004093, loss_dice: 0.478390
[10:06:04.971] TRAIN: iteration 13959 : loss : 0.185621, loss_ce: 0.001652, loss_dice: 0.369591
[10:06:05.181] TRAIN: iteration 13960 : loss : 0.240444, loss_ce: 0.005300, loss_dice: 0.475589
[10:06:05.419] TRAIN: iteration 13961 : loss : 0.129331, loss_ce: 0.002459, loss_dice: 0.256204
[10:06:05.628] TRAIN: iteration 13962 : loss : 0.251700, loss_ce: 0.003182, loss_dice: 0.500217
[10:06:05.836] TRAIN: iteration 13963 : loss : 0.092234, loss_ce: 0.006911, loss_dice: 0.177556
[10:06:06.047] TRAIN: iteration 13964 : loss : 0.176536, loss_ce: 0.005607, loss_dice: 0.347466
[10:06:06.256] TRAIN: iteration 13965 : loss : 0.212603, loss_ce: 0.002635, loss_dice: 0.422571
[10:06:06.463] TRAIN: iteration 13966 : loss : 0.084692, loss_ce: 0.007137, loss_dice: 0.162247
[10:06:06.670] TRAIN: iteration 13967 : loss : 0.031921, loss_ce: 0.001770, loss_dice: 0.062072
[10:06:06.877] TRAIN: iteration 13968 : loss : 0.085324, loss_ce: 0.003994, loss_dice: 0.166654
[10:06:07.084] TRAIN: iteration 13969 : loss : 0.080594, loss_ce: 0.005045, loss_dice: 0.156144
[10:06:07.293] TRAIN: iteration 13970 : loss : 0.047031, loss_ce: 0.007022, loss_dice: 0.087041
[10:06:07.502] TRAIN: iteration 13971 : loss : 0.095045, loss_ce: 0.019314, loss_dice: 0.170776
[10:06:07.711] TRAIN: iteration 13972 : loss : 0.100838, loss_ce: 0.005518, loss_dice: 0.196159
[10:06:07.919] TRAIN: iteration 13973 : loss : 0.149763, loss_ce: 0.006335, loss_dice: 0.293192
[10:06:08.128] TRAIN: iteration 13974 : loss : 0.251542, loss_ce: 0.002931, loss_dice: 0.500154
[10:06:08.335] TRAIN: iteration 13975 : loss : 0.099031, loss_ce: 0.004684, loss_dice: 0.193377
[10:06:08.543] TRAIN: iteration 13976 : loss : 0.214085, loss_ce: 0.003495, loss_dice: 0.424675
[10:06:08.751] TRAIN: iteration 13977 : loss : 0.251909, loss_ce: 0.004948, loss_dice: 0.498870
[10:06:08.958] TRAIN: iteration 13978 : loss : 0.079871, loss_ce: 0.003293, loss_dice: 0.156450
[10:06:09.165] TRAIN: iteration 13979 : loss : 0.195772, loss_ce: 0.006016, loss_dice: 0.385527
[10:06:09.372] TRAIN: iteration 13980 : loss : 0.055649, loss_ce: 0.002537, loss_dice: 0.108760
[10:06:09.607] TRAIN: iteration 13981 : loss : 0.074138, loss_ce: 0.004154, loss_dice: 0.144122
[10:06:09.864] TRAIN: iteration 13982 : loss : 0.021060, loss_ce: 0.001436, loss_dice: 0.040683
[10:06:10.074] TRAIN: iteration 13983 : loss : 0.104511, loss_ce: 0.003708, loss_dice: 0.205315
[10:06:10.291] TRAIN: iteration 13984 : loss : 0.245179, loss_ce: 0.005026, loss_dice: 0.485332
[10:06:10.499] TRAIN: iteration 13985 : loss : 0.251216, loss_ce: 0.002305, loss_dice: 0.500126
[10:06:10.707] TRAIN: iteration 13986 : loss : 0.138188, loss_ce: 0.004447, loss_dice: 0.271929
[10:06:10.924] TRAIN: iteration 13987 : loss : 0.098157, loss_ce: 0.005957, loss_dice: 0.190357
[10:06:11.136] TRAIN: iteration 13988 : loss : 0.242475, loss_ce: 0.003659, loss_dice: 0.481292
[10:06:11.351] TRAIN: iteration 13989 : loss : 0.102548, loss_ce: 0.003778, loss_dice: 0.201318
[10:06:11.562] TRAIN: iteration 13990 : loss : 0.110168, loss_ce: 0.002132, loss_dice: 0.218205
[10:06:11.772] TRAIN: iteration 13991 : loss : 0.106835, loss_ce: 0.003095, loss_dice: 0.210574
[10:06:11.983] TRAIN: iteration 13992 : loss : 0.087116, loss_ce: 0.001892, loss_dice: 0.172339
[10:06:12.199] TRAIN: iteration 13993 : loss : 0.251072, loss_ce: 0.002047, loss_dice: 0.500097
[10:06:12.412] TRAIN: iteration 13994 : loss : 0.082156, loss_ce: 0.007962, loss_dice: 0.156350
[10:06:12.620] TRAIN: iteration 13995 : loss : 0.039968, loss_ce: 0.001566, loss_dice: 0.078371
[10:06:12.828] TRAIN: iteration 13996 : loss : 0.251788, loss_ce: 0.003350, loss_dice: 0.500225
[10:06:13.037] TRAIN: iteration 13997 : loss : 0.177231, loss_ce: 0.004546, loss_dice: 0.349917
[10:06:13.249] TRAIN: iteration 13998 : loss : 0.251615, loss_ce: 0.003035, loss_dice: 0.500195
[10:06:13.461] TRAIN: iteration 13999 : loss : 0.113269, loss_ce: 0.009257, loss_dice: 0.217281
[10:06:13.673] TRAIN: iteration 14000 : loss : 0.245728, loss_ce: 0.003504, loss_dice: 0.487952
[10:06:13.911] TRAIN: iteration 14001 : loss : 0.251148, loss_ce: 0.002167, loss_dice: 0.500128
[10:06:14.122] TRAIN: iteration 14002 : loss : 0.145111, loss_ce: 0.013744, loss_dice: 0.276477
[10:06:14.332] TRAIN: iteration 14003 : loss : 0.152966, loss_ce: 0.004232, loss_dice: 0.301700
[10:06:14.540] TRAIN: iteration 14004 : loss : 0.030706, loss_ce: 0.003676, loss_dice: 0.057736
[10:06:14.750] TRAIN: iteration 14005 : loss : 0.154833, loss_ce: 0.012412, loss_dice: 0.297254
[10:06:14.962] TRAIN: iteration 14006 : loss : 0.251915, loss_ce: 0.003595, loss_dice: 0.500234
[10:06:15.170] TRAIN: iteration 14007 : loss : 0.081104, loss_ce: 0.011806, loss_dice: 0.150402
[10:06:15.380] TRAIN: iteration 14008 : loss : 0.122840, loss_ce: 0.012224, loss_dice: 0.233456
[10:06:15.590] TRAIN: iteration 14009 : loss : 0.211105, loss_ce: 0.006688, loss_dice: 0.415521
[10:06:15.798] TRAIN: iteration 14010 : loss : 0.053894, loss_ce: 0.003491, loss_dice: 0.104298
[10:06:16.007] TRAIN: iteration 14011 : loss : 0.065064, loss_ce: 0.011152, loss_dice: 0.118976
[10:06:16.215] TRAIN: iteration 14012 : loss : 0.095770, loss_ce: 0.009668, loss_dice: 0.181872
[10:06:16.423] TRAIN: iteration 14013 : loss : 0.161455, loss_ce: 0.002918, loss_dice: 0.319993
[10:06:16.632] TRAIN: iteration 14014 : loss : 0.186285, loss_ce: 0.010435, loss_dice: 0.362135
[10:06:16.839] TRAIN: iteration 14015 : loss : 0.072348, loss_ce: 0.003913, loss_dice: 0.140782
[10:06:17.047] TRAIN: iteration 14016 : loss : 0.251672, loss_ce: 0.003178, loss_dice: 0.500165
[10:06:17.255] TRAIN: iteration 14017 : loss : 0.241918, loss_ce: 0.004949, loss_dice: 0.478887
[10:06:17.463] TRAIN: iteration 14018 : loss : 0.070911, loss_ce: 0.005106, loss_dice: 0.136717
[10:06:17.674] TRAIN: iteration 14019 : loss : 0.252857, loss_ce: 0.005347, loss_dice: 0.500367
[10:06:17.883] TRAIN: iteration 14020 : loss : 0.043264, loss_ce: 0.005999, loss_dice: 0.080529
[10:06:18.128] TRAIN: iteration 14021 : loss : 0.241064, loss_ce: 0.005366, loss_dice: 0.476762
[10:06:18.335] TRAIN: iteration 14022 : loss : 0.029032, loss_ce: 0.002971, loss_dice: 0.055093
[10:06:18.541] TRAIN: iteration 14023 : loss : 0.165314, loss_ce: 0.003683, loss_dice: 0.326945
[10:06:18.748] TRAIN: iteration 14024 : loss : 0.229392, loss_ce: 0.002535, loss_dice: 0.456248
[10:06:18.955] TRAIN: iteration 14025 : loss : 0.191383, loss_ce: 0.005559, loss_dice: 0.377206
[10:06:19.163] TRAIN: iteration 14026 : loss : 0.092514, loss_ce: 0.003650, loss_dice: 0.181378
[10:06:19.371] TRAIN: iteration 14027 : loss : 0.251518, loss_ce: 0.002864, loss_dice: 0.500172
[10:06:19.581] TRAIN: iteration 14028 : loss : 0.184404, loss_ce: 0.003167, loss_dice: 0.365640
[10:06:19.790] TRAIN: iteration 14029 : loss : 0.141303, loss_ce: 0.009939, loss_dice: 0.272666
[10:06:20.001] TRAIN: iteration 14030 : loss : 0.251429, loss_ce: 0.002711, loss_dice: 0.500147
[10:06:20.208] TRAIN: iteration 14031 : loss : 0.035074, loss_ce: 0.005825, loss_dice: 0.064323
[10:06:20.419] TRAIN: iteration 14032 : loss : 0.055610, loss_ce: 0.003638, loss_dice: 0.107582
[10:06:20.633] TRAIN: iteration 14033 : loss : 0.069743, loss_ce: 0.003033, loss_dice: 0.136453
[10:06:20.847] TRAIN: iteration 14034 : loss : 0.252032, loss_ce: 0.003836, loss_dice: 0.500228
[10:06:21.055] TRAIN: iteration 14035 : loss : 0.253948, loss_ce: 0.010405, loss_dice: 0.497491
[10:06:21.262] TRAIN: iteration 14036 : loss : 0.187796, loss_ce: 0.003812, loss_dice: 0.371780
[10:06:21.474] TRAIN: iteration 14037 : loss : 0.121193, loss_ce: 0.003803, loss_dice: 0.238583
[10:06:21.690] TRAIN: iteration 14038 : loss : 0.125830, loss_ce: 0.003928, loss_dice: 0.247733
[10:06:21.901] TRAIN: iteration 14039 : loss : 0.123681, loss_ce: 0.002951, loss_dice: 0.244411
[10:06:22.109] TRAIN: iteration 14040 : loss : 0.251530, loss_ce: 0.002879, loss_dice: 0.500180
[10:06:22.352] TRAIN: iteration 14041 : loss : 0.251333, loss_ce: 0.002532, loss_dice: 0.500134
[10:06:22.565] TRAIN: iteration 14042 : loss : 0.158055, loss_ce: 0.002592, loss_dice: 0.313518
[10:06:22.772] TRAIN: iteration 14043 : loss : 0.061322, loss_ce: 0.005211, loss_dice: 0.117433
[10:06:22.986] TRAIN: iteration 14044 : loss : 0.228847, loss_ce: 0.001574, loss_dice: 0.456119
[10:06:23.197] TRAIN: iteration 14045 : loss : 0.104483, loss_ce: 0.017696, loss_dice: 0.191271
[10:06:23.412] TRAIN: iteration 14046 : loss : 0.242722, loss_ce: 0.003140, loss_dice: 0.482305
[10:06:23.619] TRAIN: iteration 14047 : loss : 0.086589, loss_ce: 0.002429, loss_dice: 0.170749
[10:06:23.826] TRAIN: iteration 14048 : loss : 0.251145, loss_ce: 0.002168, loss_dice: 0.500121
[10:06:24.034] TRAIN: iteration 14049 : loss : 0.150991, loss_ce: 0.002037, loss_dice: 0.299944
[10:06:24.253] TRAIN: iteration 14050 : loss : 0.243683, loss_ce: 0.003558, loss_dice: 0.483808
[10:06:24.466] TRAIN: iteration 14051 : loss : 0.250885, loss_ce: 0.022120, loss_dice: 0.479649
[10:06:24.674] TRAIN: iteration 14052 : loss : 0.091061, loss_ce: 0.005900, loss_dice: 0.176221
[10:06:24.880] TRAIN: iteration 14053 : loss : 0.160181, loss_ce: 0.002860, loss_dice: 0.317503
[10:06:25.088] TRAIN: iteration 14054 : loss : 0.061256, loss_ce: 0.003538, loss_dice: 0.118974
[10:06:25.296] TRAIN: iteration 14055 : loss : 0.170816, loss_ce: 0.002579, loss_dice: 0.339053
[10:06:25.503] TRAIN: iteration 14056 : loss : 0.118415, loss_ce: 0.004892, loss_dice: 0.231938
[10:06:25.711] TRAIN: iteration 14057 : loss : 0.165894, loss_ce: 0.001754, loss_dice: 0.330034
[10:06:25.919] TRAIN: iteration 14058 : loss : 0.106055, loss_ce: 0.001960, loss_dice: 0.210150
[10:06:26.126] TRAIN: iteration 14059 : loss : 0.115444, loss_ce: 0.002324, loss_dice: 0.228563
[10:06:26.333] TRAIN: iteration 14060 : loss : 0.057936, loss_ce: 0.001047, loss_dice: 0.114825
[10:06:26.565] TRAIN: iteration 14061 : loss : 0.249626, loss_ce: 0.001275, loss_dice: 0.497977
[10:06:26.773] TRAIN: iteration 14062 : loss : 0.101332, loss_ce: 0.005083, loss_dice: 0.197581
[10:06:26.990] TRAIN: iteration 14063 : loss : 0.251801, loss_ce: 0.003582, loss_dice: 0.500019
[10:06:27.199] TRAIN: iteration 14064 : loss : 0.087034, loss_ce: 0.002716, loss_dice: 0.171352
[10:06:27.407] TRAIN: iteration 14065 : loss : 0.223597, loss_ce: 0.002620, loss_dice: 0.444574
[10:06:27.616] TRAIN: iteration 14066 : loss : 0.115242, loss_ce: 0.007518, loss_dice: 0.222966
[10:06:27.825] TRAIN: iteration 14067 : loss : 0.072426, loss_ce: 0.001872, loss_dice: 0.142979
[10:06:28.035] TRAIN: iteration 14068 : loss : 0.115684, loss_ce: 0.003029, loss_dice: 0.228338
[10:06:28.243] TRAIN: iteration 14069 : loss : 0.124098, loss_ce: 0.001897, loss_dice: 0.246298
[10:06:28.452] TRAIN: iteration 14070 : loss : 0.099721, loss_ce: 0.004932, loss_dice: 0.194509
[10:06:28.660] TRAIN: iteration 14071 : loss : 0.251164, loss_ce: 0.002194, loss_dice: 0.500135
[10:06:28.874] TRAIN: iteration 14072 : loss : 0.088742, loss_ce: 0.005640, loss_dice: 0.171844
[10:06:29.082] TRAIN: iteration 14073 : loss : 0.036273, loss_ce: 0.002267, loss_dice: 0.070278
[10:06:29.296] TRAIN: iteration 14074 : loss : 0.090369, loss_ce: 0.002746, loss_dice: 0.177993
[10:06:29.515] TRAIN: iteration 14075 : loss : 0.069669, loss_ce: 0.008160, loss_dice: 0.131177
[10:06:29.721] TRAIN: iteration 14076 : loss : 0.160403, loss_ce: 0.013725, loss_dice: 0.307082
[10:06:29.932] TRAIN: iteration 14077 : loss : 0.043675, loss_ce: 0.004130, loss_dice: 0.083220
[10:06:30.147] TRAIN: iteration 14078 : loss : 0.102040, loss_ce: 0.011515, loss_dice: 0.192565
[10:06:30.355] TRAIN: iteration 14079 : loss : 0.133876, loss_ce: 0.004503, loss_dice: 0.263249
[10:06:30.563] TRAIN: iteration 14080 : loss : 0.084286, loss_ce: 0.004244, loss_dice: 0.164328
[10:06:30.800] TRAIN: iteration 14081 : loss : 0.137989, loss_ce: 0.004742, loss_dice: 0.271236
[10:06:31.008] TRAIN: iteration 14082 : loss : 0.188411, loss_ce: 0.003709, loss_dice: 0.373114
[10:06:31.219] TRAIN: iteration 14083 : loss : 0.042030, loss_ce: 0.006225, loss_dice: 0.077836
[10:06:31.426] TRAIN: iteration 14084 : loss : 0.058755, loss_ce: 0.007993, loss_dice: 0.109517
[10:06:31.636] TRAIN: iteration 14085 : loss : 0.240224, loss_ce: 0.005443, loss_dice: 0.475004
[10:06:31.846] TRAIN: iteration 14086 : loss : 0.060333, loss_ce: 0.005878, loss_dice: 0.114788
[10:06:32.054] TRAIN: iteration 14087 : loss : 0.252347, loss_ce: 0.004380, loss_dice: 0.500313
[10:06:32.266] TRAIN: iteration 14088 : loss : 0.096872, loss_ce: 0.009368, loss_dice: 0.184375
[10:06:32.477] TRAIN: iteration 14089 : loss : 0.247897, loss_ce: 0.002726, loss_dice: 0.493067
[10:06:32.685] TRAIN: iteration 14090 : loss : 0.252651, loss_ce: 0.006076, loss_dice: 0.499225
[10:06:32.896] TRAIN: iteration 14091 : loss : 0.076236, loss_ce: 0.009032, loss_dice: 0.143439
[10:06:33.105] TRAIN: iteration 14092 : loss : 0.080256, loss_ce: 0.004073, loss_dice: 0.156438
[10:06:33.315] TRAIN: iteration 14093 : loss : 0.167455, loss_ce: 0.005074, loss_dice: 0.329837
[10:06:33.525] TRAIN: iteration 14094 : loss : 0.160047, loss_ce: 0.003738, loss_dice: 0.316355
[10:06:33.732] TRAIN: iteration 14095 : loss : 0.054814, loss_ce: 0.009284, loss_dice: 0.100343
[10:06:33.941] TRAIN: iteration 14096 : loss : 0.073787, loss_ce: 0.008892, loss_dice: 0.138681
[10:06:34.149] TRAIN: iteration 14097 : loss : 0.051264, loss_ce: 0.002122, loss_dice: 0.100406
[10:06:34.357] TRAIN: iteration 14098 : loss : 0.058638, loss_ce: 0.006562, loss_dice: 0.110715
[10:06:34.568] TRAIN: iteration 14099 : loss : 0.247778, loss_ce: 0.003716, loss_dice: 0.491840
[10:06:34.780] TRAIN: iteration 14100 : loss : 0.056136, loss_ce: 0.003404, loss_dice: 0.108867
[10:06:35.019] TRAIN: iteration 14101 : loss : 0.093365, loss_ce: 0.005968, loss_dice: 0.180762
[10:06:35.231] TRAIN: iteration 14102 : loss : 0.074535, loss_ce: 0.006917, loss_dice: 0.142154
[10:06:35.447] TRAIN: iteration 14103 : loss : 0.165924, loss_ce: 0.013295, loss_dice: 0.318553
[10:06:35.653] TRAIN: iteration 14104 : loss : 0.052824, loss_ce: 0.003498, loss_dice: 0.102151
[10:06:35.867] TRAIN: iteration 14105 : loss : 0.067938, loss_ce: 0.002816, loss_dice: 0.133060
[10:06:36.075] TRAIN: iteration 14106 : loss : 0.250908, loss_ce: 0.002953, loss_dice: 0.498862
[10:06:36.293] TRAIN: iteration 14107 : loss : 0.082387, loss_ce: 0.005885, loss_dice: 0.158889
[10:06:36.503] TRAIN: iteration 14108 : loss : 0.249941, loss_ce: 0.003196, loss_dice: 0.496686
[10:06:36.711] TRAIN: iteration 14109 : loss : 0.128131, loss_ce: 0.021494, loss_dice: 0.234768
[10:06:36.919] TRAIN: iteration 14110 : loss : 0.081667, loss_ce: 0.006628, loss_dice: 0.156707
[10:06:37.127] TRAIN: iteration 14111 : loss : 0.254045, loss_ce: 0.007648, loss_dice: 0.500442
[10:06:37.336] TRAIN: iteration 14112 : loss : 0.048008, loss_ce: 0.004235, loss_dice: 0.091780
[10:06:37.548] TRAIN: iteration 14113 : loss : 0.236427, loss_ce: 0.006394, loss_dice: 0.466461
[10:06:37.759] TRAIN: iteration 14114 : loss : 0.173566, loss_ce: 0.004058, loss_dice: 0.343073
[10:06:37.988] TRAIN: iteration 14115 : loss : 0.194569, loss_ce: 0.004103, loss_dice: 0.385034
[10:06:38.201] TRAIN: iteration 14116 : loss : 0.252316, loss_ce: 0.004341, loss_dice: 0.500292
[10:06:38.409] TRAIN: iteration 14117 : loss : 0.255019, loss_ce: 0.010034, loss_dice: 0.500003
[10:06:38.617] TRAIN: iteration 14118 : loss : 0.096907, loss_ce: 0.006307, loss_dice: 0.187507
[10:06:38.824] TRAIN: iteration 14119 : loss : 0.081251, loss_ce: 0.002247, loss_dice: 0.160255
[10:06:39.033] TRAIN: iteration 14120 : loss : 0.054176, loss_ce: 0.010891, loss_dice: 0.097461
[10:06:39.270] TRAIN: iteration 14121 : loss : 0.196369, loss_ce: 0.004972, loss_dice: 0.387765
[10:06:39.478] TRAIN: iteration 14122 : loss : 0.166230, loss_ce: 0.003281, loss_dice: 0.329178
[10:06:39.689] TRAIN: iteration 14123 : loss : 0.233441, loss_ce: 0.004098, loss_dice: 0.462784
[10:06:39.902] TRAIN: iteration 14124 : loss : 0.204361, loss_ce: 0.005558, loss_dice: 0.403164
[10:06:40.109] TRAIN: iteration 14125 : loss : 0.248463, loss_ce: 0.006143, loss_dice: 0.490782
[10:06:40.329] TRAIN: iteration 14126 : loss : 0.227729, loss_ce: 0.006759, loss_dice: 0.448698
[10:06:40.536] TRAIN: iteration 14127 : loss : 0.138162, loss_ce: 0.003466, loss_dice: 0.272859
[10:06:40.744] TRAIN: iteration 14128 : loss : 0.171357, loss_ce: 0.002046, loss_dice: 0.340669
[10:06:40.961] TRAIN: iteration 14129 : loss : 0.103192, loss_ce: 0.008082, loss_dice: 0.198302
[10:06:41.170] TRAIN: iteration 14130 : loss : 0.157832, loss_ce: 0.003350, loss_dice: 0.312313
[10:06:41.383] TRAIN: iteration 14131 : loss : 0.035107, loss_ce: 0.002156, loss_dice: 0.068058
[10:06:41.596] TRAIN: iteration 14132 : loss : 0.215708, loss_ce: 0.002350, loss_dice: 0.429067
[10:06:41.810] TRAIN: iteration 14133 : loss : 0.252675, loss_ce: 0.005358, loss_dice: 0.499992
[10:06:42.025] TRAIN: iteration 14134 : loss : 0.247945, loss_ce: 0.005699, loss_dice: 0.490190
[10:06:42.240] TRAIN: iteration 14135 : loss : 0.089110, loss_ce: 0.002592, loss_dice: 0.175628
[10:06:42.453] TRAIN: iteration 14136 : loss : 0.107159, loss_ce: 0.002696, loss_dice: 0.211622
[10:06:42.662] TRAIN: iteration 14137 : loss : 0.070747, loss_ce: 0.004148, loss_dice: 0.137345
[10:06:42.887] TRAIN: iteration 14138 : loss : 0.064132, loss_ce: 0.003293, loss_dice: 0.124970
[10:06:43.097] TRAIN: iteration 14139 : loss : 0.060919, loss_ce: 0.001562, loss_dice: 0.120275
[10:06:43.306] TRAIN: iteration 14140 : loss : 0.250430, loss_ce: 0.000847, loss_dice: 0.500014
[10:06:43.307] NaN or Inf found in input tensor.
[10:06:43.533] TRAIN: iteration 14141 : loss : 0.216717, loss_ce: 0.003349, loss_dice: 0.430085
[10:06:43.741] TRAIN: iteration 14142 : loss : 0.250454, loss_ce: 0.000893, loss_dice: 0.500014
[10:06:43.954] TRAIN: iteration 14143 : loss : 0.116960, loss_ce: 0.001502, loss_dice: 0.232417
[10:06:44.169] TRAIN: iteration 14144 : loss : 0.250609, loss_ce: 0.001176, loss_dice: 0.500043
[10:06:44.376] TRAIN: iteration 14145 : loss : 0.189875, loss_ce: 0.008464, loss_dice: 0.371286
[10:06:44.585] TRAIN: iteration 14146 : loss : 0.250652, loss_ce: 0.001272, loss_dice: 0.500031
[10:06:44.794] TRAIN: iteration 14147 : loss : 0.250725, loss_ce: 0.001402, loss_dice: 0.500049
[10:06:45.001] TRAIN: iteration 14148 : loss : 0.176171, loss_ce: 0.005597, loss_dice: 0.346745
[10:06:45.209] TRAIN: iteration 14149 : loss : 0.245042, loss_ce: 0.003594, loss_dice: 0.486489
[10:06:45.418] TRAIN: iteration 14150 : loss : 0.198567, loss_ce: 0.016015, loss_dice: 0.381119
[10:06:45.630] TRAIN: iteration 14151 : loss : 0.251066, loss_ce: 0.002043, loss_dice: 0.500089
[10:06:45.840] TRAIN: iteration 14152 : loss : 0.104582, loss_ce: 0.003722, loss_dice: 0.205442
[10:06:46.047] TRAIN: iteration 14153 : loss : 0.102469, loss_ce: 0.003735, loss_dice: 0.201203
[10:06:46.257] TRAIN: iteration 14154 : loss : 0.166151, loss_ce: 0.004523, loss_dice: 0.327779
[10:06:46.464] TRAIN: iteration 14155 : loss : 0.071739, loss_ce: 0.004659, loss_dice: 0.138819
[10:06:46.672] TRAIN: iteration 14156 : loss : 0.115380, loss_ce: 0.004777, loss_dice: 0.225983
[10:06:46.888] TRAIN: iteration 14157 : loss : 0.098808, loss_ce: 0.003481, loss_dice: 0.194136
[10:06:47.102] TRAIN: iteration 14158 : loss : 0.096036, loss_ce: 0.003951, loss_dice: 0.188120
[10:06:47.310] TRAIN: iteration 14159 : loss : 0.122090, loss_ce: 0.004695, loss_dice: 0.239485
[10:06:47.518] TRAIN: iteration 14160 : loss : 0.252098, loss_ce: 0.003970, loss_dice: 0.500226
[10:06:47.762] TRAIN: iteration 14161 : loss : 0.083535, loss_ce: 0.011521, loss_dice: 0.155550
[10:06:47.974] TRAIN: iteration 14162 : loss : 0.168251, loss_ce: 0.004294, loss_dice: 0.332207
[10:06:48.183] TRAIN: iteration 14163 : loss : 0.111664, loss_ce: 0.011869, loss_dice: 0.211459
[10:06:48.393] TRAIN: iteration 14164 : loss : 0.230394, loss_ce: 0.005630, loss_dice: 0.455158
[10:06:48.601] TRAIN: iteration 14165 : loss : 0.135990, loss_ce: 0.006348, loss_dice: 0.265632
[10:06:48.810] TRAIN: iteration 14166 : loss : 0.166129, loss_ce: 0.031417, loss_dice: 0.300841
[10:06:49.019] TRAIN: iteration 14167 : loss : 0.237741, loss_ce: 0.003899, loss_dice: 0.471583
[10:06:49.228] TRAIN: iteration 14168 : loss : 0.072463, loss_ce: 0.003171, loss_dice: 0.141754
[10:06:49.436] TRAIN: iteration 14169 : loss : 0.145667, loss_ce: 0.005246, loss_dice: 0.286087
[10:06:49.653] TRAIN: iteration 14170 : loss : 0.242775, loss_ce: 0.005119, loss_dice: 0.480431
[10:06:49.860] TRAIN: iteration 14171 : loss : 0.087170, loss_ce: 0.007571, loss_dice: 0.166768
[10:06:50.068] TRAIN: iteration 14172 : loss : 0.252014, loss_ce: 0.003787, loss_dice: 0.500242
[10:06:50.276] TRAIN: iteration 14173 : loss : 0.159178, loss_ce: 0.004155, loss_dice: 0.314201
[10:06:50.485] TRAIN: iteration 14174 : loss : 0.251776, loss_ce: 0.003332, loss_dice: 0.500220
[10:06:50.691] TRAIN: iteration 14175 : loss : 0.252071, loss_ce: 0.003947, loss_dice: 0.500195
[10:06:50.901] TRAIN: iteration 14176 : loss : 0.116588, loss_ce: 0.010419, loss_dice: 0.222757
[10:06:51.109] TRAIN: iteration 14177 : loss : 0.122526, loss_ce: 0.004026, loss_dice: 0.241025
[10:06:51.316] TRAIN: iteration 14178 : loss : 0.106021, loss_ce: 0.003219, loss_dice: 0.208823
[10:06:51.523] TRAIN: iteration 14179 : loss : 0.251732, loss_ce: 0.003264, loss_dice: 0.500200
[10:06:51.730] TRAIN: iteration 14180 : loss : 0.052041, loss_ce: 0.004266, loss_dice: 0.099816
[10:06:51.960] TRAIN: iteration 14181 : loss : 0.107797, loss_ce: 0.009593, loss_dice: 0.206002
[10:06:52.173] TRAIN: iteration 14182 : loss : 0.074118, loss_ce: 0.002850, loss_dice: 0.145386
[10:06:52.381] TRAIN: iteration 14183 : loss : 0.047504, loss_ce: 0.005372, loss_dice: 0.089636
[10:06:52.589] TRAIN: iteration 14184 : loss : 0.071812, loss_ce: 0.004584, loss_dice: 0.139041
[10:06:52.796] TRAIN: iteration 14185 : loss : 0.049755, loss_ce: 0.006105, loss_dice: 0.093404
[10:06:53.006] TRAIN: iteration 14186 : loss : 0.250840, loss_ce: 0.001622, loss_dice: 0.500058
[10:06:53.219] TRAIN: iteration 14187 : loss : 0.105197, loss_ce: 0.003591, loss_dice: 0.206803
[10:06:53.427] TRAIN: iteration 14188 : loss : 0.180266, loss_ce: 0.004204, loss_dice: 0.356329
[10:06:53.634] TRAIN: iteration 14189 : loss : 0.092615, loss_ce: 0.002767, loss_dice: 0.182464
[10:06:53.842] TRAIN: iteration 14190 : loss : 0.117067, loss_ce: 0.004474, loss_dice: 0.229660
[10:06:54.050] TRAIN: iteration 14191 : loss : 0.233546, loss_ce: 0.004255, loss_dice: 0.462837
[10:06:54.258] TRAIN: iteration 14192 : loss : 0.085068, loss_ce: 0.009142, loss_dice: 0.160995
[10:06:54.465] TRAIN: iteration 14193 : loss : 0.109340, loss_ce: 0.007124, loss_dice: 0.211556
[10:06:54.679] TRAIN: iteration 14194 : loss : 0.085739, loss_ce: 0.002011, loss_dice: 0.169467
[10:06:54.887] TRAIN: iteration 14195 : loss : 0.110798, loss_ce: 0.004122, loss_dice: 0.217473
[10:06:55.096] TRAIN: iteration 14196 : loss : 0.154625, loss_ce: 0.006937, loss_dice: 0.302312
[10:06:55.310] TRAIN: iteration 14197 : loss : 0.126501, loss_ce: 0.010573, loss_dice: 0.242428
[10:06:55.520] TRAIN: iteration 14198 : loss : 0.047231, loss_ce: 0.004486, loss_dice: 0.089975
[10:06:55.731] TRAIN: iteration 14199 : loss : 0.249376, loss_ce: 0.004357, loss_dice: 0.494395
[10:06:55.942] TRAIN: iteration 14200 : loss : 0.097860, loss_ce: 0.011568, loss_dice: 0.184152
[10:06:56.180] TRAIN: iteration 14201 : loss : 0.158532, loss_ce: 0.002236, loss_dice: 0.314828
[10:06:56.393] TRAIN: iteration 14202 : loss : 0.070980, loss_ce: 0.002369, loss_dice: 0.139591
[10:06:56.607] TRAIN: iteration 14203 : loss : 0.077602, loss_ce: 0.006934, loss_dice: 0.148270
[10:06:56.829] TRAIN: iteration 14204 : loss : 0.233563, loss_ce: 0.001129, loss_dice: 0.465997
[10:06:57.038] TRAIN: iteration 14205 : loss : 0.250858, loss_ce: 0.001992, loss_dice: 0.499724
[10:06:57.256] TRAIN: iteration 14206 : loss : 0.058970, loss_ce: 0.001924, loss_dice: 0.116016
[10:06:57.465] TRAIN: iteration 14207 : loss : 0.100576, loss_ce: 0.014560, loss_dice: 0.186592
[10:06:57.672] TRAIN: iteration 14208 : loss : 0.106222, loss_ce: 0.003005, loss_dice: 0.209439
[10:06:57.881] TRAIN: iteration 14209 : loss : 0.251555, loss_ce: 0.002910, loss_dice: 0.500200
[10:06:58.090] TRAIN: iteration 14210 : loss : 0.102596, loss_ce: 0.004862, loss_dice: 0.200329
[10:06:58.297] TRAIN: iteration 14211 : loss : 0.130923, loss_ce: 0.006963, loss_dice: 0.254882
[10:06:58.507] TRAIN: iteration 14212 : loss : 0.127783, loss_ce: 0.002302, loss_dice: 0.253265
[10:06:58.717] TRAIN: iteration 14213 : loss : 0.251406, loss_ce: 0.002648, loss_dice: 0.500164
[10:06:58.924] TRAIN: iteration 14214 : loss : 0.110005, loss_ce: 0.003523, loss_dice: 0.216488
[10:06:59.135] TRAIN: iteration 14215 : loss : 0.042636, loss_ce: 0.002833, loss_dice: 0.082440
[10:06:59.345] TRAIN: iteration 14216 : loss : 0.251655, loss_ce: 0.003090, loss_dice: 0.500220
[10:06:59.555] TRAIN: iteration 14217 : loss : 0.060641, loss_ce: 0.002563, loss_dice: 0.118720
[10:06:59.768] TRAIN: iteration 14218 : loss : 0.204830, loss_ce: 0.005329, loss_dice: 0.404331
[10:06:59.977] TRAIN: iteration 14219 : loss : 0.251864, loss_ce: 0.006883, loss_dice: 0.496846
[10:07:00.191] TRAIN: iteration 14220 : loss : 0.074660, loss_ce: 0.005524, loss_dice: 0.143796
[10:07:00.427] TRAIN: iteration 14221 : loss : 0.053981, loss_ce: 0.001905, loss_dice: 0.106057
[10:07:00.645] TRAIN: iteration 14222 : loss : 0.251010, loss_ce: 0.004260, loss_dice: 0.497759
[10:07:00.857] TRAIN: iteration 14223 : loss : 0.242483, loss_ce: 0.003115, loss_dice: 0.481850
[10:07:01.068] TRAIN: iteration 14224 : loss : 0.079474, loss_ce: 0.003992, loss_dice: 0.154956
[10:07:01.286] TRAIN: iteration 14225 : loss : 0.135119, loss_ce: 0.002881, loss_dice: 0.267356
[10:07:01.496] TRAIN: iteration 14226 : loss : 0.250400, loss_ce: 0.000785, loss_dice: 0.500015
[10:07:01.706] TRAIN: iteration 14227 : loss : 0.129152, loss_ce: 0.006248, loss_dice: 0.252056
[10:07:01.915] TRAIN: iteration 14228 : loss : 0.156831, loss_ce: 0.003191, loss_dice: 0.310471
[10:07:02.123] TRAIN: iteration 14229 : loss : 0.112836, loss_ce: 0.007951, loss_dice: 0.217721
[10:07:02.332] TRAIN: iteration 14230 : loss : 0.118743, loss_ce: 0.002856, loss_dice: 0.234630
[10:07:02.547] TRAIN: iteration 14231 : loss : 0.065852, loss_ce: 0.012383, loss_dice: 0.119321
[10:07:02.759] TRAIN: iteration 14232 : loss : 0.074705, loss_ce: 0.005438, loss_dice: 0.143972
[10:07:02.992] TRAIN: iteration 14233 : loss : 0.130241, loss_ce: 0.004195, loss_dice: 0.256288
[10:07:03.201] TRAIN: iteration 14234 : loss : 0.062667, loss_ce: 0.002690, loss_dice: 0.122644
[10:07:03.413] TRAIN: iteration 14235 : loss : 0.066418, loss_ce: 0.006565, loss_dice: 0.126270
[10:07:03.622] TRAIN: iteration 14236 : loss : 0.036435, loss_ce: 0.002815, loss_dice: 0.070056
[10:07:03.831] TRAIN: iteration 14237 : loss : 0.046763, loss_ce: 0.002252, loss_dice: 0.091274
[10:07:04.044] TRAIN: iteration 14238 : loss : 0.063569, loss_ce: 0.005375, loss_dice: 0.121762
[10:07:04.256] TRAIN: iteration 14239 : loss : 0.113320, loss_ce: 0.002922, loss_dice: 0.223717
[10:07:04.465] TRAIN: iteration 14240 : loss : 0.182284, loss_ce: 0.004924, loss_dice: 0.359644
[10:07:04.707] TRAIN: iteration 14241 : loss : 0.252268, loss_ce: 0.004230, loss_dice: 0.500305
[10:07:04.916] TRAIN: iteration 14242 : loss : 0.036400, loss_ce: 0.001588, loss_dice: 0.071213
[10:07:05.124] TRAIN: iteration 14243 : loss : 0.107680, loss_ce: 0.003027, loss_dice: 0.212332
[10:07:05.331] TRAIN: iteration 14244 : loss : 0.139175, loss_ce: 0.005082, loss_dice: 0.273269
[10:07:05.539] TRAIN: iteration 14245 : loss : 0.131742, loss_ce: 0.005791, loss_dice: 0.257693
[10:07:05.747] TRAIN: iteration 14246 : loss : 0.251984, loss_ce: 0.004458, loss_dice: 0.499509
[10:07:05.998] TRAIN: iteration 14247 : loss : 0.076747, loss_ce: 0.003926, loss_dice: 0.149567
[10:07:06.206] TRAIN: iteration 14248 : loss : 0.253228, loss_ce: 0.006967, loss_dice: 0.499488
[10:07:06.419] TRAIN: iteration 14249 : loss : 0.154787, loss_ce: 0.002022, loss_dice: 0.307551
[10:07:06.627] TRAIN: iteration 14250 : loss : 0.062773, loss_ce: 0.003842, loss_dice: 0.121703
[10:07:06.837] TRAIN: iteration 14251 : loss : 0.153298, loss_ce: 0.011881, loss_dice: 0.294716
[10:07:07.045] TRAIN: iteration 14252 : loss : 0.026771, loss_ce: 0.001475, loss_dice: 0.052068
[10:07:07.263] TRAIN: iteration 14253 : loss : 0.117721, loss_ce: 0.005577, loss_dice: 0.229865
[10:07:07.478] TRAIN: iteration 14254 : loss : 0.229314, loss_ce: 0.007115, loss_dice: 0.451512
[10:07:07.688] TRAIN: iteration 14255 : loss : 0.185935, loss_ce: 0.004270, loss_dice: 0.367600
[10:07:07.895] TRAIN: iteration 14256 : loss : 0.099537, loss_ce: 0.003314, loss_dice: 0.195760
[10:07:08.119] TRAIN: iteration 14257 : loss : 0.013277, loss_ce: 0.001327, loss_dice: 0.025228
[10:07:08.334] TRAIN: iteration 14258 : loss : 0.137181, loss_ce: 0.010011, loss_dice: 0.264350
[10:07:08.548] TRAIN: iteration 14259 : loss : 0.048021, loss_ce: 0.004038, loss_dice: 0.092004
[10:07:08.756] TRAIN: iteration 14260 : loss : 0.251644, loss_ce: 0.003065, loss_dice: 0.500222
[10:07:09.002] TRAIN: iteration 14261 : loss : 0.041904, loss_ce: 0.006498, loss_dice: 0.077310
[10:07:09.210] TRAIN: iteration 14262 : loss : 0.095175, loss_ce: 0.005757, loss_dice: 0.184594
[10:07:09.419] TRAIN: iteration 14263 : loss : 0.203614, loss_ce: 0.004221, loss_dice: 0.403007
[10:07:09.626] TRAIN: iteration 14264 : loss : 0.251958, loss_ce: 0.003651, loss_dice: 0.500265
[10:07:09.833] TRAIN: iteration 14265 : loss : 0.261695, loss_ce: 0.023111, loss_dice: 0.500279
[10:07:10.041] TRAIN: iteration 14266 : loss : 0.153582, loss_ce: 0.008384, loss_dice: 0.298780
[10:07:10.252] TRAIN: iteration 14267 : loss : 0.114003, loss_ce: 0.002839, loss_dice: 0.225168
[10:07:11.352] TRAIN: iteration 14268 : loss : 0.090265, loss_ce: 0.005454, loss_dice: 0.175075
[10:07:11.560] TRAIN: iteration 14269 : loss : 0.079244, loss_ce: 0.011340, loss_dice: 0.147147
[10:07:11.773] TRAIN: iteration 14270 : loss : 0.113272, loss_ce: 0.013634, loss_dice: 0.212910
[10:07:11.982] TRAIN: iteration 14271 : loss : 0.086451, loss_ce: 0.003895, loss_dice: 0.169007
[10:07:12.192] TRAIN: iteration 14272 : loss : 0.089777, loss_ce: 0.004265, loss_dice: 0.175288
[10:07:12.399] TRAIN: iteration 14273 : loss : 0.056889, loss_ce: 0.001960, loss_dice: 0.111817
[10:07:12.615] TRAIN: iteration 14274 : loss : 0.122246, loss_ce: 0.004613, loss_dice: 0.239879
[10:07:12.829] TRAIN: iteration 14275 : loss : 0.182860, loss_ce: 0.012857, loss_dice: 0.352863
[10:07:13.141] TRAIN: iteration 14276 : loss : 0.067765, loss_ce: 0.004078, loss_dice: 0.131451
[10:07:13.350] TRAIN: iteration 14277 : loss : 0.251906, loss_ce: 0.003558, loss_dice: 0.500254
[10:07:13.560] TRAIN: iteration 14278 : loss : 0.251006, loss_ce: 0.001908, loss_dice: 0.500104
[10:07:13.768] TRAIN: iteration 14279 : loss : 0.042522, loss_ce: 0.007162, loss_dice: 0.077883
[10:07:13.975] TRAIN: iteration 14280 : loss : 0.250965, loss_ce: 0.004648, loss_dice: 0.497282
[10:07:14.216] TRAIN: iteration 14281 : loss : 0.250669, loss_ce: 0.001286, loss_dice: 0.500051
[10:07:14.425] TRAIN: iteration 14282 : loss : 0.230312, loss_ce: 0.003398, loss_dice: 0.457225
[10:07:14.633] TRAIN: iteration 14283 : loss : 0.066688, loss_ce: 0.003128, loss_dice: 0.130247
[10:07:14.841] TRAIN: iteration 14284 : loss : 0.105399, loss_ce: 0.003930, loss_dice: 0.206867
[10:07:15.051] TRAIN: iteration 14285 : loss : 0.188704, loss_ce: 0.004797, loss_dice: 0.372612
[10:07:15.258] TRAIN: iteration 14286 : loss : 0.169712, loss_ce: 0.003380, loss_dice: 0.336044
[10:07:15.466] TRAIN: iteration 14287 : loss : 0.058580, loss_ce: 0.002763, loss_dice: 0.114398
[10:07:15.673] TRAIN: iteration 14288 : loss : 0.074702, loss_ce: 0.003127, loss_dice: 0.146276
[10:07:15.880] TRAIN: iteration 14289 : loss : 0.088770, loss_ce: 0.019706, loss_dice: 0.157834
[10:07:16.089] TRAIN: iteration 14290 : loss : 0.075008, loss_ce: 0.007660, loss_dice: 0.142355
[10:07:16.298] TRAIN: iteration 14291 : loss : 0.125045, loss_ce: 0.006835, loss_dice: 0.243255
[10:07:16.505] TRAIN: iteration 14292 : loss : 0.103020, loss_ce: 0.004794, loss_dice: 0.201247
[10:07:16.715] TRAIN: iteration 14293 : loss : 0.253040, loss_ce: 0.008488, loss_dice: 0.497591
[10:07:16.922] TRAIN: iteration 14294 : loss : 0.252134, loss_ce: 0.003974, loss_dice: 0.500294
[10:07:17.136] TRAIN: iteration 14295 : loss : 0.102283, loss_ce: 0.002680, loss_dice: 0.201885
[10:07:17.344] TRAIN: iteration 14296 : loss : 0.251310, loss_ce: 0.002470, loss_dice: 0.500149
[10:07:17.552] TRAIN: iteration 14297 : loss : 0.077631, loss_ce: 0.002538, loss_dice: 0.152724
[10:07:17.766] TRAIN: iteration 14298 : loss : 0.169081, loss_ce: 0.004475, loss_dice: 0.333686
[10:07:17.974] TRAIN: iteration 14299 : loss : 0.075626, loss_ce: 0.003053, loss_dice: 0.148200
[10:07:18.185] TRAIN: iteration 14300 : loss : 0.129085, loss_ce: 0.005679, loss_dice: 0.252491
[10:07:18.423] TRAIN: iteration 14301 : loss : 0.156163, loss_ce: 0.003259, loss_dice: 0.309067
[10:07:18.638] TRAIN: iteration 14302 : loss : 0.093291, loss_ce: 0.007553, loss_dice: 0.179030
[10:07:18.846] TRAIN: iteration 14303 : loss : 0.243627, loss_ce: 0.004044, loss_dice: 0.483210
[10:07:19.055] TRAIN: iteration 14304 : loss : 0.183622, loss_ce: 0.005386, loss_dice: 0.361859
[10:07:19.263] TRAIN: iteration 14305 : loss : 0.253476, loss_ce: 0.019085, loss_dice: 0.487866
[10:07:19.479] TRAIN: iteration 14306 : loss : 0.184179, loss_ce: 0.005654, loss_dice: 0.362704
[10:07:19.688] TRAIN: iteration 14307 : loss : 0.030343, loss_ce: 0.005854, loss_dice: 0.054831
[10:07:19.896] TRAIN: iteration 14308 : loss : 0.185590, loss_ce: 0.002751, loss_dice: 0.368429
[10:07:20.105] TRAIN: iteration 14309 : loss : 0.048989, loss_ce: 0.002123, loss_dice: 0.095855
[10:07:20.312] TRAIN: iteration 14310 : loss : 0.251277, loss_ce: 0.002422, loss_dice: 0.500131
[10:07:20.521] TRAIN: iteration 14311 : loss : 0.030089, loss_ce: 0.004409, loss_dice: 0.055768
[10:07:20.728] TRAIN: iteration 14312 : loss : 0.140504, loss_ce: 0.009080, loss_dice: 0.271928
[10:07:20.938] TRAIN: iteration 14313 : loss : 0.135807, loss_ce: 0.019572, loss_dice: 0.252042
[10:07:21.157] TRAIN: iteration 14314 : loss : 0.139759, loss_ce: 0.002551, loss_dice: 0.276967
[10:07:21.364] TRAIN: iteration 14315 : loss : 0.251362, loss_ce: 0.002572, loss_dice: 0.500152
[10:07:21.576] TRAIN: iteration 14316 : loss : 0.071086, loss_ce: 0.003209, loss_dice: 0.138963
[10:07:21.784] TRAIN: iteration 14317 : loss : 0.077231, loss_ce: 0.003133, loss_dice: 0.151329
[10:07:21.992] TRAIN: iteration 14318 : loss : 0.050834, loss_ce: 0.002828, loss_dice: 0.098840
[10:07:22.200] TRAIN: iteration 14319 : loss : 0.108903, loss_ce: 0.001792, loss_dice: 0.216014
[10:07:22.409] TRAIN: iteration 14320 : loss : 0.090633, loss_ce: 0.006309, loss_dice: 0.174958
[10:07:22.410] NaN or Inf found in input tensor.
[10:07:22.625] TRAIN: iteration 14321 : loss : 0.107164, loss_ce: 0.006979, loss_dice: 0.207349
[10:07:22.833] TRAIN: iteration 14322 : loss : 0.136216, loss_ce: 0.005737, loss_dice: 0.266696
[10:07:23.040] TRAIN: iteration 14323 : loss : 0.059850, loss_ce: 0.002056, loss_dice: 0.117643
[10:07:23.347] TRAIN: iteration 14324 : loss : 0.251224, loss_ce: 0.002302, loss_dice: 0.500145
[10:07:23.558] TRAIN: iteration 14325 : loss : 0.150904, loss_ce: 0.017969, loss_dice: 0.283840
[10:07:23.768] TRAIN: iteration 14326 : loss : 0.098602, loss_ce: 0.002679, loss_dice: 0.194525
[10:07:23.977] TRAIN: iteration 14327 : loss : 0.117144, loss_ce: 0.002951, loss_dice: 0.231336
[10:07:24.187] TRAIN: iteration 14328 : loss : 0.250924, loss_ce: 0.001763, loss_dice: 0.500085
[10:07:24.395] TRAIN: iteration 14329 : loss : 0.235181, loss_ce: 0.001407, loss_dice: 0.468955
[10:07:24.605] TRAIN: iteration 14330 : loss : 0.102436, loss_ce: 0.004269, loss_dice: 0.200604
[10:07:24.820] TRAIN: iteration 14331 : loss : 0.184808, loss_ce: 0.006459, loss_dice: 0.363158
[10:07:25.036] TRAIN: iteration 14332 : loss : 0.201312, loss_ce: 0.002164, loss_dice: 0.400460
[10:07:25.244] TRAIN: iteration 14333 : loss : 0.177712, loss_ce: 0.004681, loss_dice: 0.350743
[10:07:25.454] TRAIN: iteration 14334 : loss : 0.208154, loss_ce: 0.005010, loss_dice: 0.411299
[10:07:25.664] TRAIN: iteration 14335 : loss : 0.134025, loss_ce: 0.005380, loss_dice: 0.262671
[10:07:25.879] TRAIN: iteration 14336 : loss : 0.076198, loss_ce: 0.001720, loss_dice: 0.150676
[10:07:26.089] TRAIN: iteration 14337 : loss : 0.109865, loss_ce: 0.003845, loss_dice: 0.215884
[10:07:26.295] TRAIN: iteration 14338 : loss : 0.030648, loss_ce: 0.001816, loss_dice: 0.059480
[10:07:26.502] TRAIN: iteration 14339 : loss : 0.046395, loss_ce: 0.006628, loss_dice: 0.086161
[10:07:26.712] TRAIN: iteration 14340 : loss : 0.235333, loss_ce: 0.002242, loss_dice: 0.468424
[10:07:26.945] TRAIN: iteration 14341 : loss : 0.087714, loss_ce: 0.009044, loss_dice: 0.166385
[10:07:27.152] TRAIN: iteration 14342 : loss : 0.161187, loss_ce: 0.005518, loss_dice: 0.316857
[10:07:27.364] TRAIN: iteration 14343 : loss : 0.119416, loss_ce: 0.008379, loss_dice: 0.230454
[10:07:27.574] TRAIN: iteration 14344 : loss : 0.193301, loss_ce: 0.004754, loss_dice: 0.381848
[10:07:27.784] TRAIN: iteration 14345 : loss : 0.049367, loss_ce: 0.004453, loss_dice: 0.094282
[10:07:27.994] TRAIN: iteration 14346 : loss : 0.023552, loss_ce: 0.002694, loss_dice: 0.044411
[10:07:28.218] TRAIN: iteration 14347 : loss : 0.095622, loss_ce: 0.002189, loss_dice: 0.189055
[10:07:28.427] TRAIN: iteration 14348 : loss : 0.124598, loss_ce: 0.002329, loss_dice: 0.246866
[10:07:28.635] TRAIN: iteration 14349 : loss : 0.251525, loss_ce: 0.002888, loss_dice: 0.500163
[10:07:28.843] TRAIN: iteration 14350 : loss : 0.251294, loss_ce: 0.002437, loss_dice: 0.500150
[10:07:29.050] TRAIN: iteration 14351 : loss : 0.127960, loss_ce: 0.002852, loss_dice: 0.253067
[10:07:29.257] TRAIN: iteration 14352 : loss : 0.073389, loss_ce: 0.008494, loss_dice: 0.138284
[10:07:29.465] TRAIN: iteration 14353 : loss : 0.139596, loss_ce: 0.003812, loss_dice: 0.275381
[10:07:29.673] TRAIN: iteration 14354 : loss : 0.251161, loss_ce: 0.002181, loss_dice: 0.500140
[10:07:29.881] TRAIN: iteration 14355 : loss : 0.161874, loss_ce: 0.004489, loss_dice: 0.319259
[10:07:30.091] TRAIN: iteration 14356 : loss : 0.050707, loss_ce: 0.002392, loss_dice: 0.099022
[10:07:30.300] TRAIN: iteration 14357 : loss : 0.135422, loss_ce: 0.004505, loss_dice: 0.266338
[10:07:30.515] TRAIN: iteration 14358 : loss : 0.054046, loss_ce: 0.005237, loss_dice: 0.102856
[10:07:30.722] TRAIN: iteration 14359 : loss : 0.028976, loss_ce: 0.002300, loss_dice: 0.055653
[10:07:30.934] TRAIN: iteration 14360 : loss : 0.079704, loss_ce: 0.002576, loss_dice: 0.156833
[10:07:31.172] TRAIN: iteration 14361 : loss : 0.115539, loss_ce: 0.022138, loss_dice: 0.208940
[10:07:31.393] TRAIN: iteration 14362 : loss : 0.192734, loss_ce: 0.010215, loss_dice: 0.375254
[10:07:31.601] TRAIN: iteration 14363 : loss : 0.248132, loss_ce: 0.011426, loss_dice: 0.484838
[10:07:31.809] TRAIN: iteration 14364 : loss : 0.064435, loss_ce: 0.009108, loss_dice: 0.119761
[10:07:32.016] TRAIN: iteration 14365 : loss : 0.097908, loss_ce: 0.001920, loss_dice: 0.193895
[10:07:32.227] TRAIN: iteration 14366 : loss : 0.251845, loss_ce: 0.003480, loss_dice: 0.500209
[10:07:32.435] TRAIN: iteration 14367 : loss : 0.031038, loss_ce: 0.006092, loss_dice: 0.055984
[10:07:32.644] TRAIN: iteration 14368 : loss : 0.251591, loss_ce: 0.002983, loss_dice: 0.500198
[10:07:32.859] TRAIN: iteration 14369 : loss : 0.148814, loss_ce: 0.004004, loss_dice: 0.293625
[10:07:33.070] TRAIN: iteration 14370 : loss : 0.070610, loss_ce: 0.009073, loss_dice: 0.132148
[10:07:33.282] TRAIN: iteration 14371 : loss : 0.147993, loss_ce: 0.006855, loss_dice: 0.289130
[10:07:33.491] TRAIN: iteration 14372 : loss : 0.091185, loss_ce: 0.003900, loss_dice: 0.178471
[10:07:35.601] TRAIN: iteration 14373 : loss : 0.077494, loss_ce: 0.006769, loss_dice: 0.148220
[10:07:35.808] TRAIN: iteration 14374 : loss : 0.175585, loss_ce: 0.002286, loss_dice: 0.348884
[10:07:36.017] TRAIN: iteration 14375 : loss : 0.063938, loss_ce: 0.005795, loss_dice: 0.122082
[10:07:36.225] TRAIN: iteration 14376 : loss : 0.249560, loss_ce: 0.002181, loss_dice: 0.496938
[10:07:36.434] TRAIN: iteration 14377 : loss : 0.208478, loss_ce: 0.003666, loss_dice: 0.413289
[10:07:36.647] TRAIN: iteration 14378 : loss : 0.115354, loss_ce: 0.004765, loss_dice: 0.225944
[10:07:36.860] TRAIN: iteration 14379 : loss : 0.074277, loss_ce: 0.002608, loss_dice: 0.145947
[10:07:37.073] TRAIN: iteration 14380 : loss : 0.109420, loss_ce: 0.002919, loss_dice: 0.215921
[10:07:37.311] TRAIN: iteration 14381 : loss : 0.071963, loss_ce: 0.005452, loss_dice: 0.138473
[10:07:37.520] TRAIN: iteration 14382 : loss : 0.058319, loss_ce: 0.005293, loss_dice: 0.111344
[10:07:37.728] TRAIN: iteration 14383 : loss : 0.195627, loss_ce: 0.011035, loss_dice: 0.380219
[10:07:37.943] TRAIN: iteration 14384 : loss : 0.119181, loss_ce: 0.007301, loss_dice: 0.231061
[10:07:38.150] TRAIN: iteration 14385 : loss : 0.226463, loss_ce: 0.003370, loss_dice: 0.449556
[10:07:38.358] TRAIN: iteration 14386 : loss : 0.168282, loss_ce: 0.003209, loss_dice: 0.333354
[10:07:38.572] TRAIN: iteration 14387 : loss : 0.184927, loss_ce: 0.004206, loss_dice: 0.365648
[10:07:38.782] TRAIN: iteration 14388 : loss : 0.174674, loss_ce: 0.003382, loss_dice: 0.345967
[10:07:38.989] TRAIN: iteration 14389 : loss : 0.126517, loss_ce: 0.006940, loss_dice: 0.246094
[10:07:39.196] TRAIN: iteration 14390 : loss : 0.238919, loss_ce: 0.003887, loss_dice: 0.473951
[10:07:39.403] TRAIN: iteration 14391 : loss : 0.052732, loss_ce: 0.003670, loss_dice: 0.101795
[10:07:39.611] TRAIN: iteration 14392 : loss : 0.051531, loss_ce: 0.002333, loss_dice: 0.100729
[10:07:39.818] TRAIN: iteration 14393 : loss : 0.038679, loss_ce: 0.003929, loss_dice: 0.073429
[10:07:40.032] TRAIN: iteration 14394 : loss : 0.062752, loss_ce: 0.006322, loss_dice: 0.119182
[10:07:40.242] TRAIN: iteration 14395 : loss : 0.168272, loss_ce: 0.005889, loss_dice: 0.330655
[10:07:40.450] TRAIN: iteration 14396 : loss : 0.170630, loss_ce: 0.007237, loss_dice: 0.334024
[10:07:40.982] TRAIN: iteration 14397 : loss : 0.036866, loss_ce: 0.002587, loss_dice: 0.071145
[10:07:41.228] TRAIN: iteration 14398 : loss : 0.057467, loss_ce: 0.002738, loss_dice: 0.112195
[10:07:41.435] TRAIN: iteration 14399 : loss : 0.094902, loss_ce: 0.001616, loss_dice: 0.188187
[10:07:41.642] TRAIN: iteration 14400 : loss : 0.196565, loss_ce: 0.004037, loss_dice: 0.389093
[10:07:41.877] TRAIN: iteration 14401 : loss : 0.227769, loss_ce: 0.003109, loss_dice: 0.452429
[10:07:42.086] TRAIN: iteration 14402 : loss : 0.030010, loss_ce: 0.002080, loss_dice: 0.057940
[10:07:42.294] TRAIN: iteration 14403 : loss : 0.092872, loss_ce: 0.005626, loss_dice: 0.180118
[10:07:42.506] TRAIN: iteration 14404 : loss : 0.035409, loss_ce: 0.002686, loss_dice: 0.068131
[10:07:45.818] TRAIN: iteration 14405 : loss : 0.099899, loss_ce: 0.005707, loss_dice: 0.194091
[10:07:46.030] TRAIN: iteration 14406 : loss : 0.048733, loss_ce: 0.004912, loss_dice: 0.092553
[10:07:46.237] TRAIN: iteration 14407 : loss : 0.251437, loss_ce: 0.002726, loss_dice: 0.500149
[10:07:46.445] TRAIN: iteration 14408 : loss : 0.102827, loss_ce: 0.004673, loss_dice: 0.200980
[10:07:46.652] TRAIN: iteration 14409 : loss : 0.213019, loss_ce: 0.003325, loss_dice: 0.422712
[10:07:46.859] TRAIN: iteration 14410 : loss : 0.128537, loss_ce: 0.003703, loss_dice: 0.253371
[10:07:47.066] TRAIN: iteration 14411 : loss : 0.246522, loss_ce: 0.004538, loss_dice: 0.488506
[10:07:47.276] TRAIN: iteration 14412 : loss : 0.142809, loss_ce: 0.019186, loss_dice: 0.266431
[10:07:47.490] TRAIN: iteration 14413 : loss : 0.101347, loss_ce: 0.015625, loss_dice: 0.187069
[10:07:47.696] TRAIN: iteration 14414 : loss : 0.174155, loss_ce: 0.004080, loss_dice: 0.344230
[10:07:47.902] TRAIN: iteration 14415 : loss : 0.089297, loss_ce: 0.006535, loss_dice: 0.172059
[10:07:48.114] TRAIN: iteration 14416 : loss : 0.251804, loss_ce: 0.003383, loss_dice: 0.500224
[10:07:48.321] TRAIN: iteration 14417 : loss : 0.103970, loss_ce: 0.003868, loss_dice: 0.204073
[10:07:48.528] TRAIN: iteration 14418 : loss : 0.043045, loss_ce: 0.002681, loss_dice: 0.083408
[10:07:48.737] TRAIN: iteration 14419 : loss : 0.044686, loss_ce: 0.004706, loss_dice: 0.084666
[10:07:48.947] TRAIN: iteration 14420 : loss : 0.235219, loss_ce: 0.010923, loss_dice: 0.459514
[10:07:49.187] TRAIN: iteration 14421 : loss : 0.106028, loss_ce: 0.005819, loss_dice: 0.206238
[10:07:49.398] TRAIN: iteration 14422 : loss : 0.051239, loss_ce: 0.005833, loss_dice: 0.096645
[10:07:49.611] TRAIN: iteration 14423 : loss : 0.120191, loss_ce: 0.008280, loss_dice: 0.232103
[10:07:49.819] TRAIN: iteration 14424 : loss : 0.160973, loss_ce: 0.005479, loss_dice: 0.316468
[10:07:50.027] TRAIN: iteration 14425 : loss : 0.251389, loss_ce: 0.002612, loss_dice: 0.500166
[10:07:50.240] TRAIN: iteration 14426 : loss : 0.230177, loss_ce: 0.008167, loss_dice: 0.452187
[10:07:50.448] TRAIN: iteration 14427 : loss : 0.251659, loss_ce: 0.003107, loss_dice: 0.500211
[10:07:50.657] TRAIN: iteration 14428 : loss : 0.164767, loss_ce: 0.002788, loss_dice: 0.326745
[10:07:50.868] TRAIN: iteration 14429 : loss : 0.052942, loss_ce: 0.003457, loss_dice: 0.102428
[10:07:51.121] TRAIN: iteration 14430 : loss : 0.061302, loss_ce: 0.009744, loss_dice: 0.112860
[10:07:51.328] TRAIN: iteration 14431 : loss : 0.035536, loss_ce: 0.001631, loss_dice: 0.069440
[10:07:51.535] TRAIN: iteration 14432 : loss : 0.046798, loss_ce: 0.002888, loss_dice: 0.090707
[10:07:51.743] TRAIN: iteration 14433 : loss : 0.175006, loss_ce: 0.002898, loss_dice: 0.347115
[10:07:51.958] TRAIN: iteration 14434 : loss : 0.065307, loss_ce: 0.003573, loss_dice: 0.127042
[10:07:52.166] TRAIN: iteration 14435 : loss : 0.145606, loss_ce: 0.004088, loss_dice: 0.287123
[10:07:52.377] TRAIN: iteration 14436 : loss : 0.090632, loss_ce: 0.002015, loss_dice: 0.179249
[10:07:52.585] TRAIN: iteration 14437 : loss : 0.245528, loss_ce: 0.007363, loss_dice: 0.483693
[10:07:52.797] TRAIN: iteration 14438 : loss : 0.080939, loss_ce: 0.002462, loss_dice: 0.159416
[10:07:53.006] TRAIN: iteration 14439 : loss : 0.125781, loss_ce: 0.011511, loss_dice: 0.240050
[10:07:53.213] TRAIN: iteration 14440 : loss : 0.157686, loss_ce: 0.007177, loss_dice: 0.308194
[10:07:53.451] TRAIN: iteration 14441 : loss : 0.064717, loss_ce: 0.006175, loss_dice: 0.123258
[10:07:53.658] TRAIN: iteration 14442 : loss : 0.205246, loss_ce: 0.009099, loss_dice: 0.401394
[10:07:53.865] TRAIN: iteration 14443 : loss : 0.094793, loss_ce: 0.003782, loss_dice: 0.185803
[10:07:54.073] TRAIN: iteration 14444 : loss : 0.099388, loss_ce: 0.006420, loss_dice: 0.192356
[10:07:54.279] TRAIN: iteration 14445 : loss : 0.129223, loss_ce: 0.003194, loss_dice: 0.255252
[10:07:56.366] TRAIN: iteration 14446 : loss : 0.047374, loss_ce: 0.004344, loss_dice: 0.090403
[10:07:56.573] TRAIN: iteration 14447 : loss : 0.122371, loss_ce: 0.008843, loss_dice: 0.235899
[10:07:56.780] TRAIN: iteration 14448 : loss : 0.254082, loss_ce: 0.007853, loss_dice: 0.500310
[10:07:56.989] TRAIN: iteration 14449 : loss : 0.220987, loss_ce: 0.005424, loss_dice: 0.436549
[10:07:57.196] TRAIN: iteration 14450 : loss : 0.156816, loss_ce: 0.002592, loss_dice: 0.311041
[10:07:57.403] TRAIN: iteration 14451 : loss : 0.127126, loss_ce: 0.006819, loss_dice: 0.247433
[10:07:57.614] TRAIN: iteration 14452 : loss : 0.182136, loss_ce: 0.007533, loss_dice: 0.356739
[10:07:57.822] TRAIN: iteration 14453 : loss : 0.168218, loss_ce: 0.028965, loss_dice: 0.307472
[10:07:58.134] TRAIN: iteration 14454 : loss : 0.162530, loss_ce: 0.007990, loss_dice: 0.317071
[10:07:58.342] TRAIN: iteration 14455 : loss : 0.073875, loss_ce: 0.006006, loss_dice: 0.141745
[10:07:58.549] TRAIN: iteration 14456 : loss : 0.113161, loss_ce: 0.007747, loss_dice: 0.218575
[10:07:58.762] TRAIN: iteration 14457 : loss : 0.251337, loss_ce: 0.002555, loss_dice: 0.500119
[10:07:58.969] TRAIN: iteration 14458 : loss : 0.060703, loss_ce: 0.003049, loss_dice: 0.118357
[10:07:59.178] TRAIN: iteration 14459 : loss : 0.235101, loss_ce: 0.003155, loss_dice: 0.467047
[10:07:59.385] TRAIN: iteration 14460 : loss : 0.131951, loss_ce: 0.002192, loss_dice: 0.261710
[10:07:59.818] TRAIN: iteration 14461 : loss : 0.042902, loss_ce: 0.002856, loss_dice: 0.082949
[10:08:00.543] TRAIN: iteration 14462 : loss : 0.049924, loss_ce: 0.005610, loss_dice: 0.094239
[10:08:00.751] TRAIN: iteration 14463 : loss : 0.053374, loss_ce: 0.003198, loss_dice: 0.103550
[10:08:00.960] TRAIN: iteration 14464 : loss : 0.158489, loss_ce: 0.020852, loss_dice: 0.296125
[10:08:01.167] TRAIN: iteration 14465 : loss : 0.040897, loss_ce: 0.006832, loss_dice: 0.074962
[10:08:01.375] TRAIN: iteration 14466 : loss : 0.255030, loss_ce: 0.009338, loss_dice: 0.500723
[10:08:01.588] TRAIN: iteration 14467 : loss : 0.132503, loss_ce: 0.004679, loss_dice: 0.260326
[10:08:01.820] TRAIN: iteration 14468 : loss : 0.085083, loss_ce: 0.005707, loss_dice: 0.164459
[10:08:02.027] TRAIN: iteration 14469 : loss : 0.137385, loss_ce: 0.013218, loss_dice: 0.261551
[10:08:02.234] TRAIN: iteration 14470 : loss : 0.252181, loss_ce: 0.005043, loss_dice: 0.499320
[10:08:02.442] TRAIN: iteration 14471 : loss : 0.080412, loss_ce: 0.003363, loss_dice: 0.157460
[10:08:02.656] TRAIN: iteration 14472 : loss : 0.229970, loss_ce: 0.004365, loss_dice: 0.455575
[10:08:02.863] TRAIN: iteration 14473 : loss : 0.046574, loss_ce: 0.003602, loss_dice: 0.089547
[10:08:03.076] TRAIN: iteration 14474 : loss : 0.173578, loss_ce: 0.016204, loss_dice: 0.330952
[10:08:03.284] TRAIN: iteration 14475 : loss : 0.120354, loss_ce: 0.003647, loss_dice: 0.237061
[10:08:03.490] TRAIN: iteration 14476 : loss : 0.230810, loss_ce: 0.006681, loss_dice: 0.454939
[10:08:03.697] TRAIN: iteration 14477 : loss : 0.242648, loss_ce: 0.002321, loss_dice: 0.482976
[10:08:03.904] TRAIN: iteration 14478 : loss : 0.190226, loss_ce: 0.011239, loss_dice: 0.369212
[10:08:04.111] TRAIN: iteration 14479 : loss : 0.152590, loss_ce: 0.005466, loss_dice: 0.299715
[10:08:04.326] TRAIN: iteration 14480 : loss : 0.085684, loss_ce: 0.008150, loss_dice: 0.163218
[10:08:04.564] TRAIN: iteration 14481 : loss : 0.063255, loss_ce: 0.002194, loss_dice: 0.124316
[10:08:04.773] TRAIN: iteration 14482 : loss : 0.206868, loss_ce: 0.005709, loss_dice: 0.408026
[10:08:04.983] TRAIN: iteration 14483 : loss : 0.120143, loss_ce: 0.006434, loss_dice: 0.233853
[10:08:05.190] TRAIN: iteration 14484 : loss : 0.174561, loss_ce: 0.002999, loss_dice: 0.346122
[10:08:05.397] TRAIN: iteration 14485 : loss : 0.138257, loss_ce: 0.011529, loss_dice: 0.264985
[10:08:05.604] TRAIN: iteration 14486 : loss : 0.048748, loss_ce: 0.004550, loss_dice: 0.092945
[10:08:05.811] TRAIN: iteration 14487 : loss : 0.059182, loss_ce: 0.005933, loss_dice: 0.112430
[10:08:06.020] TRAIN: iteration 14488 : loss : 0.251221, loss_ce: 0.002327, loss_dice: 0.500115
[10:08:06.230] TRAIN: iteration 14489 : loss : 0.139537, loss_ce: 0.004401, loss_dice: 0.274672
[10:08:07.658] TRAIN: iteration 14490 : loss : 0.042740, loss_ce: 0.004240, loss_dice: 0.081241
[10:08:07.873] TRAIN: iteration 14491 : loss : 0.082416, loss_ce: 0.003075, loss_dice: 0.161757
[10:08:08.089] TRAIN: iteration 14492 : loss : 0.125271, loss_ce: 0.004239, loss_dice: 0.246303
[10:08:08.297] TRAIN: iteration 14493 : loss : 0.176628, loss_ce: 0.010611, loss_dice: 0.342645
[10:08:08.504] TRAIN: iteration 14494 : loss : 0.091030, loss_ce: 0.002946, loss_dice: 0.179113
[10:08:08.711] TRAIN: iteration 14495 : loss : 0.082985, loss_ce: 0.003609, loss_dice: 0.162361
[10:08:08.918] TRAIN: iteration 14496 : loss : 0.247001, loss_ce: 0.001844, loss_dice: 0.492157
[10:08:09.129] TRAIN: iteration 14497 : loss : 0.036760, loss_ce: 0.002368, loss_dice: 0.071152
[10:08:09.500] TRAIN: iteration 14498 : loss : 0.037606, loss_ce: 0.002700, loss_dice: 0.072513
[10:08:09.710] TRAIN: iteration 14499 : loss : 0.112753, loss_ce: 0.007353, loss_dice: 0.218153
[10:08:09.922] TRAIN: iteration 14500 : loss : 0.141371, loss_ce: 0.003060, loss_dice: 0.279682
[10:08:10.163] TRAIN: iteration 14501 : loss : 0.232255, loss_ce: 0.002795, loss_dice: 0.461714
[10:08:10.372] TRAIN: iteration 14502 : loss : 0.178026, loss_ce: 0.003045, loss_dice: 0.353008
[10:08:10.581] TRAIN: iteration 14503 : loss : 0.184891, loss_ce: 0.002603, loss_dice: 0.367179
[10:08:10.789] TRAIN: iteration 14504 : loss : 0.250644, loss_ce: 0.001265, loss_dice: 0.500023
[10:08:10.997] TRAIN: iteration 14505 : loss : 0.075622, loss_ce: 0.003289, loss_dice: 0.147956
[10:08:11.208] TRAIN: iteration 14506 : loss : 0.251439, loss_ce: 0.002711, loss_dice: 0.500167
[10:08:11.417] TRAIN: iteration 14507 : loss : 0.250917, loss_ce: 0.001756, loss_dice: 0.500078
[10:08:11.624] TRAIN: iteration 14508 : loss : 0.224738, loss_ce: 0.013667, loss_dice: 0.435809
[10:08:11.832] TRAIN: iteration 14509 : loss : 0.171050, loss_ce: 0.005337, loss_dice: 0.336764
[10:08:12.108] TRAIN: iteration 14510 : loss : 0.251119, loss_ce: 0.002140, loss_dice: 0.500098
[10:08:12.323] TRAIN: iteration 14511 : loss : 0.060584, loss_ce: 0.005493, loss_dice: 0.115674
[10:08:12.531] TRAIN: iteration 14512 : loss : 0.122395, loss_ce: 0.007083, loss_dice: 0.237707
[10:08:12.741] TRAIN: iteration 14513 : loss : 0.034780, loss_ce: 0.004551, loss_dice: 0.065009
[10:08:12.951] TRAIN: iteration 14514 : loss : 0.070047, loss_ce: 0.001869, loss_dice: 0.138224
[10:08:13.160] TRAIN: iteration 14515 : loss : 0.251658, loss_ce: 0.003130, loss_dice: 0.500186
[10:08:13.369] TRAIN: iteration 14516 : loss : 0.194235, loss_ce: 0.005315, loss_dice: 0.383155
[10:08:14.000] TRAIN: iteration 14517 : loss : 0.176536, loss_ce: 0.002430, loss_dice: 0.350643
[10:08:14.208] TRAIN: iteration 14518 : loss : 0.251446, loss_ce: 0.002783, loss_dice: 0.500110
[10:08:14.417] TRAIN: iteration 14519 : loss : 0.058349, loss_ce: 0.003034, loss_dice: 0.113664
[10:08:14.625] TRAIN: iteration 14520 : loss : 0.249960, loss_ce: 0.002530, loss_dice: 0.497390
[10:08:14.864] TRAIN: iteration 14521 : loss : 0.251314, loss_ce: 0.002497, loss_dice: 0.500131
[10:08:15.072] TRAIN: iteration 14522 : loss : 0.086711, loss_ce: 0.008770, loss_dice: 0.164652
[10:08:15.279] TRAIN: iteration 14523 : loss : 0.140607, loss_ce: 0.012205, loss_dice: 0.269010
[10:08:15.488] TRAIN: iteration 14524 : loss : 0.201301, loss_ce: 0.008454, loss_dice: 0.394147
[10:08:15.759] TRAIN: iteration 14525 : loss : 0.161812, loss_ce: 0.004945, loss_dice: 0.318678
[10:08:15.968] TRAIN: iteration 14526 : loss : 0.147744, loss_ce: 0.004173, loss_dice: 0.291314
[10:08:16.176] TRAIN: iteration 14527 : loss : 0.116928, loss_ce: 0.004016, loss_dice: 0.229841
[10:08:16.383] TRAIN: iteration 14528 : loss : 0.251955, loss_ce: 0.003712, loss_dice: 0.500199
[10:08:16.593] TRAIN: iteration 14529 : loss : 0.243976, loss_ce: 0.003957, loss_dice: 0.483996
[10:08:16.800] TRAIN: iteration 14530 : loss : 0.252624, loss_ce: 0.004908, loss_dice: 0.500339
[10:08:17.009] TRAIN: iteration 14531 : loss : 0.223848, loss_ce: 0.005897, loss_dice: 0.441799
[10:08:17.218] TRAIN: iteration 14532 : loss : 0.031240, loss_ce: 0.004060, loss_dice: 0.058420
[10:08:17.425] TRAIN: iteration 14533 : loss : 0.221091, loss_ce: 0.003335, loss_dice: 0.438847
[10:08:17.970] TRAIN: iteration 14534 : loss : 0.125705, loss_ce: 0.006566, loss_dice: 0.244843
[10:08:19.750] TRAIN: iteration 14535 : loss : 0.081384, loss_ce: 0.004876, loss_dice: 0.157892
[10:08:19.960] TRAIN: iteration 14536 : loss : 0.216466, loss_ce: 0.014247, loss_dice: 0.418685
[10:08:20.168] TRAIN: iteration 14537 : loss : 0.055582, loss_ce: 0.001976, loss_dice: 0.109189
[10:08:20.375] TRAIN: iteration 14538 : loss : 0.038889, loss_ce: 0.003149, loss_dice: 0.074628
[10:08:20.587] TRAIN: iteration 14539 : loss : 0.141951, loss_ce: 0.003690, loss_dice: 0.280212
[10:08:20.796] TRAIN: iteration 14540 : loss : 0.044453, loss_ce: 0.001312, loss_dice: 0.087595
[10:08:21.036] TRAIN: iteration 14541 : loss : 0.251049, loss_ce: 0.002045, loss_dice: 0.500052
[10:08:21.246] TRAIN: iteration 14542 : loss : 0.044306, loss_ce: 0.003596, loss_dice: 0.085015
[10:08:21.765] TRAIN: iteration 14543 : loss : 0.123821, loss_ce: 0.001972, loss_dice: 0.245670
[10:08:21.976] TRAIN: iteration 14544 : loss : 0.071193, loss_ce: 0.001611, loss_dice: 0.140775
[10:08:22.183] TRAIN: iteration 14545 : loss : 0.169037, loss_ce: 0.004788, loss_dice: 0.333287
[10:08:22.416] TRAIN: iteration 14546 : loss : 0.162630, loss_ce: 0.001974, loss_dice: 0.323285
[10:08:22.626] TRAIN: iteration 14547 : loss : 0.231489, loss_ce: 0.003772, loss_dice: 0.459206
[10:08:22.835] TRAIN: iteration 14548 : loss : 0.205713, loss_ce: 0.028827, loss_dice: 0.382599
[10:08:23.046] TRAIN: iteration 14549 : loss : 0.042540, loss_ce: 0.003730, loss_dice: 0.081350
[10:08:23.260] TRAIN: iteration 14550 : loss : 0.149100, loss_ce: 0.002404, loss_dice: 0.295797
[10:08:23.470] TRAIN: iteration 14551 : loss : 0.156499, loss_ce: 0.007617, loss_dice: 0.305381
[10:08:23.680] TRAIN: iteration 14552 : loss : 0.164238, loss_ce: 0.001228, loss_dice: 0.327248
[10:08:23.888] TRAIN: iteration 14553 : loss : 0.250602, loss_ce: 0.001169, loss_dice: 0.500036
[10:08:24.095] TRAIN: iteration 14554 : loss : 0.174073, loss_ce: 0.001346, loss_dice: 0.346799
[10:08:24.309] TRAIN: iteration 14555 : loss : 0.053836, loss_ce: 0.008137, loss_dice: 0.099534
[10:08:24.520] TRAIN: iteration 14556 : loss : 0.050917, loss_ce: 0.005925, loss_dice: 0.095908
[10:08:24.728] TRAIN: iteration 14557 : loss : 0.105462, loss_ce: 0.002721, loss_dice: 0.208203
[10:08:24.937] TRAIN: iteration 14558 : loss : 0.125922, loss_ce: 0.001832, loss_dice: 0.250012
[10:08:25.144] TRAIN: iteration 14559 : loss : 0.082443, loss_ce: 0.001930, loss_dice: 0.162956
[10:08:26.557] TRAIN: iteration 14560 : loss : 0.037853, loss_ce: 0.004455, loss_dice: 0.071251
[10:08:26.807] TRAIN: iteration 14561 : loss : 0.122792, loss_ce: 0.009099, loss_dice: 0.236484
[10:08:27.015] TRAIN: iteration 14562 : loss : 0.171898, loss_ce: 0.009747, loss_dice: 0.334049
[10:08:27.225] TRAIN: iteration 14563 : loss : 0.083680, loss_ce: 0.002812, loss_dice: 0.164547
[10:08:27.507] TRAIN: iteration 14564 : loss : 0.148138, loss_ce: 0.022950, loss_dice: 0.273325
[10:08:27.719] TRAIN: iteration 14565 : loss : 0.076847, loss_ce: 0.002348, loss_dice: 0.151347
[10:08:27.926] TRAIN: iteration 14566 : loss : 0.206649, loss_ce: 0.006251, loss_dice: 0.407047
[10:08:28.143] TRAIN: iteration 14567 : loss : 0.089275, loss_ce: 0.002506, loss_dice: 0.176043
[10:08:28.350] TRAIN: iteration 14568 : loss : 0.193856, loss_ce: 0.002274, loss_dice: 0.385438
[10:08:28.557] TRAIN: iteration 14569 : loss : 0.251520, loss_ce: 0.002854, loss_dice: 0.500186
[10:08:28.769] TRAIN: iteration 14570 : loss : 0.030212, loss_ce: 0.001236, loss_dice: 0.059188
[10:08:28.978] TRAIN: iteration 14571 : loss : 0.105444, loss_ce: 0.003042, loss_dice: 0.207845
[10:08:29.185] TRAIN: iteration 14572 : loss : 0.116546, loss_ce: 0.007558, loss_dice: 0.225533
[10:08:29.393] TRAIN: iteration 14573 : loss : 0.124921, loss_ce: 0.002876, loss_dice: 0.246966
[10:08:29.600] TRAIN: iteration 14574 : loss : 0.251411, loss_ce: 0.002643, loss_dice: 0.500178
[10:08:29.812] TRAIN: iteration 14575 : loss : 0.244130, loss_ce: 0.001430, loss_dice: 0.486829
[10:08:30.022] TRAIN: iteration 14576 : loss : 0.070581, loss_ce: 0.004366, loss_dice: 0.136796
[10:08:30.230] TRAIN: iteration 14577 : loss : 0.250960, loss_ce: 0.001830, loss_dice: 0.500090
[10:08:30.441] TRAIN: iteration 14578 : loss : 0.105613, loss_ce: 0.001908, loss_dice: 0.209319
[10:08:30.649] TRAIN: iteration 14579 : loss : 0.157603, loss_ce: 0.030635, loss_dice: 0.284570
[10:08:30.856] TRAIN: iteration 14580 : loss : 0.184857, loss_ce: 0.002545, loss_dice: 0.367168
[10:08:31.091] TRAIN: iteration 14581 : loss : 0.254464, loss_ce: 0.008759, loss_dice: 0.500170
[10:08:31.298] TRAIN: iteration 14582 : loss : 0.115751, loss_ce: 0.003562, loss_dice: 0.227940
[10:08:31.506] TRAIN: iteration 14583 : loss : 0.252294, loss_ce: 0.007738, loss_dice: 0.496851
[10:08:31.721] TRAIN: iteration 14584 : loss : 0.048703, loss_ce: 0.003523, loss_dice: 0.093884
[10:08:31.937] TRAIN: iteration 14585 : loss : 0.124868, loss_ce: 0.004052, loss_dice: 0.245685
[10:08:32.145] TRAIN: iteration 14586 : loss : 0.181215, loss_ce: 0.002805, loss_dice: 0.359624
[10:08:32.352] TRAIN: iteration 14587 : loss : 0.135225, loss_ce: 0.006935, loss_dice: 0.263514
[10:08:32.562] TRAIN: iteration 14588 : loss : 0.250571, loss_ce: 0.005143, loss_dice: 0.495999
[10:08:32.774] TRAIN: iteration 14589 : loss : 0.211816, loss_ce: 0.002386, loss_dice: 0.421245
[10:08:32.983] TRAIN: iteration 14590 : loss : 0.102132, loss_ce: 0.004302, loss_dice: 0.199962
[10:08:33.191] TRAIN: iteration 14591 : loss : 0.165020, loss_ce: 0.001849, loss_dice: 0.328190
[10:08:33.398] TRAIN: iteration 14592 : loss : 0.167062, loss_ce: 0.002886, loss_dice: 0.331237
[10:08:33.611] TRAIN: iteration 14593 : loss : 0.103362, loss_ce: 0.009205, loss_dice: 0.197518
[10:08:33.818] TRAIN: iteration 14594 : loss : 0.161131, loss_ce: 0.003162, loss_dice: 0.319099
[10:08:34.027] TRAIN: iteration 14595 : loss : 0.068314, loss_ce: 0.002547, loss_dice: 0.134081
[10:08:34.238] TRAIN: iteration 14596 : loss : 0.037663, loss_ce: 0.002621, loss_dice: 0.072704
[10:08:34.447] TRAIN: iteration 14597 : loss : 0.171808, loss_ce: 0.006580, loss_dice: 0.337037
[10:08:34.655] TRAIN: iteration 14598 : loss : 0.130862, loss_ce: 0.003904, loss_dice: 0.257820
[10:08:34.862] TRAIN: iteration 14599 : loss : 0.112114, loss_ce: 0.004684, loss_dice: 0.219544
[10:08:35.073] TRAIN: iteration 14600 : loss : 0.095071, loss_ce: 0.004308, loss_dice: 0.185833
[10:08:35.307] TRAIN: iteration 14601 : loss : 0.251498, loss_ce: 0.002851, loss_dice: 0.500144
[10:08:35.515] TRAIN: iteration 14602 : loss : 0.055776, loss_ce: 0.003328, loss_dice: 0.108224
[10:08:35.729] TRAIN: iteration 14603 : loss : 0.091504, loss_ce: 0.007311, loss_dice: 0.175697
[10:08:35.942] TRAIN: iteration 14604 : loss : 0.251163, loss_ce: 0.002232, loss_dice: 0.500095
[10:08:36.150] TRAIN: iteration 14605 : loss : 0.134446, loss_ce: 0.006511, loss_dice: 0.262381
[10:08:36.358] TRAIN: iteration 14606 : loss : 0.058216, loss_ce: 0.003703, loss_dice: 0.112730
[10:08:36.565] TRAIN: iteration 14607 : loss : 0.099242, loss_ce: 0.007120, loss_dice: 0.191365
[10:08:36.789] TRAIN: iteration 14608 : loss : 0.128920, loss_ce: 0.005977, loss_dice: 0.251864
[10:08:37.057] TRAIN: iteration 14609 : loss : 0.121159, loss_ce: 0.004284, loss_dice: 0.238035
[10:08:37.265] TRAIN: iteration 14610 : loss : 0.048829, loss_ce: 0.004244, loss_dice: 0.093414
[10:08:37.788] TRAIN: iteration 14611 : loss : 0.094303, loss_ce: 0.004306, loss_dice: 0.184299
[10:08:38.000] TRAIN: iteration 14612 : loss : 0.189627, loss_ce: 0.013571, loss_dice: 0.365683
[10:08:38.210] TRAIN: iteration 14613 : loss : 0.132349, loss_ce: 0.004234, loss_dice: 0.260465
[10:08:38.423] TRAIN: iteration 14614 : loss : 0.124020, loss_ce: 0.003851, loss_dice: 0.244190
[10:08:38.638] TRAIN: iteration 14615 : loss : 0.098690, loss_ce: 0.002984, loss_dice: 0.194396
[10:08:38.852] TRAIN: iteration 14616 : loss : 0.063091, loss_ce: 0.002556, loss_dice: 0.123627
[10:08:39.061] TRAIN: iteration 14617 : loss : 0.208771, loss_ce: 0.003943, loss_dice: 0.413598
[10:08:39.276] TRAIN: iteration 14618 : loss : 0.126342, loss_ce: 0.002819, loss_dice: 0.249864
[10:08:39.483] TRAIN: iteration 14619 : loss : 0.086909, loss_ce: 0.003974, loss_dice: 0.169843
[10:08:39.690] TRAIN: iteration 14620 : loss : 0.154899, loss_ce: 0.012959, loss_dice: 0.296838
[10:08:39.930] TRAIN: iteration 14621 : loss : 0.060439, loss_ce: 0.008060, loss_dice: 0.112819
[10:08:40.139] TRAIN: iteration 14622 : loss : 0.128623, loss_ce: 0.007409, loss_dice: 0.249837
[10:08:40.346] TRAIN: iteration 14623 : loss : 0.086767, loss_ce: 0.004162, loss_dice: 0.169373
[10:08:40.553] TRAIN: iteration 14624 : loss : 0.054459, loss_ce: 0.005729, loss_dice: 0.103190
[10:08:40.771] TRAIN: iteration 14625 : loss : 0.057537, loss_ce: 0.007366, loss_dice: 0.107707
[10:08:40.983] TRAIN: iteration 14626 : loss : 0.058802, loss_ce: 0.003940, loss_dice: 0.113665
[10:08:41.191] TRAIN: iteration 14627 : loss : 0.104055, loss_ce: 0.004028, loss_dice: 0.204082
[10:08:41.400] TRAIN: iteration 14628 : loss : 0.066517, loss_ce: 0.007100, loss_dice: 0.125933
[10:08:41.615] TRAIN: iteration 14629 : loss : 0.123867, loss_ce: 0.009126, loss_dice: 0.238608
[10:08:41.823] TRAIN: iteration 14630 : loss : 0.080991, loss_ce: 0.004034, loss_dice: 0.157948
[10:08:42.030] TRAIN: iteration 14631 : loss : 0.071473, loss_ce: 0.006908, loss_dice: 0.136039
[10:08:42.239] TRAIN: iteration 14632 : loss : 0.105883, loss_ce: 0.009957, loss_dice: 0.201810
[10:08:42.448] TRAIN: iteration 14633 : loss : 0.047987, loss_ce: 0.005731, loss_dice: 0.090243
[10:08:42.655] TRAIN: iteration 14634 : loss : 0.251726, loss_ce: 0.003399, loss_dice: 0.500052
[10:08:42.863] TRAIN: iteration 14635 : loss : 0.038676, loss_ce: 0.007385, loss_dice: 0.069967
[10:08:43.841] TRAIN: iteration 14636 : loss : 0.101875, loss_ce: 0.007066, loss_dice: 0.196684
[10:08:44.051] TRAIN: iteration 14637 : loss : 0.033383, loss_ce: 0.005341, loss_dice: 0.061424
[10:08:44.259] TRAIN: iteration 14638 : loss : 0.090945, loss_ce: 0.004969, loss_dice: 0.176920
[10:08:44.466] TRAIN: iteration 14639 : loss : 0.078947, loss_ce: 0.013971, loss_dice: 0.143922
[10:08:44.677] TRAIN: iteration 14640 : loss : 0.241065, loss_ce: 0.007986, loss_dice: 0.474145
[10:08:44.921] TRAIN: iteration 14641 : loss : 0.107368, loss_ce: 0.002607, loss_dice: 0.212128
[10:08:45.129] TRAIN: iteration 14642 : loss : 0.088509, loss_ce: 0.004644, loss_dice: 0.172374
[10:08:45.343] TRAIN: iteration 14643 : loss : 0.251441, loss_ce: 0.002730, loss_dice: 0.500151
[10:08:46.902] TRAIN: iteration 14644 : loss : 0.251468, loss_ce: 0.002763, loss_dice: 0.500173
[10:08:47.112] TRAIN: iteration 14645 : loss : 0.158433, loss_ce: 0.008034, loss_dice: 0.308833
[10:08:47.319] TRAIN: iteration 14646 : loss : 0.253625, loss_ce: 0.006711, loss_dice: 0.500539
[10:08:47.526] TRAIN: iteration 14647 : loss : 0.148276, loss_ce: 0.003733, loss_dice: 0.292818
[10:08:47.734] TRAIN: iteration 14648 : loss : 0.086761, loss_ce: 0.003035, loss_dice: 0.170486
[10:08:47.942] TRAIN: iteration 14649 : loss : 0.069986, loss_ce: 0.008821, loss_dice: 0.131152
[10:08:48.151] TRAIN: iteration 14650 : loss : 0.173211, loss_ce: 0.027141, loss_dice: 0.319281
[10:08:48.359] TRAIN: iteration 14651 : loss : 0.251637, loss_ce: 0.003057, loss_dice: 0.500218
[10:08:50.822] TRAIN: iteration 14652 : loss : 0.089614, loss_ce: 0.004520, loss_dice: 0.174709
[10:08:51.030] TRAIN: iteration 14653 : loss : 0.076528, loss_ce: 0.002339, loss_dice: 0.150717
[10:08:51.242] TRAIN: iteration 14654 : loss : 0.159349, loss_ce: 0.012356, loss_dice: 0.306342
[10:08:51.451] TRAIN: iteration 14655 : loss : 0.164290, loss_ce: 0.006212, loss_dice: 0.322368
[10:08:51.659] TRAIN: iteration 14656 : loss : 0.184889, loss_ce: 0.004479, loss_dice: 0.365299
[10:08:51.867] TRAIN: iteration 14657 : loss : 0.201019, loss_ce: 0.003567, loss_dice: 0.398472
[10:08:52.074] TRAIN: iteration 14658 : loss : 0.071024, loss_ce: 0.004103, loss_dice: 0.137945
[10:08:52.281] TRAIN: iteration 14659 : loss : 0.079067, loss_ce: 0.003757, loss_dice: 0.154378
[10:08:52.495] TRAIN: iteration 14660 : loss : 0.081903, loss_ce: 0.002311, loss_dice: 0.161495
[10:08:52.730] TRAIN: iteration 14661 : loss : 0.180814, loss_ce: 0.005739, loss_dice: 0.355889
[10:08:52.942] TRAIN: iteration 14662 : loss : 0.105377, loss_ce: 0.005818, loss_dice: 0.204937
[10:08:53.150] TRAIN: iteration 14663 : loss : 0.177205, loss_ce: 0.003288, loss_dice: 0.351123
[10:08:53.356] TRAIN: iteration 14664 : loss : 0.246052, loss_ce: 0.003082, loss_dice: 0.489022
[10:08:53.566] TRAIN: iteration 14665 : loss : 0.162804, loss_ce: 0.005274, loss_dice: 0.320335
[10:08:53.776] TRAIN: iteration 14666 : loss : 0.124512, loss_ce: 0.005358, loss_dice: 0.243666
[10:08:53.985] TRAIN: iteration 14667 : loss : 0.165356, loss_ce: 0.010010, loss_dice: 0.320702
[10:08:54.204] TRAIN: iteration 14668 : loss : 0.102689, loss_ce: 0.012263, loss_dice: 0.193115
[10:08:54.411] TRAIN: iteration 14669 : loss : 0.097482, loss_ce: 0.008410, loss_dice: 0.186554
[10:08:54.619] TRAIN: iteration 14670 : loss : 0.046128, loss_ce: 0.002227, loss_dice: 0.090029
[10:08:54.826] TRAIN: iteration 14671 : loss : 0.251563, loss_ce: 0.002928, loss_dice: 0.500198
[10:08:55.039] TRAIN: iteration 14672 : loss : 0.061061, loss_ce: 0.002331, loss_dice: 0.119792
[10:08:55.247] TRAIN: iteration 14673 : loss : 0.040162, loss_ce: 0.002003, loss_dice: 0.078320
[10:08:55.464] TRAIN: iteration 14674 : loss : 0.251892, loss_ce: 0.003603, loss_dice: 0.500181
[10:08:55.674] TRAIN: iteration 14675 : loss : 0.107002, loss_ce: 0.004391, loss_dice: 0.209613
[10:08:55.881] TRAIN: iteration 14676 : loss : 0.100801, loss_ce: 0.003341, loss_dice: 0.198261
[10:08:56.088] TRAIN: iteration 14677 : loss : 0.148245, loss_ce: 0.016537, loss_dice: 0.279952
[10:08:56.295] TRAIN: iteration 14678 : loss : 0.252159, loss_ce: 0.004038, loss_dice: 0.500281
[10:08:56.503] TRAIN: iteration 14679 : loss : 0.101785, loss_ce: 0.005824, loss_dice: 0.197746
[10:08:56.712] TRAIN: iteration 14680 : loss : 0.240030, loss_ce: 0.002628, loss_dice: 0.477433
[10:08:56.950] TRAIN: iteration 14681 : loss : 0.053469, loss_ce: 0.002360, loss_dice: 0.104578
[10:08:57.476] TRAIN: iteration 14682 : loss : 0.089559, loss_ce: 0.005632, loss_dice: 0.173486
[10:08:57.682] TRAIN: iteration 14683 : loss : 0.120096, loss_ce: 0.005375, loss_dice: 0.234816
[10:08:57.929] TRAIN: iteration 14684 : loss : 0.020139, loss_ce: 0.002207, loss_dice: 0.038070
[10:08:58.137] TRAIN: iteration 14685 : loss : 0.165776, loss_ce: 0.010695, loss_dice: 0.320857
[10:08:58.353] TRAIN: iteration 14686 : loss : 0.055750, loss_ce: 0.003152, loss_dice: 0.108349
[10:08:58.563] TRAIN: iteration 14687 : loss : 0.115728, loss_ce: 0.005320, loss_dice: 0.226136
[10:08:58.770] TRAIN: iteration 14688 : loss : 0.052459, loss_ce: 0.004668, loss_dice: 0.100250
[10:08:58.978] TRAIN: iteration 14689 : loss : 0.251987, loss_ce: 0.003731, loss_dice: 0.500243
[10:09:01.576] TRAIN: iteration 14690 : loss : 0.055811, loss_ce: 0.002741, loss_dice: 0.108882
[10:09:01.788] TRAIN: iteration 14691 : loss : 0.231245, loss_ce: 0.003325, loss_dice: 0.459164
[10:09:01.998] TRAIN: iteration 14692 : loss : 0.084067, loss_ce: 0.003910, loss_dice: 0.164225
[10:09:02.208] TRAIN: iteration 14693 : loss : 0.115727, loss_ce: 0.004547, loss_dice: 0.226907
[10:09:02.421] TRAIN: iteration 14694 : loss : 0.210336, loss_ce: 0.003673, loss_dice: 0.417000
[10:09:02.633] TRAIN: iteration 14695 : loss : 0.030660, loss_ce: 0.002920, loss_dice: 0.058400
[10:09:02.842] TRAIN: iteration 14696 : loss : 0.026596, loss_ce: 0.001628, loss_dice: 0.051565
[10:09:03.059] TRAIN: iteration 14697 : loss : 0.161044, loss_ce: 0.005880, loss_dice: 0.316208
[10:09:03.272] TRAIN: iteration 14698 : loss : 0.177711, loss_ce: 0.006240, loss_dice: 0.349182
[10:09:03.483] TRAIN: iteration 14699 : loss : 0.224710, loss_ce: 0.002665, loss_dice: 0.446755
[10:09:03.694] TRAIN: iteration 14700 : loss : 0.034204, loss_ce: 0.001944, loss_dice: 0.066464
[10:09:03.932] TRAIN: iteration 14701 : loss : 0.154423, loss_ce: 0.004944, loss_dice: 0.303901
[10:09:04.143] TRAIN: iteration 14702 : loss : 0.096218, loss_ce: 0.004810, loss_dice: 0.187626
[10:09:04.352] TRAIN: iteration 14703 : loss : 0.248382, loss_ce: 0.002906, loss_dice: 0.493859
[10:09:04.561] TRAIN: iteration 14704 : loss : 0.100128, loss_ce: 0.002930, loss_dice: 0.197326
[10:09:04.769] TRAIN: iteration 14705 : loss : 0.130984, loss_ce: 0.019689, loss_dice: 0.242278
[10:09:04.981] TRAIN: iteration 14706 : loss : 0.219718, loss_ce: 0.004056, loss_dice: 0.435380
[10:09:05.190] TRAIN: iteration 14707 : loss : 0.140216, loss_ce: 0.001801, loss_dice: 0.278632
[10:09:05.405] TRAIN: iteration 14708 : loss : 0.193748, loss_ce: 0.001956, loss_dice: 0.385540
[10:09:05.643] TRAIN: iteration 14709 : loss : 0.108445, loss_ce: 0.002203, loss_dice: 0.214688
[10:09:05.851] TRAIN: iteration 14710 : loss : 0.208529, loss_ce: 0.003855, loss_dice: 0.413203
[10:09:06.058] TRAIN: iteration 14711 : loss : 0.057751, loss_ce: 0.003837, loss_dice: 0.111665
[10:09:06.268] TRAIN: iteration 14712 : loss : 0.251157, loss_ce: 0.002171, loss_dice: 0.500143
[10:09:06.476] TRAIN: iteration 14713 : loss : 0.041912, loss_ce: 0.003121, loss_dice: 0.080704
[10:09:06.684] TRAIN: iteration 14714 : loss : 0.237389, loss_ce: 0.002424, loss_dice: 0.472354
[10:09:06.891] TRAIN: iteration 14715 : loss : 0.250824, loss_ce: 0.001549, loss_dice: 0.500099
[10:09:07.102] TRAIN: iteration 14716 : loss : 0.250585, loss_ce: 0.001120, loss_dice: 0.500050
[10:09:07.311] TRAIN: iteration 14717 : loss : 0.210882, loss_ce: 0.005127, loss_dice: 0.416636
[10:09:07.518] TRAIN: iteration 14718 : loss : 0.049054, loss_ce: 0.001270, loss_dice: 0.096838
[10:09:07.725] TRAIN: iteration 14719 : loss : 0.134227, loss_ce: 0.005552, loss_dice: 0.262901
[10:09:07.933] TRAIN: iteration 14720 : loss : 0.126044, loss_ce: 0.022418, loss_dice: 0.229669
[10:09:08.181] TRAIN: iteration 14721 : loss : 0.099301, loss_ce: 0.002218, loss_dice: 0.196385
[10:09:08.389] TRAIN: iteration 14722 : loss : 0.250997, loss_ce: 0.001881, loss_dice: 0.500114
[10:09:08.596] TRAIN: iteration 14723 : loss : 0.083699, loss_ce: 0.005885, loss_dice: 0.161513
[10:09:08.803] TRAIN: iteration 14724 : loss : 0.106289, loss_ce: 0.002623, loss_dice: 0.209955
[10:09:09.010] TRAIN: iteration 14725 : loss : 0.126398, loss_ce: 0.003308, loss_dice: 0.249488
[10:09:09.901] TRAIN: iteration 14726 : loss : 0.056608, loss_ce: 0.004397, loss_dice: 0.108819
[10:09:10.109] TRAIN: iteration 14727 : loss : 0.074780, loss_ce: 0.002869, loss_dice: 0.146692
[10:09:10.318] TRAIN: iteration 14728 : loss : 0.119539, loss_ce: 0.002454, loss_dice: 0.236624
[10:09:10.525] TRAIN: iteration 14729 : loss : 0.116597, loss_ce: 0.002832, loss_dice: 0.230362
[10:09:10.733] TRAIN: iteration 14730 : loss : 0.153143, loss_ce: 0.004641, loss_dice: 0.301646
[10:09:10.941] TRAIN: iteration 14731 : loss : 0.036925, loss_ce: 0.002246, loss_dice: 0.071604
[10:09:11.153] TRAIN: iteration 14732 : loss : 0.128656, loss_ce: 0.002464, loss_dice: 0.254848
[10:09:11.360] TRAIN: iteration 14733 : loss : 0.250873, loss_ce: 0.001665, loss_dice: 0.500081
[10:09:11.570] TRAIN: iteration 14734 : loss : 0.178606, loss_ce: 0.007279, loss_dice: 0.349932
[10:09:11.777] TRAIN: iteration 14735 : loss : 0.062878, loss_ce: 0.001653, loss_dice: 0.124103
[10:09:11.986] TRAIN: iteration 14736 : loss : 0.193088, loss_ce: 0.007820, loss_dice: 0.378355
[10:09:12.203] TRAIN: iteration 14737 : loss : 0.176505, loss_ce: 0.014002, loss_dice: 0.339008
[10:09:12.409] TRAIN: iteration 14738 : loss : 0.150770, loss_ce: 0.014275, loss_dice: 0.287265
[10:09:12.616] TRAIN: iteration 14739 : loss : 0.184203, loss_ce: 0.008509, loss_dice: 0.359897
[10:09:12.824] TRAIN: iteration 14740 : loss : 0.251493, loss_ce: 0.003596, loss_dice: 0.499389
[10:09:13.059] TRAIN: iteration 14741 : loss : 0.157424, loss_ce: 0.005755, loss_dice: 0.309094
[10:09:15.100] TRAIN: iteration 14742 : loss : 0.089419, loss_ce: 0.003645, loss_dice: 0.175193
[10:09:15.314] TRAIN: iteration 14743 : loss : 0.105689, loss_ce: 0.009249, loss_dice: 0.202129
[10:09:15.522] TRAIN: iteration 14744 : loss : 0.126163, loss_ce: 0.003553, loss_dice: 0.248773
[10:09:15.732] TRAIN: iteration 14745 : loss : 0.116049, loss_ce: 0.005075, loss_dice: 0.227022
[10:09:15.940] TRAIN: iteration 14746 : loss : 0.113042, loss_ce: 0.024047, loss_dice: 0.202037
[10:09:16.149] TRAIN: iteration 14747 : loss : 0.060080, loss_ce: 0.002165, loss_dice: 0.117995
[10:09:16.358] TRAIN: iteration 14748 : loss : 0.178573, loss_ce: 0.017350, loss_dice: 0.339796
[10:09:16.568] TRAIN: iteration 14749 : loss : 0.154663, loss_ce: 0.007180, loss_dice: 0.302147
[10:09:18.370] TRAIN: iteration 14750 : loss : 0.128090, loss_ce: 0.004648, loss_dice: 0.251531
[10:09:18.577] TRAIN: iteration 14751 : loss : 0.124730, loss_ce: 0.003142, loss_dice: 0.246318
[10:09:18.783] TRAIN: iteration 14752 : loss : 0.252005, loss_ce: 0.003766, loss_dice: 0.500245
[10:09:18.997] TRAIN: iteration 14753 : loss : 0.251829, loss_ce: 0.003459, loss_dice: 0.500199
[10:09:19.294] TRAIN: iteration 14754 : loss : 0.044764, loss_ce: 0.003798, loss_dice: 0.085729
[10:09:19.504] TRAIN: iteration 14755 : loss : 0.060860, loss_ce: 0.003213, loss_dice: 0.118508
[10:09:19.720] TRAIN: iteration 14756 : loss : 0.253044, loss_ce: 0.006420, loss_dice: 0.499668
[10:09:19.929] TRAIN: iteration 14757 : loss : 0.086112, loss_ce: 0.002286, loss_dice: 0.169937
[10:09:21.306] TRAIN: iteration 14758 : loss : 0.046411, loss_ce: 0.004295, loss_dice: 0.088527
[10:09:21.515] TRAIN: iteration 14759 : loss : 0.174305, loss_ce: 0.007494, loss_dice: 0.341115
[10:09:21.723] TRAIN: iteration 14760 : loss : 0.218417, loss_ce: 0.014295, loss_dice: 0.422540
[10:09:21.959] TRAIN: iteration 14761 : loss : 0.086563, loss_ce: 0.004733, loss_dice: 0.168392
[10:09:22.172] TRAIN: iteration 14762 : loss : 0.120965, loss_ce: 0.004813, loss_dice: 0.237117
[10:09:22.380] TRAIN: iteration 14763 : loss : 0.088998, loss_ce: 0.002444, loss_dice: 0.175553
[10:09:22.589] TRAIN: iteration 14764 : loss : 0.235041, loss_ce: 0.004425, loss_dice: 0.465658
[10:09:22.796] TRAIN: iteration 14765 : loss : 0.133297, loss_ce: 0.005481, loss_dice: 0.261113
[10:09:23.005] TRAIN: iteration 14766 : loss : 0.125579, loss_ce: 0.004791, loss_dice: 0.246367
[10:09:23.216] TRAIN: iteration 14767 : loss : 0.049395, loss_ce: 0.002282, loss_dice: 0.096509
[10:09:23.423] TRAIN: iteration 14768 : loss : 0.099394, loss_ce: 0.001654, loss_dice: 0.197135
[10:09:23.666] TRAIN: iteration 14769 : loss : 0.251881, loss_ce: 0.003511, loss_dice: 0.500251
[10:09:23.881] TRAIN: iteration 14770 : loss : 0.130811, loss_ce: 0.005811, loss_dice: 0.255812
[10:09:24.089] TRAIN: iteration 14771 : loss : 0.119311, loss_ce: 0.004586, loss_dice: 0.234035
[10:09:24.302] TRAIN: iteration 14772 : loss : 0.250976, loss_ce: 0.001838, loss_dice: 0.500115
[10:09:24.509] TRAIN: iteration 14773 : loss : 0.052021, loss_ce: 0.002461, loss_dice: 0.101581
[10:09:24.717] TRAIN: iteration 14774 : loss : 0.086942, loss_ce: 0.002243, loss_dice: 0.171641
[10:09:24.927] TRAIN: iteration 14775 : loss : 0.086440, loss_ce: 0.000954, loss_dice: 0.171926
[10:09:25.134] TRAIN: iteration 14776 : loss : 0.080362, loss_ce: 0.004134, loss_dice: 0.156590
[10:09:25.340] TRAIN: iteration 14777 : loss : 0.033096, loss_ce: 0.001925, loss_dice: 0.064266
[10:09:25.548] TRAIN: iteration 14778 : loss : 0.138311, loss_ce: 0.003695, loss_dice: 0.272927
[10:09:25.757] TRAIN: iteration 14779 : loss : 0.250336, loss_ce: 0.000662, loss_dice: 0.500009
[10:09:25.972] TRAIN: iteration 14780 : loss : 0.219442, loss_ce: 0.002100, loss_dice: 0.436783
[10:09:26.219] TRAIN: iteration 14781 : loss : 0.250437, loss_ce: 0.000847, loss_dice: 0.500027
[10:09:26.830] TRAIN: iteration 14782 : loss : 0.250704, loss_ce: 0.001354, loss_dice: 0.500053
[10:09:27.056] TRAIN: iteration 14783 : loss : 0.138768, loss_ce: 0.003424, loss_dice: 0.274111
[10:09:27.268] TRAIN: iteration 14784 : loss : 0.109218, loss_ce: 0.003843, loss_dice: 0.214593
[10:09:27.479] TRAIN: iteration 14785 : loss : 0.193315, loss_ce: 0.003868, loss_dice: 0.382761
[10:09:27.689] TRAIN: iteration 14786 : loss : 0.113419, loss_ce: 0.002680, loss_dice: 0.224159
[10:09:28.503] TRAIN: iteration 14787 : loss : 0.207593, loss_ce: 0.003532, loss_dice: 0.411654
[10:09:28.731] TRAIN: iteration 14788 : loss : 0.187170, loss_ce: 0.012377, loss_dice: 0.361963
[10:09:28.937] TRAIN: iteration 14789 : loss : 0.096218, loss_ce: 0.002619, loss_dice: 0.189817
[10:09:29.563] TRAIN: iteration 14790 : loss : 0.023945, loss_ce: 0.001951, loss_dice: 0.045940
[10:09:29.773] TRAIN: iteration 14791 : loss : 0.144717, loss_ce: 0.007065, loss_dice: 0.282370
[10:09:29.981] TRAIN: iteration 14792 : loss : 0.127084, loss_ce: 0.002039, loss_dice: 0.252129
[10:09:30.190] TRAIN: iteration 14793 : loss : 0.106301, loss_ce: 0.002666, loss_dice: 0.209935
[10:09:30.397] TRAIN: iteration 14794 : loss : 0.243440, loss_ce: 0.003883, loss_dice: 0.482998
[10:09:31.858] TRAIN: iteration 14795 : loss : 0.107129, loss_ce: 0.004369, loss_dice: 0.209890
[10:09:32.069] TRAIN: iteration 14796 : loss : 0.168378, loss_ce: 0.006762, loss_dice: 0.329994
[10:09:32.276] TRAIN: iteration 14797 : loss : 0.100789, loss_ce: 0.002398, loss_dice: 0.199180
[10:09:32.763] TRAIN: iteration 14798 : loss : 0.082302, loss_ce: 0.001822, loss_dice: 0.162783
[10:09:32.970] TRAIN: iteration 14799 : loss : 0.125652, loss_ce: 0.002645, loss_dice: 0.248659
[10:09:33.178] TRAIN: iteration 14800 : loss : 0.243959, loss_ce: 0.003224, loss_dice: 0.484693
[10:09:33.418] TRAIN: iteration 14801 : loss : 0.225431, loss_ce: 0.003275, loss_dice: 0.447586
[10:09:33.686] TRAIN: iteration 14802 : loss : 0.140434, loss_ce: 0.005679, loss_dice: 0.275189
[10:09:33.894] TRAIN: iteration 14803 : loss : 0.232644, loss_ce: 0.009340, loss_dice: 0.455949
[10:09:34.101] TRAIN: iteration 14804 : loss : 0.135461, loss_ce: 0.005497, loss_dice: 0.265425
[10:09:34.311] TRAIN: iteration 14805 : loss : 0.094288, loss_ce: 0.003554, loss_dice: 0.185022
[10:09:34.521] TRAIN: iteration 14806 : loss : 0.217374, loss_ce: 0.004571, loss_dice: 0.430176
[10:09:34.728] TRAIN: iteration 14807 : loss : 0.246873, loss_ce: 0.002171, loss_dice: 0.491575
[10:09:34.937] TRAIN: iteration 14808 : loss : 0.106691, loss_ce: 0.002065, loss_dice: 0.211318
[10:09:35.147] TRAIN: iteration 14809 : loss : 0.248597, loss_ce: 0.006302, loss_dice: 0.490892
[10:09:35.355] TRAIN: iteration 14810 : loss : 0.083006, loss_ce: 0.009930, loss_dice: 0.156082
[10:09:35.565] TRAIN: iteration 14811 : loss : 0.124563, loss_ce: 0.002438, loss_dice: 0.246687
[10:09:35.781] TRAIN: iteration 14812 : loss : 0.251564, loss_ce: 0.002950, loss_dice: 0.500178
[10:09:35.992] TRAIN: iteration 14813 : loss : 0.033655, loss_ce: 0.001763, loss_dice: 0.065547
[10:09:36.199] TRAIN: iteration 14814 : loss : 0.250393, loss_ce: 0.002776, loss_dice: 0.498010
[10:09:36.407] TRAIN: iteration 14815 : loss : 0.251477, loss_ce: 0.002777, loss_dice: 0.500177
[10:09:36.618] TRAIN: iteration 14816 : loss : 0.165686, loss_ce: 0.003948, loss_dice: 0.327425
[10:09:36.863] TRAIN: iteration 14817 : loss : 0.058040, loss_ce: 0.002332, loss_dice: 0.113749
[10:09:37.080] TRAIN: iteration 14818 : loss : 0.160641, loss_ce: 0.023492, loss_dice: 0.297790
[10:09:37.289] TRAIN: iteration 14819 : loss : 0.126430, loss_ce: 0.005994, loss_dice: 0.246866
[10:09:37.498] TRAIN: iteration 14820 : loss : 0.164173, loss_ce: 0.003586, loss_dice: 0.324760
[10:09:37.736] TRAIN: iteration 14821 : loss : 0.034917, loss_ce: 0.001841, loss_dice: 0.067992
[10:09:37.946] TRAIN: iteration 14822 : loss : 0.046260, loss_ce: 0.001418, loss_dice: 0.091103
[10:09:38.154] TRAIN: iteration 14823 : loss : 0.154629, loss_ce: 0.006955, loss_dice: 0.302303
[10:09:38.364] TRAIN: iteration 14824 : loss : 0.044473, loss_ce: 0.002544, loss_dice: 0.086403
[10:09:39.526] TRAIN: iteration 14825 : loss : 0.039335, loss_ce: 0.003545, loss_dice: 0.075125
[10:09:39.736] TRAIN: iteration 14826 : loss : 0.154288, loss_ce: 0.004462, loss_dice: 0.304113
[10:09:39.946] TRAIN: iteration 14827 : loss : 0.072618, loss_ce: 0.009548, loss_dice: 0.135687
[10:09:40.153] TRAIN: iteration 14828 : loss : 0.102183, loss_ce: 0.009779, loss_dice: 0.194587
[10:09:40.363] TRAIN: iteration 14829 : loss : 0.144882, loss_ce: 0.003314, loss_dice: 0.286450
[10:09:40.570] TRAIN: iteration 14830 : loss : 0.213017, loss_ce: 0.005050, loss_dice: 0.420984
[10:09:40.778] TRAIN: iteration 14831 : loss : 0.052562, loss_ce: 0.003209, loss_dice: 0.101916
[10:09:40.988] TRAIN: iteration 14832 : loss : 0.108661, loss_ce: 0.004419, loss_dice: 0.212902
[10:09:41.194] TRAIN: iteration 14833 : loss : 0.233676, loss_ce: 0.006658, loss_dice: 0.460693
[10:09:41.403] TRAIN: iteration 14834 : loss : 0.123834, loss_ce: 0.003419, loss_dice: 0.244250
[10:09:41.609] TRAIN: iteration 14835 : loss : 0.143726, loss_ce: 0.003239, loss_dice: 0.284214
[10:09:41.815] TRAIN: iteration 14836 : loss : 0.220947, loss_ce: 0.006982, loss_dice: 0.434913
[10:09:42.023] TRAIN: iteration 14837 : loss : 0.235572, loss_ce: 0.002726, loss_dice: 0.468418
[10:09:42.230] TRAIN: iteration 14838 : loss : 0.103297, loss_ce: 0.002158, loss_dice: 0.204436
[10:09:42.439] TRAIN: iteration 14839 : loss : 0.115418, loss_ce: 0.005338, loss_dice: 0.225498
[10:09:42.650] TRAIN: iteration 14840 : loss : 0.226087, loss_ce: 0.004301, loss_dice: 0.447872
[10:09:44.016] TRAIN: iteration 14841 : loss : 0.250628, loss_ce: 0.001218, loss_dice: 0.500037
[10:09:44.224] TRAIN: iteration 14842 : loss : 0.057209, loss_ce: 0.002250, loss_dice: 0.112168
[10:09:44.438] TRAIN: iteration 14843 : loss : 0.079433, loss_ce: 0.006821, loss_dice: 0.152045
[10:09:44.645] TRAIN: iteration 14844 : loss : 0.131935, loss_ce: 0.004863, loss_dice: 0.259007
[10:09:44.858] TRAIN: iteration 14845 : loss : 0.097065, loss_ce: 0.002147, loss_dice: 0.191983
[10:09:45.065] TRAIN: iteration 14846 : loss : 0.090874, loss_ce: 0.006133, loss_dice: 0.175615
[10:09:45.274] TRAIN: iteration 14847 : loss : 0.103392, loss_ce: 0.003636, loss_dice: 0.203148
[10:09:45.482] TRAIN: iteration 14848 : loss : 0.121526, loss_ce: 0.004635, loss_dice: 0.238416
[10:09:46.912] TRAIN: iteration 14849 : loss : 0.026964, loss_ce: 0.002399, loss_dice: 0.051528
[10:09:47.118] TRAIN: iteration 14850 : loss : 0.120626, loss_ce: 0.014893, loss_dice: 0.226359
[10:09:47.324] TRAIN: iteration 14851 : loss : 0.116844, loss_ce: 0.002510, loss_dice: 0.231179
[10:09:47.531] TRAIN: iteration 14852 : loss : 0.115365, loss_ce: 0.003476, loss_dice: 0.227255
[10:09:47.738] TRAIN: iteration 14853 : loss : 0.110346, loss_ce: 0.002549, loss_dice: 0.218143
[10:09:47.945] TRAIN: iteration 14854 : loss : 0.134918, loss_ce: 0.002957, loss_dice: 0.266879
[10:09:48.153] TRAIN: iteration 14855 : loss : 0.240996, loss_ce: 0.003798, loss_dice: 0.478194
[10:09:48.361] TRAIN: iteration 14856 : loss : 0.231467, loss_ce: 0.004689, loss_dice: 0.458246
[10:09:49.542] TRAIN: iteration 14857 : loss : 0.075273, loss_ce: 0.007003, loss_dice: 0.143543
[10:09:49.748] TRAIN: iteration 14858 : loss : 0.065591, loss_ce: 0.001721, loss_dice: 0.129461
[10:09:49.956] TRAIN: iteration 14859 : loss : 0.227461, loss_ce: 0.002641, loss_dice: 0.452280
[10:09:50.165] TRAIN: iteration 14860 : loss : 0.081834, loss_ce: 0.004394, loss_dice: 0.159274
[10:09:50.399] TRAIN: iteration 14861 : loss : 0.086648, loss_ce: 0.004690, loss_dice: 0.168606
[10:09:50.607] TRAIN: iteration 14862 : loss : 0.118263, loss_ce: 0.004583, loss_dice: 0.231944
[10:09:50.814] TRAIN: iteration 14863 : loss : 0.088421, loss_ce: 0.013275, loss_dice: 0.163566
[10:09:51.023] TRAIN: iteration 14864 : loss : 0.235358, loss_ce: 0.005125, loss_dice: 0.465591
[10:09:51.353] TRAIN: iteration 14865 : loss : 0.092464, loss_ce: 0.003462, loss_dice: 0.181465
[10:09:51.561] TRAIN: iteration 14866 : loss : 0.121829, loss_ce: 0.004075, loss_dice: 0.239582
[10:09:51.769] TRAIN: iteration 14867 : loss : 0.252110, loss_ce: 0.005130, loss_dice: 0.499089
[10:09:51.977] TRAIN: iteration 14868 : loss : 0.082591, loss_ce: 0.010679, loss_dice: 0.154503
[10:09:52.186] TRAIN: iteration 14869 : loss : 0.138472, loss_ce: 0.003115, loss_dice: 0.273829
[10:09:52.395] TRAIN: iteration 14870 : loss : 0.102422, loss_ce: 0.002757, loss_dice: 0.202088
[10:09:52.602] TRAIN: iteration 14871 : loss : 0.251946, loss_ce: 0.003648, loss_dice: 0.500243
[10:09:52.811] TRAIN: iteration 14872 : loss : 0.249654, loss_ce: 0.003097, loss_dice: 0.496211
[10:09:54.333] TRAIN: iteration 14873 : loss : 0.109085, loss_ce: 0.009370, loss_dice: 0.208799
[10:09:54.540] TRAIN: iteration 14874 : loss : 0.251596, loss_ce: 0.002993, loss_dice: 0.500198
[10:09:54.747] TRAIN: iteration 14875 : loss : 0.117540, loss_ce: 0.005269, loss_dice: 0.229811
[10:09:54.955] TRAIN: iteration 14876 : loss : 0.251847, loss_ce: 0.003435, loss_dice: 0.500259
[10:09:55.164] TRAIN: iteration 14877 : loss : 0.082127, loss_ce: 0.006029, loss_dice: 0.158226
[10:09:55.379] TRAIN: iteration 14878 : loss : 0.134248, loss_ce: 0.002585, loss_dice: 0.265912
[10:09:55.586] TRAIN: iteration 14879 : loss : 0.046665, loss_ce: 0.004436, loss_dice: 0.088895
[10:09:55.794] TRAIN: iteration 14880 : loss : 0.196402, loss_ce: 0.005717, loss_dice: 0.387086
[10:09:56.684] TRAIN: iteration 14881 : loss : 0.183525, loss_ce: 0.002690, loss_dice: 0.364360
[10:09:56.892] TRAIN: iteration 14882 : loss : 0.116692, loss_ce: 0.010252, loss_dice: 0.223131
[10:09:57.100] TRAIN: iteration 14883 : loss : 0.054236, loss_ce: 0.003276, loss_dice: 0.105195
[10:09:57.308] TRAIN: iteration 14884 : loss : 0.043857, loss_ce: 0.002221, loss_dice: 0.085492
[10:09:57.516] TRAIN: iteration 14885 : loss : 0.142298, loss_ce: 0.005677, loss_dice: 0.278920
[10:09:57.724] TRAIN: iteration 14886 : loss : 0.044260, loss_ce: 0.002011, loss_dice: 0.086508
[10:09:57.935] TRAIN: iteration 14887 : loss : 0.073774, loss_ce: 0.005236, loss_dice: 0.142311
[10:09:58.144] TRAIN: iteration 14888 : loss : 0.251406, loss_ce: 0.002618, loss_dice: 0.500193
[10:09:58.352] TRAIN: iteration 14889 : loss : 0.074820, loss_ce: 0.004275, loss_dice: 0.145366
[10:09:58.814] TRAIN: iteration 14890 : loss : 0.141723, loss_ce: 0.005715, loss_dice: 0.277731
[10:09:59.023] TRAIN: iteration 14891 : loss : 0.251570, loss_ce: 0.004420, loss_dice: 0.498720
[10:09:59.230] TRAIN: iteration 14892 : loss : 0.081198, loss_ce: 0.002178, loss_dice: 0.160219
[10:09:59.439] TRAIN: iteration 14893 : loss : 0.250666, loss_ce: 0.001271, loss_dice: 0.500061
[10:09:59.653] TRAIN: iteration 14894 : loss : 0.080737, loss_ce: 0.006140, loss_dice: 0.155334
[10:09:59.862] TRAIN: iteration 14895 : loss : 0.098343, loss_ce: 0.002139, loss_dice: 0.194547
[10:10:00.071] TRAIN: iteration 14896 : loss : 0.094895, loss_ce: 0.005599, loss_dice: 0.184190
[10:10:00.280] TRAIN: iteration 14897 : loss : 0.090322, loss_ce: 0.005426, loss_dice: 0.175217
[10:10:01.381] TRAIN: iteration 14898 : loss : 0.250961, loss_ce: 0.001798, loss_dice: 0.500123
[10:10:01.590] TRAIN: iteration 14899 : loss : 0.127436, loss_ce: 0.005752, loss_dice: 0.249119
[10:10:01.796] TRAIN: iteration 14900 : loss : 0.080437, loss_ce: 0.014020, loss_dice: 0.146854
[10:10:02.037] TRAIN: iteration 14901 : loss : 0.251206, loss_ce: 0.002254, loss_dice: 0.500159
[10:10:02.248] TRAIN: iteration 14902 : loss : 0.108342, loss_ce: 0.006383, loss_dice: 0.210300
[10:10:02.464] TRAIN: iteration 14903 : loss : 0.102184, loss_ce: 0.004194, loss_dice: 0.200174
[10:10:02.672] TRAIN: iteration 14904 : loss : 0.201511, loss_ce: 0.003882, loss_dice: 0.399140
[10:10:02.881] TRAIN: iteration 14905 : loss : 0.240470, loss_ce: 0.003464, loss_dice: 0.477476
[10:10:05.249] TRAIN: iteration 14906 : loss : 0.088978, loss_ce: 0.002256, loss_dice: 0.175699
[10:10:05.460] TRAIN: iteration 14907 : loss : 0.251920, loss_ce: 0.003664, loss_dice: 0.500175
[10:10:05.666] TRAIN: iteration 14908 : loss : 0.146620, loss_ce: 0.020176, loss_dice: 0.273064
[10:10:05.874] TRAIN: iteration 14909 : loss : 0.161712, loss_ce: 0.008895, loss_dice: 0.314529
[10:10:06.082] TRAIN: iteration 14910 : loss : 0.058004, loss_ce: 0.003611, loss_dice: 0.112396
[10:10:06.298] TRAIN: iteration 14911 : loss : 0.011617, loss_ce: 0.001725, loss_dice: 0.021510
[10:10:06.508] TRAIN: iteration 14912 : loss : 0.048614, loss_ce: 0.002560, loss_dice: 0.094668
[10:10:06.716] TRAIN: iteration 14913 : loss : 0.251537, loss_ce: 0.002874, loss_dice: 0.500201
[10:10:07.908] TRAIN: iteration 14914 : loss : 0.168864, loss_ce: 0.009582, loss_dice: 0.328145
[10:10:08.116] TRAIN: iteration 14915 : loss : 0.243093, loss_ce: 0.006094, loss_dice: 0.480092
[10:10:08.323] TRAIN: iteration 14916 : loss : 0.085936, loss_ce: 0.009426, loss_dice: 0.162446
[10:10:08.531] TRAIN: iteration 14917 : loss : 0.252458, loss_ce: 0.004935, loss_dice: 0.499981
[10:10:08.739] TRAIN: iteration 14918 : loss : 0.124754, loss_ce: 0.003290, loss_dice: 0.246218
[10:10:08.947] TRAIN: iteration 14919 : loss : 0.252058, loss_ce: 0.004567, loss_dice: 0.499549
[10:10:09.156] TRAIN: iteration 14920 : loss : 0.160230, loss_ce: 0.002353, loss_dice: 0.318107
[10:10:09.393] TRAIN: iteration 14921 : loss : 0.188265, loss_ce: 0.005201, loss_dice: 0.371330
[10:10:09.604] TRAIN: iteration 14922 : loss : 0.066881, loss_ce: 0.006152, loss_dice: 0.127610
[10:10:09.812] TRAIN: iteration 14923 : loss : 0.052701, loss_ce: 0.003306, loss_dice: 0.102096
[10:10:10.020] TRAIN: iteration 14924 : loss : 0.038710, loss_ce: 0.003301, loss_dice: 0.074120
[10:10:10.227] TRAIN: iteration 14925 : loss : 0.246607, loss_ce: 0.005298, loss_dice: 0.487916
[10:10:10.437] TRAIN: iteration 14926 : loss : 0.250957, loss_ce: 0.001822, loss_dice: 0.500093
[10:10:10.646] TRAIN: iteration 14927 : loss : 0.063058, loss_ce: 0.003492, loss_dice: 0.122625
[10:10:10.855] TRAIN: iteration 14928 : loss : 0.139369, loss_ce: 0.007213, loss_dice: 0.271525
[10:10:14.168] TRAIN: iteration 14929 : loss : 0.046532, loss_ce: 0.001940, loss_dice: 0.091123
[10:10:14.375] TRAIN: iteration 14930 : loss : 0.159335, loss_ce: 0.004371, loss_dice: 0.314298
[10:10:14.583] TRAIN: iteration 14931 : loss : 0.154914, loss_ce: 0.004251, loss_dice: 0.305577
[10:10:14.791] TRAIN: iteration 14932 : loss : 0.086791, loss_ce: 0.008895, loss_dice: 0.164686
[10:10:14.999] TRAIN: iteration 14933 : loss : 0.235342, loss_ce: 0.004395, loss_dice: 0.466290
[10:10:15.216] TRAIN: iteration 14934 : loss : 0.062420, loss_ce: 0.001920, loss_dice: 0.122919
[10:10:15.423] TRAIN: iteration 14935 : loss : 0.064099, loss_ce: 0.002589, loss_dice: 0.125610
[10:10:15.636] TRAIN: iteration 14936 : loss : 0.251057, loss_ce: 0.001989, loss_dice: 0.500125
[10:10:16.614] TRAIN: iteration 14937 : loss : 0.251036, loss_ce: 0.001947, loss_dice: 0.500124
[10:10:16.822] TRAIN: iteration 14938 : loss : 0.075051, loss_ce: 0.005784, loss_dice: 0.144318
[10:10:17.028] TRAIN: iteration 14939 : loss : 0.048221, loss_ce: 0.001342, loss_dice: 0.095100
[10:10:17.236] TRAIN: iteration 14940 : loss : 0.110994, loss_ce: 0.002304, loss_dice: 0.219683
[10:10:17.486] TRAIN: iteration 14941 : loss : 0.235437, loss_ce: 0.041379, loss_dice: 0.429494
[10:10:17.693] TRAIN: iteration 14942 : loss : 0.251157, loss_ce: 0.002162, loss_dice: 0.500153
[10:10:17.899] TRAIN: iteration 14943 : loss : 0.067560, loss_ce: 0.006312, loss_dice: 0.128808
[10:10:18.106] TRAIN: iteration 14944 : loss : 0.160280, loss_ce: 0.003171, loss_dice: 0.317389
[10:10:20.995] TRAIN: iteration 14945 : loss : 0.117794, loss_ce: 0.002010, loss_dice: 0.233578
[10:10:21.202] TRAIN: iteration 14946 : loss : 0.247856, loss_ce: 0.001601, loss_dice: 0.494112
[10:10:21.410] TRAIN: iteration 14947 : loss : 0.250975, loss_ce: 0.001840, loss_dice: 0.500110
[10:10:21.618] TRAIN: iteration 14948 : loss : 0.076215, loss_ce: 0.001273, loss_dice: 0.151156
[10:10:21.826] TRAIN: iteration 14949 : loss : 0.239531, loss_ce: 0.020123, loss_dice: 0.458939
[10:10:22.034] TRAIN: iteration 14950 : loss : 0.132325, loss_ce: 0.007612, loss_dice: 0.257039
[10:10:22.242] TRAIN: iteration 14951 : loss : 0.136336, loss_ce: 0.004216, loss_dice: 0.268456
[10:10:22.451] TRAIN: iteration 14952 : loss : 0.205173, loss_ce: 0.014506, loss_dice: 0.395840
[10:10:24.823] TRAIN: iteration 14953 : loss : 0.150756, loss_ce: 0.024015, loss_dice: 0.277497
[10:10:25.031] TRAIN: iteration 14954 : loss : 0.133364, loss_ce: 0.002160, loss_dice: 0.264568
[10:10:25.246] TRAIN: iteration 14955 : loss : 0.250877, loss_ce: 0.001671, loss_dice: 0.500083
[10:10:25.456] TRAIN: iteration 14956 : loss : 0.108386, loss_ce: 0.005649, loss_dice: 0.211123
[10:10:25.664] TRAIN: iteration 14957 : loss : 0.109880, loss_ce: 0.007351, loss_dice: 0.212409
[10:10:25.872] TRAIN: iteration 14958 : loss : 0.074311, loss_ce: 0.004088, loss_dice: 0.144534
[10:10:26.087] TRAIN: iteration 14959 : loss : 0.251361, loss_ce: 0.002587, loss_dice: 0.500135
[10:10:26.295] TRAIN: iteration 14960 : loss : 0.207018, loss_ce: 0.003787, loss_dice: 0.410248
[10:10:26.296] NaN or Inf found in input tensor.
[10:10:27.930] TRAIN: iteration 14961 : loss : 0.136131, loss_ce: 0.005910, loss_dice: 0.266353
[10:10:28.139] TRAIN: iteration 14962 : loss : 0.252636, loss_ce: 0.007113, loss_dice: 0.498159
[10:10:28.349] TRAIN: iteration 14963 : loss : 0.095241, loss_ce: 0.008585, loss_dice: 0.181897
[10:10:28.557] TRAIN: iteration 14964 : loss : 0.115957, loss_ce: 0.006239, loss_dice: 0.225674
[10:10:28.797] TRAIN: iteration 14965 : loss : 0.118965, loss_ce: 0.010814, loss_dice: 0.227115
[10:10:29.007] TRAIN: iteration 14966 : loss : 0.067874, loss_ce: 0.005935, loss_dice: 0.129812
[10:10:29.214] TRAIN: iteration 14967 : loss : 0.250930, loss_ce: 0.001789, loss_dice: 0.500072
[10:10:29.422] TRAIN: iteration 14968 : loss : 0.166177, loss_ce: 0.004871, loss_dice: 0.327484
[10:10:29.801] TRAIN: iteration 14969 : loss : 0.072805, loss_ce: 0.005780, loss_dice: 0.139830
[10:10:30.010] TRAIN: iteration 14970 : loss : 0.053094, loss_ce: 0.001916, loss_dice: 0.104273
[10:10:30.220] TRAIN: iteration 14971 : loss : 0.184530, loss_ce: 0.008546, loss_dice: 0.360515
[10:10:30.428] TRAIN: iteration 14972 : loss : 0.251686, loss_ce: 0.003169, loss_dice: 0.500203
[10:10:30.636] TRAIN: iteration 14973 : loss : 0.251339, loss_ce: 0.002520, loss_dice: 0.500159
[10:10:30.845] TRAIN: iteration 14974 : loss : 0.024582, loss_ce: 0.001856, loss_dice: 0.047308
[10:10:31.053] TRAIN: iteration 14975 : loss : 0.107618, loss_ce: 0.003417, loss_dice: 0.211818
[10:10:31.263] TRAIN: iteration 14976 : loss : 0.089718, loss_ce: 0.019669, loss_dice: 0.159767
[10:10:31.967] TRAIN: iteration 14977 : loss : 0.227865, loss_ce: 0.003630, loss_dice: 0.452101
[10:10:32.175] TRAIN: iteration 14978 : loss : 0.250903, loss_ce: 0.001737, loss_dice: 0.500069
[10:10:32.390] TRAIN: iteration 14979 : loss : 0.097414, loss_ce: 0.003720, loss_dice: 0.191108
[10:10:33.085] TRAIN: iteration 14980 : loss : 0.063773, loss_ce: 0.003551, loss_dice: 0.123996
[10:10:33.316] TRAIN: iteration 14981 : loss : 0.057774, loss_ce: 0.001385, loss_dice: 0.114164
[10:10:33.526] TRAIN: iteration 14982 : loss : 0.113735, loss_ce: 0.003029, loss_dice: 0.224441
[10:10:33.733] TRAIN: iteration 14983 : loss : 0.234346, loss_ce: 0.004894, loss_dice: 0.463798
[10:10:33.940] TRAIN: iteration 14984 : loss : 0.239468, loss_ce: 0.003387, loss_dice: 0.475548
[10:10:34.146] TRAIN: iteration 14985 : loss : 0.042507, loss_ce: 0.001760, loss_dice: 0.083254
[10:10:34.355] TRAIN: iteration 14986 : loss : 0.085466, loss_ce: 0.003240, loss_dice: 0.167692
[10:10:35.954] TRAIN: iteration 14987 : loss : 0.251267, loss_ce: 0.002429, loss_dice: 0.500106
[10:10:37.795] TRAIN: iteration 14988 : loss : 0.251386, loss_ce: 0.002646, loss_dice: 0.500126
[10:10:38.005] TRAIN: iteration 14989 : loss : 0.143549, loss_ce: 0.008699, loss_dice: 0.278399
[10:10:38.219] TRAIN: iteration 14990 : loss : 0.125012, loss_ce: 0.001847, loss_dice: 0.248176
[10:10:38.426] TRAIN: iteration 14991 : loss : 0.190126, loss_ce: 0.010357, loss_dice: 0.369895
[10:10:38.641] TRAIN: iteration 14992 : loss : 0.248382, loss_ce: 0.002583, loss_dice: 0.494181
[10:10:38.855] TRAIN: iteration 14993 : loss : 0.215725, loss_ce: 0.006736, loss_dice: 0.424715
[10:10:39.066] TRAIN: iteration 14994 : loss : 0.056639, loss_ce: 0.002228, loss_dice: 0.111050
[10:10:39.281] TRAIN: iteration 14995 : loss : 0.201670, loss_ce: 0.003601, loss_dice: 0.399739
[10:10:40.670] TRAIN: iteration 14996 : loss : 0.032701, loss_ce: 0.001700, loss_dice: 0.063702
[10:10:40.878] TRAIN: iteration 14997 : loss : 0.053059, loss_ce: 0.002943, loss_dice: 0.103174
[10:10:41.087] TRAIN: iteration 14998 : loss : 0.030718, loss_ce: 0.003590, loss_dice: 0.057846
[10:10:41.294] TRAIN: iteration 14999 : loss : 0.122061, loss_ce: 0.006592, loss_dice: 0.237531
[10:10:41.503] TRAIN: iteration 15000 : loss : 0.108737, loss_ce: 0.004799, loss_dice: 0.212675
[10:10:41.745] TRAIN: iteration 15001 : loss : 0.185272, loss_ce: 0.003197, loss_dice: 0.367347
[10:10:41.960] TRAIN: iteration 15002 : loss : 0.250973, loss_ce: 0.001869, loss_dice: 0.500078
[10:10:42.171] TRAIN: iteration 15003 : loss : 0.023081, loss_ce: 0.002867, loss_dice: 0.043296
[10:10:42.382] TRAIN: iteration 15004 : loss : 0.251019, loss_ce: 0.003892, loss_dice: 0.498145
[10:10:42.590] TRAIN: iteration 15005 : loss : 0.251704, loss_ce: 0.003220, loss_dice: 0.500189
[10:10:42.805] TRAIN: iteration 15006 : loss : 0.106241, loss_ce: 0.005769, loss_dice: 0.206712
[10:10:43.013] TRAIN: iteration 15007 : loss : 0.202532, loss_ce: 0.008449, loss_dice: 0.396615
[10:10:43.221] TRAIN: iteration 15008 : loss : 0.098308, loss_ce: 0.003608, loss_dice: 0.193007
[10:10:43.429] TRAIN: iteration 15009 : loss : 0.102049, loss_ce: 0.003130, loss_dice: 0.200967
[10:10:43.638] TRAIN: iteration 15010 : loss : 0.237633, loss_ce: 0.004820, loss_dice: 0.470446
[10:10:45.036] TRAIN: iteration 15011 : loss : 0.190351, loss_ce: 0.005615, loss_dice: 0.375088
[10:10:45.244] TRAIN: iteration 15012 : loss : 0.115252, loss_ce: 0.002258, loss_dice: 0.228245
[10:10:45.451] TRAIN: iteration 15013 : loss : 0.129907, loss_ce: 0.002528, loss_dice: 0.257285
[10:10:45.657] TRAIN: iteration 15014 : loss : 0.083716, loss_ce: 0.005035, loss_dice: 0.162397
[10:10:45.864] TRAIN: iteration 15015 : loss : 0.144393, loss_ce: 0.020551, loss_dice: 0.268234
[10:10:46.072] TRAIN: iteration 15016 : loss : 0.079379, loss_ce: 0.002721, loss_dice: 0.156037
[10:10:46.280] TRAIN: iteration 15017 : loss : 0.144812, loss_ce: 0.002535, loss_dice: 0.287089
[10:10:46.489] TRAIN: iteration 15018 : loss : 0.082155, loss_ce: 0.002085, loss_dice: 0.162225
[10:10:47.442] TRAIN: iteration 15019 : loss : 0.059148, loss_ce: 0.003357, loss_dice: 0.114939
[10:10:47.649] TRAIN: iteration 15020 : loss : 0.044550, loss_ce: 0.002129, loss_dice: 0.086972
[10:10:47.884] TRAIN: iteration 15021 : loss : 0.081976, loss_ce: 0.002648, loss_dice: 0.161304
[10:10:48.092] TRAIN: iteration 15022 : loss : 0.102191, loss_ce: 0.020170, loss_dice: 0.184212
[10:10:48.306] TRAIN: iteration 15023 : loss : 0.158134, loss_ce: 0.001950, loss_dice: 0.314319
[10:10:48.523] TRAIN: iteration 15024 : loss : 0.068164, loss_ce: 0.002239, loss_dice: 0.134089
[10:10:48.730] TRAIN: iteration 15025 : loss : 0.216371, loss_ce: 0.014746, loss_dice: 0.417997
[10:10:48.938] TRAIN: iteration 15026 : loss : 0.025900, loss_ce: 0.001947, loss_dice: 0.049853
[10:10:49.757] TRAIN: iteration 15027 : loss : 0.095024, loss_ce: 0.011358, loss_dice: 0.178691
[10:10:49.964] TRAIN: iteration 15028 : loss : 0.104503, loss_ce: 0.005058, loss_dice: 0.203948
[10:10:50.173] TRAIN: iteration 15029 : loss : 0.246418, loss_ce: 0.006269, loss_dice: 0.486567
[10:10:50.381] TRAIN: iteration 15030 : loss : 0.250672, loss_ce: 0.001296, loss_dice: 0.500048
[10:10:50.589] TRAIN: iteration 15031 : loss : 0.170541, loss_ce: 0.005066, loss_dice: 0.336015
[10:10:50.797] TRAIN: iteration 15032 : loss : 0.132834, loss_ce: 0.009574, loss_dice: 0.256094
[10:10:51.007] TRAIN: iteration 15033 : loss : 0.121415, loss_ce: 0.008307, loss_dice: 0.234524
[10:10:51.226] TRAIN: iteration 15034 : loss : 0.100690, loss_ce: 0.010428, loss_dice: 0.190951
[10:10:52.012] TRAIN: iteration 15035 : loss : 0.039187, loss_ce: 0.004908, loss_dice: 0.073466
[10:10:52.220] TRAIN: iteration 15036 : loss : 0.079456, loss_ce: 0.004069, loss_dice: 0.154843
[10:10:52.435] TRAIN: iteration 15037 : loss : 0.054034, loss_ce: 0.003363, loss_dice: 0.104705
[10:10:52.645] TRAIN: iteration 15038 : loss : 0.140852, loss_ce: 0.003589, loss_dice: 0.278116
[10:10:53.594] TRAIN: iteration 15039 : loss : 0.044618, loss_ce: 0.002722, loss_dice: 0.086515
[10:10:53.804] TRAIN: iteration 15040 : loss : 0.054694, loss_ce: 0.005217, loss_dice: 0.104171
[10:10:54.041] TRAIN: iteration 15041 : loss : 0.082729, loss_ce: 0.003005, loss_dice: 0.162454
[10:10:54.250] TRAIN: iteration 15042 : loss : 0.050844, loss_ce: 0.002777, loss_dice: 0.098910
[10:10:54.458] TRAIN: iteration 15043 : loss : 0.251507, loss_ce: 0.004519, loss_dice: 0.498495
[10:10:55.116] TRAIN: iteration 15044 : loss : 0.111340, loss_ce: 0.003635, loss_dice: 0.219044
[10:10:55.323] TRAIN: iteration 15045 : loss : 0.248176, loss_ce: 0.005878, loss_dice: 0.490475
[10:10:55.532] TRAIN: iteration 15046 : loss : 0.136820, loss_ce: 0.002847, loss_dice: 0.270794
[10:10:56.943] TRAIN: iteration 15047 : loss : 0.077986, loss_ce: 0.001893, loss_dice: 0.154080
[10:10:57.149] TRAIN: iteration 15048 : loss : 0.053233, loss_ce: 0.001348, loss_dice: 0.105118
[10:10:57.356] TRAIN: iteration 15049 : loss : 0.187501, loss_ce: 0.002241, loss_dice: 0.372761
[10:10:57.562] TRAIN: iteration 15050 : loss : 0.128061, loss_ce: 0.004080, loss_dice: 0.252043
[10:10:59.475] TRAIN: iteration 15051 : loss : 0.117092, loss_ce: 0.006411, loss_dice: 0.227773
[10:10:59.687] TRAIN: iteration 15052 : loss : 0.214712, loss_ce: 0.003228, loss_dice: 0.426195
[10:10:59.894] TRAIN: iteration 15053 : loss : 0.250453, loss_ce: 0.000890, loss_dice: 0.500015
[10:11:00.101] TRAIN: iteration 15054 : loss : 0.214517, loss_ce: 0.004547, loss_dice: 0.424487
[10:11:00.892] TRAIN: iteration 15055 : loss : 0.107465, loss_ce: 0.003944, loss_dice: 0.210987
[10:11:01.099] TRAIN: iteration 15056 : loss : 0.100881, loss_ce: 0.005411, loss_dice: 0.196351
[10:11:01.306] TRAIN: iteration 15057 : loss : 0.188830, loss_ce: 0.001688, loss_dice: 0.375972
[10:11:01.513] TRAIN: iteration 15058 : loss : 0.196327, loss_ce: 0.005894, loss_dice: 0.386761
[10:11:01.719] TRAIN: iteration 15059 : loss : 0.065335, loss_ce: 0.002540, loss_dice: 0.128130
[10:11:01.925] TRAIN: iteration 15060 : loss : 0.029266, loss_ce: 0.002166, loss_dice: 0.056366
[10:11:02.168] TRAIN: iteration 15061 : loss : 0.086294, loss_ce: 0.004244, loss_dice: 0.168343
[10:11:02.376] TRAIN: iteration 15062 : loss : 0.135335, loss_ce: 0.005954, loss_dice: 0.264716
[10:11:02.584] TRAIN: iteration 15063 : loss : 0.074739, loss_ce: 0.003488, loss_dice: 0.145989
[10:11:02.836] TRAIN: iteration 15064 : loss : 0.232225, loss_ce: 0.002802, loss_dice: 0.461647
[10:11:03.043] TRAIN: iteration 15065 : loss : 0.066487, loss_ce: 0.002655, loss_dice: 0.130319
[10:11:03.251] TRAIN: iteration 15066 : loss : 0.093720, loss_ce: 0.002480, loss_dice: 0.184961
[10:11:03.521] TRAIN: iteration 15067 : loss : 0.199905, loss_ce: 0.002905, loss_dice: 0.396905
[10:11:03.731] TRAIN: iteration 15068 : loss : 0.160440, loss_ce: 0.004089, loss_dice: 0.316791
[10:11:03.940] TRAIN: iteration 15069 : loss : 0.092867, loss_ce: 0.004326, loss_dice: 0.181408
[10:11:04.154] TRAIN: iteration 15070 : loss : 0.124841, loss_ce: 0.004429, loss_dice: 0.245254
[10:11:04.747] TRAIN: iteration 15071 : loss : 0.086202, loss_ce: 0.002324, loss_dice: 0.170080
[10:11:05.225] TRAIN: iteration 15072 : loss : 0.239430, loss_ce: 0.005367, loss_dice: 0.473494
[10:11:05.433] TRAIN: iteration 15073 : loss : 0.138212, loss_ce: 0.009097, loss_dice: 0.267327
[10:11:05.640] TRAIN: iteration 15074 : loss : 0.250964, loss_ce: 0.001823, loss_dice: 0.500105
[10:11:05.851] TRAIN: iteration 15075 : loss : 0.113576, loss_ce: 0.005049, loss_dice: 0.222103
[10:11:06.791] TRAIN: iteration 15076 : loss : 0.251460, loss_ce: 0.002750, loss_dice: 0.500171
[10:11:06.998] TRAIN: iteration 15077 : loss : 0.102978, loss_ce: 0.002541, loss_dice: 0.203415
[10:11:07.209] TRAIN: iteration 15078 : loss : 0.250786, loss_ce: 0.001509, loss_dice: 0.500063
[10:11:07.416] TRAIN: iteration 15079 : loss : 0.148680, loss_ce: 0.013714, loss_dice: 0.283646
[10:11:07.623] TRAIN: iteration 15080 : loss : 0.146660, loss_ce: 0.001983, loss_dice: 0.291337
[10:11:07.624] NaN or Inf found in input tensor.
[10:11:07.849] TRAIN: iteration 15081 : loss : 0.223718, loss_ce: 0.002753, loss_dice: 0.444683
[10:11:08.060] TRAIN: iteration 15082 : loss : 0.150018, loss_ce: 0.005846, loss_dice: 0.294191
[10:11:08.266] TRAIN: iteration 15083 : loss : 0.108716, loss_ce: 0.004551, loss_dice: 0.212880
[10:11:10.211] TRAIN: iteration 15084 : loss : 0.063395, loss_ce: 0.007878, loss_dice: 0.118912
[10:11:10.422] TRAIN: iteration 15085 : loss : 0.081092, loss_ce: 0.007253, loss_dice: 0.154931
[10:11:10.630] TRAIN: iteration 15086 : loss : 0.059102, loss_ce: 0.003381, loss_dice: 0.114823
[10:11:10.837] TRAIN: iteration 15087 : loss : 0.110544, loss_ce: 0.004302, loss_dice: 0.216785
[10:11:11.045] TRAIN: iteration 15088 : loss : 0.058010, loss_ce: 0.004937, loss_dice: 0.111084
[10:11:11.260] TRAIN: iteration 15089 : loss : 0.065215, loss_ce: 0.002318, loss_dice: 0.128111
[10:11:11.467] TRAIN: iteration 15090 : loss : 0.040242, loss_ce: 0.002684, loss_dice: 0.077800
[10:11:11.681] TRAIN: iteration 15091 : loss : 0.124060, loss_ce: 0.005077, loss_dice: 0.243043
[10:11:12.353] TRAIN: iteration 15092 : loss : 0.164287, loss_ce: 0.005144, loss_dice: 0.323431
[10:11:12.560] TRAIN: iteration 15093 : loss : 0.251528, loss_ce: 0.002860, loss_dice: 0.500195
[10:11:12.768] TRAIN: iteration 15094 : loss : 0.044254, loss_ce: 0.003089, loss_dice: 0.085418
[10:11:12.975] TRAIN: iteration 15095 : loss : 0.159641, loss_ce: 0.003079, loss_dice: 0.316203
[10:11:13.182] TRAIN: iteration 15096 : loss : 0.251375, loss_ce: 0.002595, loss_dice: 0.500155
[10:11:13.390] TRAIN: iteration 15097 : loss : 0.127079, loss_ce: 0.001119, loss_dice: 0.253039
[10:11:13.598] TRAIN: iteration 15098 : loss : 0.249532, loss_ce: 0.001998, loss_dice: 0.497067
[10:11:13.808] TRAIN: iteration 15099 : loss : 0.147859, loss_ce: 0.003278, loss_dice: 0.292439
[10:11:17.607] TRAIN: iteration 15100 : loss : 0.085733, loss_ce: 0.008196, loss_dice: 0.163270
[10:11:17.842] TRAIN: iteration 15101 : loss : 0.245802, loss_ce: 0.002603, loss_dice: 0.489001
[10:11:18.049] TRAIN: iteration 15102 : loss : 0.233377, loss_ce: 0.007268, loss_dice: 0.459486
[10:11:18.257] TRAIN: iteration 15103 : loss : 0.227273, loss_ce: 0.004394, loss_dice: 0.450152
[10:11:18.466] TRAIN: iteration 15104 : loss : 0.026961, loss_ce: 0.002088, loss_dice: 0.051834
[10:11:18.680] TRAIN: iteration 15105 : loss : 0.162963, loss_ce: 0.006930, loss_dice: 0.318996
[10:11:18.887] TRAIN: iteration 15106 : loss : 0.250379, loss_ce: 0.000742, loss_dice: 0.500016
[10:11:19.095] TRAIN: iteration 15107 : loss : 0.250223, loss_ce: 0.000444, loss_dice: 0.500003
[10:11:20.176] TRAIN: iteration 15108 : loss : 0.144884, loss_ce: 0.003278, loss_dice: 0.286490
[10:11:20.384] TRAIN: iteration 15109 : loss : 0.024561, loss_ce: 0.001370, loss_dice: 0.047752
[10:11:20.590] TRAIN: iteration 15110 : loss : 0.236938, loss_ce: 0.001798, loss_dice: 0.472077
[10:11:20.800] TRAIN: iteration 15111 : loss : 0.116952, loss_ce: 0.003238, loss_dice: 0.230665
[10:11:21.011] TRAIN: iteration 15112 : loss : 0.084634, loss_ce: 0.008217, loss_dice: 0.161052
[10:11:21.222] TRAIN: iteration 15113 : loss : 0.147956, loss_ce: 0.005002, loss_dice: 0.290909
[10:11:21.429] TRAIN: iteration 15114 : loss : 0.209447, loss_ce: 0.009297, loss_dice: 0.409597
[10:11:21.638] TRAIN: iteration 15115 : loss : 0.090819, loss_ce: 0.003594, loss_dice: 0.178044
[10:11:23.950] TRAIN: iteration 15116 : loss : 0.116533, loss_ce: 0.004061, loss_dice: 0.229005
[10:11:24.157] TRAIN: iteration 15117 : loss : 0.160036, loss_ce: 0.004380, loss_dice: 0.315692
[10:11:24.364] TRAIN: iteration 15118 : loss : 0.059987, loss_ce: 0.004076, loss_dice: 0.115898
[10:11:24.574] TRAIN: iteration 15119 : loss : 0.114027, loss_ce: 0.007036, loss_dice: 0.221018
[10:11:24.784] TRAIN: iteration 15120 : loss : 0.097880, loss_ce: 0.003136, loss_dice: 0.192623
[10:11:25.026] TRAIN: iteration 15121 : loss : 0.251136, loss_ce: 0.002179, loss_dice: 0.500093
[10:11:25.267] TRAIN: iteration 15122 : loss : 0.239173, loss_ce: 0.003428, loss_dice: 0.474918
[10:11:25.475] TRAIN: iteration 15123 : loss : 0.198336, loss_ce: 0.016656, loss_dice: 0.380017
[10:11:27.970] TRAIN: iteration 15124 : loss : 0.114876, loss_ce: 0.004053, loss_dice: 0.225698
[10:11:28.178] TRAIN: iteration 15125 : loss : 0.061956, loss_ce: 0.002750, loss_dice: 0.121161
[10:11:28.389] TRAIN: iteration 15126 : loss : 0.066440, loss_ce: 0.002997, loss_dice: 0.129884
[10:11:28.596] TRAIN: iteration 15127 : loss : 0.252581, loss_ce: 0.006974, loss_dice: 0.498189
[10:11:28.809] TRAIN: iteration 15128 : loss : 0.240549, loss_ce: 0.005304, loss_dice: 0.475795
[10:11:29.016] TRAIN: iteration 15129 : loss : 0.056190, loss_ce: 0.005821, loss_dice: 0.106559
[10:11:29.223] TRAIN: iteration 15130 : loss : 0.120316, loss_ce: 0.009621, loss_dice: 0.231012
[10:11:29.431] TRAIN: iteration 15131 : loss : 0.147368, loss_ce: 0.013547, loss_dice: 0.281188
[10:11:31.819] TRAIN: iteration 15132 : loss : 0.142861, loss_ce: 0.003802, loss_dice: 0.281920
[10:11:32.026] TRAIN: iteration 15133 : loss : 0.051955, loss_ce: 0.002514, loss_dice: 0.101396
[10:11:32.233] TRAIN: iteration 15134 : loss : 0.251323, loss_ce: 0.002511, loss_dice: 0.500135
[10:11:32.441] TRAIN: iteration 15135 : loss : 0.202786, loss_ce: 0.005898, loss_dice: 0.399674
[10:11:32.649] TRAIN: iteration 15136 : loss : 0.112185, loss_ce: 0.016730, loss_dice: 0.207639
[10:11:32.856] TRAIN: iteration 15137 : loss : 0.251048, loss_ce: 0.002005, loss_dice: 0.500090
[10:11:33.063] TRAIN: iteration 15138 : loss : 0.089323, loss_ce: 0.004644, loss_dice: 0.174002
[10:11:33.276] TRAIN: iteration 15139 : loss : 0.066593, loss_ce: 0.007030, loss_dice: 0.126157
[10:11:33.680] TRAIN: iteration 15140 : loss : 0.219405, loss_ce: 0.004930, loss_dice: 0.433880
[10:11:33.923] TRAIN: iteration 15141 : loss : 0.108904, loss_ce: 0.013678, loss_dice: 0.204130
[10:11:35.031] TRAIN: iteration 15142 : loss : 0.061229, loss_ce: 0.003484, loss_dice: 0.118975
[10:11:35.238] TRAIN: iteration 15143 : loss : 0.143979, loss_ce: 0.004907, loss_dice: 0.283052
[10:11:35.447] TRAIN: iteration 15144 : loss : 0.056375, loss_ce: 0.006150, loss_dice: 0.106600
[10:11:35.653] TRAIN: iteration 15145 : loss : 0.063605, loss_ce: 0.007535, loss_dice: 0.119674
[10:11:35.860] TRAIN: iteration 15146 : loss : 0.140039, loss_ce: 0.003594, loss_dice: 0.276484
[10:11:36.068] TRAIN: iteration 15147 : loss : 0.206386, loss_ce: 0.004337, loss_dice: 0.408435
[10:11:36.274] TRAIN: iteration 15148 : loss : 0.084504, loss_ce: 0.006796, loss_dice: 0.162213
[10:11:36.482] TRAIN: iteration 15149 : loss : 0.162417, loss_ce: 0.008610, loss_dice: 0.316223
[10:11:36.689] TRAIN: iteration 15150 : loss : 0.161274, loss_ce: 0.004902, loss_dice: 0.317647
[10:11:36.897] TRAIN: iteration 15151 : loss : 0.038967, loss_ce: 0.003875, loss_dice: 0.074059
[10:11:37.105] TRAIN: iteration 15152 : loss : 0.252263, loss_ce: 0.004255, loss_dice: 0.500271
[10:11:37.399] TRAIN: iteration 15153 : loss : 0.113634, loss_ce: 0.005211, loss_dice: 0.222058
[10:11:37.608] TRAIN: iteration 15154 : loss : 0.038522, loss_ce: 0.003926, loss_dice: 0.073119
[10:11:37.816] TRAIN: iteration 15155 : loss : 0.153072, loss_ce: 0.009627, loss_dice: 0.296517
[10:11:38.026] TRAIN: iteration 15156 : loss : 0.072663, loss_ce: 0.004951, loss_dice: 0.140374
[10:11:38.236] TRAIN: iteration 15157 : loss : 0.162934, loss_ce: 0.007580, loss_dice: 0.318288
[10:11:38.736] TRAIN: iteration 15158 : loss : 0.251358, loss_ce: 0.002575, loss_dice: 0.500141
[10:11:39.343] TRAIN: iteration 15159 : loss : 0.252059, loss_ce: 0.004323, loss_dice: 0.499795
[10:11:39.552] TRAIN: iteration 15160 : loss : 0.101218, loss_ce: 0.005673, loss_dice: 0.196764
[10:11:41.293] TRAIN: iteration 15161 : loss : 0.097849, loss_ce: 0.002981, loss_dice: 0.192716
[10:11:41.503] TRAIN: iteration 15162 : loss : 0.139753, loss_ce: 0.005557, loss_dice: 0.273950
[10:11:41.712] TRAIN: iteration 15163 : loss : 0.037938, loss_ce: 0.001467, loss_dice: 0.074408
[10:11:41.920] TRAIN: iteration 15164 : loss : 0.126011, loss_ce: 0.004328, loss_dice: 0.247693
[10:11:42.129] TRAIN: iteration 15165 : loss : 0.251314, loss_ce: 0.002489, loss_dice: 0.500140
[10:11:42.339] TRAIN: iteration 15166 : loss : 0.042670, loss_ce: 0.003317, loss_dice: 0.082024
[10:11:42.547] TRAIN: iteration 15167 : loss : 0.160249, loss_ce: 0.003942, loss_dice: 0.316556
[10:11:42.756] TRAIN: iteration 15168 : loss : 0.093593, loss_ce: 0.003566, loss_dice: 0.183620
[10:11:45.492] TRAIN: iteration 15169 : loss : 0.121818, loss_ce: 0.003645, loss_dice: 0.239992
[10:11:45.700] TRAIN: iteration 15170 : loss : 0.126570, loss_ce: 0.003046, loss_dice: 0.250093
[10:11:45.908] TRAIN: iteration 15171 : loss : 0.251999, loss_ce: 0.003742, loss_dice: 0.500256
[10:11:46.115] TRAIN: iteration 15172 : loss : 0.090325, loss_ce: 0.006096, loss_dice: 0.174554
[10:11:46.323] TRAIN: iteration 15173 : loss : 0.071507, loss_ce: 0.003332, loss_dice: 0.139682
[10:11:46.530] TRAIN: iteration 15174 : loss : 0.213471, loss_ce: 0.003694, loss_dice: 0.423247
[10:11:46.743] TRAIN: iteration 15175 : loss : 0.081708, loss_ce: 0.004821, loss_dice: 0.158595
[10:11:46.950] TRAIN: iteration 15176 : loss : 0.238957, loss_ce: 0.003052, loss_dice: 0.474863
[10:11:50.493] TRAIN: iteration 15177 : loss : 0.071960, loss_ce: 0.004791, loss_dice: 0.139129
[10:11:50.703] TRAIN: iteration 15178 : loss : 0.077755, loss_ce: 0.002927, loss_dice: 0.152583
[10:11:50.910] TRAIN: iteration 15179 : loss : 0.052247, loss_ce: 0.003071, loss_dice: 0.101423
[10:11:51.118] TRAIN: iteration 15180 : loss : 0.141447, loss_ce: 0.005554, loss_dice: 0.277340
[10:11:51.361] TRAIN: iteration 15181 : loss : 0.122068, loss_ce: 0.003546, loss_dice: 0.240590
[10:11:51.569] TRAIN: iteration 15182 : loss : 0.223438, loss_ce: 0.002825, loss_dice: 0.444052
[10:11:51.776] TRAIN: iteration 15183 : loss : 0.227277, loss_ce: 0.004361, loss_dice: 0.450193
[10:11:51.986] TRAIN: iteration 15184 : loss : 0.171154, loss_ce: 0.003234, loss_dice: 0.339075
[10:11:52.702] TRAIN: iteration 15185 : loss : 0.068988, loss_ce: 0.003632, loss_dice: 0.134344
[10:11:52.913] TRAIN: iteration 15186 : loss : 0.200734, loss_ce: 0.004132, loss_dice: 0.397336
[10:11:53.121] TRAIN: iteration 15187 : loss : 0.179053, loss_ce: 0.005202, loss_dice: 0.352904
[10:11:53.333] TRAIN: iteration 15188 : loss : 0.067965, loss_ce: 0.002627, loss_dice: 0.133304
[10:11:53.543] TRAIN: iteration 15189 : loss : 0.075082, loss_ce: 0.001766, loss_dice: 0.148399
[10:11:53.751] TRAIN: iteration 15190 : loss : 0.250998, loss_ce: 0.001888, loss_dice: 0.500108
[10:11:53.960] TRAIN: iteration 15191 : loss : 0.247579, loss_ce: 0.003092, loss_dice: 0.492066
[10:11:54.168] TRAIN: iteration 15192 : loss : 0.250673, loss_ce: 0.001311, loss_dice: 0.500035
[10:11:56.330] TRAIN: iteration 15193 : loss : 0.087031, loss_ce: 0.002224, loss_dice: 0.171838
[10:11:56.543] TRAIN: iteration 15194 : loss : 0.143787, loss_ce: 0.010203, loss_dice: 0.277371
[10:11:56.750] TRAIN: iteration 15195 : loss : 0.082879, loss_ce: 0.003383, loss_dice: 0.162374
[10:11:56.958] TRAIN: iteration 15196 : loss : 0.103309, loss_ce: 0.004628, loss_dice: 0.201991
[10:11:58.492] TRAIN: iteration 15197 : loss : 0.064664, loss_ce: 0.002366, loss_dice: 0.126962
[10:11:58.700] TRAIN: iteration 15198 : loss : 0.096799, loss_ce: 0.007358, loss_dice: 0.186240
[10:11:59.934] TRAIN: iteration 15199 : loss : 0.082242, loss_ce: 0.003225, loss_dice: 0.161258
[10:12:00.141] TRAIN: iteration 15200 : loss : 0.030311, loss_ce: 0.001888, loss_dice: 0.058733
[10:12:00.384] TRAIN: iteration 15201 : loss : 0.232285, loss_ce: 0.002094, loss_dice: 0.462476
[10:12:00.591] TRAIN: iteration 15202 : loss : 0.058439, loss_ce: 0.007042, loss_dice: 0.109836
[10:12:00.980] TRAIN: iteration 15203 : loss : 0.118784, loss_ce: 0.005729, loss_dice: 0.231840
[10:12:01.188] TRAIN: iteration 15204 : loss : 0.166455, loss_ce: 0.005956, loss_dice: 0.326954
[10:12:01.397] TRAIN: iteration 15205 : loss : 0.110105, loss_ce: 0.007851, loss_dice: 0.212358
[10:12:02.070] TRAIN: iteration 15206 : loss : 0.026080, loss_ce: 0.003021, loss_dice: 0.049139
[10:12:03.763] TRAIN: iteration 15207 : loss : 0.156525, loss_ce: 0.002705, loss_dice: 0.310346
[10:12:03.973] TRAIN: iteration 15208 : loss : 0.247802, loss_ce: 0.004565, loss_dice: 0.491039
[10:12:04.185] TRAIN: iteration 15209 : loss : 0.251204, loss_ce: 0.002277, loss_dice: 0.500132
[10:12:04.394] TRAIN: iteration 15210 : loss : 0.105594, loss_ce: 0.002900, loss_dice: 0.208287
[10:12:05.144] TRAIN: iteration 15211 : loss : 0.130155, loss_ce: 0.005822, loss_dice: 0.254489
[10:12:05.352] TRAIN: iteration 15212 : loss : 0.136119, loss_ce: 0.004693, loss_dice: 0.267546
[10:12:05.560] TRAIN: iteration 15213 : loss : 0.111335, loss_ce: 0.004427, loss_dice: 0.218243
[10:12:06.276] TRAIN: iteration 15214 : loss : 0.148780, loss_ce: 0.003694, loss_dice: 0.293867
[10:12:08.619] TRAIN: iteration 15215 : loss : 0.129535, loss_ce: 0.005363, loss_dice: 0.253708
[10:12:08.829] TRAIN: iteration 15216 : loss : 0.118136, loss_ce: 0.004006, loss_dice: 0.232267
[10:12:09.040] TRAIN: iteration 15217 : loss : 0.222600, loss_ce: 0.004228, loss_dice: 0.440971
[10:12:09.248] TRAIN: iteration 15218 : loss : 0.041613, loss_ce: 0.002022, loss_dice: 0.081204
[10:12:09.456] TRAIN: iteration 15219 : loss : 0.180788, loss_ce: 0.004754, loss_dice: 0.356822
[10:12:09.665] TRAIN: iteration 15220 : loss : 0.045093, loss_ce: 0.003132, loss_dice: 0.087054
[10:12:09.908] TRAIN: iteration 15221 : loss : 0.251881, loss_ce: 0.003529, loss_dice: 0.500232
[10:12:10.117] TRAIN: iteration 15222 : loss : 0.089855, loss_ce: 0.005766, loss_dice: 0.173944
[10:12:12.697] TRAIN: iteration 15223 : loss : 0.084495, loss_ce: 0.002850, loss_dice: 0.166139
[10:12:12.904] TRAIN: iteration 15224 : loss : 0.100915, loss_ce: 0.006727, loss_dice: 0.195104
[10:12:13.113] TRAIN: iteration 15225 : loss : 0.126022, loss_ce: 0.005501, loss_dice: 0.246542
[10:12:13.320] TRAIN: iteration 15226 : loss : 0.083907, loss_ce: 0.012739, loss_dice: 0.155075
[10:12:13.846] TRAIN: iteration 15227 : loss : 0.096630, loss_ce: 0.003137, loss_dice: 0.190124
[10:12:14.053] TRAIN: iteration 15228 : loss : 0.128554, loss_ce: 0.003771, loss_dice: 0.253338
[10:12:14.263] TRAIN: iteration 15229 : loss : 0.039578, loss_ce: 0.001953, loss_dice: 0.077203
[10:12:14.470] TRAIN: iteration 15230 : loss : 0.076121, loss_ce: 0.002006, loss_dice: 0.150236
[10:12:14.678] TRAIN: iteration 15231 : loss : 0.100593, loss_ce: 0.003039, loss_dice: 0.198148
[10:12:14.886] TRAIN: iteration 15232 : loss : 0.088589, loss_ce: 0.006493, loss_dice: 0.170685
[10:12:15.095] TRAIN: iteration 15233 : loss : 0.206503, loss_ce: 0.010881, loss_dice: 0.402125
[10:12:15.303] TRAIN: iteration 15234 : loss : 0.253382, loss_ce: 0.006992, loss_dice: 0.499772
[10:12:15.745] TRAIN: iteration 15235 : loss : 0.076404, loss_ce: 0.011596, loss_dice: 0.141212
[10:12:15.955] TRAIN: iteration 15236 : loss : 0.129495, loss_ce: 0.003881, loss_dice: 0.255109
[10:12:16.164] TRAIN: iteration 15237 : loss : 0.176620, loss_ce: 0.001967, loss_dice: 0.351273
[10:12:16.373] TRAIN: iteration 15238 : loss : 0.088446, loss_ce: 0.002357, loss_dice: 0.174534
[10:12:16.583] TRAIN: iteration 15239 : loss : 0.037576, loss_ce: 0.001553, loss_dice: 0.073600
[10:12:16.792] TRAIN: iteration 15240 : loss : 0.046754, loss_ce: 0.002316, loss_dice: 0.091191
[10:12:17.028] TRAIN: iteration 15241 : loss : 0.034425, loss_ce: 0.001916, loss_dice: 0.066934
[10:12:17.269] TRAIN: iteration 15242 : loss : 0.114885, loss_ce: 0.004822, loss_dice: 0.224949
[10:12:20.546] TRAIN: iteration 15243 : loss : 0.067925, loss_ce: 0.003462, loss_dice: 0.132387
[10:12:20.759] TRAIN: iteration 15244 : loss : 0.231948, loss_ce: 0.007628, loss_dice: 0.456269
[10:12:20.967] TRAIN: iteration 15245 : loss : 0.121361, loss_ce: 0.002043, loss_dice: 0.240680
[10:12:21.178] TRAIN: iteration 15246 : loss : 0.107393, loss_ce: 0.006060, loss_dice: 0.208726
[10:12:21.392] TRAIN: iteration 15247 : loss : 0.231337, loss_ce: 0.003791, loss_dice: 0.458884
[10:12:21.600] TRAIN: iteration 15248 : loss : 0.176939, loss_ce: 0.002205, loss_dice: 0.351674
[10:12:21.807] TRAIN: iteration 15249 : loss : 0.250864, loss_ce: 0.001671, loss_dice: 0.500056
[10:12:22.017] TRAIN: iteration 15250 : loss : 0.099689, loss_ce: 0.003466, loss_dice: 0.195911
[10:12:23.720] TRAIN: iteration 15251 : loss : 0.120111, loss_ce: 0.002352, loss_dice: 0.237869
[10:12:23.928] TRAIN: iteration 15252 : loss : 0.160329, loss_ce: 0.003750, loss_dice: 0.316907
[10:12:24.197] TRAIN: iteration 15253 : loss : 0.063756, loss_ce: 0.003115, loss_dice: 0.124396
[10:12:25.165] TRAIN: iteration 15254 : loss : 0.089550, loss_ce: 0.008118, loss_dice: 0.170982
[10:12:26.488] TRAIN: iteration 15255 : loss : 0.111019, loss_ce: 0.003162, loss_dice: 0.218876
[10:12:26.695] TRAIN: iteration 15256 : loss : 0.080557, loss_ce: 0.003904, loss_dice: 0.157210
[10:12:26.906] TRAIN: iteration 15257 : loss : 0.251622, loss_ce: 0.003058, loss_dice: 0.500186
[10:12:27.115] TRAIN: iteration 15258 : loss : 0.145749, loss_ce: 0.007301, loss_dice: 0.284196
[10:12:27.355] TRAIN: iteration 15259 : loss : 0.137491, loss_ce: 0.003040, loss_dice: 0.271942
[10:12:27.565] TRAIN: iteration 15260 : loss : 0.253659, loss_ce: 0.007771, loss_dice: 0.499547
[10:12:27.927] TRAIN: iteration 15261 : loss : 0.114824, loss_ce: 0.005288, loss_dice: 0.224360
[10:12:28.136] TRAIN: iteration 15262 : loss : 0.138687, loss_ce: 0.032687, loss_dice: 0.244686
[10:12:28.898] TRAIN: iteration 15263 : loss : 0.095994, loss_ce: 0.004123, loss_dice: 0.187866
[10:12:29.114] TRAIN: iteration 15264 : loss : 0.116954, loss_ce: 0.004087, loss_dice: 0.229821
[10:12:29.325] TRAIN: iteration 15265 : loss : 0.124407, loss_ce: 0.005839, loss_dice: 0.242975
[10:12:29.533] TRAIN: iteration 15266 : loss : 0.037534, loss_ce: 0.001849, loss_dice: 0.073219
[10:12:30.359] TRAIN: iteration 15267 : loss : 0.147857, loss_ce: 0.011994, loss_dice: 0.283719
[10:12:32.339] TRAIN: iteration 15268 : loss : 0.166231, loss_ce: 0.004444, loss_dice: 0.328018
[10:12:32.692] TRAIN: iteration 15269 : loss : 0.252556, loss_ce: 0.004770, loss_dice: 0.500341
[10:12:32.900] TRAIN: iteration 15270 : loss : 0.116875, loss_ce: 0.003718, loss_dice: 0.230033
[10:12:33.304] TRAIN: iteration 15271 : loss : 0.251943, loss_ce: 0.003659, loss_dice: 0.500226
[10:12:33.512] TRAIN: iteration 15272 : loss : 0.130778, loss_ce: 0.007769, loss_dice: 0.253787
[10:12:33.719] TRAIN: iteration 15273 : loss : 0.146328, loss_ce: 0.012516, loss_dice: 0.280140
[10:12:33.926] TRAIN: iteration 15274 : loss : 0.119157, loss_ce: 0.007162, loss_dice: 0.231152
[10:12:34.137] TRAIN: iteration 15275 : loss : 0.022984, loss_ce: 0.001860, loss_dice: 0.044108
[10:12:36.526] TRAIN: iteration 15276 : loss : 0.123758, loss_ce: 0.007967, loss_dice: 0.239548
[10:12:36.736] TRAIN: iteration 15277 : loss : 0.130917, loss_ce: 0.002955, loss_dice: 0.258879
[10:12:36.946] TRAIN: iteration 15278 : loss : 0.114723, loss_ce: 0.002874, loss_dice: 0.226571
[10:12:37.154] TRAIN: iteration 15279 : loss : 0.045430, loss_ce: 0.004326, loss_dice: 0.086534
[10:12:37.363] TRAIN: iteration 15280 : loss : 0.084225, loss_ce: 0.002365, loss_dice: 0.166084
[10:12:37.590] TRAIN: iteration 15281 : loss : 0.065951, loss_ce: 0.007762, loss_dice: 0.124141
[10:12:37.798] TRAIN: iteration 15282 : loss : 0.056114, loss_ce: 0.003856, loss_dice: 0.108372
[10:12:38.005] TRAIN: iteration 15283 : loss : 0.047381, loss_ce: 0.003365, loss_dice: 0.091397
[10:12:40.868] TRAIN: iteration 15284 : loss : 0.122642, loss_ce: 0.004007, loss_dice: 0.241278
[10:12:41.077] TRAIN: iteration 15285 : loss : 0.075809, loss_ce: 0.005519, loss_dice: 0.146098
[10:12:41.285] TRAIN: iteration 15286 : loss : 0.197290, loss_ce: 0.010249, loss_dice: 0.384331
[10:12:41.496] TRAIN: iteration 15287 : loss : 0.243930, loss_ce: 0.002728, loss_dice: 0.485132
[10:12:41.707] TRAIN: iteration 15288 : loss : 0.100979, loss_ce: 0.006477, loss_dice: 0.195480
[10:12:41.916] TRAIN: iteration 15289 : loss : 0.049702, loss_ce: 0.002150, loss_dice: 0.097254
[10:12:42.124] TRAIN: iteration 15290 : loss : 0.162189, loss_ce: 0.003172, loss_dice: 0.321205
[10:12:42.497] TRAIN: iteration 15291 : loss : 0.141597, loss_ce: 0.005364, loss_dice: 0.277830
[10:12:47.256] TRAIN: iteration 15292 : loss : 0.252271, loss_ce: 0.004241, loss_dice: 0.500300
[10:12:47.468] TRAIN: iteration 15293 : loss : 0.122067, loss_ce: 0.006375, loss_dice: 0.237759
[10:12:47.676] TRAIN: iteration 15294 : loss : 0.098441, loss_ce: 0.002832, loss_dice: 0.194050
[10:12:47.883] TRAIN: iteration 15295 : loss : 0.047520, loss_ce: 0.004341, loss_dice: 0.090698
[10:12:48.097] TRAIN: iteration 15296 : loss : 0.251333, loss_ce: 0.002535, loss_dice: 0.500132
[10:12:48.307] TRAIN: iteration 15297 : loss : 0.110850, loss_ce: 0.002884, loss_dice: 0.218816
[10:12:48.515] TRAIN: iteration 15298 : loss : 0.112735, loss_ce: 0.004819, loss_dice: 0.220651
[10:12:49.595] TRAIN: iteration 15299 : loss : 0.118539, loss_ce: 0.003924, loss_dice: 0.233154
[10:12:51.359] TRAIN: iteration 15300 : loss : 0.115581, loss_ce: 0.005622, loss_dice: 0.225540
[10:12:51.603] TRAIN: iteration 15301 : loss : 0.109429, loss_ce: 0.013928, loss_dice: 0.204929
[10:12:51.811] TRAIN: iteration 15302 : loss : 0.134058, loss_ce: 0.002033, loss_dice: 0.266084
[10:12:52.020] TRAIN: iteration 15303 : loss : 0.147861, loss_ce: 0.001713, loss_dice: 0.294008
[10:12:52.228] TRAIN: iteration 15304 : loss : 0.052752, loss_ce: 0.002821, loss_dice: 0.102683
[10:12:52.440] TRAIN: iteration 15305 : loss : 0.133484, loss_ce: 0.004486, loss_dice: 0.262482
[10:12:52.649] TRAIN: iteration 15306 : loss : 0.250947, loss_ce: 0.001805, loss_dice: 0.500090
[10:12:53.999] TRAIN: iteration 15307 : loss : 0.242774, loss_ce: 0.003391, loss_dice: 0.482156
[10:12:56.538] TRAIN: iteration 15308 : loss : 0.250917, loss_ce: 0.001731, loss_dice: 0.500103
[10:12:56.749] TRAIN: iteration 15309 : loss : 0.076067, loss_ce: 0.003120, loss_dice: 0.149013
[10:12:56.958] TRAIN: iteration 15310 : loss : 0.175287, loss_ce: 0.003949, loss_dice: 0.346626
[10:12:57.166] TRAIN: iteration 15311 : loss : 0.071654, loss_ce: 0.002373, loss_dice: 0.140935
[10:12:57.373] TRAIN: iteration 15312 : loss : 0.198650, loss_ce: 0.019818, loss_dice: 0.377482
[10:12:57.581] TRAIN: iteration 15313 : loss : 0.105858, loss_ce: 0.004750, loss_dice: 0.206965
[10:12:57.789] TRAIN: iteration 15314 : loss : 0.248567, loss_ce: 0.016303, loss_dice: 0.480831
[10:12:57.996] TRAIN: iteration 15315 : loss : 0.247338, loss_ce: 0.004839, loss_dice: 0.489836
[10:13:01.374] TRAIN: iteration 15316 : loss : 0.136873, loss_ce: 0.005364, loss_dice: 0.268382
[10:13:01.582] TRAIN: iteration 15317 : loss : 0.071016, loss_ce: 0.006243, loss_dice: 0.135788
[10:13:01.789] TRAIN: iteration 15318 : loss : 0.137965, loss_ce: 0.012397, loss_dice: 0.263532
[10:13:01.997] TRAIN: iteration 15319 : loss : 0.251466, loss_ce: 0.006738, loss_dice: 0.496193
[10:13:02.205] TRAIN: iteration 15320 : loss : 0.124221, loss_ce: 0.005520, loss_dice: 0.242922
[10:13:02.206] NaN or Inf found in input tensor.
[10:13:02.420] TRAIN: iteration 15321 : loss : 0.052744, loss_ce: 0.005802, loss_dice: 0.099685
[10:13:02.628] TRAIN: iteration 15322 : loss : 0.250876, loss_ce: 0.003397, loss_dice: 0.498355
[10:13:02.836] TRAIN: iteration 15323 : loss : 0.251183, loss_ce: 0.002208, loss_dice: 0.500158
[10:13:05.647] TRAIN: iteration 15324 : loss : 0.188827, loss_ce: 0.005949, loss_dice: 0.371704
[10:13:05.857] TRAIN: iteration 15325 : loss : 0.089844, loss_ce: 0.002745, loss_dice: 0.176942
[10:13:06.066] TRAIN: iteration 15326 : loss : 0.243563, loss_ce: 0.003440, loss_dice: 0.483687
[10:13:06.275] TRAIN: iteration 15327 : loss : 0.251033, loss_ce: 0.001928, loss_dice: 0.500139
[10:13:06.485] TRAIN: iteration 15328 : loss : 0.049373, loss_ce: 0.008238, loss_dice: 0.090509
[10:13:06.695] TRAIN: iteration 15329 : loss : 0.128167, loss_ce: 0.003090, loss_dice: 0.253244
[10:13:06.903] TRAIN: iteration 15330 : loss : 0.250795, loss_ce: 0.001492, loss_dice: 0.500097
[10:13:07.574] TRAIN: iteration 15331 : loss : 0.198007, loss_ce: 0.003177, loss_dice: 0.392836
[10:13:10.353] TRAIN: iteration 15332 : loss : 0.250449, loss_ce: 0.000851, loss_dice: 0.500046
[10:13:10.561] TRAIN: iteration 15333 : loss : 0.199492, loss_ce: 0.005526, loss_dice: 0.393458
[10:13:10.769] TRAIN: iteration 15334 : loss : 0.126355, loss_ce: 0.013356, loss_dice: 0.239355
[10:13:10.976] TRAIN: iteration 15335 : loss : 0.191001, loss_ce: 0.002558, loss_dice: 0.379443
[10:13:11.184] TRAIN: iteration 15336 : loss : 0.108016, loss_ce: 0.003396, loss_dice: 0.212637
[10:13:11.393] TRAIN: iteration 15337 : loss : 0.218436, loss_ce: 0.003438, loss_dice: 0.433433
[10:13:11.603] TRAIN: iteration 15338 : loss : 0.185698, loss_ce: 0.002754, loss_dice: 0.368642
[10:13:15.067] TRAIN: iteration 15339 : loss : 0.069096, loss_ce: 0.001364, loss_dice: 0.136829
[10:13:15.275] TRAIN: iteration 15340 : loss : 0.143874, loss_ce: 0.009401, loss_dice: 0.278347
[10:13:15.514] TRAIN: iteration 15341 : loss : 0.250896, loss_ce: 0.001724, loss_dice: 0.500067
[10:13:15.723] TRAIN: iteration 15342 : loss : 0.128219, loss_ce: 0.002699, loss_dice: 0.253739
[10:13:15.992] TRAIN: iteration 15343 : loss : 0.129100, loss_ce: 0.002261, loss_dice: 0.255938
[10:13:16.199] TRAIN: iteration 15344 : loss : 0.059175, loss_ce: 0.003201, loss_dice: 0.115149
[10:13:16.408] TRAIN: iteration 15345 : loss : 0.156960, loss_ce: 0.022585, loss_dice: 0.291336
[10:13:16.615] TRAIN: iteration 15346 : loss : 0.129868, loss_ce: 0.005901, loss_dice: 0.253835
[10:13:20.906] TRAIN: iteration 15347 : loss : 0.198115, loss_ce: 0.004066, loss_dice: 0.392163
[10:13:21.114] TRAIN: iteration 15348 : loss : 0.064917, loss_ce: 0.002249, loss_dice: 0.127585
[10:13:21.321] TRAIN: iteration 15349 : loss : 0.113822, loss_ce: 0.009572, loss_dice: 0.218072
[10:13:21.528] TRAIN: iteration 15350 : loss : 0.056335, loss_ce: 0.002944, loss_dice: 0.109726
[10:13:21.736] TRAIN: iteration 15351 : loss : 0.147100, loss_ce: 0.005649, loss_dice: 0.288550
[10:13:21.943] TRAIN: iteration 15352 : loss : 0.066650, loss_ce: 0.005130, loss_dice: 0.128171
[10:13:22.152] TRAIN: iteration 15353 : loss : 0.251412, loss_ce: 0.002703, loss_dice: 0.500121
[10:13:22.361] TRAIN: iteration 15354 : loss : 0.027944, loss_ce: 0.001513, loss_dice: 0.054375
[10:13:28.178] TRAIN: iteration 15355 : loss : 0.085543, loss_ce: 0.004128, loss_dice: 0.166957
[10:13:28.385] TRAIN: iteration 15356 : loss : 0.252651, loss_ce: 0.004968, loss_dice: 0.500333
[10:13:28.592] TRAIN: iteration 15357 : loss : 0.035930, loss_ce: 0.002386, loss_dice: 0.069473
[10:13:30.462] TRAIN: iteration 15358 : loss : 0.248992, loss_ce: 0.003546, loss_dice: 0.494439
[10:13:30.668] TRAIN: iteration 15359 : loss : 0.248835, loss_ce: 0.002904, loss_dice: 0.494767
[10:13:30.878] TRAIN: iteration 15360 : loss : 0.085836, loss_ce: 0.012626, loss_dice: 0.159047
[10:13:31.122] TRAIN: iteration 15361 : loss : 0.036934, loss_ce: 0.006311, loss_dice: 0.067558
[10:13:31.330] TRAIN: iteration 15362 : loss : 0.118422, loss_ce: 0.004511, loss_dice: 0.232332
[10:13:32.630] TRAIN: iteration 15363 : loss : 0.203040, loss_ce: 0.006325, loss_dice: 0.399756
[10:13:32.837] TRAIN: iteration 15364 : loss : 0.140047, loss_ce: 0.014085, loss_dice: 0.266008
[10:13:33.045] TRAIN: iteration 15365 : loss : 0.087288, loss_ce: 0.009332, loss_dice: 0.165244
[10:13:36.515] TRAIN: iteration 15366 : loss : 0.249451, loss_ce: 0.007107, loss_dice: 0.491796
[10:13:36.723] TRAIN: iteration 15367 : loss : 0.186007, loss_ce: 0.006602, loss_dice: 0.365411
[10:13:36.931] TRAIN: iteration 15368 : loss : 0.121564, loss_ce: 0.004442, loss_dice: 0.238686
[10:13:37.141] TRAIN: iteration 15369 : loss : 0.077811, loss_ce: 0.007773, loss_dice: 0.147849
[10:13:37.354] TRAIN: iteration 15370 : loss : 0.140550, loss_ce: 0.004523, loss_dice: 0.276577
[10:13:39.137] TRAIN: iteration 15371 : loss : 0.108092, loss_ce: 0.007544, loss_dice: 0.208640
[10:13:39.345] TRAIN: iteration 15372 : loss : 0.148761, loss_ce: 0.003855, loss_dice: 0.293667
[10:13:39.553] TRAIN: iteration 15373 : loss : 0.251831, loss_ce: 0.003484, loss_dice: 0.500179
[10:13:42.768] TRAIN: iteration 15374 : loss : 0.174190, loss_ce: 0.010436, loss_dice: 0.337944
[10:13:42.977] TRAIN: iteration 15375 : loss : 0.216714, loss_ce: 0.005221, loss_dice: 0.428208
[10:13:43.186] TRAIN: iteration 15376 : loss : 0.049958, loss_ce: 0.002634, loss_dice: 0.097281
[10:13:43.394] TRAIN: iteration 15377 : loss : 0.149366, loss_ce: 0.004503, loss_dice: 0.294229
[10:13:43.604] TRAIN: iteration 15378 : loss : 0.066518, loss_ce: 0.003534, loss_dice: 0.129503
[10:13:46.815] TRAIN: iteration 15379 : loss : 0.044173, loss_ce: 0.002478, loss_dice: 0.085867
[10:13:47.031] TRAIN: iteration 15380 : loss : 0.243382, loss_ce: 0.003600, loss_dice: 0.483165
[10:13:47.269] TRAIN: iteration 15381 : loss : 0.087124, loss_ce: 0.006954, loss_dice: 0.167294
[10:13:48.555] TRAIN: iteration 15382 : loss : 0.250794, loss_ce: 0.003202, loss_dice: 0.498387
[10:13:48.763] TRAIN: iteration 15383 : loss : 0.212915, loss_ce: 0.003617, loss_dice: 0.422213
[10:13:48.971] TRAIN: iteration 15384 : loss : 0.132492, loss_ce: 0.004719, loss_dice: 0.260265
[10:13:49.179] TRAIN: iteration 15385 : loss : 0.168670, loss_ce: 0.003512, loss_dice: 0.333827
[10:13:49.387] TRAIN: iteration 15386 : loss : 0.090026, loss_ce: 0.002740, loss_dice: 0.177312
[10:13:53.363] TRAIN: iteration 15387 : loss : 0.104444, loss_ce: 0.005504, loss_dice: 0.203384
[10:13:53.577] TRAIN: iteration 15388 : loss : 0.118788, loss_ce: 0.002341, loss_dice: 0.235235
[10:13:53.785] TRAIN: iteration 15389 : loss : 0.165343, loss_ce: 0.008912, loss_dice: 0.321774
[10:13:53.994] TRAIN: iteration 15390 : loss : 0.113275, loss_ce: 0.011441, loss_dice: 0.215109
[10:13:54.202] TRAIN: iteration 15391 : loss : 0.077133, loss_ce: 0.003840, loss_dice: 0.150427
[10:13:54.410] TRAIN: iteration 15392 : loss : 0.189013, loss_ce: 0.002646, loss_dice: 0.375379
[10:13:54.618] TRAIN: iteration 15393 : loss : 0.194260, loss_ce: 0.006248, loss_dice: 0.382271
[10:13:54.827] TRAIN: iteration 15394 : loss : 0.251235, loss_ce: 0.002378, loss_dice: 0.500091
[10:13:59.635] TRAIN: iteration 15395 : loss : 0.078400, loss_ce: 0.006806, loss_dice: 0.149993
[10:13:59.843] TRAIN: iteration 15396 : loss : 0.126237, loss_ce: 0.006764, loss_dice: 0.245710
[10:14:00.051] TRAIN: iteration 15397 : loss : 0.040881, loss_ce: 0.004152, loss_dice: 0.077611
[10:14:02.124] TRAIN: iteration 15398 : loss : 0.065094, loss_ce: 0.003226, loss_dice: 0.126962
[10:14:02.335] TRAIN: iteration 15399 : loss : 0.180805, loss_ce: 0.005508, loss_dice: 0.356102
[10:14:02.550] TRAIN: iteration 15400 : loss : 0.245704, loss_ce: 0.003357, loss_dice: 0.488052
[10:14:02.788] TRAIN: iteration 15401 : loss : 0.099125, loss_ce: 0.004895, loss_dice: 0.193355
[10:14:02.996] TRAIN: iteration 15402 : loss : 0.082864, loss_ce: 0.008700, loss_dice: 0.157028
[10:14:08.433] TRAIN: iteration 15403 : loss : 0.229593, loss_ce: 0.006219, loss_dice: 0.452966
[10:14:08.646] TRAIN: iteration 15404 : loss : 0.085453, loss_ce: 0.003244, loss_dice: 0.167662
[10:14:08.853] TRAIN: iteration 15405 : loss : 0.250649, loss_ce: 0.003019, loss_dice: 0.498278
[10:14:09.062] TRAIN: iteration 15406 : loss : 0.051713, loss_ce: 0.002682, loss_dice: 0.100744
[10:14:09.276] TRAIN: iteration 15407 : loss : 0.165650, loss_ce: 0.005744, loss_dice: 0.325556
[10:14:09.483] TRAIN: iteration 15408 : loss : 0.165775, loss_ce: 0.011319, loss_dice: 0.320231
[10:14:09.695] TRAIN: iteration 15409 : loss : 0.151128, loss_ce: 0.005593, loss_dice: 0.296663
[10:14:09.905] TRAIN: iteration 15410 : loss : 0.032747, loss_ce: 0.002147, loss_dice: 0.063347
[10:14:15.224] TRAIN: iteration 15411 : loss : 0.134668, loss_ce: 0.002530, loss_dice: 0.266806
[10:14:15.432] TRAIN: iteration 15412 : loss : 0.054031, loss_ce: 0.003638, loss_dice: 0.104424
[10:14:15.641] TRAIN: iteration 15413 : loss : 0.235219, loss_ce: 0.002460, loss_dice: 0.467978
[10:14:16.388] TRAIN: iteration 15414 : loss : 0.251573, loss_ce: 0.002976, loss_dice: 0.500171
[10:14:16.596] TRAIN: iteration 15415 : loss : 0.253484, loss_ce: 0.006930, loss_dice: 0.500038
[10:14:16.805] TRAIN: iteration 15416 : loss : 0.059076, loss_ce: 0.001759, loss_dice: 0.116393
[10:14:17.012] TRAIN: iteration 15417 : loss : 0.125772, loss_ce: 0.002254, loss_dice: 0.249290
[10:14:17.223] TRAIN: iteration 15418 : loss : 0.105737, loss_ce: 0.006142, loss_dice: 0.205331
[10:14:22.130] TRAIN: iteration 15419 : loss : 0.207872, loss_ce: 0.002132, loss_dice: 0.413612
[10:14:22.339] TRAIN: iteration 15420 : loss : 0.094112, loss_ce: 0.008911, loss_dice: 0.179313
[10:14:22.575] TRAIN: iteration 15421 : loss : 0.079644, loss_ce: 0.003893, loss_dice: 0.155396
[10:14:24.304] TRAIN: iteration 15422 : loss : 0.081913, loss_ce: 0.005502, loss_dice: 0.158325
[10:14:24.512] TRAIN: iteration 15423 : loss : 0.178712, loss_ce: 0.003173, loss_dice: 0.354251
[10:14:24.719] TRAIN: iteration 15424 : loss : 0.143499, loss_ce: 0.004862, loss_dice: 0.282136
[10:14:24.926] TRAIN: iteration 15425 : loss : 0.091458, loss_ce: 0.002034, loss_dice: 0.180883
[10:14:25.135] TRAIN: iteration 15426 : loss : 0.251414, loss_ce: 0.002672, loss_dice: 0.500156
[10:14:28.239] TRAIN: iteration 15427 : loss : 0.068375, loss_ce: 0.002670, loss_dice: 0.134080
[10:14:28.449] TRAIN: iteration 15428 : loss : 0.101672, loss_ce: 0.003729, loss_dice: 0.199616
[10:14:28.657] TRAIN: iteration 15429 : loss : 0.078035, loss_ce: 0.004358, loss_dice: 0.151712
[10:14:32.355] TRAIN: iteration 15430 : loss : 0.144482, loss_ce: 0.006040, loss_dice: 0.282923
[10:14:32.565] TRAIN: iteration 15431 : loss : 0.223211, loss_ce: 0.002610, loss_dice: 0.443812
[10:14:32.773] TRAIN: iteration 15432 : loss : 0.163421, loss_ce: 0.004186, loss_dice: 0.322655
[10:14:32.983] TRAIN: iteration 15433 : loss : 0.081513, loss_ce: 0.002273, loss_dice: 0.160753
[10:14:33.195] TRAIN: iteration 15434 : loss : 0.074525, loss_ce: 0.004678, loss_dice: 0.144371
[10:14:35.470] TRAIN: iteration 15435 : loss : 0.127051, loss_ce: 0.001964, loss_dice: 0.252138
[10:14:35.676] TRAIN: iteration 15436 : loss : 0.078863, loss_ce: 0.006983, loss_dice: 0.150743
[10:14:35.884] TRAIN: iteration 15437 : loss : 0.244913, loss_ce: 0.004152, loss_dice: 0.485675
[10:14:37.049] TRAIN: iteration 15438 : loss : 0.094818, loss_ce: 0.006825, loss_dice: 0.182811
[10:14:37.257] TRAIN: iteration 15439 : loss : 0.088225, loss_ce: 0.005947, loss_dice: 0.170502
[10:14:37.466] TRAIN: iteration 15440 : loss : 0.236343, loss_ce: 0.001740, loss_dice: 0.470945
[10:14:37.704] TRAIN: iteration 15441 : loss : 0.085557, loss_ce: 0.001831, loss_dice: 0.169283
[10:14:37.912] TRAIN: iteration 15442 : loss : 0.083236, loss_ce: 0.004102, loss_dice: 0.162370
[10:14:41.261] TRAIN: iteration 15443 : loss : 0.122029, loss_ce: 0.003272, loss_dice: 0.240785
[10:14:41.469] TRAIN: iteration 15444 : loss : 0.116713, loss_ce: 0.006148, loss_dice: 0.227279
[10:14:41.677] TRAIN: iteration 15445 : loss : 0.074024, loss_ce: 0.004025, loss_dice: 0.144023
[10:14:44.496] TRAIN: iteration 15446 : loss : 0.108654, loss_ce: 0.003247, loss_dice: 0.214060
[10:14:44.705] TRAIN: iteration 15447 : loss : 0.250815, loss_ce: 0.001576, loss_dice: 0.500055
[10:14:44.918] TRAIN: iteration 15448 : loss : 0.162851, loss_ce: 0.006036, loss_dice: 0.319666
[10:14:45.126] TRAIN: iteration 15449 : loss : 0.251939, loss_ce: 0.003618, loss_dice: 0.500260
[10:14:45.334] TRAIN: iteration 15450 : loss : 0.032866, loss_ce: 0.004416, loss_dice: 0.061317
[10:14:49.300] TRAIN: iteration 15451 : loss : 0.070814, loss_ce: 0.003934, loss_dice: 0.137694
[10:14:49.508] TRAIN: iteration 15452 : loss : 0.251597, loss_ce: 0.003675, loss_dice: 0.499519
[10:14:49.718] TRAIN: iteration 15453 : loss : 0.080830, loss_ce: 0.002095, loss_dice: 0.159565
[10:14:51.310] TRAIN: iteration 15454 : loss : 0.251489, loss_ce: 0.002796, loss_dice: 0.500181
[10:14:51.518] TRAIN: iteration 15455 : loss : 0.038574, loss_ce: 0.001776, loss_dice: 0.075371
[10:14:52.391] TRAIN: iteration 15456 : loss : 0.054987, loss_ce: 0.003918, loss_dice: 0.106055
[10:14:52.599] TRAIN: iteration 15457 : loss : 0.043962, loss_ce: 0.002473, loss_dice: 0.085451
[10:14:52.809] TRAIN: iteration 15458 : loss : 0.071665, loss_ce: 0.004958, loss_dice: 0.138371
[10:14:57.644] TRAIN: iteration 15459 : loss : 0.140338, loss_ce: 0.005854, loss_dice: 0.274822
[10:14:57.856] TRAIN: iteration 15460 : loss : 0.048690, loss_ce: 0.002835, loss_dice: 0.094546
[10:14:58.095] TRAIN: iteration 15461 : loss : 0.240948, loss_ce: 0.004230, loss_dice: 0.477665
[10:14:59.855] TRAIN: iteration 15462 : loss : 0.097781, loss_ce: 0.003021, loss_dice: 0.192541
[10:15:00.063] TRAIN: iteration 15463 : loss : 0.210809, loss_ce: 0.008799, loss_dice: 0.412818
[10:15:02.562] TRAIN: iteration 15464 : loss : 0.062315, loss_ce: 0.003889, loss_dice: 0.120740
[10:15:02.770] TRAIN: iteration 15465 : loss : 0.251520, loss_ce: 0.002847, loss_dice: 0.500193
[10:15:02.979] TRAIN: iteration 15466 : loss : 0.121675, loss_ce: 0.014729, loss_dice: 0.228622
[10:15:04.101] TRAIN: iteration 15467 : loss : 0.253093, loss_ce: 0.005736, loss_dice: 0.500449
[10:15:04.310] TRAIN: iteration 15468 : loss : 0.040512, loss_ce: 0.002013, loss_dice: 0.079011
[10:15:04.519] TRAIN: iteration 15469 : loss : 0.070392, loss_ce: 0.001502, loss_dice: 0.139281
[10:15:08.179] TRAIN: iteration 15470 : loss : 0.042832, loss_ce: 0.004133, loss_dice: 0.081532
[10:15:08.387] TRAIN: iteration 15471 : loss : 0.034540, loss_ce: 0.000852, loss_dice: 0.068229
[10:15:10.108] TRAIN: iteration 15472 : loss : 0.125430, loss_ce: 0.003163, loss_dice: 0.247696
[10:15:10.316] TRAIN: iteration 15473 : loss : 0.250772, loss_ce: 0.001458, loss_dice: 0.500085
[10:15:10.524] TRAIN: iteration 15474 : loss : 0.135158, loss_ce: 0.008804, loss_dice: 0.261512
[10:15:13.253] TRAIN: iteration 15475 : loss : 0.162113, loss_ce: 0.003789, loss_dice: 0.320437
[10:15:13.461] TRAIN: iteration 15476 : loss : 0.211764, loss_ce: 0.004273, loss_dice: 0.419256
[10:15:13.669] TRAIN: iteration 15477 : loss : 0.082675, loss_ce: 0.004843, loss_dice: 0.160508
[10:15:15.727] TRAIN: iteration 15478 : loss : 0.063966, loss_ce: 0.002630, loss_dice: 0.125302
[10:15:15.935] TRAIN: iteration 15479 : loss : 0.144869, loss_ce: 0.005463, loss_dice: 0.284274
[10:15:19.144] TRAIN: iteration 15480 : loss : 0.119932, loss_ce: 0.003844, loss_dice: 0.236021
[10:15:19.377] TRAIN: iteration 15481 : loss : 0.077624, loss_ce: 0.002702, loss_dice: 0.152546
[10:15:19.591] TRAIN: iteration 15482 : loss : 0.140685, loss_ce: 0.003559, loss_dice: 0.277812
[10:15:22.846] TRAIN: iteration 15483 : loss : 0.186214, loss_ce: 0.002384, loss_dice: 0.370045
[10:15:23.056] TRAIN: iteration 15484 : loss : 0.251428, loss_ce: 0.002677, loss_dice: 0.500178
[10:15:23.319] TRAIN: iteration 15485 : loss : 0.200276, loss_ce: 0.002620, loss_dice: 0.397931
[10:15:25.478] TRAIN: iteration 15486 : loss : 0.195385, loss_ce: 0.006358, loss_dice: 0.384411
[10:15:25.686] TRAIN: iteration 15487 : loss : 0.103171, loss_ce: 0.003094, loss_dice: 0.203249
[10:15:27.966] TRAIN: iteration 15488 : loss : 0.091060, loss_ce: 0.001995, loss_dice: 0.180125
[10:15:28.175] TRAIN: iteration 15489 : loss : 0.090990, loss_ce: 0.006111, loss_dice: 0.175869
[10:15:28.383] TRAIN: iteration 15490 : loss : 0.237263, loss_ce: 0.001781, loss_dice: 0.472745
[10:15:32.779] TRAIN: iteration 15491 : loss : 0.095484, loss_ce: 0.005609, loss_dice: 0.185360
[10:15:32.988] TRAIN: iteration 15492 : loss : 0.066849, loss_ce: 0.001803, loss_dice: 0.131894
[10:15:33.196] TRAIN: iteration 15493 : loss : 0.092458, loss_ce: 0.004719, loss_dice: 0.180197
[10:15:33.404] TRAIN: iteration 15494 : loss : 0.187715, loss_ce: 0.013242, loss_dice: 0.362189
[10:15:33.613] TRAIN: iteration 15495 : loss : 0.195864, loss_ce: 0.005903, loss_dice: 0.385826
[10:15:36.546] TRAIN: iteration 15496 : loss : 0.240252, loss_ce: 0.003040, loss_dice: 0.477463
[10:15:36.754] TRAIN: iteration 15497 : loss : 0.107771, loss_ce: 0.005734, loss_dice: 0.209809
[10:15:36.961] TRAIN: iteration 15498 : loss : 0.130338, loss_ce: 0.007440, loss_dice: 0.253237
[10:15:41.585] TRAIN: iteration 15499 : loss : 0.054733, loss_ce: 0.001369, loss_dice: 0.108098
[10:15:41.792] TRAIN: iteration 15500 : loss : 0.134977, loss_ce: 0.002952, loss_dice: 0.267003
[10:15:42.029] TRAIN: iteration 15501 : loss : 0.071195, loss_ce: 0.003503, loss_dice: 0.138887
[10:15:43.883] TRAIN: iteration 15502 : loss : 0.251093, loss_ce: 0.002430, loss_dice: 0.499755
[10:15:44.093] TRAIN: iteration 15503 : loss : 0.204271, loss_ce: 0.001279, loss_dice: 0.407263
[10:15:44.303] TRAIN: iteration 15504 : loss : 0.188054, loss_ce: 0.002113, loss_dice: 0.373995
[10:15:44.511] TRAIN: iteration 15505 : loss : 0.073718, loss_ce: 0.003166, loss_dice: 0.144270
[10:15:44.720] TRAIN: iteration 15506 : loss : 0.098109, loss_ce: 0.013980, loss_dice: 0.182239
[10:15:49.923] TRAIN: iteration 15507 : loss : 0.085520, loss_ce: 0.002436, loss_dice: 0.168603
[10:15:50.132] TRAIN: iteration 15508 : loss : 0.250094, loss_ce: 0.004651, loss_dice: 0.495537
[10:15:50.341] TRAIN: iteration 15509 : loss : 0.252596, loss_ce: 0.004877, loss_dice: 0.500314
[10:15:52.340] TRAIN: iteration 15510 : loss : 0.097341, loss_ce: 0.003551, loss_dice: 0.191131
[10:15:52.550] TRAIN: iteration 15511 : loss : 0.251933, loss_ce: 0.003626, loss_dice: 0.500239
[10:15:52.761] TRAIN: iteration 15512 : loss : 0.093554, loss_ce: 0.002269, loss_dice: 0.184840
[10:15:52.969] TRAIN: iteration 15513 : loss : 0.091986, loss_ce: 0.002837, loss_dice: 0.181135
[10:15:53.179] TRAIN: iteration 15514 : loss : 0.130922, loss_ce: 0.008489, loss_dice: 0.253355
[10:15:56.708] TRAIN: iteration 15515 : loss : 0.091882, loss_ce: 0.004588, loss_dice: 0.179176
[10:15:56.917] TRAIN: iteration 15516 : loss : 0.150407, loss_ce: 0.012894, loss_dice: 0.287921
[10:15:57.125] TRAIN: iteration 15517 : loss : 0.247063, loss_ce: 0.002935, loss_dice: 0.491190
[10:16:02.614] TRAIN: iteration 15518 : loss : 0.251204, loss_ce: 0.002302, loss_dice: 0.500105
[10:16:02.824] TRAIN: iteration 15519 : loss : 0.072459, loss_ce: 0.002358, loss_dice: 0.142560
[10:16:03.033] TRAIN: iteration 15520 : loss : 0.089036, loss_ce: 0.010517, loss_dice: 0.167554
[10:16:03.271] TRAIN: iteration 15521 : loss : 0.054224, loss_ce: 0.003864, loss_dice: 0.104585
[10:16:03.481] TRAIN: iteration 15522 : loss : 0.140339, loss_ce: 0.006038, loss_dice: 0.274640
[10:16:03.688] TRAIN: iteration 15523 : loss : 0.113882, loss_ce: 0.004812, loss_dice: 0.222952
[10:16:03.896] TRAIN: iteration 15524 : loss : 0.118330, loss_ce: 0.003271, loss_dice: 0.233389
[10:16:04.104] TRAIN: iteration 15525 : loss : 0.045961, loss_ce: 0.003742, loss_dice: 0.088179
[10:16:10.988] TRAIN: iteration 15526 : loss : 0.043735, loss_ce: 0.004162, loss_dice: 0.083308
[10:16:11.196] TRAIN: iteration 15527 : loss : 0.122642, loss_ce: 0.007885, loss_dice: 0.237399
[10:16:11.407] TRAIN: iteration 15528 : loss : 0.161187, loss_ce: 0.003630, loss_dice: 0.318744
[10:16:11.616] TRAIN: iteration 15529 : loss : 0.106625, loss_ce: 0.006813, loss_dice: 0.206437
[10:16:11.824] TRAIN: iteration 15530 : loss : 0.110436, loss_ce: 0.006416, loss_dice: 0.214455
[10:16:14.007] TRAIN: iteration 15531 : loss : 0.147786, loss_ce: 0.003530, loss_dice: 0.292042
[10:16:14.214] TRAIN: iteration 15532 : loss : 0.246765, loss_ce: 0.001999, loss_dice: 0.491531
[10:16:14.422] TRAIN: iteration 15533 : loss : 0.254879, loss_ce: 0.015148, loss_dice: 0.494609
[10:16:21.461] TRAIN: iteration 15534 : loss : 0.101089, loss_ce: 0.003574, loss_dice: 0.198605
[10:16:21.669] TRAIN: iteration 15535 : loss : 0.090849, loss_ce: 0.006614, loss_dice: 0.175084
[10:16:21.878] TRAIN: iteration 15536 : loss : 0.057141, loss_ce: 0.003647, loss_dice: 0.110635
[10:16:22.086] TRAIN: iteration 15537 : loss : 0.146027, loss_ce: 0.004079, loss_dice: 0.287975
[10:16:22.296] TRAIN: iteration 15538 : loss : 0.248695, loss_ce: 0.003166, loss_dice: 0.494223
[10:16:23.102] TRAIN: iteration 15539 : loss : 0.081303, loss_ce: 0.002489, loss_dice: 0.160118
[10:16:23.311] TRAIN: iteration 15540 : loss : 0.219846, loss_ce: 0.004395, loss_dice: 0.435298
[10:16:23.547] TRAIN: iteration 15541 : loss : 0.193555, loss_ce: 0.005924, loss_dice: 0.381186
[10:16:31.037] TRAIN: iteration 15542 : loss : 0.129093, loss_ce: 0.007885, loss_dice: 0.250300
[10:16:31.248] TRAIN: iteration 15543 : loss : 0.093448, loss_ce: 0.004177, loss_dice: 0.182720
[10:16:31.457] TRAIN: iteration 15544 : loss : 0.101038, loss_ce: 0.002779, loss_dice: 0.199297
[10:16:31.667] TRAIN: iteration 15545 : loss : 0.226089, loss_ce: 0.001339, loss_dice: 0.450839
[10:16:31.876] TRAIN: iteration 15546 : loss : 0.248638, loss_ce: 0.001027, loss_dice: 0.496249
[10:16:32.086] TRAIN: iteration 15547 : loss : 0.104389, loss_ce: 0.002344, loss_dice: 0.206435
[10:16:32.297] TRAIN: iteration 15548 : loss : 0.048763, loss_ce: 0.002002, loss_dice: 0.095523
[10:16:32.506] TRAIN: iteration 15549 : loss : 0.132227, loss_ce: 0.011347, loss_dice: 0.253108
[10:16:41.103] TRAIN: iteration 15550 : loss : 0.117405, loss_ce: 0.010336, loss_dice: 0.224475
[10:16:41.312] TRAIN: iteration 15551 : loss : 0.129376, loss_ce: 0.007470, loss_dice: 0.251282
[10:16:41.519] TRAIN: iteration 15552 : loss : 0.139343, loss_ce: 0.006757, loss_dice: 0.271928
[10:16:41.728] TRAIN: iteration 15553 : loss : 0.128836, loss_ce: 0.002983, loss_dice: 0.254689
[10:16:41.935] TRAIN: iteration 15554 : loss : 0.226965, loss_ce: 0.004740, loss_dice: 0.449191
[10:16:42.392] TRAIN: iteration 15555 : loss : 0.174841, loss_ce: 0.011140, loss_dice: 0.338541
[10:16:42.599] TRAIN: iteration 15556 : loss : 0.229689, loss_ce: 0.003907, loss_dice: 0.455470
[10:16:42.806] TRAIN: iteration 15557 : loss : 0.251636, loss_ce: 0.003072, loss_dice: 0.500199
[10:16:49.073] TRAIN: iteration 15558 : loss : 0.107949, loss_ce: 0.005478, loss_dice: 0.210420
[10:16:49.467] TRAIN: iteration 15559 : loss : 0.101095, loss_ce: 0.003517, loss_dice: 0.198674
[10:16:49.675] TRAIN: iteration 15560 : loss : 0.057244, loss_ce: 0.002479, loss_dice: 0.112009
[10:16:49.912] TRAIN: iteration 15561 : loss : 0.098839, loss_ce: 0.005940, loss_dice: 0.191738
[10:16:50.120] TRAIN: iteration 15562 : loss : 0.186855, loss_ce: 0.003579, loss_dice: 0.370132
[10:16:52.008] TRAIN: iteration 15563 : loss : 0.109749, loss_ce: 0.005369, loss_dice: 0.214129
[10:16:52.215] TRAIN: iteration 15564 : loss : 0.199176, loss_ce: 0.005151, loss_dice: 0.393201
[10:16:52.424] TRAIN: iteration 15565 : loss : 0.084094, loss_ce: 0.003945, loss_dice: 0.164243
[10:16:58.305] TRAIN: iteration 15566 : loss : 0.158831, loss_ce: 0.005402, loss_dice: 0.312260
[10:16:58.873] TRAIN: iteration 15567 : loss : 0.142327, loss_ce: 0.004835, loss_dice: 0.279820
[10:16:59.081] TRAIN: iteration 15568 : loss : 0.049085, loss_ce: 0.002149, loss_dice: 0.096022
[10:16:59.289] TRAIN: iteration 15569 : loss : 0.167128, loss_ce: 0.004647, loss_dice: 0.329609
[10:16:59.498] TRAIN: iteration 15570 : loss : 0.084167, loss_ce: 0.005359, loss_dice: 0.162975
[10:17:00.733] TRAIN: iteration 15571 : loss : 0.251795, loss_ce: 0.003370, loss_dice: 0.500220
[10:17:00.941] TRAIN: iteration 15572 : loss : 0.029843, loss_ce: 0.001938, loss_dice: 0.057749
[10:17:01.150] TRAIN: iteration 15573 : loss : 0.228697, loss_ce: 0.003852, loss_dice: 0.453543
[10:17:08.453] TRAIN: iteration 15574 : loss : 0.061212, loss_ce: 0.001950, loss_dice: 0.120474
[10:17:09.608] TRAIN: iteration 15575 : loss : 0.150659, loss_ce: 0.003544, loss_dice: 0.297774
[10:17:09.815] TRAIN: iteration 15576 : loss : 0.152933, loss_ce: 0.007670, loss_dice: 0.298197
[10:17:10.024] TRAIN: iteration 15577 : loss : 0.201317, loss_ce: 0.005385, loss_dice: 0.397248
[10:17:10.234] TRAIN: iteration 15578 : loss : 0.144902, loss_ce: 0.022708, loss_dice: 0.267096
[10:17:10.879] TRAIN: iteration 15579 : loss : 0.133861, loss_ce: 0.002120, loss_dice: 0.265601
[10:17:11.088] TRAIN: iteration 15580 : loss : 0.178491, loss_ce: 0.002680, loss_dice: 0.354301
[10:17:11.328] TRAIN: iteration 15581 : loss : 0.112930, loss_ce: 0.003337, loss_dice: 0.222523
[10:17:18.980] TRAIN: iteration 15582 : loss : 0.117639, loss_ce: 0.004421, loss_dice: 0.230857
[10:17:19.517] TRAIN: iteration 15583 : loss : 0.211836, loss_ce: 0.002362, loss_dice: 0.421310
[10:17:19.724] TRAIN: iteration 15584 : loss : 0.054085, loss_ce: 0.003759, loss_dice: 0.104411
[10:17:19.931] TRAIN: iteration 15585 : loss : 0.153165, loss_ce: 0.011913, loss_dice: 0.294416
[10:17:20.144] TRAIN: iteration 15586 : loss : 0.224661, loss_ce: 0.025066, loss_dice: 0.424257
[10:17:20.353] TRAIN: iteration 15587 : loss : 0.045950, loss_ce: 0.002072, loss_dice: 0.089827
[10:17:20.561] TRAIN: iteration 15588 : loss : 0.249835, loss_ce: 0.003029, loss_dice: 0.496642
[10:17:20.769] TRAIN: iteration 15589 : loss : 0.116104, loss_ce: 0.010888, loss_dice: 0.221319
[10:17:27.856] TRAIN: iteration 15590 : loss : 0.063732, loss_ce: 0.006210, loss_dice: 0.121255
[10:17:30.386] TRAIN: iteration 15591 : loss : 0.176484, loss_ce: 0.005049, loss_dice: 0.347920
[10:17:30.595] TRAIN: iteration 15592 : loss : 0.125470, loss_ce: 0.010842, loss_dice: 0.240098
[10:17:30.805] TRAIN: iteration 15593 : loss : 0.048145, loss_ce: 0.001562, loss_dice: 0.094729
[10:17:31.015] TRAIN: iteration 15594 : loss : 0.159060, loss_ce: 0.003479, loss_dice: 0.314641
[10:17:31.227] TRAIN: iteration 15595 : loss : 0.202393, loss_ce: 0.004041, loss_dice: 0.400746
[10:17:31.435] TRAIN: iteration 15596 : loss : 0.090141, loss_ce: 0.006787, loss_dice: 0.173494
[10:17:31.643] TRAIN: iteration 15597 : loss : 0.063477, loss_ce: 0.003124, loss_dice: 0.123830
[10:17:36.940] TRAIN: iteration 15598 : loss : 0.251489, loss_ce: 0.002857, loss_dice: 0.500121
[10:17:40.088] TRAIN: iteration 15599 : loss : 0.252226, loss_ce: 0.004175, loss_dice: 0.500276
[10:17:40.296] TRAIN: iteration 15600 : loss : 0.105935, loss_ce: 0.003772, loss_dice: 0.208099
[10:17:40.533] TRAIN: iteration 15601 : loss : 0.138550, loss_ce: 0.004173, loss_dice: 0.272926
[10:17:40.739] TRAIN: iteration 15602 : loss : 0.158737, loss_ce: 0.003617, loss_dice: 0.313857
[10:17:40.945] TRAIN: iteration 15603 : loss : 0.145288, loss_ce: 0.020982, loss_dice: 0.269594
[10:17:41.153] TRAIN: iteration 15604 : loss : 0.142877, loss_ce: 0.003131, loss_dice: 0.282623
[10:17:41.364] TRAIN: iteration 15605 : loss : 0.251990, loss_ce: 0.003731, loss_dice: 0.500249
[10:17:45.945] TRAIN: iteration 15606 : loss : 0.192751, loss_ce: 0.004264, loss_dice: 0.381237
[10:17:49.589] TRAIN: iteration 15607 : loss : 0.251665, loss_ce: 0.003136, loss_dice: 0.500194
[10:17:49.797] TRAIN: iteration 15608 : loss : 0.251253, loss_ce: 0.002410, loss_dice: 0.500096
[10:17:50.006] TRAIN: iteration 15609 : loss : 0.183664, loss_ce: 0.023755, loss_dice: 0.343572
[10:17:50.214] TRAIN: iteration 15610 : loss : 0.039074, loss_ce: 0.004695, loss_dice: 0.073452
[10:17:50.421] TRAIN: iteration 15611 : loss : 0.168029, loss_ce: 0.007882, loss_dice: 0.328176
[10:17:50.629] TRAIN: iteration 15612 : loss : 0.165714, loss_ce: 0.003831, loss_dice: 0.327598
[10:17:50.836] TRAIN: iteration 15613 : loss : 0.210952, loss_ce: 0.004813, loss_dice: 0.417091
[10:17:56.256] TRAIN: iteration 15614 : loss : 0.118684, loss_ce: 0.004642, loss_dice: 0.232726
[10:17:56.353] TRAIN: iteration 15615 : loss : 0.251112, loss_ce: 0.002160, loss_dice: 0.500064
[10:23:29.372] VALIDATION: iteration 8 : loss : 0.133819, loss_ce: 0.005274, loss_dice: 0.262365
[10:23:30.225] TRAIN: iteration 15616 : loss : 0.087764, loss_ce: 0.002452, loss_dice: 0.173077
[10:23:31.135] TRAIN: iteration 15617 : loss : 0.197506, loss_ce: 0.003815, loss_dice: 0.391196
[10:23:31.539] TRAIN: iteration 15618 : loss : 0.251156, loss_ce: 0.002204, loss_dice: 0.500107
[10:23:31.751] TRAIN: iteration 15619 : loss : 0.126440, loss_ce: 0.013945, loss_dice: 0.238935
[10:23:31.962] TRAIN: iteration 15620 : loss : 0.090115, loss_ce: 0.006993, loss_dice: 0.173236
[10:23:31.963] NaN or Inf found in input tensor.
[10:23:32.187] TRAIN: iteration 15621 : loss : 0.063726, loss_ce: 0.004572, loss_dice: 0.122880
[10:23:32.401] TRAIN: iteration 15622 : loss : 0.083893, loss_ce: 0.003486, loss_dice: 0.164301
[10:23:32.611] TRAIN: iteration 15623 : loss : 0.082628, loss_ce: 0.003276, loss_dice: 0.161979
[10:23:32.821] TRAIN: iteration 15624 : loss : 0.098951, loss_ce: 0.003424, loss_dice: 0.194478
[10:23:33.035] TRAIN: iteration 15625 : loss : 0.089358, loss_ce: 0.005534, loss_dice: 0.173182
[10:23:33.248] TRAIN: iteration 15626 : loss : 0.170018, loss_ce: 0.006093, loss_dice: 0.333943
[10:23:33.466] TRAIN: iteration 15627 : loss : 0.030061, loss_ce: 0.002419, loss_dice: 0.057703
[10:23:33.675] TRAIN: iteration 15628 : loss : 0.249391, loss_ce: 0.003779, loss_dice: 0.495002
[10:23:33.892] TRAIN: iteration 15629 : loss : 0.088343, loss_ce: 0.004783, loss_dice: 0.171903
[10:23:34.104] TRAIN: iteration 15630 : loss : 0.121759, loss_ce: 0.007168, loss_dice: 0.236349
[10:23:34.316] TRAIN: iteration 15631 : loss : 0.222882, loss_ce: 0.002956, loss_dice: 0.442809
[10:23:34.527] TRAIN: iteration 15632 : loss : 0.251310, loss_ce: 0.002485, loss_dice: 0.500135
[10:23:34.740] TRAIN: iteration 15633 : loss : 0.101468, loss_ce: 0.008779, loss_dice: 0.194158
[10:23:34.950] TRAIN: iteration 15634 : loss : 0.069892, loss_ce: 0.001715, loss_dice: 0.138070
[10:23:35.162] TRAIN: iteration 15635 : loss : 0.250990, loss_ce: 0.001881, loss_dice: 0.500099
[10:23:35.377] TRAIN: iteration 15636 : loss : 0.210980, loss_ce: 0.002313, loss_dice: 0.419647
[10:23:35.589] TRAIN: iteration 15637 : loss : 0.052034, loss_ce: 0.001723, loss_dice: 0.102344
[10:23:35.802] TRAIN: iteration 15638 : loss : 0.204647, loss_ce: 0.002529, loss_dice: 0.406766
[10:23:36.023] TRAIN: iteration 15639 : loss : 0.209576, loss_ce: 0.005201, loss_dice: 0.413952
[10:23:36.233] TRAIN: iteration 15640 : loss : 0.189560, loss_ce: 0.006015, loss_dice: 0.373105
[10:23:36.473] TRAIN: iteration 15641 : loss : 0.063378, loss_ce: 0.002349, loss_dice: 0.124407
[10:23:36.684] TRAIN: iteration 15642 : loss : 0.079196, loss_ce: 0.011423, loss_dice: 0.146970
[10:23:36.895] TRAIN: iteration 15643 : loss : 0.250556, loss_ce: 0.002345, loss_dice: 0.498768
[10:23:37.104] TRAIN: iteration 15644 : loss : 0.111222, loss_ce: 0.003112, loss_dice: 0.219333
[10:23:37.314] TRAIN: iteration 15645 : loss : 0.197342, loss_ce: 0.004074, loss_dice: 0.390609
[10:23:37.524] TRAIN: iteration 15646 : loss : 0.251299, loss_ce: 0.002506, loss_dice: 0.500091
[10:23:37.740] TRAIN: iteration 15647 : loss : 0.150598, loss_ce: 0.010332, loss_dice: 0.290864
[10:23:37.949] TRAIN: iteration 15648 : loss : 0.143102, loss_ce: 0.002036, loss_dice: 0.284168
[10:23:38.160] TRAIN: iteration 15649 : loss : 0.251440, loss_ce: 0.002753, loss_dice: 0.500128
[10:23:38.374] TRAIN: iteration 15650 : loss : 0.252314, loss_ce: 0.005641, loss_dice: 0.498988
[10:23:38.585] TRAIN: iteration 15651 : loss : 0.253728, loss_ce: 0.007399, loss_dice: 0.500057
[10:23:38.796] TRAIN: iteration 15652 : loss : 0.251965, loss_ce: 0.003709, loss_dice: 0.500220
[10:23:39.008] TRAIN: iteration 15653 : loss : 0.095152, loss_ce: 0.010664, loss_dice: 0.179640
[10:23:39.219] TRAIN: iteration 15654 : loss : 0.085833, loss_ce: 0.002785, loss_dice: 0.168880
[10:23:39.428] TRAIN: iteration 15655 : loss : 0.117209, loss_ce: 0.004515, loss_dice: 0.229902
[10:23:39.639] TRAIN: iteration 15656 : loss : 0.200981, loss_ce: 0.005049, loss_dice: 0.396913
[10:23:39.852] TRAIN: iteration 15657 : loss : 0.072787, loss_ce: 0.009547, loss_dice: 0.136026
[10:23:40.062] TRAIN: iteration 15658 : loss : 0.035111, loss_ce: 0.002199, loss_dice: 0.068022
[10:23:40.272] TRAIN: iteration 15659 : loss : 0.251495, loss_ce: 0.002854, loss_dice: 0.500135
[10:23:40.488] TRAIN: iteration 15660 : loss : 0.148898, loss_ce: 0.017270, loss_dice: 0.280525
[10:23:40.737] TRAIN: iteration 15661 : loss : 0.084917, loss_ce: 0.002169, loss_dice: 0.167665
[10:23:40.945] TRAIN: iteration 15662 : loss : 0.251214, loss_ce: 0.002298, loss_dice: 0.500130
[10:23:41.155] TRAIN: iteration 15663 : loss : 0.041245, loss_ce: 0.001948, loss_dice: 0.080542
[10:23:41.365] TRAIN: iteration 15664 : loss : 0.240990, loss_ce: 0.003469, loss_dice: 0.478511
[10:23:41.587] TRAIN: iteration 15665 : loss : 0.090264, loss_ce: 0.002604, loss_dice: 0.177923
[10:23:41.797] TRAIN: iteration 15666 : loss : 0.054480, loss_ce: 0.002544, loss_dice: 0.106417
[10:23:42.006] TRAIN: iteration 15667 : loss : 0.173633, loss_ce: 0.006985, loss_dice: 0.340280
[10:23:42.216] TRAIN: iteration 15668 : loss : 0.234482, loss_ce: 0.002604, loss_dice: 0.466359
[10:23:42.425] TRAIN: iteration 15669 : loss : 0.251630, loss_ce: 0.003077, loss_dice: 0.500182
[10:23:42.634] TRAIN: iteration 15670 : loss : 0.074032, loss_ce: 0.011766, loss_dice: 0.136299
[10:23:42.844] TRAIN: iteration 15671 : loss : 0.185643, loss_ce: 0.008559, loss_dice: 0.362728
[10:23:43.057] TRAIN: iteration 15672 : loss : 0.065582, loss_ce: 0.003885, loss_dice: 0.127279
[10:23:43.275] TRAIN: iteration 15673 : loss : 0.156281, loss_ce: 0.003760, loss_dice: 0.308803
[10:23:43.486] TRAIN: iteration 15674 : loss : 0.250769, loss_ce: 0.003814, loss_dice: 0.497724
[10:23:43.697] TRAIN: iteration 15675 : loss : 0.159301, loss_ce: 0.004996, loss_dice: 0.313606
[10:23:43.911] TRAIN: iteration 15676 : loss : 0.201575, loss_ce: 0.004302, loss_dice: 0.398848
[10:23:44.129] TRAIN: iteration 15677 : loss : 0.223273, loss_ce: 0.005555, loss_dice: 0.440990
[10:23:44.337] TRAIN: iteration 15678 : loss : 0.053479, loss_ce: 0.008158, loss_dice: 0.098800
[10:23:44.547] TRAIN: iteration 15679 : loss : 0.220841, loss_ce: 0.003803, loss_dice: 0.437879
[10:23:44.757] TRAIN: iteration 15680 : loss : 0.238312, loss_ce: 0.003024, loss_dice: 0.473600
[10:23:45.116] TRAIN: iteration 15681 : loss : 0.160454, loss_ce: 0.007623, loss_dice: 0.313285
[10:23:45.326] TRAIN: iteration 15682 : loss : 0.199028, loss_ce: 0.010504, loss_dice: 0.387551
[10:23:45.535] TRAIN: iteration 15683 : loss : 0.253167, loss_ce: 0.006013, loss_dice: 0.500322
[10:23:45.745] TRAIN: iteration 15684 : loss : 0.252448, loss_ce: 0.004598, loss_dice: 0.500297
[10:23:45.958] TRAIN: iteration 15685 : loss : 0.225227, loss_ce: 0.008191, loss_dice: 0.442262
[10:23:46.167] TRAIN: iteration 15686 : loss : 0.169222, loss_ce: 0.005873, loss_dice: 0.332571
[10:23:46.383] TRAIN: iteration 15687 : loss : 0.062927, loss_ce: 0.005636, loss_dice: 0.120218
[10:23:46.592] TRAIN: iteration 15688 : loss : 0.043877, loss_ce: 0.002522, loss_dice: 0.085231
[10:23:46.808] TRAIN: iteration 15689 : loss : 0.071728, loss_ce: 0.003796, loss_dice: 0.139659
[10:23:47.017] TRAIN: iteration 15690 : loss : 0.251626, loss_ce: 0.003037, loss_dice: 0.500216
[10:23:47.227] TRAIN: iteration 15691 : loss : 0.058240, loss_ce: 0.003538, loss_dice: 0.112942
[10:23:47.442] TRAIN: iteration 15692 : loss : 0.103884, loss_ce: 0.014874, loss_dice: 0.192895
[10:23:47.654] TRAIN: iteration 15693 : loss : 0.092165, loss_ce: 0.005824, loss_dice: 0.178506
[10:23:47.866] TRAIN: iteration 15694 : loss : 0.111121, loss_ce: 0.006635, loss_dice: 0.215608
[10:23:48.082] TRAIN: iteration 15695 : loss : 0.228908, loss_ce: 0.002526, loss_dice: 0.455289
[10:23:48.292] TRAIN: iteration 15696 : loss : 0.147615, loss_ce: 0.004328, loss_dice: 0.290903
[10:23:48.508] TRAIN: iteration 15697 : loss : 0.112422, loss_ce: 0.005023, loss_dice: 0.219820
[10:23:48.720] TRAIN: iteration 15698 : loss : 0.064981, loss_ce: 0.007624, loss_dice: 0.122338
[10:23:48.936] TRAIN: iteration 15699 : loss : 0.151681, loss_ce: 0.010183, loss_dice: 0.293180
[10:23:49.148] TRAIN: iteration 15700 : loss : 0.203383, loss_ce: 0.002629, loss_dice: 0.404137
[10:23:49.391] TRAIN: iteration 15701 : loss : 0.147489, loss_ce: 0.007106, loss_dice: 0.287871
[10:23:49.603] TRAIN: iteration 15702 : loss : 0.075671, loss_ce: 0.007475, loss_dice: 0.143867
[10:23:49.813] TRAIN: iteration 15703 : loss : 0.034657, loss_ce: 0.002437, loss_dice: 0.066878
[10:23:50.027] TRAIN: iteration 15704 : loss : 0.116188, loss_ce: 0.001955, loss_dice: 0.230422
[10:23:50.243] TRAIN: iteration 15705 : loss : 0.057111, loss_ce: 0.005058, loss_dice: 0.109163
[10:23:50.459] TRAIN: iteration 15706 : loss : 0.108821, loss_ce: 0.003391, loss_dice: 0.214251
[10:23:50.668] TRAIN: iteration 15707 : loss : 0.109787, loss_ce: 0.002064, loss_dice: 0.217511
[10:23:50.923] TRAIN: iteration 15708 : loss : 0.075519, loss_ce: 0.002522, loss_dice: 0.148516
[10:23:51.135] TRAIN: iteration 15709 : loss : 0.247276, loss_ce: 0.003199, loss_dice: 0.491354
[10:23:51.344] TRAIN: iteration 15710 : loss : 0.205808, loss_ce: 0.015863, loss_dice: 0.395753
[10:23:51.554] TRAIN: iteration 15711 : loss : 0.066622, loss_ce: 0.001348, loss_dice: 0.131896
[10:23:51.764] TRAIN: iteration 15712 : loss : 0.037854, loss_ce: 0.000885, loss_dice: 0.074823
[10:23:51.978] TRAIN: iteration 15713 : loss : 0.222037, loss_ce: 0.005510, loss_dice: 0.438563
[10:23:52.194] TRAIN: iteration 15714 : loss : 0.156319, loss_ce: 0.008853, loss_dice: 0.303784
[10:23:52.405] TRAIN: iteration 15715 : loss : 0.109053, loss_ce: 0.007052, loss_dice: 0.211054
[10:23:52.616] TRAIN: iteration 15716 : loss : 0.102675, loss_ce: 0.003596, loss_dice: 0.201755
[10:23:52.828] TRAIN: iteration 15717 : loss : 0.244219, loss_ce: 0.005919, loss_dice: 0.482518
[10:23:53.040] TRAIN: iteration 15718 : loss : 0.062391, loss_ce: 0.002473, loss_dice: 0.122309
[10:23:53.248] TRAIN: iteration 15719 : loss : 0.142128, loss_ce: 0.003527, loss_dice: 0.280729
[10:23:53.457] TRAIN: iteration 15720 : loss : 0.070711, loss_ce: 0.002308, loss_dice: 0.139113
[10:23:53.696] TRAIN: iteration 15721 : loss : 0.251256, loss_ce: 0.002349, loss_dice: 0.500163
[10:23:53.905] TRAIN: iteration 15722 : loss : 0.251105, loss_ce: 0.002075, loss_dice: 0.500136
[10:23:54.117] TRAIN: iteration 15723 : loss : 0.113552, loss_ce: 0.007813, loss_dice: 0.219292
[10:23:54.326] TRAIN: iteration 15724 : loss : 0.086718, loss_ce: 0.004526, loss_dice: 0.168910
[10:23:54.537] TRAIN: iteration 15725 : loss : 0.237741, loss_ce: 0.006180, loss_dice: 0.469301
[10:23:54.747] TRAIN: iteration 15726 : loss : 0.115902, loss_ce: 0.005821, loss_dice: 0.225983
[10:23:54.958] TRAIN: iteration 15727 : loss : 0.123309, loss_ce: 0.005490, loss_dice: 0.241128
[10:23:55.167] TRAIN: iteration 15728 : loss : 0.068091, loss_ce: 0.001785, loss_dice: 0.134397
[10:23:55.378] TRAIN: iteration 15729 : loss : 0.158132, loss_ce: 0.004747, loss_dice: 0.311518
[10:23:55.588] TRAIN: iteration 15730 : loss : 0.069911, loss_ce: 0.003726, loss_dice: 0.136096
[10:23:56.019] TRAIN: iteration 15731 : loss : 0.248273, loss_ce: 0.002223, loss_dice: 0.494322
[10:23:56.232] TRAIN: iteration 15732 : loss : 0.085851, loss_ce: 0.014375, loss_dice: 0.157327
[10:23:56.442] TRAIN: iteration 15733 : loss : 0.203234, loss_ce: 0.003773, loss_dice: 0.402696
[10:23:56.652] TRAIN: iteration 15734 : loss : 0.121025, loss_ce: 0.005470, loss_dice: 0.236581
[10:23:56.862] TRAIN: iteration 15735 : loss : 0.196269, loss_ce: 0.007070, loss_dice: 0.385468
[10:23:57.072] TRAIN: iteration 15736 : loss : 0.252193, loss_ce: 0.005155, loss_dice: 0.499231
[10:23:57.283] TRAIN: iteration 15737 : loss : 0.177807, loss_ce: 0.003340, loss_dice: 0.352275
[10:23:57.493] TRAIN: iteration 15738 : loss : 0.068687, loss_ce: 0.005293, loss_dice: 0.132082
[10:23:57.703] TRAIN: iteration 15739 : loss : 0.126373, loss_ce: 0.004325, loss_dice: 0.248420
[10:23:57.913] TRAIN: iteration 15740 : loss : 0.062540, loss_ce: 0.006544, loss_dice: 0.118536
[10:23:58.153] TRAIN: iteration 15741 : loss : 0.076891, loss_ce: 0.005669, loss_dice: 0.148114
[10:23:58.363] TRAIN: iteration 15742 : loss : 0.051253, loss_ce: 0.003192, loss_dice: 0.099313
[10:23:58.574] TRAIN: iteration 15743 : loss : 0.120897, loss_ce: 0.015305, loss_dice: 0.226489
[10:23:58.785] TRAIN: iteration 15744 : loss : 0.051740, loss_ce: 0.006856, loss_dice: 0.096624
[10:23:59.012] TRAIN: iteration 15745 : loss : 0.206544, loss_ce: 0.002311, loss_dice: 0.410778
[10:23:59.223] TRAIN: iteration 15746 : loss : 0.249461, loss_ce: 0.004421, loss_dice: 0.494501
[10:23:59.435] TRAIN: iteration 15747 : loss : 0.252358, loss_ce: 0.004387, loss_dice: 0.500328
[10:23:59.649] TRAIN: iteration 15748 : loss : 0.067130, loss_ce: 0.007455, loss_dice: 0.126805
[10:23:59.865] TRAIN: iteration 15749 : loss : 0.115666, loss_ce: 0.002426, loss_dice: 0.228907
[10:24:00.074] TRAIN: iteration 15750 : loss : 0.099024, loss_ce: 0.009674, loss_dice: 0.188373
[10:24:00.287] TRAIN: iteration 15751 : loss : 0.069182, loss_ce: 0.005324, loss_dice: 0.133040
[10:24:00.496] TRAIN: iteration 15752 : loss : 0.124111, loss_ce: 0.003119, loss_dice: 0.245102
[10:24:00.704] TRAIN: iteration 15753 : loss : 0.086138, loss_ce: 0.003479, loss_dice: 0.168797
[10:24:00.913] TRAIN: iteration 15754 : loss : 0.251511, loss_ce: 0.002846, loss_dice: 0.500176
[10:24:01.122] TRAIN: iteration 15755 : loss : 0.126927, loss_ce: 0.003265, loss_dice: 0.250589
[10:24:01.331] TRAIN: iteration 15756 : loss : 0.150477, loss_ce: 0.005320, loss_dice: 0.295633
[10:24:01.541] TRAIN: iteration 15757 : loss : 0.249025, loss_ce: 0.003991, loss_dice: 0.494060
[10:24:01.750] TRAIN: iteration 15758 : loss : 0.043142, loss_ce: 0.001547, loss_dice: 0.084737
[10:24:01.960] TRAIN: iteration 15759 : loss : 0.149443, loss_ce: 0.003080, loss_dice: 0.295805
[10:24:02.173] TRAIN: iteration 15760 : loss : 0.250697, loss_ce: 0.001345, loss_dice: 0.500050
[10:24:02.413] TRAIN: iteration 15761 : loss : 0.099538, loss_ce: 0.025622, loss_dice: 0.173453
[10:24:02.627] TRAIN: iteration 15762 : loss : 0.191729, loss_ce: 0.001250, loss_dice: 0.382209
[10:24:02.837] TRAIN: iteration 15763 : loss : 0.080754, loss_ce: 0.001494, loss_dice: 0.160014
[10:24:03.050] TRAIN: iteration 15764 : loss : 0.087754, loss_ce: 0.002033, loss_dice: 0.173475
[10:24:03.260] TRAIN: iteration 15765 : loss : 0.036435, loss_ce: 0.001974, loss_dice: 0.070896
[10:24:03.476] TRAIN: iteration 15766 : loss : 0.128165, loss_ce: 0.017577, loss_dice: 0.238753
[10:24:03.687] TRAIN: iteration 15767 : loss : 0.104030, loss_ce: 0.002254, loss_dice: 0.205806
[10:24:03.896] TRAIN: iteration 15768 : loss : 0.092508, loss_ce: 0.003548, loss_dice: 0.181468
[10:24:04.112] TRAIN: iteration 15769 : loss : 0.014966, loss_ce: 0.001489, loss_dice: 0.028444
[10:24:04.321] TRAIN: iteration 15770 : loss : 0.034716, loss_ce: 0.001277, loss_dice: 0.068154
[10:24:04.531] TRAIN: iteration 15771 : loss : 0.081407, loss_ce: 0.003006, loss_dice: 0.159808
[10:24:04.740] TRAIN: iteration 15772 : loss : 0.127688, loss_ce: 0.002408, loss_dice: 0.252969
[10:24:04.949] TRAIN: iteration 15773 : loss : 0.039927, loss_ce: 0.002286, loss_dice: 0.077568
[10:24:05.159] TRAIN: iteration 15774 : loss : 0.060114, loss_ce: 0.011769, loss_dice: 0.108458
[10:24:05.370] TRAIN: iteration 15775 : loss : 0.096343, loss_ce: 0.005412, loss_dice: 0.187275
[10:24:05.583] TRAIN: iteration 15776 : loss : 0.091142, loss_ce: 0.006354, loss_dice: 0.175930
[10:24:05.793] TRAIN: iteration 15777 : loss : 0.078119, loss_ce: 0.001343, loss_dice: 0.154895
[10:24:06.005] TRAIN: iteration 15778 : loss : 0.083499, loss_ce: 0.004834, loss_dice: 0.162165
[10:24:06.214] TRAIN: iteration 15779 : loss : 0.103381, loss_ce: 0.005152, loss_dice: 0.201610
[10:24:06.423] TRAIN: iteration 15780 : loss : 0.126802, loss_ce: 0.003720, loss_dice: 0.249883
[10:24:06.660] TRAIN: iteration 15781 : loss : 0.074244, loss_ce: 0.004136, loss_dice: 0.144351
[10:24:06.871] TRAIN: iteration 15782 : loss : 0.138362, loss_ce: 0.002713, loss_dice: 0.274010
[10:24:07.085] TRAIN: iteration 15783 : loss : 0.101869, loss_ce: 0.004472, loss_dice: 0.199266
[10:24:07.298] TRAIN: iteration 15784 : loss : 0.169551, loss_ce: 0.008689, loss_dice: 0.330414
[10:24:07.507] TRAIN: iteration 15785 : loss : 0.061035, loss_ce: 0.002404, loss_dice: 0.119666
[10:24:07.716] TRAIN: iteration 15786 : loss : 0.250402, loss_ce: 0.001341, loss_dice: 0.499464
[10:24:07.925] TRAIN: iteration 15787 : loss : 0.109611, loss_ce: 0.004995, loss_dice: 0.214227
[10:24:08.136] TRAIN: iteration 15788 : loss : 0.250760, loss_ce: 0.001434, loss_dice: 0.500085
[10:24:08.346] TRAIN: iteration 15789 : loss : 0.024475, loss_ce: 0.000818, loss_dice: 0.048131
[10:24:08.555] TRAIN: iteration 15790 : loss : 0.230386, loss_ce: 0.009285, loss_dice: 0.451487
[10:24:08.766] TRAIN: iteration 15791 : loss : 0.094143, loss_ce: 0.003122, loss_dice: 0.185164
[10:24:08.976] TRAIN: iteration 15792 : loss : 0.244151, loss_ce: 0.000775, loss_dice: 0.487526
[10:24:09.186] TRAIN: iteration 15793 : loss : 0.230498, loss_ce: 0.010826, loss_dice: 0.450171
[10:24:09.397] TRAIN: iteration 15794 : loss : 0.121288, loss_ce: 0.005728, loss_dice: 0.236848
[10:24:09.606] TRAIN: iteration 15795 : loss : 0.144014, loss_ce: 0.002765, loss_dice: 0.285264
[10:24:09.816] TRAIN: iteration 15796 : loss : 0.130123, loss_ce: 0.001622, loss_dice: 0.258625
[10:24:10.025] TRAIN: iteration 15797 : loss : 0.137915, loss_ce: 0.006147, loss_dice: 0.269682
[10:24:10.244] TRAIN: iteration 15798 : loss : 0.138502, loss_ce: 0.001368, loss_dice: 0.275635
[10:24:10.460] TRAIN: iteration 15799 : loss : 0.137565, loss_ce: 0.007135, loss_dice: 0.267995
[10:24:10.672] TRAIN: iteration 15800 : loss : 0.114491, loss_ce: 0.001768, loss_dice: 0.227214
[10:24:10.911] TRAIN: iteration 15801 : loss : 0.166451, loss_ce: 0.013132, loss_dice: 0.319770
[10:24:11.120] TRAIN: iteration 15802 : loss : 0.053152, loss_ce: 0.000907, loss_dice: 0.105397
[10:24:11.330] TRAIN: iteration 15803 : loss : 0.077208, loss_ce: 0.004821, loss_dice: 0.149595
[10:24:11.539] TRAIN: iteration 15804 : loss : 0.066772, loss_ce: 0.006307, loss_dice: 0.127238
[10:24:11.747] TRAIN: iteration 15805 : loss : 0.229127, loss_ce: 0.016157, loss_dice: 0.442098
[10:24:11.959] TRAIN: iteration 15806 : loss : 0.090490, loss_ce: 0.005189, loss_dice: 0.175792
[10:24:12.167] TRAIN: iteration 15807 : loss : 0.072323, loss_ce: 0.002890, loss_dice: 0.141756
[10:24:12.376] TRAIN: iteration 15808 : loss : 0.153867, loss_ce: 0.005533, loss_dice: 0.302201
[10:24:12.588] TRAIN: iteration 15809 : loss : 0.263518, loss_ce: 0.032188, loss_dice: 0.494847
[10:24:12.797] TRAIN: iteration 15810 : loss : 0.204345, loss_ce: 0.048074, loss_dice: 0.360617
[10:24:13.006] TRAIN: iteration 15811 : loss : 0.072684, loss_ce: 0.005532, loss_dice: 0.139836
[10:24:13.217] TRAIN: iteration 15812 : loss : 0.251258, loss_ce: 0.002362, loss_dice: 0.500155
[10:24:13.436] TRAIN: iteration 15813 : loss : 0.067475, loss_ce: 0.004920, loss_dice: 0.130029
[10:24:13.651] TRAIN: iteration 15814 : loss : 0.087533, loss_ce: 0.003044, loss_dice: 0.172023
[10:24:13.861] TRAIN: iteration 15815 : loss : 0.088352, loss_ce: 0.004514, loss_dice: 0.172190
[10:24:14.073] TRAIN: iteration 15816 : loss : 0.064507, loss_ce: 0.005268, loss_dice: 0.123746
[10:24:14.289] TRAIN: iteration 15817 : loss : 0.110796, loss_ce: 0.005458, loss_dice: 0.216133
[10:24:14.498] TRAIN: iteration 15818 : loss : 0.091674, loss_ce: 0.002218, loss_dice: 0.181130
[10:24:14.707] TRAIN: iteration 15819 : loss : 0.175070, loss_ce: 0.006509, loss_dice: 0.343631
[10:24:14.923] TRAIN: iteration 15820 : loss : 0.091267, loss_ce: 0.010659, loss_dice: 0.171875
[10:24:15.172] TRAIN: iteration 15821 : loss : 0.251746, loss_ce: 0.003302, loss_dice: 0.500190
[10:24:15.386] TRAIN: iteration 15822 : loss : 0.151335, loss_ce: 0.011695, loss_dice: 0.290975
[10:24:15.600] TRAIN: iteration 15823 : loss : 0.252417, loss_ce: 0.004542, loss_dice: 0.500291
[10:24:15.817] TRAIN: iteration 15824 : loss : 0.232538, loss_ce: 0.006383, loss_dice: 0.458693
[10:24:16.087] TRAIN: iteration 15825 : loss : 0.252413, loss_ce: 0.004516, loss_dice: 0.500311
[10:24:16.303] TRAIN: iteration 15826 : loss : 0.142643, loss_ce: 0.006475, loss_dice: 0.278812
[10:24:16.514] TRAIN: iteration 15827 : loss : 0.252637, loss_ce: 0.004949, loss_dice: 0.500326
[10:24:16.724] TRAIN: iteration 15828 : loss : 0.251869, loss_ce: 0.003518, loss_dice: 0.500220
[10:24:16.933] TRAIN: iteration 15829 : loss : 0.143164, loss_ce: 0.006086, loss_dice: 0.280242
[10:24:17.142] TRAIN: iteration 15830 : loss : 0.180704, loss_ce: 0.003604, loss_dice: 0.357804
[10:24:17.352] TRAIN: iteration 15831 : loss : 0.100305, loss_ce: 0.004334, loss_dice: 0.196276
[10:24:17.561] TRAIN: iteration 15832 : loss : 0.141565, loss_ce: 0.003198, loss_dice: 0.279932
[10:24:17.778] TRAIN: iteration 15833 : loss : 0.255492, loss_ce: 0.012568, loss_dice: 0.498416
[10:24:17.988] TRAIN: iteration 15834 : loss : 0.088477, loss_ce: 0.005197, loss_dice: 0.171757
[10:24:18.198] TRAIN: iteration 15835 : loss : 0.099006, loss_ce: 0.001856, loss_dice: 0.196155
[10:24:18.408] TRAIN: iteration 15836 : loss : 0.091840, loss_ce: 0.003463, loss_dice: 0.180217
[10:24:18.616] TRAIN: iteration 15837 : loss : 0.022752, loss_ce: 0.001118, loss_dice: 0.044386
[10:24:18.828] TRAIN: iteration 15838 : loss : 0.115673, loss_ce: 0.005446, loss_dice: 0.225900
[10:24:19.039] TRAIN: iteration 15839 : loss : 0.216444, loss_ce: 0.004216, loss_dice: 0.428672
[10:24:19.249] TRAIN: iteration 15840 : loss : 0.250969, loss_ce: 0.001843, loss_dice: 0.500095
[10:24:19.495] TRAIN: iteration 15841 : loss : 0.125740, loss_ce: 0.002407, loss_dice: 0.249073
[10:24:19.704] TRAIN: iteration 15842 : loss : 0.075098, loss_ce: 0.004493, loss_dice: 0.145702
[10:24:19.919] TRAIN: iteration 15843 : loss : 0.111509, loss_ce: 0.003799, loss_dice: 0.219218
[10:24:20.130] TRAIN: iteration 15844 : loss : 0.087397, loss_ce: 0.001813, loss_dice: 0.172981
[10:24:20.722] TRAIN: iteration 15845 : loss : 0.070596, loss_ce: 0.003235, loss_dice: 0.137957
[10:24:20.932] TRAIN: iteration 15846 : loss : 0.103047, loss_ce: 0.007865, loss_dice: 0.198230
[10:24:21.141] TRAIN: iteration 15847 : loss : 0.147218, loss_ce: 0.005986, loss_dice: 0.288451
[10:24:21.350] TRAIN: iteration 15848 : loss : 0.103097, loss_ce: 0.002592, loss_dice: 0.203603
[10:24:21.559] TRAIN: iteration 15849 : loss : 0.100381, loss_ce: 0.002480, loss_dice: 0.198282
[10:24:21.777] TRAIN: iteration 15850 : loss : 0.100448, loss_ce: 0.002964, loss_dice: 0.197931
[10:24:21.991] TRAIN: iteration 15851 : loss : 0.144405, loss_ce: 0.004989, loss_dice: 0.283820
[10:24:22.207] TRAIN: iteration 15852 : loss : 0.216891, loss_ce: 0.003525, loss_dice: 0.430256
[10:24:22.415] TRAIN: iteration 15853 : loss : 0.079952, loss_ce: 0.002814, loss_dice: 0.157090
[10:24:22.626] TRAIN: iteration 15854 : loss : 0.209547, loss_ce: 0.005906, loss_dice: 0.413189
[10:24:22.835] TRAIN: iteration 15855 : loss : 0.251452, loss_ce: 0.002709, loss_dice: 0.500194
[10:24:23.044] TRAIN: iteration 15856 : loss : 0.039894, loss_ce: 0.006650, loss_dice: 0.073138
[10:24:23.255] TRAIN: iteration 15857 : loss : 0.179922, loss_ce: 0.003292, loss_dice: 0.356552
[10:24:23.464] TRAIN: iteration 15858 : loss : 0.136950, loss_ce: 0.005274, loss_dice: 0.268625
[10:24:23.673] TRAIN: iteration 15859 : loss : 0.104047, loss_ce: 0.006244, loss_dice: 0.201850
[10:24:23.882] TRAIN: iteration 15860 : loss : 0.019510, loss_ce: 0.001977, loss_dice: 0.037042
[10:24:24.119] TRAIN: iteration 15861 : loss : 0.238803, loss_ce: 0.004746, loss_dice: 0.472859
[10:24:24.328] TRAIN: iteration 15862 : loss : 0.130834, loss_ce: 0.011951, loss_dice: 0.249716
[10:24:24.539] TRAIN: iteration 15863 : loss : 0.227860, loss_ce: 0.022778, loss_dice: 0.432942
[10:24:24.752] TRAIN: iteration 15864 : loss : 0.048394, loss_ce: 0.003592, loss_dice: 0.093197
[10:24:24.963] TRAIN: iteration 15865 : loss : 0.094080, loss_ce: 0.003605, loss_dice: 0.184555
[10:24:25.173] TRAIN: iteration 15866 : loss : 0.219345, loss_ce: 0.002925, loss_dice: 0.435764
[10:24:25.381] TRAIN: iteration 15867 : loss : 0.060217, loss_ce: 0.003682, loss_dice: 0.116753
[10:24:25.598] TRAIN: iteration 15868 : loss : 0.088940, loss_ce: 0.009938, loss_dice: 0.167942
[10:24:25.807] TRAIN: iteration 15869 : loss : 0.251532, loss_ce: 0.003666, loss_dice: 0.499399
[10:24:26.017] TRAIN: iteration 15870 : loss : 0.251768, loss_ce: 0.003309, loss_dice: 0.500227
[10:24:26.227] TRAIN: iteration 15871 : loss : 0.251567, loss_ce: 0.002964, loss_dice: 0.500170
[10:24:26.438] TRAIN: iteration 15872 : loss : 0.111424, loss_ce: 0.007094, loss_dice: 0.215754
[10:24:26.647] TRAIN: iteration 15873 : loss : 0.048983, loss_ce: 0.001578, loss_dice: 0.096388
[10:24:26.859] TRAIN: iteration 15874 : loss : 0.192320, loss_ce: 0.003380, loss_dice: 0.381260
[10:24:27.078] TRAIN: iteration 15875 : loss : 0.086323, loss_ce: 0.004904, loss_dice: 0.167741
[10:24:27.289] TRAIN: iteration 15876 : loss : 0.247541, loss_ce: 0.003091, loss_dice: 0.491990
[10:24:27.499] TRAIN: iteration 15877 : loss : 0.246643, loss_ce: 0.004271, loss_dice: 0.489016
[10:24:27.709] TRAIN: iteration 15878 : loss : 0.251052, loss_ce: 0.001986, loss_dice: 0.500118
[10:24:27.921] TRAIN: iteration 15879 : loss : 0.127326, loss_ce: 0.002793, loss_dice: 0.251860
[10:24:28.134] TRAIN: iteration 15880 : loss : 0.075885, loss_ce: 0.004024, loss_dice: 0.147747
[10:24:28.377] TRAIN: iteration 15881 : loss : 0.064947, loss_ce: 0.002798, loss_dice: 0.127097
[10:24:28.588] TRAIN: iteration 15882 : loss : 0.155947, loss_ce: 0.004849, loss_dice: 0.307045
[10:24:28.797] TRAIN: iteration 15883 : loss : 0.142856, loss_ce: 0.002030, loss_dice: 0.283682
[10:24:29.005] TRAIN: iteration 15884 : loss : 0.093380, loss_ce: 0.001837, loss_dice: 0.184923
[10:24:29.217] TRAIN: iteration 15885 : loss : 0.073391, loss_ce: 0.002018, loss_dice: 0.144763
[10:24:29.426] TRAIN: iteration 15886 : loss : 0.175008, loss_ce: 0.004248, loss_dice: 0.345769
[10:24:29.635] TRAIN: iteration 15887 : loss : 0.231931, loss_ce: 0.004023, loss_dice: 0.459839
[10:24:29.844] TRAIN: iteration 15888 : loss : 0.250833, loss_ce: 0.001589, loss_dice: 0.500077
[10:24:30.053] TRAIN: iteration 15889 : loss : 0.092597, loss_ce: 0.002365, loss_dice: 0.182830
[10:24:30.264] TRAIN: iteration 15890 : loss : 0.110625, loss_ce: 0.012020, loss_dice: 0.209230
[10:24:30.473] TRAIN: iteration 15891 : loss : 0.114395, loss_ce: 0.002633, loss_dice: 0.226157
[10:24:30.682] TRAIN: iteration 15892 : loss : 0.017805, loss_ce: 0.001068, loss_dice: 0.034542
[10:24:30.891] TRAIN: iteration 15893 : loss : 0.063010, loss_ce: 0.002172, loss_dice: 0.123848
[10:24:31.102] TRAIN: iteration 15894 : loss : 0.069329, loss_ce: 0.004585, loss_dice: 0.134074
[10:24:31.312] TRAIN: iteration 15895 : loss : 0.251208, loss_ce: 0.002268, loss_dice: 0.500149
[10:24:31.529] TRAIN: iteration 15896 : loss : 0.250986, loss_ce: 0.001880, loss_dice: 0.500092
[10:24:31.742] TRAIN: iteration 15897 : loss : 0.090082, loss_ce: 0.011076, loss_dice: 0.169089
[10:24:31.952] TRAIN: iteration 15898 : loss : 0.064554, loss_ce: 0.004597, loss_dice: 0.124511
[10:24:32.162] TRAIN: iteration 15899 : loss : 0.251738, loss_ce: 0.004557, loss_dice: 0.498918
[10:24:32.378] TRAIN: iteration 15900 : loss : 0.158812, loss_ce: 0.009590, loss_dice: 0.308034
[10:24:32.629] TRAIN: iteration 15901 : loss : 0.078557, loss_ce: 0.002884, loss_dice: 0.154231
[10:24:32.842] TRAIN: iteration 15902 : loss : 0.042985, loss_ce: 0.002582, loss_dice: 0.083387
[10:24:33.050] TRAIN: iteration 15903 : loss : 0.266427, loss_ce: 0.036812, loss_dice: 0.496041
[10:24:33.259] TRAIN: iteration 15904 : loss : 0.051574, loss_ce: 0.001418, loss_dice: 0.101729
[10:24:33.468] TRAIN: iteration 15905 : loss : 0.251015, loss_ce: 0.001924, loss_dice: 0.500107
[10:24:33.676] TRAIN: iteration 15906 : loss : 0.119891, loss_ce: 0.008390, loss_dice: 0.231393
[10:24:33.891] TRAIN: iteration 15907 : loss : 0.096166, loss_ce: 0.003558, loss_dice: 0.188774
[10:24:34.102] TRAIN: iteration 15908 : loss : 0.105050, loss_ce: 0.007237, loss_dice: 0.202863
[10:24:34.319] TRAIN: iteration 15909 : loss : 0.053322, loss_ce: 0.005694, loss_dice: 0.100951
[10:24:34.527] TRAIN: iteration 15910 : loss : 0.092416, loss_ce: 0.003002, loss_dice: 0.181830
[10:24:34.743] TRAIN: iteration 15911 : loss : 0.238955, loss_ce: 0.010220, loss_dice: 0.467691
[10:24:34.953] TRAIN: iteration 15912 : loss : 0.108735, loss_ce: 0.003052, loss_dice: 0.214418
[10:24:35.161] TRAIN: iteration 15913 : loss : 0.148254, loss_ce: 0.023905, loss_dice: 0.272603
[10:24:35.374] TRAIN: iteration 15914 : loss : 0.245310, loss_ce: 0.002802, loss_dice: 0.487818
[10:24:35.583] TRAIN: iteration 15915 : loss : 0.093981, loss_ce: 0.003997, loss_dice: 0.183965
[10:24:35.793] TRAIN: iteration 15916 : loss : 0.086368, loss_ce: 0.006176, loss_dice: 0.166560
[10:24:36.004] TRAIN: iteration 15917 : loss : 0.251260, loss_ce: 0.002392, loss_dice: 0.500129
[10:24:36.214] TRAIN: iteration 15918 : loss : 0.251914, loss_ce: 0.003583, loss_dice: 0.500245
[10:24:36.430] TRAIN: iteration 15919 : loss : 0.150212, loss_ce: 0.005626, loss_dice: 0.294798
[10:24:36.643] TRAIN: iteration 15920 : loss : 0.059503, loss_ce: 0.002368, loss_dice: 0.116637
[10:24:36.644] NaN or Inf found in input tensor.
[10:24:36.864] TRAIN: iteration 15921 : loss : 0.158931, loss_ce: 0.004329, loss_dice: 0.313534
[10:24:37.075] TRAIN: iteration 15922 : loss : 0.105929, loss_ce: 0.003249, loss_dice: 0.208609
[10:24:37.283] TRAIN: iteration 15923 : loss : 0.059206, loss_ce: 0.009349, loss_dice: 0.109063
[10:24:37.492] TRAIN: iteration 15924 : loss : 0.028374, loss_ce: 0.004716, loss_dice: 0.052033
[10:24:37.701] TRAIN: iteration 15925 : loss : 0.049633, loss_ce: 0.002003, loss_dice: 0.097263
[10:24:37.911] TRAIN: iteration 15926 : loss : 0.250891, loss_ce: 0.001692, loss_dice: 0.500090
[10:24:38.125] TRAIN: iteration 15927 : loss : 0.158941, loss_ce: 0.006730, loss_dice: 0.311152
[10:24:38.334] TRAIN: iteration 15928 : loss : 0.100634, loss_ce: 0.002943, loss_dice: 0.198326
[10:24:38.544] TRAIN: iteration 15929 : loss : 0.161075, loss_ce: 0.006908, loss_dice: 0.315242
[10:24:38.753] TRAIN: iteration 15930 : loss : 0.214208, loss_ce: 0.002584, loss_dice: 0.425832
[10:24:38.963] TRAIN: iteration 15931 : loss : 0.033492, loss_ce: 0.004652, loss_dice: 0.062333
[10:24:39.173] TRAIN: iteration 15932 : loss : 0.077391, loss_ce: 0.004285, loss_dice: 0.150497
[10:24:39.384] TRAIN: iteration 15933 : loss : 0.055870, loss_ce: 0.002051, loss_dice: 0.109689
[10:24:39.596] TRAIN: iteration 15934 : loss : 0.090714, loss_ce: 0.002468, loss_dice: 0.178960
[10:24:39.805] TRAIN: iteration 15935 : loss : 0.022415, loss_ce: 0.002481, loss_dice: 0.042350
[10:24:40.017] TRAIN: iteration 15936 : loss : 0.035168, loss_ce: 0.002172, loss_dice: 0.068164
[10:24:40.229] TRAIN: iteration 15937 : loss : 0.203867, loss_ce: 0.003587, loss_dice: 0.404146
[10:24:40.446] TRAIN: iteration 15938 : loss : 0.067342, loss_ce: 0.001458, loss_dice: 0.133225
[10:24:40.662] TRAIN: iteration 15939 : loss : 0.231391, loss_ce: 0.002620, loss_dice: 0.460161
[10:24:40.873] TRAIN: iteration 15940 : loss : 0.122615, loss_ce: 0.008525, loss_dice: 0.236704
[10:24:41.120] TRAIN: iteration 15941 : loss : 0.106873, loss_ce: 0.002757, loss_dice: 0.210989
[10:24:41.331] TRAIN: iteration 15942 : loss : 0.229191, loss_ce: 0.002559, loss_dice: 0.455823
[10:24:41.540] TRAIN: iteration 15943 : loss : 0.083168, loss_ce: 0.002896, loss_dice: 0.163439
[10:24:41.749] TRAIN: iteration 15944 : loss : 0.173428, loss_ce: 0.012960, loss_dice: 0.333896
[10:24:41.966] TRAIN: iteration 15945 : loss : 0.037881, loss_ce: 0.003313, loss_dice: 0.072449
[10:24:42.177] TRAIN: iteration 15946 : loss : 0.193935, loss_ce: 0.002067, loss_dice: 0.385803
[10:24:42.386] TRAIN: iteration 15947 : loss : 0.240067, loss_ce: 0.002803, loss_dice: 0.477332
[10:24:42.595] TRAIN: iteration 15948 : loss : 0.104003, loss_ce: 0.002268, loss_dice: 0.205738
[10:24:42.806] TRAIN: iteration 15949 : loss : 0.240792, loss_ce: 0.005288, loss_dice: 0.476297
[10:24:43.014] TRAIN: iteration 15950 : loss : 0.231869, loss_ce: 0.003247, loss_dice: 0.460491
[10:24:43.227] TRAIN: iteration 15951 : loss : 0.105418, loss_ce: 0.005021, loss_dice: 0.205814
[10:24:43.444] TRAIN: iteration 15952 : loss : 0.113757, loss_ce: 0.005100, loss_dice: 0.222415
[10:24:43.683] TRAIN: iteration 15953 : loss : 0.239297, loss_ce: 0.020962, loss_dice: 0.457632
[10:24:43.893] TRAIN: iteration 15954 : loss : 0.033196, loss_ce: 0.001936, loss_dice: 0.064456
[10:24:44.101] TRAIN: iteration 15955 : loss : 0.180021, loss_ce: 0.003827, loss_dice: 0.356216
[10:24:44.311] TRAIN: iteration 15956 : loss : 0.197278, loss_ce: 0.002998, loss_dice: 0.391558
[10:24:44.526] TRAIN: iteration 15957 : loss : 0.066636, loss_ce: 0.009155, loss_dice: 0.124117
[10:24:44.736] TRAIN: iteration 15958 : loss : 0.127016, loss_ce: 0.003561, loss_dice: 0.250471
[10:24:44.945] TRAIN: iteration 15959 : loss : 0.119356, loss_ce: 0.008154, loss_dice: 0.230558
[10:24:45.156] TRAIN: iteration 15960 : loss : 0.156538, loss_ce: 0.005822, loss_dice: 0.307253
[10:24:45.403] TRAIN: iteration 15961 : loss : 0.070473, loss_ce: 0.003128, loss_dice: 0.137818
[10:24:45.613] TRAIN: iteration 15962 : loss : 0.219266, loss_ce: 0.006910, loss_dice: 0.431622
[10:24:45.823] TRAIN: iteration 15963 : loss : 0.063086, loss_ce: 0.004618, loss_dice: 0.121553
[10:24:46.040] TRAIN: iteration 15964 : loss : 0.197910, loss_ce: 0.015399, loss_dice: 0.380422
[10:24:46.264] TRAIN: iteration 15965 : loss : 0.228648, loss_ce: 0.003630, loss_dice: 0.453665
[10:24:46.474] TRAIN: iteration 15966 : loss : 0.093049, loss_ce: 0.003225, loss_dice: 0.182874
[10:24:46.683] TRAIN: iteration 15967 : loss : 0.230810, loss_ce: 0.005373, loss_dice: 0.456246
[10:24:46.894] TRAIN: iteration 15968 : loss : 0.057240, loss_ce: 0.004712, loss_dice: 0.109769
[10:24:47.107] TRAIN: iteration 15969 : loss : 0.086229, loss_ce: 0.018462, loss_dice: 0.153996
[10:24:47.318] TRAIN: iteration 15970 : loss : 0.174550, loss_ce: 0.011990, loss_dice: 0.337110
[10:24:47.526] TRAIN: iteration 15971 : loss : 0.117431, loss_ce: 0.006642, loss_dice: 0.228221
[10:24:47.737] TRAIN: iteration 15972 : loss : 0.062621, loss_ce: 0.002723, loss_dice: 0.122520
[10:24:47.947] TRAIN: iteration 15973 : loss : 0.179863, loss_ce: 0.008635, loss_dice: 0.351092
[10:24:48.155] TRAIN: iteration 15974 : loss : 0.132583, loss_ce: 0.005134, loss_dice: 0.260031
[10:24:48.364] TRAIN: iteration 15975 : loss : 0.104287, loss_ce: 0.003319, loss_dice: 0.205256
[10:24:48.573] TRAIN: iteration 15976 : loss : 0.131102, loss_ce: 0.006573, loss_dice: 0.255630
[10:24:48.783] TRAIN: iteration 15977 : loss : 0.153643, loss_ce: 0.007687, loss_dice: 0.299598
[10:24:48.992] TRAIN: iteration 15978 : loss : 0.141540, loss_ce: 0.005931, loss_dice: 0.277149
[10:24:49.203] TRAIN: iteration 15979 : loss : 0.066318, loss_ce: 0.003001, loss_dice: 0.129636
[10:24:49.412] TRAIN: iteration 15980 : loss : 0.131379, loss_ce: 0.015210, loss_dice: 0.247548
[10:24:49.657] TRAIN: iteration 15981 : loss : 0.143864, loss_ce: 0.007450, loss_dice: 0.280278
[10:24:49.867] TRAIN: iteration 15982 : loss : 0.177797, loss_ce: 0.002277, loss_dice: 0.353316
[10:24:50.076] TRAIN: iteration 15983 : loss : 0.075982, loss_ce: 0.005802, loss_dice: 0.146161
[10:24:50.285] TRAIN: iteration 15984 : loss : 0.115096, loss_ce: 0.003446, loss_dice: 0.226746
[10:24:50.500] TRAIN: iteration 15985 : loss : 0.116739, loss_ce: 0.011463, loss_dice: 0.222015
[10:24:50.710] TRAIN: iteration 15986 : loss : 0.102067, loss_ce: 0.002492, loss_dice: 0.201641
[10:24:50.921] TRAIN: iteration 15987 : loss : 0.069357, loss_ce: 0.002626, loss_dice: 0.136088
[10:24:51.137] TRAIN: iteration 15988 : loss : 0.043266, loss_ce: 0.003272, loss_dice: 0.083261
[10:24:51.352] TRAIN: iteration 15989 : loss : 0.126729, loss_ce: 0.008322, loss_dice: 0.245135
[10:24:51.565] TRAIN: iteration 15990 : loss : 0.238679, loss_ce: 0.003149, loss_dice: 0.474209
[10:24:51.775] TRAIN: iteration 15991 : loss : 0.188458, loss_ce: 0.014469, loss_dice: 0.362447
[10:24:51.990] TRAIN: iteration 15992 : loss : 0.062059, loss_ce: 0.001630, loss_dice: 0.122488
[10:24:52.201] TRAIN: iteration 15993 : loss : 0.195133, loss_ce: 0.007618, loss_dice: 0.382649
[10:24:52.413] TRAIN: iteration 15994 : loss : 0.101259, loss_ce: 0.005666, loss_dice: 0.196852
[10:24:52.622] TRAIN: iteration 15995 : loss : 0.063876, loss_ce: 0.004172, loss_dice: 0.123581
[10:24:52.860] TRAIN: iteration 15996 : loss : 0.140661, loss_ce: 0.003726, loss_dice: 0.277596
[10:24:53.070] TRAIN: iteration 15997 : loss : 0.236242, loss_ce: 0.010077, loss_dice: 0.462407
[10:24:53.279] TRAIN: iteration 15998 : loss : 0.075585, loss_ce: 0.002671, loss_dice: 0.148499
[10:24:53.488] TRAIN: iteration 15999 : loss : 0.130591, loss_ce: 0.007396, loss_dice: 0.253786
[10:24:53.698] TRAIN: iteration 16000 : loss : 0.102313, loss_ce: 0.001570, loss_dice: 0.203055
[10:24:53.934] TRAIN: iteration 16001 : loss : 0.202991, loss_ce: 0.002521, loss_dice: 0.403461
[10:24:56.080] TRAIN: iteration 16002 : loss : 0.080006, loss_ce: 0.002092, loss_dice: 0.157919
[10:24:56.291] TRAIN: iteration 16003 : loss : 0.090241, loss_ce: 0.003191, loss_dice: 0.177292
[10:24:56.502] TRAIN: iteration 16004 : loss : 0.047415, loss_ce: 0.002265, loss_dice: 0.092564
[10:24:56.719] TRAIN: iteration 16005 : loss : 0.104909, loss_ce: 0.004657, loss_dice: 0.205160
[10:24:56.929] TRAIN: iteration 16006 : loss : 0.077625, loss_ce: 0.002968, loss_dice: 0.152283
[10:24:57.149] TRAIN: iteration 16007 : loss : 0.117186, loss_ce: 0.004223, loss_dice: 0.230149
[10:24:57.360] TRAIN: iteration 16008 : loss : 0.198106, loss_ce: 0.010905, loss_dice: 0.385306
[10:24:57.574] TRAIN: iteration 16009 : loss : 0.187676, loss_ce: 0.001938, loss_dice: 0.373414
[10:24:57.784] TRAIN: iteration 16010 : loss : 0.251096, loss_ce: 0.002071, loss_dice: 0.500120
[10:24:57.998] TRAIN: iteration 16011 : loss : 0.152218, loss_ce: 0.002867, loss_dice: 0.301570
[10:24:58.209] TRAIN: iteration 16012 : loss : 0.120325, loss_ce: 0.001885, loss_dice: 0.238765
[10:24:58.419] TRAIN: iteration 16013 : loss : 0.157624, loss_ce: 0.003929, loss_dice: 0.311318
[10:24:58.632] TRAIN: iteration 16014 : loss : 0.251091, loss_ce: 0.002066, loss_dice: 0.500117
[10:24:58.842] TRAIN: iteration 16015 : loss : 0.148240, loss_ce: 0.015643, loss_dice: 0.280836
[10:24:59.053] TRAIN: iteration 16016 : loss : 0.085452, loss_ce: 0.002973, loss_dice: 0.167930
[10:24:59.264] TRAIN: iteration 16017 : loss : 0.175899, loss_ce: 0.003204, loss_dice: 0.348594
[10:24:59.474] TRAIN: iteration 16018 : loss : 0.066637, loss_ce: 0.002378, loss_dice: 0.130897
[10:24:59.685] TRAIN: iteration 16019 : loss : 0.133209, loss_ce: 0.003642, loss_dice: 0.262776
[10:24:59.894] TRAIN: iteration 16020 : loss : 0.181592, loss_ce: 0.005454, loss_dice: 0.357730
[10:25:00.131] TRAIN: iteration 16021 : loss : 0.250802, loss_ce: 0.001545, loss_dice: 0.500059
[10:25:00.345] TRAIN: iteration 16022 : loss : 0.092198, loss_ce: 0.002036, loss_dice: 0.182360
[10:25:00.555] TRAIN: iteration 16023 : loss : 0.077807, loss_ce: 0.001438, loss_dice: 0.154176
[10:25:00.771] TRAIN: iteration 16024 : loss : 0.126254, loss_ce: 0.004083, loss_dice: 0.248425
[10:25:00.983] TRAIN: iteration 16025 : loss : 0.069851, loss_ce: 0.003851, loss_dice: 0.135852
[10:25:01.193] TRAIN: iteration 16026 : loss : 0.089550, loss_ce: 0.005717, loss_dice: 0.173384
[10:25:01.403] TRAIN: iteration 16027 : loss : 0.087047, loss_ce: 0.002964, loss_dice: 0.171129
[10:25:01.614] TRAIN: iteration 16028 : loss : 0.062360, loss_ce: 0.003636, loss_dice: 0.121083
[10:25:01.825] TRAIN: iteration 16029 : loss : 0.084126, loss_ce: 0.001655, loss_dice: 0.166596
[10:25:02.050] TRAIN: iteration 16030 : loss : 0.146604, loss_ce: 0.004089, loss_dice: 0.289119
[10:25:02.260] TRAIN: iteration 16031 : loss : 0.144699, loss_ce: 0.001070, loss_dice: 0.288327
[10:25:02.470] TRAIN: iteration 16032 : loss : 0.106260, loss_ce: 0.004282, loss_dice: 0.208238
[10:25:02.680] TRAIN: iteration 16033 : loss : 0.200510, loss_ce: 0.002447, loss_dice: 0.398572
[10:25:02.894] TRAIN: iteration 16034 : loss : 0.225798, loss_ce: 0.004544, loss_dice: 0.447052
[10:25:03.103] TRAIN: iteration 16035 : loss : 0.031171, loss_ce: 0.001208, loss_dice: 0.061133
[10:25:03.313] TRAIN: iteration 16036 : loss : 0.119070, loss_ce: 0.006132, loss_dice: 0.232008
[10:25:03.522] TRAIN: iteration 16037 : loss : 0.247625, loss_ce: 0.001878, loss_dice: 0.493372
[10:25:03.735] TRAIN: iteration 16038 : loss : 0.066790, loss_ce: 0.004826, loss_dice: 0.128754
[10:25:03.948] TRAIN: iteration 16039 : loss : 0.067923, loss_ce: 0.001587, loss_dice: 0.134259
[10:25:04.164] TRAIN: iteration 16040 : loss : 0.051973, loss_ce: 0.004635, loss_dice: 0.099311
[10:25:04.411] TRAIN: iteration 16041 : loss : 0.089685, loss_ce: 0.006132, loss_dice: 0.173239
[10:25:04.622] TRAIN: iteration 16042 : loss : 0.113289, loss_ce: 0.002712, loss_dice: 0.223866
[10:25:04.831] TRAIN: iteration 16043 : loss : 0.049564, loss_ce: 0.005973, loss_dice: 0.093154
[10:25:05.045] TRAIN: iteration 16044 : loss : 0.122810, loss_ce: 0.003654, loss_dice: 0.241965
[10:25:05.257] TRAIN: iteration 16045 : loss : 0.077356, loss_ce: 0.002002, loss_dice: 0.152710
[10:25:05.468] TRAIN: iteration 16046 : loss : 0.047588, loss_ce: 0.002601, loss_dice: 0.092574
[10:25:05.682] TRAIN: iteration 16047 : loss : 0.223224, loss_ce: 0.001259, loss_dice: 0.445189
[10:25:05.895] TRAIN: iteration 16048 : loss : 0.246289, loss_ce: 0.002490, loss_dice: 0.490087
[10:25:06.107] TRAIN: iteration 16049 : loss : 0.153808, loss_ce: 0.005758, loss_dice: 0.301858
[10:25:06.323] TRAIN: iteration 16050 : loss : 0.182357, loss_ce: 0.011093, loss_dice: 0.353620
[10:25:06.532] TRAIN: iteration 16051 : loss : 0.147209, loss_ce: 0.003093, loss_dice: 0.291325
[10:25:06.749] TRAIN: iteration 16052 : loss : 0.095221, loss_ce: 0.003126, loss_dice: 0.187316
[10:25:06.958] TRAIN: iteration 16053 : loss : 0.032890, loss_ce: 0.001143, loss_dice: 0.064636
[10:25:07.167] TRAIN: iteration 16054 : loss : 0.243235, loss_ce: 0.003178, loss_dice: 0.483291
[10:25:07.376] TRAIN: iteration 16055 : loss : 0.077955, loss_ce: 0.007814, loss_dice: 0.148096
[10:25:07.628] TRAIN: iteration 16056 : loss : 0.050287, loss_ce: 0.007056, loss_dice: 0.093518
[10:25:07.838] TRAIN: iteration 16057 : loss : 0.039580, loss_ce: 0.004373, loss_dice: 0.074786
[10:25:08.052] TRAIN: iteration 16058 : loss : 0.049243, loss_ce: 0.003503, loss_dice: 0.094983
[10:25:08.269] TRAIN: iteration 16059 : loss : 0.080595, loss_ce: 0.004062, loss_dice: 0.157128
[10:25:08.483] TRAIN: iteration 16060 : loss : 0.093696, loss_ce: 0.003456, loss_dice: 0.183936
[10:25:08.729] TRAIN: iteration 16061 : loss : 0.070280, loss_ce: 0.004625, loss_dice: 0.135936
[10:25:08.946] TRAIN: iteration 16062 : loss : 0.188362, loss_ce: 0.006866, loss_dice: 0.369858
[10:25:09.162] TRAIN: iteration 16063 : loss : 0.186603, loss_ce: 0.018590, loss_dice: 0.354616
[10:25:09.372] TRAIN: iteration 16064 : loss : 0.072076, loss_ce: 0.004468, loss_dice: 0.139683
[10:25:09.584] TRAIN: iteration 16065 : loss : 0.099565, loss_ce: 0.006603, loss_dice: 0.192527
[10:25:09.792] TRAIN: iteration 16066 : loss : 0.051941, loss_ce: 0.002903, loss_dice: 0.100980
[10:25:10.001] TRAIN: iteration 16067 : loss : 0.250946, loss_ce: 0.001810, loss_dice: 0.500083
[10:25:10.218] TRAIN: iteration 16068 : loss : 0.196329, loss_ce: 0.010264, loss_dice: 0.382393
[10:25:10.429] TRAIN: iteration 16069 : loss : 0.252880, loss_ce: 0.005397, loss_dice: 0.500363
[10:25:10.640] TRAIN: iteration 16070 : loss : 0.175773, loss_ce: 0.003069, loss_dice: 0.348476
[10:25:10.850] TRAIN: iteration 16071 : loss : 0.237309, loss_ce: 0.011387, loss_dice: 0.463230
[10:25:11.059] TRAIN: iteration 16072 : loss : 0.252423, loss_ce: 0.004531, loss_dice: 0.500315
[10:25:11.267] TRAIN: iteration 16073 : loss : 0.235467, loss_ce: 0.007705, loss_dice: 0.463228
[10:25:11.478] TRAIN: iteration 16074 : loss : 0.083380, loss_ce: 0.005816, loss_dice: 0.160944
[10:25:11.690] TRAIN: iteration 16075 : loss : 0.251947, loss_ce: 0.004263, loss_dice: 0.499632
[10:25:11.900] TRAIN: iteration 16076 : loss : 0.135051, loss_ce: 0.003491, loss_dice: 0.266612
[10:25:12.110] TRAIN: iteration 16077 : loss : 0.096529, loss_ce: 0.003359, loss_dice: 0.189698
[10:25:12.319] TRAIN: iteration 16078 : loss : 0.072442, loss_ce: 0.006088, loss_dice: 0.138796
[10:25:12.529] TRAIN: iteration 16079 : loss : 0.056974, loss_ce: 0.003072, loss_dice: 0.110876
[10:25:12.738] TRAIN: iteration 16080 : loss : 0.251394, loss_ce: 0.002632, loss_dice: 0.500156
[10:25:12.986] TRAIN: iteration 16081 : loss : 0.251308, loss_ce: 0.002480, loss_dice: 0.500135
[10:25:13.198] TRAIN: iteration 16082 : loss : 0.097128, loss_ce: 0.002512, loss_dice: 0.191745
[10:25:13.409] TRAIN: iteration 16083 : loss : 0.144431, loss_ce: 0.002070, loss_dice: 0.286791
[10:25:13.620] TRAIN: iteration 16084 : loss : 0.251113, loss_ce: 0.002097, loss_dice: 0.500130
[10:25:13.829] TRAIN: iteration 16085 : loss : 0.042973, loss_ce: 0.002311, loss_dice: 0.083636
[10:25:14.073] TRAIN: iteration 16086 : loss : 0.134143, loss_ce: 0.004438, loss_dice: 0.263848
[10:25:14.283] TRAIN: iteration 16087 : loss : 0.056237, loss_ce: 0.005343, loss_dice: 0.107130
[10:25:14.492] TRAIN: iteration 16088 : loss : 0.251640, loss_ce: 0.005957, loss_dice: 0.497324
[10:25:14.700] TRAIN: iteration 16089 : loss : 0.014223, loss_ce: 0.001634, loss_dice: 0.026813
[10:25:14.940] TRAIN: iteration 16090 : loss : 0.118295, loss_ce: 0.003393, loss_dice: 0.233196
[10:25:15.149] TRAIN: iteration 16091 : loss : 0.155592, loss_ce: 0.003109, loss_dice: 0.308075
[10:25:15.358] TRAIN: iteration 16092 : loss : 0.114714, loss_ce: 0.003682, loss_dice: 0.225746
[10:25:15.575] TRAIN: iteration 16093 : loss : 0.251213, loss_ce: 0.002294, loss_dice: 0.500133
[10:25:15.789] TRAIN: iteration 16094 : loss : 0.052916, loss_ce: 0.005371, loss_dice: 0.100461
[10:25:15.998] TRAIN: iteration 16095 : loss : 0.250797, loss_ce: 0.001504, loss_dice: 0.500090
[10:25:16.209] TRAIN: iteration 16096 : loss : 0.251084, loss_ce: 0.002044, loss_dice: 0.500123
[10:25:16.418] TRAIN: iteration 16097 : loss : 0.132135, loss_ce: 0.001460, loss_dice: 0.262811
[10:25:16.629] TRAIN: iteration 16098 : loss : 0.237464, loss_ce: 0.009310, loss_dice: 0.465617
[10:25:16.838] TRAIN: iteration 16099 : loss : 0.128978, loss_ce: 0.003445, loss_dice: 0.254511
[10:25:17.050] TRAIN: iteration 16100 : loss : 0.059203, loss_ce: 0.005109, loss_dice: 0.113296
[10:25:17.298] TRAIN: iteration 16101 : loss : 0.039939, loss_ce: 0.004090, loss_dice: 0.075789
[10:25:17.519] TRAIN: iteration 16102 : loss : 0.169528, loss_ce: 0.003161, loss_dice: 0.335895
[10:25:17.735] TRAIN: iteration 16103 : loss : 0.025179, loss_ce: 0.001341, loss_dice: 0.049018
[10:25:17.945] TRAIN: iteration 16104 : loss : 0.066021, loss_ce: 0.003497, loss_dice: 0.128545
[10:25:18.153] TRAIN: iteration 16105 : loss : 0.250586, loss_ce: 0.001115, loss_dice: 0.500058
[10:25:19.262] TRAIN: iteration 16106 : loss : 0.104410, loss_ce: 0.024651, loss_dice: 0.184169
[10:25:19.473] TRAIN: iteration 16107 : loss : 0.099116, loss_ce: 0.002086, loss_dice: 0.196146
[10:25:19.682] TRAIN: iteration 16108 : loss : 0.055389, loss_ce: 0.004694, loss_dice: 0.106084
[10:25:19.891] TRAIN: iteration 16109 : loss : 0.097197, loss_ce: 0.005381, loss_dice: 0.189013
[10:25:20.102] TRAIN: iteration 16110 : loss : 0.117768, loss_ce: 0.002099, loss_dice: 0.233437
[10:25:20.318] TRAIN: iteration 16111 : loss : 0.217262, loss_ce: 0.003159, loss_dice: 0.431364
[10:25:20.530] TRAIN: iteration 16112 : loss : 0.078183, loss_ce: 0.005249, loss_dice: 0.151117
[10:25:20.761] TRAIN: iteration 16113 : loss : 0.138044, loss_ce: 0.004379, loss_dice: 0.271708
[10:25:20.970] TRAIN: iteration 16114 : loss : 0.249094, loss_ce: 0.006085, loss_dice: 0.492102
[10:25:21.181] TRAIN: iteration 16115 : loss : 0.047399, loss_ce: 0.002432, loss_dice: 0.092365
[10:25:21.396] TRAIN: iteration 16116 : loss : 0.103037, loss_ce: 0.004208, loss_dice: 0.201867
[10:25:21.611] TRAIN: iteration 16117 : loss : 0.113484, loss_ce: 0.002499, loss_dice: 0.224469
[10:25:21.823] TRAIN: iteration 16118 : loss : 0.051779, loss_ce: 0.004016, loss_dice: 0.099541
[10:25:22.032] TRAIN: iteration 16119 : loss : 0.179108, loss_ce: 0.016475, loss_dice: 0.341740
[10:25:22.241] TRAIN: iteration 16120 : loss : 0.106970, loss_ce: 0.006989, loss_dice: 0.206951
[10:25:22.480] TRAIN: iteration 16121 : loss : 0.194360, loss_ce: 0.002116, loss_dice: 0.386603
[10:25:22.689] TRAIN: iteration 16122 : loss : 0.185026, loss_ce: 0.023491, loss_dice: 0.346562
[10:25:22.899] TRAIN: iteration 16123 : loss : 0.213850, loss_ce: 0.006096, loss_dice: 0.421604
[10:25:23.108] TRAIN: iteration 16124 : loss : 0.190541, loss_ce: 0.004469, loss_dice: 0.376613
[10:25:23.323] TRAIN: iteration 16125 : loss : 0.089199, loss_ce: 0.001990, loss_dice: 0.176408
[10:25:23.540] TRAIN: iteration 16126 : loss : 0.087408, loss_ce: 0.003869, loss_dice: 0.170947
[10:25:23.752] TRAIN: iteration 16127 : loss : 0.251861, loss_ce: 0.003491, loss_dice: 0.500232
[10:25:23.961] TRAIN: iteration 16128 : loss : 0.108225, loss_ce: 0.002340, loss_dice: 0.214109
[10:25:24.186] TRAIN: iteration 16129 : loss : 0.055867, loss_ce: 0.006816, loss_dice: 0.104918
[10:25:24.521] TRAIN: iteration 16130 : loss : 0.207027, loss_ce: 0.002061, loss_dice: 0.411994
[10:25:24.732] TRAIN: iteration 16131 : loss : 0.132102, loss_ce: 0.002060, loss_dice: 0.262144
[10:25:25.558] TRAIN: iteration 16132 : loss : 0.111939, loss_ce: 0.001879, loss_dice: 0.221999
[10:25:25.767] TRAIN: iteration 16133 : loss : 0.238903, loss_ce: 0.001914, loss_dice: 0.475891
[10:25:25.977] TRAIN: iteration 16134 : loss : 0.120946, loss_ce: 0.002038, loss_dice: 0.239854
[10:25:26.188] TRAIN: iteration 16135 : loss : 0.098869, loss_ce: 0.005240, loss_dice: 0.192497
[10:25:26.398] TRAIN: iteration 16136 : loss : 0.079013, loss_ce: 0.002255, loss_dice: 0.155771
[10:25:26.607] TRAIN: iteration 16137 : loss : 0.073040, loss_ce: 0.004219, loss_dice: 0.141861
[10:25:26.823] TRAIN: iteration 16138 : loss : 0.136097, loss_ce: 0.001630, loss_dice: 0.270565
[10:25:27.034] TRAIN: iteration 16139 : loss : 0.227684, loss_ce: 0.002723, loss_dice: 0.452644
[10:25:27.243] TRAIN: iteration 16140 : loss : 0.244224, loss_ce: 0.016847, loss_dice: 0.471600
[10:25:27.484] TRAIN: iteration 16141 : loss : 0.093764, loss_ce: 0.001748, loss_dice: 0.185779
[10:25:27.705] TRAIN: iteration 16142 : loss : 0.084755, loss_ce: 0.004223, loss_dice: 0.165288
[10:25:27.923] TRAIN: iteration 16143 : loss : 0.250157, loss_ce: 0.001476, loss_dice: 0.498838
[10:25:28.135] TRAIN: iteration 16144 : loss : 0.110764, loss_ce: 0.006511, loss_dice: 0.215016
[10:25:28.343] TRAIN: iteration 16145 : loss : 0.212858, loss_ce: 0.003082, loss_dice: 0.422634
[10:25:28.562] TRAIN: iteration 16146 : loss : 0.250830, loss_ce: 0.001608, loss_dice: 0.500052
[10:25:28.775] TRAIN: iteration 16147 : loss : 0.044764, loss_ce: 0.003584, loss_dice: 0.085944
[10:25:28.988] TRAIN: iteration 16148 : loss : 0.213636, loss_ce: 0.006346, loss_dice: 0.420926
[10:25:29.200] TRAIN: iteration 16149 : loss : 0.110648, loss_ce: 0.004047, loss_dice: 0.217248
[10:25:29.416] TRAIN: iteration 16150 : loss : 0.099288, loss_ce: 0.002009, loss_dice: 0.196566
[10:25:29.629] TRAIN: iteration 16151 : loss : 0.207437, loss_ce: 0.004462, loss_dice: 0.410411
[10:25:29.842] TRAIN: iteration 16152 : loss : 0.091944, loss_ce: 0.009718, loss_dice: 0.174171
[10:25:30.052] TRAIN: iteration 16153 : loss : 0.091673, loss_ce: 0.002586, loss_dice: 0.180760
[10:25:30.261] TRAIN: iteration 16154 : loss : 0.184355, loss_ce: 0.011236, loss_dice: 0.357475
[10:25:30.470] TRAIN: iteration 16155 : loss : 0.126574, loss_ce: 0.006671, loss_dice: 0.246476
[10:25:30.679] TRAIN: iteration 16156 : loss : 0.099963, loss_ce: 0.001911, loss_dice: 0.198014
[10:25:30.889] TRAIN: iteration 16157 : loss : 0.048884, loss_ce: 0.004454, loss_dice: 0.093314
[10:25:31.105] TRAIN: iteration 16158 : loss : 0.147602, loss_ce: 0.004173, loss_dice: 0.291031
[10:25:31.339] TRAIN: iteration 16159 : loss : 0.149380, loss_ce: 0.008473, loss_dice: 0.290288
[10:25:31.549] TRAIN: iteration 16160 : loss : 0.087654, loss_ce: 0.002593, loss_dice: 0.172714
[10:25:31.794] TRAIN: iteration 16161 : loss : 0.227978, loss_ce: 0.002344, loss_dice: 0.453613
[10:25:32.002] TRAIN: iteration 16162 : loss : 0.091076, loss_ce: 0.009776, loss_dice: 0.172377
[10:25:32.211] TRAIN: iteration 16163 : loss : 0.251048, loss_ce: 0.002003, loss_dice: 0.500092
[10:25:32.445] TRAIN: iteration 16164 : loss : 0.058678, loss_ce: 0.001942, loss_dice: 0.115415
[10:25:32.656] TRAIN: iteration 16165 : loss : 0.149647, loss_ce: 0.007464, loss_dice: 0.291830
[10:25:32.872] TRAIN: iteration 16166 : loss : 0.163647, loss_ce: 0.001352, loss_dice: 0.325942
[10:25:33.081] TRAIN: iteration 16167 : loss : 0.231807, loss_ce: 0.002185, loss_dice: 0.461428
[10:25:33.291] TRAIN: iteration 16168 : loss : 0.077941, loss_ce: 0.013581, loss_dice: 0.142301
[10:25:33.501] TRAIN: iteration 16169 : loss : 0.077972, loss_ce: 0.009929, loss_dice: 0.146015
[10:25:33.715] TRAIN: iteration 16170 : loss : 0.142316, loss_ce: 0.007217, loss_dice: 0.277414
[10:25:33.929] TRAIN: iteration 16171 : loss : 0.217518, loss_ce: 0.011474, loss_dice: 0.423561
[10:25:34.142] TRAIN: iteration 16172 : loss : 0.159930, loss_ce: 0.003889, loss_dice: 0.315970
[10:25:34.353] TRAIN: iteration 16173 : loss : 0.044972, loss_ce: 0.003350, loss_dice: 0.086593
[10:25:34.568] TRAIN: iteration 16174 : loss : 0.248982, loss_ce: 0.002216, loss_dice: 0.495747
[10:25:35.800] TRAIN: iteration 16175 : loss : 0.074186, loss_ce: 0.006015, loss_dice: 0.142357
[10:25:36.015] TRAIN: iteration 16176 : loss : 0.251124, loss_ce: 0.002116, loss_dice: 0.500131
[10:25:36.224] TRAIN: iteration 16177 : loss : 0.115183, loss_ce: 0.006229, loss_dice: 0.224137
[10:25:36.432] TRAIN: iteration 16178 : loss : 0.073048, loss_ce: 0.003592, loss_dice: 0.142504
[10:25:36.641] TRAIN: iteration 16179 : loss : 0.068715, loss_ce: 0.002916, loss_dice: 0.134513
[10:25:36.850] TRAIN: iteration 16180 : loss : 0.073029, loss_ce: 0.001806, loss_dice: 0.144251
[10:25:37.102] TRAIN: iteration 16181 : loss : 0.063738, loss_ce: 0.003012, loss_dice: 0.124464
[10:25:37.311] TRAIN: iteration 16182 : loss : 0.054570, loss_ce: 0.002492, loss_dice: 0.106647
[10:25:37.521] TRAIN: iteration 16183 : loss : 0.090715, loss_ce: 0.003571, loss_dice: 0.177859
[10:25:37.737] TRAIN: iteration 16184 : loss : 0.132245, loss_ce: 0.001881, loss_dice: 0.262609
[10:25:37.953] TRAIN: iteration 16185 : loss : 0.172482, loss_ce: 0.002737, loss_dice: 0.342228
[10:25:38.164] TRAIN: iteration 16186 : loss : 0.250814, loss_ce: 0.001573, loss_dice: 0.500055
[10:25:38.374] TRAIN: iteration 16187 : loss : 0.096116, loss_ce: 0.002157, loss_dice: 0.190075
[10:25:38.584] TRAIN: iteration 16188 : loss : 0.251492, loss_ce: 0.003018, loss_dice: 0.499965
[10:25:38.793] TRAIN: iteration 16189 : loss : 0.138982, loss_ce: 0.003469, loss_dice: 0.274495
[10:25:39.023] TRAIN: iteration 16190 : loss : 0.250457, loss_ce: 0.000886, loss_dice: 0.500028
[10:25:39.238] TRAIN: iteration 16191 : loss : 0.105693, loss_ce: 0.004618, loss_dice: 0.206767
[10:25:39.447] TRAIN: iteration 16192 : loss : 0.209580, loss_ce: 0.003255, loss_dice: 0.415904
[10:25:39.656] TRAIN: iteration 16193 : loss : 0.049266, loss_ce: 0.001725, loss_dice: 0.096807
[10:25:39.871] TRAIN: iteration 16194 : loss : 0.044686, loss_ce: 0.006022, loss_dice: 0.083350
[10:25:40.085] TRAIN: iteration 16195 : loss : 0.030165, loss_ce: 0.002276, loss_dice: 0.058054
[10:25:40.294] TRAIN: iteration 16196 : loss : 0.052153, loss_ce: 0.002294, loss_dice: 0.102012
[10:25:40.507] TRAIN: iteration 16197 : loss : 0.180419, loss_ce: 0.023101, loss_dice: 0.337737
[10:25:40.718] TRAIN: iteration 16198 : loss : 0.072456, loss_ce: 0.005128, loss_dice: 0.139784
[10:25:40.927] TRAIN: iteration 16199 : loss : 0.251257, loss_ce: 0.002355, loss_dice: 0.500159
[10:25:41.137] TRAIN: iteration 16200 : loss : 0.124942, loss_ce: 0.006539, loss_dice: 0.243346
[10:25:41.370] TRAIN: iteration 16201 : loss : 0.211141, loss_ce: 0.003002, loss_dice: 0.419281
[10:25:41.583] TRAIN: iteration 16202 : loss : 0.210479, loss_ce: 0.006408, loss_dice: 0.414549
[10:25:41.793] TRAIN: iteration 16203 : loss : 0.082227, loss_ce: 0.003805, loss_dice: 0.160649
[10:25:42.007] TRAIN: iteration 16204 : loss : 0.074541, loss_ce: 0.002776, loss_dice: 0.146306
[10:25:42.217] TRAIN: iteration 16205 : loss : 0.253077, loss_ce: 0.005741, loss_dice: 0.500414
[10:25:42.428] TRAIN: iteration 16206 : loss : 0.055679, loss_ce: 0.003293, loss_dice: 0.108066
[10:25:42.657] TRAIN: iteration 16207 : loss : 0.251862, loss_ce: 0.003502, loss_dice: 0.500223
[10:25:42.866] TRAIN: iteration 16208 : loss : 0.098776, loss_ce: 0.003778, loss_dice: 0.193774
[10:25:43.075] TRAIN: iteration 16209 : loss : 0.121692, loss_ce: 0.003014, loss_dice: 0.240369
[10:25:43.285] TRAIN: iteration 16210 : loss : 0.072262, loss_ce: 0.009822, loss_dice: 0.134703
[10:25:43.494] TRAIN: iteration 16211 : loss : 0.192681, loss_ce: 0.005381, loss_dice: 0.379981
[10:25:43.710] TRAIN: iteration 16212 : loss : 0.029279, loss_ce: 0.005111, loss_dice: 0.053446
[10:25:43.920] TRAIN: iteration 16213 : loss : 0.251994, loss_ce: 0.003717, loss_dice: 0.500270
[10:25:44.131] TRAIN: iteration 16214 : loss : 0.212767, loss_ce: 0.002900, loss_dice: 0.422634
[10:25:44.339] TRAIN: iteration 16215 : loss : 0.251297, loss_ce: 0.002435, loss_dice: 0.500160
[10:25:44.549] TRAIN: iteration 16216 : loss : 0.117338, loss_ce: 0.002823, loss_dice: 0.231853
[10:25:44.914] TRAIN: iteration 16217 : loss : 0.096441, loss_ce: 0.002979, loss_dice: 0.189904
[10:25:45.123] TRAIN: iteration 16218 : loss : 0.127029, loss_ce: 0.003246, loss_dice: 0.250812
[10:25:45.332] TRAIN: iteration 16219 : loss : 0.183315, loss_ce: 0.002166, loss_dice: 0.364463
[10:25:45.541] TRAIN: iteration 16220 : loss : 0.251043, loss_ce: 0.001966, loss_dice: 0.500121
[10:25:45.541] NaN or Inf found in input tensor.
[10:25:45.757] TRAIN: iteration 16221 : loss : 0.173062, loss_ce: 0.009932, loss_dice: 0.336191
[10:25:45.970] TRAIN: iteration 16222 : loss : 0.243170, loss_ce: 0.004116, loss_dice: 0.482224
[10:25:46.182] TRAIN: iteration 16223 : loss : 0.124931, loss_ce: 0.004174, loss_dice: 0.245687
[10:25:46.393] TRAIN: iteration 16224 : loss : 0.250523, loss_ce: 0.001023, loss_dice: 0.500024
[10:25:48.593] TRAIN: iteration 16225 : loss : 0.166211, loss_ce: 0.001292, loss_dice: 0.331129
[10:25:48.808] TRAIN: iteration 16226 : loss : 0.169946, loss_ce: 0.004357, loss_dice: 0.335536
[10:25:49.017] TRAIN: iteration 16227 : loss : 0.131199, loss_ce: 0.002426, loss_dice: 0.259973
[10:25:49.226] TRAIN: iteration 16228 : loss : 0.108197, loss_ce: 0.005793, loss_dice: 0.210601
[10:25:49.438] TRAIN: iteration 16229 : loss : 0.094675, loss_ce: 0.004187, loss_dice: 0.185163
[10:25:49.647] TRAIN: iteration 16230 : loss : 0.119218, loss_ce: 0.003512, loss_dice: 0.234923
[10:25:49.857] TRAIN: iteration 16231 : loss : 0.030279, loss_ce: 0.001949, loss_dice: 0.058609
[10:25:50.067] TRAIN: iteration 16232 : loss : 0.031610, loss_ce: 0.003550, loss_dice: 0.059670
[10:25:51.198] TRAIN: iteration 16233 : loss : 0.151240, loss_ce: 0.008079, loss_dice: 0.294401
[10:25:51.409] TRAIN: iteration 16234 : loss : 0.232326, loss_ce: 0.004704, loss_dice: 0.459949
[10:25:51.620] TRAIN: iteration 16235 : loss : 0.071770, loss_ce: 0.004982, loss_dice: 0.138559
[10:25:51.833] TRAIN: iteration 16236 : loss : 0.252624, loss_ce: 0.004901, loss_dice: 0.500347
[10:25:52.048] TRAIN: iteration 16237 : loss : 0.127704, loss_ce: 0.004372, loss_dice: 0.251035
[10:25:52.266] TRAIN: iteration 16238 : loss : 0.197356, loss_ce: 0.005726, loss_dice: 0.388985
[10:25:52.477] TRAIN: iteration 16239 : loss : 0.046022, loss_ce: 0.004054, loss_dice: 0.087990
[10:25:52.688] TRAIN: iteration 16240 : loss : 0.041357, loss_ce: 0.008040, loss_dice: 0.074674
[10:25:52.943] TRAIN: iteration 16241 : loss : 0.251331, loss_ce: 0.002517, loss_dice: 0.500144
[10:25:53.154] TRAIN: iteration 16242 : loss : 0.071611, loss_ce: 0.013738, loss_dice: 0.129485
[10:25:53.363] TRAIN: iteration 16243 : loss : 0.104390, loss_ce: 0.009249, loss_dice: 0.199530
[10:25:53.575] TRAIN: iteration 16244 : loss : 0.163266, loss_ce: 0.004091, loss_dice: 0.322440
[10:25:53.787] TRAIN: iteration 16245 : loss : 0.120452, loss_ce: 0.026929, loss_dice: 0.213975
[10:25:53.999] TRAIN: iteration 16246 : loss : 0.251598, loss_ce: 0.003017, loss_dice: 0.500179
[10:25:54.208] TRAIN: iteration 16247 : loss : 0.100603, loss_ce: 0.005051, loss_dice: 0.196155
[10:25:54.421] TRAIN: iteration 16248 : loss : 0.189092, loss_ce: 0.005158, loss_dice: 0.373025
[10:25:54.973] TRAIN: iteration 16249 : loss : 0.071405, loss_ce: 0.002865, loss_dice: 0.139944
[10:25:55.182] TRAIN: iteration 16250 : loss : 0.120216, loss_ce: 0.008795, loss_dice: 0.231638
[10:25:55.395] TRAIN: iteration 16251 : loss : 0.091189, loss_ce: 0.006161, loss_dice: 0.176216
[10:25:55.606] TRAIN: iteration 16252 : loss : 0.094098, loss_ce: 0.002524, loss_dice: 0.185672
[10:25:55.822] TRAIN: iteration 16253 : loss : 0.142347, loss_ce: 0.004633, loss_dice: 0.280061
[10:25:56.030] TRAIN: iteration 16254 : loss : 0.069296, loss_ce: 0.006739, loss_dice: 0.131854
[10:25:56.340] TRAIN: iteration 16255 : loss : 0.109996, loss_ce: 0.005535, loss_dice: 0.214458
[10:25:56.549] TRAIN: iteration 16256 : loss : 0.091690, loss_ce: 0.003933, loss_dice: 0.179446
[10:25:56.760] TRAIN: iteration 16257 : loss : 0.078062, loss_ce: 0.001747, loss_dice: 0.154376
[10:25:56.973] TRAIN: iteration 16258 : loss : 0.218312, loss_ce: 0.001735, loss_dice: 0.434889
[10:25:57.182] TRAIN: iteration 16259 : loss : 0.075509, loss_ce: 0.003509, loss_dice: 0.147509
[10:25:57.556] TRAIN: iteration 16260 : loss : 0.073335, loss_ce: 0.002892, loss_dice: 0.143778
[10:25:57.789] TRAIN: iteration 16261 : loss : 0.051853, loss_ce: 0.003072, loss_dice: 0.100633
[10:25:58.006] TRAIN: iteration 16262 : loss : 0.139404, loss_ce: 0.009156, loss_dice: 0.269651
[10:25:58.231] TRAIN: iteration 16263 : loss : 0.086851, loss_ce: 0.012141, loss_dice: 0.161561
[10:25:58.441] TRAIN: iteration 16264 : loss : 0.110368, loss_ce: 0.005208, loss_dice: 0.215529
[10:25:58.652] TRAIN: iteration 16265 : loss : 0.070800, loss_ce: 0.003572, loss_dice: 0.138028
[10:25:58.867] TRAIN: iteration 16266 : loss : 0.094779, loss_ce: 0.002538, loss_dice: 0.187020
[10:25:59.078] TRAIN: iteration 16267 : loss : 0.153521, loss_ce: 0.007231, loss_dice: 0.299810
[10:26:00.505] TRAIN: iteration 16268 : loss : 0.038761, loss_ce: 0.001973, loss_dice: 0.075550
[10:26:00.715] TRAIN: iteration 16269 : loss : 0.250453, loss_ce: 0.003055, loss_dice: 0.497852
[10:26:00.924] TRAIN: iteration 16270 : loss : 0.119256, loss_ce: 0.007365, loss_dice: 0.231147
[10:26:01.134] TRAIN: iteration 16271 : loss : 0.251249, loss_ce: 0.002365, loss_dice: 0.500133
[10:26:01.343] TRAIN: iteration 16272 : loss : 0.070312, loss_ce: 0.002137, loss_dice: 0.138487
[10:26:01.552] TRAIN: iteration 16273 : loss : 0.074561, loss_ce: 0.003717, loss_dice: 0.145405
[10:26:01.762] TRAIN: iteration 16274 : loss : 0.109263, loss_ce: 0.005390, loss_dice: 0.213136
[10:26:01.973] TRAIN: iteration 16275 : loss : 0.251185, loss_ce: 0.002224, loss_dice: 0.500146
[10:26:02.181] TRAIN: iteration 16276 : loss : 0.127551, loss_ce: 0.002949, loss_dice: 0.252153
[10:26:02.392] TRAIN: iteration 16277 : loss : 0.079283, loss_ce: 0.002715, loss_dice: 0.155851
[10:26:02.603] TRAIN: iteration 16278 : loss : 0.068446, loss_ce: 0.001563, loss_dice: 0.135329
[10:26:02.813] TRAIN: iteration 16279 : loss : 0.227182, loss_ce: 0.002360, loss_dice: 0.452003
[10:26:03.024] TRAIN: iteration 16280 : loss : 0.133619, loss_ce: 0.004217, loss_dice: 0.263021
[10:26:03.262] TRAIN: iteration 16281 : loss : 0.250660, loss_ce: 0.001262, loss_dice: 0.500059
[10:26:03.473] TRAIN: iteration 16282 : loss : 0.188313, loss_ce: 0.002635, loss_dice: 0.373991
[10:26:03.682] TRAIN: iteration 16283 : loss : 0.066400, loss_ce: 0.001444, loss_dice: 0.131355
[10:26:04.204] TRAIN: iteration 16284 : loss : 0.245548, loss_ce: 0.002809, loss_dice: 0.488288
[10:26:04.414] TRAIN: iteration 16285 : loss : 0.129856, loss_ce: 0.002736, loss_dice: 0.256976
[10:26:04.626] TRAIN: iteration 16286 : loss : 0.053267, loss_ce: 0.001590, loss_dice: 0.104944
[10:26:04.848] TRAIN: iteration 16287 : loss : 0.231974, loss_ce: 0.001330, loss_dice: 0.462617
[10:26:05.063] TRAIN: iteration 16288 : loss : 0.250489, loss_ce: 0.000947, loss_dice: 0.500032
[10:26:05.274] TRAIN: iteration 16289 : loss : 0.067014, loss_ce: 0.003704, loss_dice: 0.130323
[10:26:05.484] TRAIN: iteration 16290 : loss : 0.122091, loss_ce: 0.001696, loss_dice: 0.242485
[10:26:05.695] TRAIN: iteration 16291 : loss : 0.132810, loss_ce: 0.006786, loss_dice: 0.258833
[10:26:05.911] TRAIN: iteration 16292 : loss : 0.089709, loss_ce: 0.009444, loss_dice: 0.169974
[10:26:06.125] TRAIN: iteration 16293 : loss : 0.250894, loss_ce: 0.001699, loss_dice: 0.500089
[10:26:06.337] TRAIN: iteration 16294 : loss : 0.200581, loss_ce: 0.026958, loss_dice: 0.374204
[10:26:06.546] TRAIN: iteration 16295 : loss : 0.066816, loss_ce: 0.003518, loss_dice: 0.130115
[10:26:06.756] TRAIN: iteration 16296 : loss : 0.039481, loss_ce: 0.004505, loss_dice: 0.074457
[10:26:06.966] TRAIN: iteration 16297 : loss : 0.048348, loss_ce: 0.001635, loss_dice: 0.095062
[10:26:07.176] TRAIN: iteration 16298 : loss : 0.155542, loss_ce: 0.002318, loss_dice: 0.308766
[10:26:07.559] TRAIN: iteration 16299 : loss : 0.189822, loss_ce: 0.003302, loss_dice: 0.376342
[10:26:07.768] TRAIN: iteration 16300 : loss : 0.085897, loss_ce: 0.002276, loss_dice: 0.169518
[10:26:08.006] TRAIN: iteration 16301 : loss : 0.187328, loss_ce: 0.003544, loss_dice: 0.371112
[10:26:08.217] TRAIN: iteration 16302 : loss : 0.074065, loss_ce: 0.005102, loss_dice: 0.143027
[10:26:08.428] TRAIN: iteration 16303 : loss : 0.250905, loss_ce: 0.001733, loss_dice: 0.500077
[10:26:08.638] TRAIN: iteration 16304 : loss : 0.089256, loss_ce: 0.003048, loss_dice: 0.175464
[10:26:08.848] TRAIN: iteration 16305 : loss : 0.061121, loss_ce: 0.004196, loss_dice: 0.118046
[10:26:09.065] TRAIN: iteration 16306 : loss : 0.121923, loss_ce: 0.002760, loss_dice: 0.241087
[10:26:09.274] TRAIN: iteration 16307 : loss : 0.057710, loss_ce: 0.003624, loss_dice: 0.111795
[10:26:09.483] TRAIN: iteration 16308 : loss : 0.222340, loss_ce: 0.014140, loss_dice: 0.430541
[10:26:09.704] TRAIN: iteration 16309 : loss : 0.126327, loss_ce: 0.003037, loss_dice: 0.249618
[10:26:09.920] TRAIN: iteration 16310 : loss : 0.056955, loss_ce: 0.004848, loss_dice: 0.109061
[10:26:10.129] TRAIN: iteration 16311 : loss : 0.092858, loss_ce: 0.002418, loss_dice: 0.183298
[10:26:10.338] TRAIN: iteration 16312 : loss : 0.164775, loss_ce: 0.002970, loss_dice: 0.326580
[10:26:10.551] TRAIN: iteration 16313 : loss : 0.246047, loss_ce: 0.009263, loss_dice: 0.482831
[10:26:10.763] TRAIN: iteration 16314 : loss : 0.085666, loss_ce: 0.006817, loss_dice: 0.164515
[10:26:10.971] TRAIN: iteration 16315 : loss : 0.252911, loss_ce: 0.006116, loss_dice: 0.499707
[10:26:11.191] TRAIN: iteration 16316 : loss : 0.076239, loss_ce: 0.001961, loss_dice: 0.150517
[10:26:11.399] TRAIN: iteration 16317 : loss : 0.250868, loss_ce: 0.002737, loss_dice: 0.499000
[10:26:11.608] TRAIN: iteration 16318 : loss : 0.230044, loss_ce: 0.021126, loss_dice: 0.438963
[10:26:11.817] TRAIN: iteration 16319 : loss : 0.071676, loss_ce: 0.003493, loss_dice: 0.139858
[10:26:12.026] TRAIN: iteration 16320 : loss : 0.216813, loss_ce: 0.002678, loss_dice: 0.430948
[10:26:12.268] TRAIN: iteration 16321 : loss : 0.218004, loss_ce: 0.002078, loss_dice: 0.433930
[10:26:12.478] TRAIN: iteration 16322 : loss : 0.160571, loss_ce: 0.008487, loss_dice: 0.312655
[10:26:12.687] TRAIN: iteration 16323 : loss : 0.043922, loss_ce: 0.001486, loss_dice: 0.086359
[10:26:12.897] TRAIN: iteration 16324 : loss : 0.060412, loss_ce: 0.002425, loss_dice: 0.118399
[10:26:13.108] TRAIN: iteration 16325 : loss : 0.065647, loss_ce: 0.002162, loss_dice: 0.129132
[10:26:13.319] TRAIN: iteration 16326 : loss : 0.056396, loss_ce: 0.004694, loss_dice: 0.108097
[10:26:13.531] TRAIN: iteration 16327 : loss : 0.029252, loss_ce: 0.001218, loss_dice: 0.057286
[10:26:13.742] TRAIN: iteration 16328 : loss : 0.165972, loss_ce: 0.004383, loss_dice: 0.327561
[10:26:13.951] TRAIN: iteration 16329 : loss : 0.124616, loss_ce: 0.003251, loss_dice: 0.245982
[10:26:14.160] TRAIN: iteration 16330 : loss : 0.059852, loss_ce: 0.011127, loss_dice: 0.108577
[10:26:14.384] TRAIN: iteration 16331 : loss : 0.226087, loss_ce: 0.006546, loss_dice: 0.445628
[10:26:14.594] TRAIN: iteration 16332 : loss : 0.251335, loss_ce: 0.004528, loss_dice: 0.498143
[10:26:14.806] TRAIN: iteration 16333 : loss : 0.126819, loss_ce: 0.005759, loss_dice: 0.247879
[10:26:15.015] TRAIN: iteration 16334 : loss : 0.207781, loss_ce: 0.007080, loss_dice: 0.408482
[10:26:15.224] TRAIN: iteration 16335 : loss : 0.251644, loss_ce: 0.003079, loss_dice: 0.500208
[10:26:15.435] TRAIN: iteration 16336 : loss : 0.168315, loss_ce: 0.002329, loss_dice: 0.334301
[10:26:15.651] TRAIN: iteration 16337 : loss : 0.098528, loss_ce: 0.002814, loss_dice: 0.194242
[10:26:15.861] TRAIN: iteration 16338 : loss : 0.101675, loss_ce: 0.007429, loss_dice: 0.195921
[10:26:17.224] TRAIN: iteration 16339 : loss : 0.158366, loss_ce: 0.006466, loss_dice: 0.310266
[10:26:17.433] TRAIN: iteration 16340 : loss : 0.120934, loss_ce: 0.008418, loss_dice: 0.233450
[10:26:17.678] TRAIN: iteration 16341 : loss : 0.152896, loss_ce: 0.015569, loss_dice: 0.290222
[10:26:17.890] TRAIN: iteration 16342 : loss : 0.083799, loss_ce: 0.004680, loss_dice: 0.162918
[10:26:18.103] TRAIN: iteration 16343 : loss : 0.045756, loss_ce: 0.008797, loss_dice: 0.082715
[10:26:18.322] TRAIN: iteration 16344 : loss : 0.034762, loss_ce: 0.002097, loss_dice: 0.067427
[10:26:18.540] TRAIN: iteration 16345 : loss : 0.069237, loss_ce: 0.002502, loss_dice: 0.135972
[10:26:18.750] TRAIN: iteration 16346 : loss : 0.110226, loss_ce: 0.001695, loss_dice: 0.218757
[10:26:18.959] TRAIN: iteration 16347 : loss : 0.053744, loss_ce: 0.001893, loss_dice: 0.105594
[10:26:19.168] TRAIN: iteration 16348 : loss : 0.197965, loss_ce: 0.003268, loss_dice: 0.392662
[10:26:19.384] TRAIN: iteration 16349 : loss : 0.143920, loss_ce: 0.002467, loss_dice: 0.285372
[10:26:19.594] TRAIN: iteration 16350 : loss : 0.096012, loss_ce: 0.006064, loss_dice: 0.185961
[10:26:19.803] TRAIN: iteration 16351 : loss : 0.066830, loss_ce: 0.005358, loss_dice: 0.128303
[10:26:20.013] TRAIN: iteration 16352 : loss : 0.067480, loss_ce: 0.004336, loss_dice: 0.130624
[10:26:21.067] TRAIN: iteration 16353 : loss : 0.069303, loss_ce: 0.007144, loss_dice: 0.131461
[10:26:21.282] TRAIN: iteration 16354 : loss : 0.079278, loss_ce: 0.003304, loss_dice: 0.155253
[10:26:21.496] TRAIN: iteration 16355 : loss : 0.131127, loss_ce: 0.026109, loss_dice: 0.236145
[10:26:21.707] TRAIN: iteration 16356 : loss : 0.250849, loss_ce: 0.001610, loss_dice: 0.500088
[10:26:21.917] TRAIN: iteration 16357 : loss : 0.244683, loss_ce: 0.001902, loss_dice: 0.487464
[10:26:22.146] TRAIN: iteration 16358 : loss : 0.194484, loss_ce: 0.025203, loss_dice: 0.363766
[10:26:22.357] TRAIN: iteration 16359 : loss : 0.096670, loss_ce: 0.006451, loss_dice: 0.186890
[10:26:22.573] TRAIN: iteration 16360 : loss : 0.152978, loss_ce: 0.001290, loss_dice: 0.304666
[10:26:22.811] TRAIN: iteration 16361 : loss : 0.059840, loss_ce: 0.003959, loss_dice: 0.115722
[10:26:23.024] TRAIN: iteration 16362 : loss : 0.039131, loss_ce: 0.002958, loss_dice: 0.075303
[10:26:23.235] TRAIN: iteration 16363 : loss : 0.053653, loss_ce: 0.001806, loss_dice: 0.105500
[10:26:23.445] TRAIN: iteration 16364 : loss : 0.215905, loss_ce: 0.005321, loss_dice: 0.426488
[10:26:23.655] TRAIN: iteration 16365 : loss : 0.159480, loss_ce: 0.005557, loss_dice: 0.313403
[10:26:23.865] TRAIN: iteration 16366 : loss : 0.068586, loss_ce: 0.002625, loss_dice: 0.134547
[10:26:24.079] TRAIN: iteration 16367 : loss : 0.046476, loss_ce: 0.002458, loss_dice: 0.090494
[10:26:24.291] TRAIN: iteration 16368 : loss : 0.126807, loss_ce: 0.004160, loss_dice: 0.249454
[10:26:24.503] TRAIN: iteration 16369 : loss : 0.169444, loss_ce: 0.009648, loss_dice: 0.329240
[10:26:24.718] TRAIN: iteration 16370 : loss : 0.066902, loss_ce: 0.002001, loss_dice: 0.131803
[10:26:24.927] TRAIN: iteration 16371 : loss : 0.228836, loss_ce: 0.013389, loss_dice: 0.444283
[10:26:25.136] TRAIN: iteration 16372 : loss : 0.168473, loss_ce: 0.004307, loss_dice: 0.332640
[10:26:25.347] TRAIN: iteration 16373 : loss : 0.251640, loss_ce: 0.003067, loss_dice: 0.500213
[10:26:25.557] TRAIN: iteration 16374 : loss : 0.203850, loss_ce: 0.003419, loss_dice: 0.404280
[10:26:25.772] TRAIN: iteration 16375 : loss : 0.131203, loss_ce: 0.005525, loss_dice: 0.256881
[10:26:25.987] TRAIN: iteration 16376 : loss : 0.081957, loss_ce: 0.003867, loss_dice: 0.160046
[10:26:26.197] TRAIN: iteration 16377 : loss : 0.084499, loss_ce: 0.006630, loss_dice: 0.162368
[10:26:26.405] TRAIN: iteration 16378 : loss : 0.227637, loss_ce: 0.006574, loss_dice: 0.448700
[10:26:26.614] TRAIN: iteration 16379 : loss : 0.249867, loss_ce: 0.002319, loss_dice: 0.497416
[10:26:26.825] TRAIN: iteration 16380 : loss : 0.107468, loss_ce: 0.006529, loss_dice: 0.208407
[10:26:27.064] TRAIN: iteration 16381 : loss : 0.245967, loss_ce: 0.004119, loss_dice: 0.487815
[10:26:27.278] TRAIN: iteration 16382 : loss : 0.041701, loss_ce: 0.002586, loss_dice: 0.080816
[10:26:27.487] TRAIN: iteration 16383 : loss : 0.025321, loss_ce: 0.002197, loss_dice: 0.048445
[10:26:27.697] TRAIN: iteration 16384 : loss : 0.075525, loss_ce: 0.003615, loss_dice: 0.147436
[10:26:27.907] TRAIN: iteration 16385 : loss : 0.043262, loss_ce: 0.002340, loss_dice: 0.084185
[10:26:28.124] TRAIN: iteration 16386 : loss : 0.072421, loss_ce: 0.002347, loss_dice: 0.142494
[10:26:28.830] TRAIN: iteration 16387 : loss : 0.132647, loss_ce: 0.001828, loss_dice: 0.263466
[10:26:29.043] TRAIN: iteration 16388 : loss : 0.251189, loss_ce: 0.002259, loss_dice: 0.500118
[10:26:29.259] TRAIN: iteration 16389 : loss : 0.094963, loss_ce: 0.003272, loss_dice: 0.186655
[10:26:29.473] TRAIN: iteration 16390 : loss : 0.139801, loss_ce: 0.005319, loss_dice: 0.274284
[10:26:29.690] TRAIN: iteration 16391 : loss : 0.185714, loss_ce: 0.006584, loss_dice: 0.364843
[10:26:29.904] TRAIN: iteration 16392 : loss : 0.124029, loss_ce: 0.001950, loss_dice: 0.246107
[10:26:30.115] TRAIN: iteration 16393 : loss : 0.250616, loss_ce: 0.003761, loss_dice: 0.497470
[10:26:30.325] TRAIN: iteration 16394 : loss : 0.204034, loss_ce: 0.002738, loss_dice: 0.405330
[10:26:30.539] TRAIN: iteration 16395 : loss : 0.104915, loss_ce: 0.002656, loss_dice: 0.207173
[10:26:30.748] TRAIN: iteration 16396 : loss : 0.139307, loss_ce: 0.012027, loss_dice: 0.266586
[10:26:30.960] TRAIN: iteration 16397 : loss : 0.133065, loss_ce: 0.001879, loss_dice: 0.264250
[10:26:31.173] TRAIN: iteration 16398 : loss : 0.075473, loss_ce: 0.004230, loss_dice: 0.146715
[10:26:31.385] TRAIN: iteration 16399 : loss : 0.049929, loss_ce: 0.002285, loss_dice: 0.097574
[10:26:31.841] TRAIN: iteration 16400 : loss : 0.071447, loss_ce: 0.013586, loss_dice: 0.129308
[10:26:32.079] TRAIN: iteration 16401 : loss : 0.155667, loss_ce: 0.006163, loss_dice: 0.305172
[10:26:32.289] TRAIN: iteration 16402 : loss : 0.232376, loss_ce: 0.004143, loss_dice: 0.460608
[10:26:32.499] TRAIN: iteration 16403 : loss : 0.208621, loss_ce: 0.010987, loss_dice: 0.406256
[10:26:32.710] TRAIN: iteration 16404 : loss : 0.251081, loss_ce: 0.003133, loss_dice: 0.499030
[10:26:32.918] TRAIN: iteration 16405 : loss : 0.252258, loss_ce: 0.004800, loss_dice: 0.499716
[10:26:33.130] TRAIN: iteration 16406 : loss : 0.098210, loss_ce: 0.008812, loss_dice: 0.187607
[10:26:33.364] TRAIN: iteration 16407 : loss : 0.135042, loss_ce: 0.005594, loss_dice: 0.264491
[10:26:34.729] TRAIN: iteration 16408 : loss : 0.251209, loss_ce: 0.002317, loss_dice: 0.500101
[10:26:34.942] TRAIN: iteration 16409 : loss : 0.251882, loss_ce: 0.003845, loss_dice: 0.499920
[10:26:35.152] TRAIN: iteration 16410 : loss : 0.250194, loss_ce: 0.004091, loss_dice: 0.496297
[10:26:35.364] TRAIN: iteration 16411 : loss : 0.230842, loss_ce: 0.005995, loss_dice: 0.455689
[10:26:35.575] TRAIN: iteration 16412 : loss : 0.098575, loss_ce: 0.006631, loss_dice: 0.190520
[10:26:35.785] TRAIN: iteration 16413 : loss : 0.251894, loss_ce: 0.003562, loss_dice: 0.500226
[10:26:35.994] TRAIN: iteration 16414 : loss : 0.251460, loss_ce: 0.002755, loss_dice: 0.500165
[10:26:36.204] TRAIN: iteration 16415 : loss : 0.226023, loss_ce: 0.009953, loss_dice: 0.442094
[10:26:36.415] TRAIN: iteration 16416 : loss : 0.032446, loss_ce: 0.006422, loss_dice: 0.058469
[10:26:36.626] TRAIN: iteration 16417 : loss : 0.059155, loss_ce: 0.003875, loss_dice: 0.114436
[10:26:36.835] TRAIN: iteration 16418 : loss : 0.251459, loss_ce: 0.002752, loss_dice: 0.500166
[10:26:37.044] TRAIN: iteration 16419 : loss : 0.102310, loss_ce: 0.003862, loss_dice: 0.200759
[10:26:37.263] TRAIN: iteration 16420 : loss : 0.058452, loss_ce: 0.005067, loss_dice: 0.111837
[10:26:37.264] NaN or Inf found in input tensor.
[10:26:37.484] TRAIN: iteration 16421 : loss : 0.081932, loss_ce: 0.002077, loss_dice: 0.161787
[10:26:37.693] TRAIN: iteration 16422 : loss : 0.250450, loss_ce: 0.003014, loss_dice: 0.497887
[10:26:37.902] TRAIN: iteration 16423 : loss : 0.139546, loss_ce: 0.007500, loss_dice: 0.271593
[10:26:38.126] TRAIN: iteration 16424 : loss : 0.104165, loss_ce: 0.002458, loss_dice: 0.205871
[10:26:38.335] TRAIN: iteration 16425 : loss : 0.081044, loss_ce: 0.002419, loss_dice: 0.159669
[10:26:38.544] TRAIN: iteration 16426 : loss : 0.251653, loss_ce: 0.003106, loss_dice: 0.500201
[10:26:38.754] TRAIN: iteration 16427 : loss : 0.250841, loss_ce: 0.001612, loss_dice: 0.500071
[10:26:38.963] TRAIN: iteration 16428 : loss : 0.164541, loss_ce: 0.002551, loss_dice: 0.326530
[10:26:39.172] TRAIN: iteration 16429 : loss : 0.056843, loss_ce: 0.003047, loss_dice: 0.110638
[10:26:39.381] TRAIN: iteration 16430 : loss : 0.050419, loss_ce: 0.006298, loss_dice: 0.094539
[10:26:39.677] TRAIN: iteration 16431 : loss : 0.068584, loss_ce: 0.002086, loss_dice: 0.135083
[10:26:39.888] TRAIN: iteration 16432 : loss : 0.238224, loss_ce: 0.009082, loss_dice: 0.467366
[10:26:40.099] TRAIN: iteration 16433 : loss : 0.168608, loss_ce: 0.008205, loss_dice: 0.329011
[10:26:40.309] TRAIN: iteration 16434 : loss : 0.251392, loss_ce: 0.002610, loss_dice: 0.500174
[10:26:40.518] TRAIN: iteration 16435 : loss : 0.251294, loss_ce: 0.002428, loss_dice: 0.500160
[10:26:40.729] TRAIN: iteration 16436 : loss : 0.080894, loss_ce: 0.005957, loss_dice: 0.155831
[10:26:40.942] TRAIN: iteration 16437 : loss : 0.131285, loss_ce: 0.004430, loss_dice: 0.258140
[10:26:41.161] TRAIN: iteration 16438 : loss : 0.014090, loss_ce: 0.001058, loss_dice: 0.027122
[10:26:43.507] TRAIN: iteration 16439 : loss : 0.197851, loss_ce: 0.009413, loss_dice: 0.386290
[10:26:43.722] TRAIN: iteration 16440 : loss : 0.140725, loss_ce: 0.011460, loss_dice: 0.269991
[10:26:43.964] TRAIN: iteration 16441 : loss : 0.084546, loss_ce: 0.003753, loss_dice: 0.165340
[10:26:44.173] TRAIN: iteration 16442 : loss : 0.146151, loss_ce: 0.025649, loss_dice: 0.266654
[10:26:44.382] TRAIN: iteration 16443 : loss : 0.109284, loss_ce: 0.004443, loss_dice: 0.214125
[10:26:44.592] TRAIN: iteration 16444 : loss : 0.054787, loss_ce: 0.002798, loss_dice: 0.106777
[10:26:44.802] TRAIN: iteration 16445 : loss : 0.235022, loss_ce: 0.001907, loss_dice: 0.468138
[10:26:45.013] TRAIN: iteration 16446 : loss : 0.214054, loss_ce: 0.003016, loss_dice: 0.425091
[10:26:45.223] TRAIN: iteration 16447 : loss : 0.073171, loss_ce: 0.003381, loss_dice: 0.142960
[10:26:45.433] TRAIN: iteration 16448 : loss : 0.042887, loss_ce: 0.003059, loss_dice: 0.082714
[10:26:45.643] TRAIN: iteration 16449 : loss : 0.053859, loss_ce: 0.003095, loss_dice: 0.104623
[10:26:45.853] TRAIN: iteration 16450 : loss : 0.086029, loss_ce: 0.005755, loss_dice: 0.166303
[10:26:46.101] TRAIN: iteration 16451 : loss : 0.081730, loss_ce: 0.004346, loss_dice: 0.159115
[10:26:46.316] TRAIN: iteration 16452 : loss : 0.110641, loss_ce: 0.008192, loss_dice: 0.213090
[10:26:46.525] TRAIN: iteration 16453 : loss : 0.239064, loss_ce: 0.004128, loss_dice: 0.474000
[10:26:46.735] TRAIN: iteration 16454 : loss : 0.021857, loss_ce: 0.002109, loss_dice: 0.041605
[10:26:46.946] TRAIN: iteration 16455 : loss : 0.225449, loss_ce: 0.003434, loss_dice: 0.447465
[10:26:47.156] TRAIN: iteration 16456 : loss : 0.063717, loss_ce: 0.009638, loss_dice: 0.117797
[10:26:47.374] TRAIN: iteration 16457 : loss : 0.126426, loss_ce: 0.002648, loss_dice: 0.250204
[10:26:47.586] TRAIN: iteration 16458 : loss : 0.031959, loss_ce: 0.001625, loss_dice: 0.062293
[10:26:47.795] TRAIN: iteration 16459 : loss : 0.251518, loss_ce: 0.002882, loss_dice: 0.500155
[10:26:48.003] TRAIN: iteration 16460 : loss : 0.151706, loss_ce: 0.002291, loss_dice: 0.301120
[10:26:48.224] TRAIN: iteration 16461 : loss : 0.109092, loss_ce: 0.007938, loss_dice: 0.210245
[10:26:48.433] TRAIN: iteration 16462 : loss : 0.025795, loss_ce: 0.001436, loss_dice: 0.050154
[10:26:48.647] TRAIN: iteration 16463 : loss : 0.251599, loss_ce: 0.003008, loss_dice: 0.500190
[10:26:48.857] TRAIN: iteration 16464 : loss : 0.042230, loss_ce: 0.002229, loss_dice: 0.082232
[10:26:49.070] TRAIN: iteration 16465 : loss : 0.124470, loss_ce: 0.009935, loss_dice: 0.239006
[10:26:49.290] TRAIN: iteration 16466 : loss : 0.244479, loss_ce: 0.011847, loss_dice: 0.477110
[10:26:49.499] TRAIN: iteration 16467 : loss : 0.250769, loss_ce: 0.003031, loss_dice: 0.498507
[10:26:53.209] TRAIN: iteration 16468 : loss : 0.251007, loss_ce: 0.001955, loss_dice: 0.500058
[10:26:53.416] TRAIN: iteration 16469 : loss : 0.239866, loss_ce: 0.004795, loss_dice: 0.474936
[10:26:53.625] TRAIN: iteration 16470 : loss : 0.212698, loss_ce: 0.003987, loss_dice: 0.421409
[10:26:53.835] TRAIN: iteration 16471 : loss : 0.105700, loss_ce: 0.004173, loss_dice: 0.207226
[10:26:54.044] TRAIN: iteration 16472 : loss : 0.153930, loss_ce: 0.003753, loss_dice: 0.304107
[10:26:54.254] TRAIN: iteration 16473 : loss : 0.138371, loss_ce: 0.003606, loss_dice: 0.273137
[10:26:54.507] TRAIN: iteration 16474 : loss : 0.253174, loss_ce: 0.018042, loss_dice: 0.488307
[10:26:54.725] TRAIN: iteration 16475 : loss : 0.092373, loss_ce: 0.005397, loss_dice: 0.179350
[10:26:54.934] TRAIN: iteration 16476 : loss : 0.251208, loss_ce: 0.002315, loss_dice: 0.500101
[10:26:55.144] TRAIN: iteration 16477 : loss : 0.170964, loss_ce: 0.002293, loss_dice: 0.339635
[10:26:55.359] TRAIN: iteration 16478 : loss : 0.050275, loss_ce: 0.002153, loss_dice: 0.098397
[10:26:55.578] TRAIN: iteration 16479 : loss : 0.094182, loss_ce: 0.007500, loss_dice: 0.180864
[10:26:55.787] TRAIN: iteration 16480 : loss : 0.113404, loss_ce: 0.006801, loss_dice: 0.220007
[10:26:56.028] TRAIN: iteration 16481 : loss : 0.152685, loss_ce: 0.003461, loss_dice: 0.301909
[10:26:56.238] TRAIN: iteration 16482 : loss : 0.125488, loss_ce: 0.002043, loss_dice: 0.248932
[10:26:56.447] TRAIN: iteration 16483 : loss : 0.250910, loss_ce: 0.001773, loss_dice: 0.500046
[10:26:56.658] TRAIN: iteration 16484 : loss : 0.114375, loss_ce: 0.008658, loss_dice: 0.220092
[10:26:56.872] TRAIN: iteration 16485 : loss : 0.176332, loss_ce: 0.006530, loss_dice: 0.346135
[10:26:57.084] TRAIN: iteration 16486 : loss : 0.110883, loss_ce: 0.002364, loss_dice: 0.219401
[10:26:57.296] TRAIN: iteration 16487 : loss : 0.093178, loss_ce: 0.004168, loss_dice: 0.182188
[10:26:57.511] TRAIN: iteration 16488 : loss : 0.066493, loss_ce: 0.002215, loss_dice: 0.130771
[10:26:59.123] TRAIN: iteration 16489 : loss : 0.251144, loss_ce: 0.002165, loss_dice: 0.500122
[10:26:59.336] TRAIN: iteration 16490 : loss : 0.131848, loss_ce: 0.006379, loss_dice: 0.257318
[10:26:59.549] TRAIN: iteration 16491 : loss : 0.084005, loss_ce: 0.002153, loss_dice: 0.165858
[10:26:59.765] TRAIN: iteration 16492 : loss : 0.109026, loss_ce: 0.016471, loss_dice: 0.201582
[10:27:00.034] TRAIN: iteration 16493 : loss : 0.050517, loss_ce: 0.002842, loss_dice: 0.098191
[10:27:00.246] TRAIN: iteration 16494 : loss : 0.047615, loss_ce: 0.001843, loss_dice: 0.093387
[10:27:00.455] TRAIN: iteration 16495 : loss : 0.067827, loss_ce: 0.001611, loss_dice: 0.134042
[10:27:00.668] TRAIN: iteration 16496 : loss : 0.206540, loss_ce: 0.002942, loss_dice: 0.410139
[10:27:00.883] TRAIN: iteration 16497 : loss : 0.035706, loss_ce: 0.006090, loss_dice: 0.065322
[10:27:01.093] TRAIN: iteration 16498 : loss : 0.176731, loss_ce: 0.002069, loss_dice: 0.351393
[10:27:01.301] TRAIN: iteration 16499 : loss : 0.143361, loss_ce: 0.004181, loss_dice: 0.282542
[10:27:01.511] TRAIN: iteration 16500 : loss : 0.116580, loss_ce: 0.002209, loss_dice: 0.230951
[10:27:01.512] NaN or Inf found in input tensor.
[10:27:01.734] TRAIN: iteration 16501 : loss : 0.240233, loss_ce: 0.007162, loss_dice: 0.473303
[10:27:01.943] TRAIN: iteration 16502 : loss : 0.153028, loss_ce: 0.004970, loss_dice: 0.301086
[10:27:02.152] TRAIN: iteration 16503 : loss : 0.090275, loss_ce: 0.008181, loss_dice: 0.172368
[10:27:02.368] TRAIN: iteration 16504 : loss : 0.251116, loss_ce: 0.002092, loss_dice: 0.500140
[10:27:02.580] TRAIN: iteration 16505 : loss : 0.250411, loss_ce: 0.000808, loss_dice: 0.500014
[10:27:02.789] TRAIN: iteration 16506 : loss : 0.085362, loss_ce: 0.001914, loss_dice: 0.168810
[10:27:02.999] TRAIN: iteration 16507 : loss : 0.142793, loss_ce: 0.004360, loss_dice: 0.281226
[10:27:03.208] TRAIN: iteration 16508 : loss : 0.250582, loss_ce: 0.001135, loss_dice: 0.500029
[10:27:03.437] TRAIN: iteration 16509 : loss : 0.044094, loss_ce: 0.002994, loss_dice: 0.085194
[10:27:03.645] TRAIN: iteration 16510 : loss : 0.135780, loss_ce: 0.024513, loss_dice: 0.247047
[10:27:03.854] TRAIN: iteration 16511 : loss : 0.245527, loss_ce: 0.002656, loss_dice: 0.488398
[10:27:04.064] TRAIN: iteration 16512 : loss : 0.106806, loss_ce: 0.009577, loss_dice: 0.204036
[10:27:04.274] TRAIN: iteration 16513 : loss : 0.115820, loss_ce: 0.008707, loss_dice: 0.222933
[10:27:04.483] TRAIN: iteration 16514 : loss : 0.042999, loss_ce: 0.004802, loss_dice: 0.081197
[10:27:04.691] TRAIN: iteration 16515 : loss : 0.161334, loss_ce: 0.012506, loss_dice: 0.310162
[10:27:04.914] TRAIN: iteration 16516 : loss : 0.153814, loss_ce: 0.007469, loss_dice: 0.300158
[10:27:05.123] TRAIN: iteration 16517 : loss : 0.066657, loss_ce: 0.001655, loss_dice: 0.131659
[10:27:05.613] TRAIN: iteration 16518 : loss : 0.028657, loss_ce: 0.002838, loss_dice: 0.054476
[10:27:07.756] TRAIN: iteration 16519 : loss : 0.100816, loss_ce: 0.010165, loss_dice: 0.191467
[10:27:07.965] TRAIN: iteration 16520 : loss : 0.024502, loss_ce: 0.001570, loss_dice: 0.047433
[10:27:08.213] TRAIN: iteration 16521 : loss : 0.069916, loss_ce: 0.003624, loss_dice: 0.136208
[10:27:08.421] TRAIN: iteration 16522 : loss : 0.089327, loss_ce: 0.002688, loss_dice: 0.175967
[10:27:08.630] TRAIN: iteration 16523 : loss : 0.040441, loss_ce: 0.008226, loss_dice: 0.072656
[10:27:08.839] TRAIN: iteration 16524 : loss : 0.085019, loss_ce: 0.007202, loss_dice: 0.162836
[10:27:09.049] TRAIN: iteration 16525 : loss : 0.087646, loss_ce: 0.002636, loss_dice: 0.172656
[10:27:09.259] TRAIN: iteration 16526 : loss : 0.018670, loss_ce: 0.003770, loss_dice: 0.033570
[10:27:09.467] TRAIN: iteration 16527 : loss : 0.198690, loss_ce: 0.014307, loss_dice: 0.383072
[10:27:09.727] TRAIN: iteration 16528 : loss : 0.252122, loss_ce: 0.003949, loss_dice: 0.500296
[10:27:09.940] TRAIN: iteration 16529 : loss : 0.131391, loss_ce: 0.006666, loss_dice: 0.256116
[10:27:10.150] TRAIN: iteration 16530 : loss : 0.057959, loss_ce: 0.002957, loss_dice: 0.112961
[10:27:10.361] TRAIN: iteration 16531 : loss : 0.135223, loss_ce: 0.002141, loss_dice: 0.268305
[10:27:10.573] TRAIN: iteration 16532 : loss : 0.037370, loss_ce: 0.004428, loss_dice: 0.070312
[10:27:10.782] TRAIN: iteration 16533 : loss : 0.174457, loss_ce: 0.007148, loss_dice: 0.341767
[10:27:10.993] TRAIN: iteration 16534 : loss : 0.079403, loss_ce: 0.002064, loss_dice: 0.156742
[10:27:11.610] TRAIN: iteration 16535 : loss : 0.250242, loss_ce: 0.000481, loss_dice: 0.500003
[10:27:11.819] TRAIN: iteration 16536 : loss : 0.145917, loss_ce: 0.005148, loss_dice: 0.286685
[10:27:12.027] TRAIN: iteration 16537 : loss : 0.088519, loss_ce: 0.004132, loss_dice: 0.172907
[10:27:12.237] TRAIN: iteration 16538 : loss : 0.124386, loss_ce: 0.005872, loss_dice: 0.242899
[10:27:12.528] TRAIN: iteration 16539 : loss : 0.139097, loss_ce: 0.002312, loss_dice: 0.275881
[10:27:12.737] TRAIN: iteration 16540 : loss : 0.080942, loss_ce: 0.003166, loss_dice: 0.158719
[10:27:12.991] TRAIN: iteration 16541 : loss : 0.071128, loss_ce: 0.009889, loss_dice: 0.132367
[10:27:13.200] TRAIN: iteration 16542 : loss : 0.089045, loss_ce: 0.003858, loss_dice: 0.174232
[10:27:14.227] TRAIN: iteration 16543 : loss : 0.173640, loss_ce: 0.002786, loss_dice: 0.344495
[10:27:14.436] TRAIN: iteration 16544 : loss : 0.127250, loss_ce: 0.006380, loss_dice: 0.248120
[10:27:14.652] TRAIN: iteration 16545 : loss : 0.126969, loss_ce: 0.001871, loss_dice: 0.252067
[10:27:14.865] TRAIN: iteration 16546 : loss : 0.031562, loss_ce: 0.001322, loss_dice: 0.061802
[10:27:15.075] TRAIN: iteration 16547 : loss : 0.137307, loss_ce: 0.008825, loss_dice: 0.265790
[10:27:15.288] TRAIN: iteration 16548 : loss : 0.091580, loss_ce: 0.004269, loss_dice: 0.178891
[10:27:15.500] TRAIN: iteration 16549 : loss : 0.090949, loss_ce: 0.002890, loss_dice: 0.179008
[10:27:15.710] TRAIN: iteration 16550 : loss : 0.074852, loss_ce: 0.002686, loss_dice: 0.147018
[10:27:15.919] TRAIN: iteration 16551 : loss : 0.113985, loss_ce: 0.005352, loss_dice: 0.222618
[10:27:16.130] TRAIN: iteration 16552 : loss : 0.071786, loss_ce: 0.008603, loss_dice: 0.134969
[10:27:16.340] TRAIN: iteration 16553 : loss : 0.084213, loss_ce: 0.004102, loss_dice: 0.164323
[10:27:16.551] TRAIN: iteration 16554 : loss : 0.225412, loss_ce: 0.001918, loss_dice: 0.448906
[10:27:16.763] TRAIN: iteration 16555 : loss : 0.083517, loss_ce: 0.001675, loss_dice: 0.165358
[10:27:16.976] TRAIN: iteration 16556 : loss : 0.069301, loss_ce: 0.005461, loss_dice: 0.133142
[10:27:17.187] TRAIN: iteration 16557 : loss : 0.252464, loss_ce: 0.004976, loss_dice: 0.499952
[10:27:17.402] TRAIN: iteration 16558 : loss : 0.251840, loss_ce: 0.003452, loss_dice: 0.500229
[10:27:17.612] TRAIN: iteration 16559 : loss : 0.083140, loss_ce: 0.003572, loss_dice: 0.162708
[10:27:17.820] TRAIN: iteration 16560 : loss : 0.091130, loss_ce: 0.014027, loss_dice: 0.168233
[10:27:18.063] TRAIN: iteration 16561 : loss : 0.232180, loss_ce: 0.004692, loss_dice: 0.459669
[10:27:18.271] TRAIN: iteration 16562 : loss : 0.044495, loss_ce: 0.004499, loss_dice: 0.084490
[10:27:18.481] TRAIN: iteration 16563 : loss : 0.095076, loss_ce: 0.003908, loss_dice: 0.186244
[10:27:18.689] TRAIN: iteration 16564 : loss : 0.118380, loss_ce: 0.003003, loss_dice: 0.233757
[10:27:18.897] TRAIN: iteration 16565 : loss : 0.106800, loss_ce: 0.005277, loss_dice: 0.208324
[10:27:19.388] TRAIN: iteration 16566 : loss : 0.226954, loss_ce: 0.007852, loss_dice: 0.446055
[10:27:19.602] TRAIN: iteration 16567 : loss : 0.079298, loss_ce: 0.004003, loss_dice: 0.154594
[10:27:19.810] TRAIN: iteration 16568 : loss : 0.057418, loss_ce: 0.003697, loss_dice: 0.111140
[10:27:20.026] TRAIN: iteration 16569 : loss : 0.067704, loss_ce: 0.004711, loss_dice: 0.130698
[10:27:20.236] TRAIN: iteration 16570 : loss : 0.201200, loss_ce: 0.003609, loss_dice: 0.398791
[10:27:20.453] TRAIN: iteration 16571 : loss : 0.065635, loss_ce: 0.002756, loss_dice: 0.128515
[10:27:20.711] TRAIN: iteration 16572 : loss : 0.143636, loss_ce: 0.004363, loss_dice: 0.282908
[10:27:20.920] TRAIN: iteration 16573 : loss : 0.160588, loss_ce: 0.002838, loss_dice: 0.318339
[10:27:22.539] TRAIN: iteration 16574 : loss : 0.091189, loss_ce: 0.002238, loss_dice: 0.180139
[10:27:22.751] TRAIN: iteration 16575 : loss : 0.144338, loss_ce: 0.006960, loss_dice: 0.281715
[10:27:22.966] TRAIN: iteration 16576 : loss : 0.250403, loss_ce: 0.000779, loss_dice: 0.500027
[10:27:23.177] TRAIN: iteration 16577 : loss : 0.154968, loss_ce: 0.010337, loss_dice: 0.299599
[10:27:23.387] TRAIN: iteration 16578 : loss : 0.057056, loss_ce: 0.001978, loss_dice: 0.112134
[10:27:23.597] TRAIN: iteration 16579 : loss : 0.066495, loss_ce: 0.004812, loss_dice: 0.128177
[10:27:23.806] TRAIN: iteration 16580 : loss : 0.137399, loss_ce: 0.006461, loss_dice: 0.268336
[10:27:24.051] TRAIN: iteration 16581 : loss : 0.243063, loss_ce: 0.003004, loss_dice: 0.483121
[10:27:24.266] TRAIN: iteration 16582 : loss : 0.069284, loss_ce: 0.001219, loss_dice: 0.137349
[10:27:24.476] TRAIN: iteration 16583 : loss : 0.235517, loss_ce: 0.003790, loss_dice: 0.467244
[10:27:24.686] TRAIN: iteration 16584 : loss : 0.172942, loss_ce: 0.010767, loss_dice: 0.335116
[10:27:24.898] TRAIN: iteration 16585 : loss : 0.205896, loss_ce: 0.003248, loss_dice: 0.408543
[10:27:25.108] TRAIN: iteration 16586 : loss : 0.045094, loss_ce: 0.001406, loss_dice: 0.088783
[10:27:25.318] TRAIN: iteration 16587 : loss : 0.069544, loss_ce: 0.002508, loss_dice: 0.136581
[10:27:26.043] TRAIN: iteration 16588 : loss : 0.251361, loss_ce: 0.002595, loss_dice: 0.500128
[10:27:26.254] TRAIN: iteration 16589 : loss : 0.148708, loss_ce: 0.002988, loss_dice: 0.294429
[10:27:26.464] TRAIN: iteration 16590 : loss : 0.117556, loss_ce: 0.006885, loss_dice: 0.228227
[10:27:26.673] TRAIN: iteration 16591 : loss : 0.252342, loss_ce: 0.004377, loss_dice: 0.500307
[10:27:26.881] TRAIN: iteration 16592 : loss : 0.115501, loss_ce: 0.005652, loss_dice: 0.225351
[10:27:27.091] TRAIN: iteration 16593 : loss : 0.059234, loss_ce: 0.001635, loss_dice: 0.116833
[10:27:27.301] TRAIN: iteration 16594 : loss : 0.109350, loss_ce: 0.019095, loss_dice: 0.199605
[10:27:27.514] TRAIN: iteration 16595 : loss : 0.082681, loss_ce: 0.009357, loss_dice: 0.156005
[10:27:27.734] TRAIN: iteration 16596 : loss : 0.118649, loss_ce: 0.004003, loss_dice: 0.233294
[10:27:27.945] TRAIN: iteration 16597 : loss : 0.232013, loss_ce: 0.009443, loss_dice: 0.454583
[10:27:28.155] TRAIN: iteration 16598 : loss : 0.047005, loss_ce: 0.003394, loss_dice: 0.090616
[10:27:28.371] TRAIN: iteration 16599 : loss : 0.160180, loss_ce: 0.004261, loss_dice: 0.316099
[10:27:28.581] TRAIN: iteration 16600 : loss : 0.099559, loss_ce: 0.002404, loss_dice: 0.196715
[10:27:28.819] TRAIN: iteration 16601 : loss : 0.030579, loss_ce: 0.002284, loss_dice: 0.058874
[10:27:29.163] TRAIN: iteration 16602 : loss : 0.087130, loss_ce: 0.005435, loss_dice: 0.168826
[10:27:29.377] TRAIN: iteration 16603 : loss : 0.061924, loss_ce: 0.003773, loss_dice: 0.120075
[10:27:31.730] TRAIN: iteration 16604 : loss : 0.146041, loss_ce: 0.011094, loss_dice: 0.280988
[10:27:31.943] TRAIN: iteration 16605 : loss : 0.041679, loss_ce: 0.003505, loss_dice: 0.079853
[10:27:32.155] TRAIN: iteration 16606 : loss : 0.251699, loss_ce: 0.003189, loss_dice: 0.500209
[10:27:32.365] TRAIN: iteration 16607 : loss : 0.075055, loss_ce: 0.004007, loss_dice: 0.146104
[10:27:32.574] TRAIN: iteration 16608 : loss : 0.251101, loss_ce: 0.002086, loss_dice: 0.500116
[10:27:32.782] TRAIN: iteration 16609 : loss : 0.235576, loss_ce: 0.003139, loss_dice: 0.468013
[10:27:32.991] TRAIN: iteration 16610 : loss : 0.105392, loss_ce: 0.002089, loss_dice: 0.208695
[10:27:33.199] TRAIN: iteration 16611 : loss : 0.250350, loss_ce: 0.002315, loss_dice: 0.498385
[10:27:35.994] TRAIN: iteration 16612 : loss : 0.047834, loss_ce: 0.001670, loss_dice: 0.093998
[10:27:36.206] TRAIN: iteration 16613 : loss : 0.056477, loss_ce: 0.001837, loss_dice: 0.111117
[10:27:36.417] TRAIN: iteration 16614 : loss : 0.198827, loss_ce: 0.001658, loss_dice: 0.395997
[10:27:36.625] TRAIN: iteration 16615 : loss : 0.116032, loss_ce: 0.003847, loss_dice: 0.228218
[10:27:36.928] TRAIN: iteration 16616 : loss : 0.097402, loss_ce: 0.002705, loss_dice: 0.192099
[10:27:37.141] TRAIN: iteration 16617 : loss : 0.250355, loss_ce: 0.000698, loss_dice: 0.500013
[10:27:37.351] TRAIN: iteration 16618 : loss : 0.084500, loss_ce: 0.002368, loss_dice: 0.166632
[10:27:37.559] TRAIN: iteration 16619 : loss : 0.057515, loss_ce: 0.003094, loss_dice: 0.111936
[10:27:38.714] TRAIN: iteration 16620 : loss : 0.184099, loss_ce: 0.020412, loss_dice: 0.347786
[10:27:38.952] TRAIN: iteration 16621 : loss : 0.165281, loss_ce: 0.001316, loss_dice: 0.329246
[10:27:39.160] TRAIN: iteration 16622 : loss : 0.042534, loss_ce: 0.002479, loss_dice: 0.082589
[10:27:39.369] TRAIN: iteration 16623 : loss : 0.050197, loss_ce: 0.002526, loss_dice: 0.097868
[10:27:39.576] TRAIN: iteration 16624 : loss : 0.058066, loss_ce: 0.007204, loss_dice: 0.108928
[10:27:39.784] TRAIN: iteration 16625 : loss : 0.125709, loss_ce: 0.014084, loss_dice: 0.237333
[10:27:39.999] TRAIN: iteration 16626 : loss : 0.107788, loss_ce: 0.006984, loss_dice: 0.208593
[10:27:40.215] TRAIN: iteration 16627 : loss : 0.148178, loss_ce: 0.007526, loss_dice: 0.288831
[10:27:42.390] TRAIN: iteration 16628 : loss : 0.187397, loss_ce: 0.007074, loss_dice: 0.367720
[10:27:42.599] TRAIN: iteration 16629 : loss : 0.055786, loss_ce: 0.009379, loss_dice: 0.102194
[10:27:42.809] TRAIN: iteration 16630 : loss : 0.038683, loss_ce: 0.005128, loss_dice: 0.072237
[10:27:43.022] TRAIN: iteration 16631 : loss : 0.065390, loss_ce: 0.004668, loss_dice: 0.126112
[10:27:43.233] TRAIN: iteration 16632 : loss : 0.119869, loss_ce: 0.003437, loss_dice: 0.236301
[10:27:43.442] TRAIN: iteration 16633 : loss : 0.105995, loss_ce: 0.003896, loss_dice: 0.208095
[10:27:43.651] TRAIN: iteration 16634 : loss : 0.098315, loss_ce: 0.005935, loss_dice: 0.190695
[10:27:43.865] TRAIN: iteration 16635 : loss : 0.091198, loss_ce: 0.005495, loss_dice: 0.176900
[10:27:44.074] TRAIN: iteration 16636 : loss : 0.049822, loss_ce: 0.002044, loss_dice: 0.097599
[10:27:44.283] TRAIN: iteration 16637 : loss : 0.206207, loss_ce: 0.012139, loss_dice: 0.400275
[10:27:44.501] TRAIN: iteration 16638 : loss : 0.045459, loss_ce: 0.002025, loss_dice: 0.088893
[10:27:44.709] TRAIN: iteration 16639 : loss : 0.182926, loss_ce: 0.006306, loss_dice: 0.359546
[10:27:44.918] TRAIN: iteration 16640 : loss : 0.251826, loss_ce: 0.004878, loss_dice: 0.498774
[10:27:45.159] TRAIN: iteration 16641 : loss : 0.162717, loss_ce: 0.017044, loss_dice: 0.308390
[10:27:45.367] TRAIN: iteration 16642 : loss : 0.100569, loss_ce: 0.001691, loss_dice: 0.199446
[10:27:45.577] TRAIN: iteration 16643 : loss : 0.132214, loss_ce: 0.003299, loss_dice: 0.261129
[10:27:45.792] TRAIN: iteration 16644 : loss : 0.092906, loss_ce: 0.001632, loss_dice: 0.184180
[10:27:46.039] TRAIN: iteration 16645 : loss : 0.070902, loss_ce: 0.004748, loss_dice: 0.137057
[10:27:47.227] TRAIN: iteration 16646 : loss : 0.052667, loss_ce: 0.004836, loss_dice: 0.100498
[10:27:47.435] TRAIN: iteration 16647 : loss : 0.224818, loss_ce: 0.006901, loss_dice: 0.442736
[10:27:47.642] TRAIN: iteration 16648 : loss : 0.156141, loss_ce: 0.002744, loss_dice: 0.309538
[10:27:47.856] TRAIN: iteration 16649 : loss : 0.248897, loss_ce: 0.002755, loss_dice: 0.495040
[10:27:48.065] TRAIN: iteration 16650 : loss : 0.103279, loss_ce: 0.005926, loss_dice: 0.200632
[10:27:48.274] TRAIN: iteration 16651 : loss : 0.061891, loss_ce: 0.007813, loss_dice: 0.115969
[10:27:48.483] TRAIN: iteration 16652 : loss : 0.119779, loss_ce: 0.004267, loss_dice: 0.235291
[10:27:48.692] TRAIN: iteration 16653 : loss : 0.174896, loss_ce: 0.002281, loss_dice: 0.347510
[10:27:50.813] TRAIN: iteration 16654 : loss : 0.103075, loss_ce: 0.002326, loss_dice: 0.203824
[10:27:51.022] TRAIN: iteration 16655 : loss : 0.170063, loss_ce: 0.009683, loss_dice: 0.330443
[10:27:51.230] TRAIN: iteration 16656 : loss : 0.213575, loss_ce: 0.003432, loss_dice: 0.423717
[10:27:51.438] TRAIN: iteration 16657 : loss : 0.193852, loss_ce: 0.005589, loss_dice: 0.382115
[10:27:51.647] TRAIN: iteration 16658 : loss : 0.160038, loss_ce: 0.003748, loss_dice: 0.316327
[10:27:51.862] TRAIN: iteration 16659 : loss : 0.096835, loss_ce: 0.002850, loss_dice: 0.190820
[10:27:52.075] TRAIN: iteration 16660 : loss : 0.253105, loss_ce: 0.005782, loss_dice: 0.500428
[10:27:52.315] TRAIN: iteration 16661 : loss : 0.047968, loss_ce: 0.007766, loss_dice: 0.088169
[10:27:52.525] TRAIN: iteration 16662 : loss : 0.129098, loss_ce: 0.020187, loss_dice: 0.238008
[10:27:52.732] TRAIN: iteration 16663 : loss : 0.129510, loss_ce: 0.005256, loss_dice: 0.253764
[10:27:52.943] TRAIN: iteration 16664 : loss : 0.251980, loss_ce: 0.003725, loss_dice: 0.500236
[10:27:53.152] TRAIN: iteration 16665 : loss : 0.156021, loss_ce: 0.004789, loss_dice: 0.307252
[10:27:53.360] TRAIN: iteration 16666 : loss : 0.228779, loss_ce: 0.003346, loss_dice: 0.454211
[10:27:53.572] TRAIN: iteration 16667 : loss : 0.189575, loss_ce: 0.008221, loss_dice: 0.370930
[10:27:53.832] TRAIN: iteration 16668 : loss : 0.079791, loss_ce: 0.004222, loss_dice: 0.155360
[10:27:54.040] TRAIN: iteration 16669 : loss : 0.032512, loss_ce: 0.001060, loss_dice: 0.063964
[10:27:54.284] TRAIN: iteration 16670 : loss : 0.196313, loss_ce: 0.005877, loss_dice: 0.386749
[10:27:55.490] TRAIN: iteration 16671 : loss : 0.111485, loss_ce: 0.016771, loss_dice: 0.206199
[10:27:55.700] TRAIN: iteration 16672 : loss : 0.077663, loss_ce: 0.004602, loss_dice: 0.150724
[10:27:55.938] TRAIN: iteration 16673 : loss : 0.235934, loss_ce: 0.004625, loss_dice: 0.467243
[10:27:56.158] TRAIN: iteration 16674 : loss : 0.105185, loss_ce: 0.006047, loss_dice: 0.204324
[10:27:56.368] TRAIN: iteration 16675 : loss : 0.067341, loss_ce: 0.005808, loss_dice: 0.128873
[10:27:56.585] TRAIN: iteration 16676 : loss : 0.030367, loss_ce: 0.002595, loss_dice: 0.058139
[10:27:56.794] TRAIN: iteration 16677 : loss : 0.166775, loss_ce: 0.003546, loss_dice: 0.330005
[10:27:57.011] TRAIN: iteration 16678 : loss : 0.246976, loss_ce: 0.003286, loss_dice: 0.490665
[10:27:57.219] TRAIN: iteration 16679 : loss : 0.065091, loss_ce: 0.003828, loss_dice: 0.126354
[10:27:57.428] TRAIN: iteration 16680 : loss : 0.078587, loss_ce: 0.002136, loss_dice: 0.155039
[10:27:57.669] TRAIN: iteration 16681 : loss : 0.085412, loss_ce: 0.005237, loss_dice: 0.165588
[10:27:57.879] TRAIN: iteration 16682 : loss : 0.150935, loss_ce: 0.013688, loss_dice: 0.288181
[10:27:58.088] TRAIN: iteration 16683 : loss : 0.100938, loss_ce: 0.004358, loss_dice: 0.197517
[10:27:58.297] TRAIN: iteration 16684 : loss : 0.199898, loss_ce: 0.009615, loss_dice: 0.390180
[10:27:58.514] TRAIN: iteration 16685 : loss : 0.223265, loss_ce: 0.003003, loss_dice: 0.443527
[10:27:59.229] TRAIN: iteration 16686 : loss : 0.029625, loss_ce: 0.002680, loss_dice: 0.056570
[10:27:59.438] TRAIN: iteration 16687 : loss : 0.114546, loss_ce: 0.002933, loss_dice: 0.226158
[10:27:59.647] TRAIN: iteration 16688 : loss : 0.134893, loss_ce: 0.005305, loss_dice: 0.264480
[10:27:59.856] TRAIN: iteration 16689 : loss : 0.120166, loss_ce: 0.004127, loss_dice: 0.236205
[10:28:00.065] TRAIN: iteration 16690 : loss : 0.215071, loss_ce: 0.004233, loss_dice: 0.425908
[10:28:00.274] TRAIN: iteration 16691 : loss : 0.247248, loss_ce: 0.004139, loss_dice: 0.490357
[10:28:02.264] TRAIN: iteration 16692 : loss : 0.055440, loss_ce: 0.002641, loss_dice: 0.108239
[10:28:02.473] TRAIN: iteration 16693 : loss : 0.215263, loss_ce: 0.009481, loss_dice: 0.421044
[10:28:02.684] TRAIN: iteration 16694 : loss : 0.131385, loss_ce: 0.005003, loss_dice: 0.257768
[10:28:02.892] TRAIN: iteration 16695 : loss : 0.195586, loss_ce: 0.007829, loss_dice: 0.383344
[10:28:03.106] TRAIN: iteration 16696 : loss : 0.141498, loss_ce: 0.003606, loss_dice: 0.279389
[10:28:03.317] TRAIN: iteration 16697 : loss : 0.069478, loss_ce: 0.007263, loss_dice: 0.131694
[10:28:03.529] TRAIN: iteration 16698 : loss : 0.229163, loss_ce: 0.005704, loss_dice: 0.452623
[10:28:03.737] TRAIN: iteration 16699 : loss : 0.247120, loss_ce: 0.003067, loss_dice: 0.491173
[10:28:05.358] TRAIN: iteration 16700 : loss : 0.128091, loss_ce: 0.002750, loss_dice: 0.253433
[10:28:05.595] TRAIN: iteration 16701 : loss : 0.057122, loss_ce: 0.004942, loss_dice: 0.109302
[10:28:05.803] TRAIN: iteration 16702 : loss : 0.154317, loss_ce: 0.003844, loss_dice: 0.304790
[10:28:06.012] TRAIN: iteration 16703 : loss : 0.242099, loss_ce: 0.002416, loss_dice: 0.481782
[10:28:06.228] TRAIN: iteration 16704 : loss : 0.251192, loss_ce: 0.002274, loss_dice: 0.500110
[10:28:06.436] TRAIN: iteration 16705 : loss : 0.068355, loss_ce: 0.001535, loss_dice: 0.135175
[10:28:06.643] TRAIN: iteration 16706 : loss : 0.145425, loss_ce: 0.006118, loss_dice: 0.284731
[10:28:06.852] TRAIN: iteration 16707 : loss : 0.097475, loss_ce: 0.003945, loss_dice: 0.191005
[10:28:07.074] TRAIN: iteration 16708 : loss : 0.200295, loss_ce: 0.037928, loss_dice: 0.362662
[10:28:07.285] TRAIN: iteration 16709 : loss : 0.110451, loss_ce: 0.011489, loss_dice: 0.209413
[10:28:07.494] TRAIN: iteration 16710 : loss : 0.191896, loss_ce: 0.002861, loss_dice: 0.380931
[10:28:07.705] TRAIN: iteration 16711 : loss : 0.103510, loss_ce: 0.004054, loss_dice: 0.202965
[10:28:07.914] TRAIN: iteration 16712 : loss : 0.166106, loss_ce: 0.015932, loss_dice: 0.316279
[10:28:08.127] TRAIN: iteration 16713 : loss : 0.150707, loss_ce: 0.006487, loss_dice: 0.294926
[10:28:08.336] TRAIN: iteration 16714 : loss : 0.069860, loss_ce: 0.004969, loss_dice: 0.134751
[10:28:08.546] TRAIN: iteration 16715 : loss : 0.192467, loss_ce: 0.004176, loss_dice: 0.380758
[10:28:09.523] TRAIN: iteration 16716 : loss : 0.064728, loss_ce: 0.007310, loss_dice: 0.122145
[10:28:09.732] TRAIN: iteration 16717 : loss : 0.251674, loss_ce: 0.003193, loss_dice: 0.500155
[10:28:09.941] TRAIN: iteration 16718 : loss : 0.056827, loss_ce: 0.003129, loss_dice: 0.110525
[10:28:10.159] TRAIN: iteration 16719 : loss : 0.056495, loss_ce: 0.002841, loss_dice: 0.110148
[10:28:10.367] TRAIN: iteration 16720 : loss : 0.246403, loss_ce: 0.004310, loss_dice: 0.488496
[10:28:10.642] TRAIN: iteration 16721 : loss : 0.077140, loss_ce: 0.003854, loss_dice: 0.150426
[10:28:10.850] TRAIN: iteration 16722 : loss : 0.032989, loss_ce: 0.002153, loss_dice: 0.063825
[10:28:11.060] TRAIN: iteration 16723 : loss : 0.027124, loss_ce: 0.003273, loss_dice: 0.050975
[10:28:11.273] TRAIN: iteration 16724 : loss : 0.109647, loss_ce: 0.004553, loss_dice: 0.214741
[10:28:11.482] TRAIN: iteration 16725 : loss : 0.087647, loss_ce: 0.006188, loss_dice: 0.169107
[10:28:12.635] TRAIN: iteration 16726 : loss : 0.098270, loss_ce: 0.004979, loss_dice: 0.191561
[10:28:12.846] TRAIN: iteration 16727 : loss : 0.195870, loss_ce: 0.007396, loss_dice: 0.384343
[10:28:13.055] TRAIN: iteration 16728 : loss : 0.185859, loss_ce: 0.009058, loss_dice: 0.362660
[10:28:13.265] TRAIN: iteration 16729 : loss : 0.106609, loss_ce: 0.002246, loss_dice: 0.210973
[10:28:13.476] TRAIN: iteration 16730 : loss : 0.083346, loss_ce: 0.004221, loss_dice: 0.162472
[10:28:13.684] TRAIN: iteration 16731 : loss : 0.162191, loss_ce: 0.005990, loss_dice: 0.318392
[10:28:13.897] TRAIN: iteration 16732 : loss : 0.087706, loss_ce: 0.008032, loss_dice: 0.167379
[10:28:14.111] TRAIN: iteration 16733 : loss : 0.035611, loss_ce: 0.001774, loss_dice: 0.069449
[10:28:14.326] TRAIN: iteration 16734 : loss : 0.206844, loss_ce: 0.031556, loss_dice: 0.382132
[10:28:14.535] TRAIN: iteration 16735 : loss : 0.106867, loss_ce: 0.001947, loss_dice: 0.211787
[10:28:14.742] TRAIN: iteration 16736 : loss : 0.251353, loss_ce: 0.002573, loss_dice: 0.500133
[10:28:14.951] TRAIN: iteration 16737 : loss : 0.092364, loss_ce: 0.003355, loss_dice: 0.181374
[10:28:15.160] TRAIN: iteration 16738 : loss : 0.194020, loss_ce: 0.003736, loss_dice: 0.384303
[10:28:15.368] TRAIN: iteration 16739 : loss : 0.069875, loss_ce: 0.006721, loss_dice: 0.133029
[10:28:15.576] TRAIN: iteration 16740 : loss : 0.251580, loss_ce: 0.002976, loss_dice: 0.500183
[10:28:15.814] TRAIN: iteration 16741 : loss : 0.143984, loss_ce: 0.003072, loss_dice: 0.284895
[10:28:17.341] TRAIN: iteration 16742 : loss : 0.249988, loss_ce: 0.004599, loss_dice: 0.495377
[10:28:17.549] TRAIN: iteration 16743 : loss : 0.120794, loss_ce: 0.007207, loss_dice: 0.234381
[10:28:17.758] TRAIN: iteration 16744 : loss : 0.251173, loss_ce: 0.002207, loss_dice: 0.500139
[10:28:17.971] TRAIN: iteration 16745 : loss : 0.126130, loss_ce: 0.007168, loss_dice: 0.245093
[10:28:18.181] TRAIN: iteration 16746 : loss : 0.140073, loss_ce: 0.007224, loss_dice: 0.272922
[10:28:18.391] TRAIN: iteration 16747 : loss : 0.154202, loss_ce: 0.003086, loss_dice: 0.305318
[10:28:18.600] TRAIN: iteration 16748 : loss : 0.251611, loss_ce: 0.003005, loss_dice: 0.500217
[10:28:18.817] TRAIN: iteration 16749 : loss : 0.251101, loss_ce: 0.002112, loss_dice: 0.500091
[10:28:19.028] TRAIN: iteration 16750 : loss : 0.089097, loss_ce: 0.003439, loss_dice: 0.174756
[10:28:19.240] TRAIN: iteration 16751 : loss : 0.074326, loss_ce: 0.006732, loss_dice: 0.141921
[10:28:19.449] TRAIN: iteration 16752 : loss : 0.077931, loss_ce: 0.008339, loss_dice: 0.147522
[10:28:19.659] TRAIN: iteration 16753 : loss : 0.141735, loss_ce: 0.008879, loss_dice: 0.274592
[10:28:19.867] TRAIN: iteration 16754 : loss : 0.209764, loss_ce: 0.008630, loss_dice: 0.410897
[10:28:20.077] TRAIN: iteration 16755 : loss : 0.250633, loss_ce: 0.005165, loss_dice: 0.496100
[10:28:20.289] TRAIN: iteration 16756 : loss : 0.251021, loss_ce: 0.001959, loss_dice: 0.500083
[10:28:20.499] TRAIN: iteration 16757 : loss : 0.147820, loss_ce: 0.005476, loss_dice: 0.290165
[10:28:20.709] TRAIN: iteration 16758 : loss : 0.050119, loss_ce: 0.001909, loss_dice: 0.098330
[10:28:20.917] TRAIN: iteration 16759 : loss : 0.060548, loss_ce: 0.007893, loss_dice: 0.113203
[10:28:21.127] TRAIN: iteration 16760 : loss : 0.040299, loss_ce: 0.002319, loss_dice: 0.078278
[10:28:21.365] TRAIN: iteration 16761 : loss : 0.065259, loss_ce: 0.002357, loss_dice: 0.128160
[10:28:21.576] TRAIN: iteration 16762 : loss : 0.107820, loss_ce: 0.009046, loss_dice: 0.206594
[10:28:23.048] TRAIN: iteration 16763 : loss : 0.163326, loss_ce: 0.003641, loss_dice: 0.323011
[10:28:23.256] TRAIN: iteration 16764 : loss : 0.037419, loss_ce: 0.001189, loss_dice: 0.073650
[10:28:23.468] TRAIN: iteration 16765 : loss : 0.112059, loss_ce: 0.004060, loss_dice: 0.220059
[10:28:23.677] TRAIN: iteration 16766 : loss : 0.147537, loss_ce: 0.002961, loss_dice: 0.292114
[10:28:23.884] TRAIN: iteration 16767 : loss : 0.126909, loss_ce: 0.005671, loss_dice: 0.248148
[10:28:24.093] TRAIN: iteration 16768 : loss : 0.120684, loss_ce: 0.003153, loss_dice: 0.238215
[10:28:24.308] TRAIN: iteration 16769 : loss : 0.073599, loss_ce: 0.006879, loss_dice: 0.140319
[10:28:24.517] TRAIN: iteration 16770 : loss : 0.187958, loss_ce: 0.002403, loss_dice: 0.373512
[10:28:25.869] TRAIN: iteration 16771 : loss : 0.112396, loss_ce: 0.002488, loss_dice: 0.222304
[10:28:26.079] TRAIN: iteration 16772 : loss : 0.093016, loss_ce: 0.013038, loss_dice: 0.172993
[10:28:26.289] TRAIN: iteration 16773 : loss : 0.251270, loss_ce: 0.002384, loss_dice: 0.500155
[10:28:26.499] TRAIN: iteration 16774 : loss : 0.080804, loss_ce: 0.001835, loss_dice: 0.159773
[10:28:26.707] TRAIN: iteration 16775 : loss : 0.232187, loss_ce: 0.001078, loss_dice: 0.463295
[10:28:26.918] TRAIN: iteration 16776 : loss : 0.095815, loss_ce: 0.009346, loss_dice: 0.182283
[10:28:27.128] TRAIN: iteration 16777 : loss : 0.085267, loss_ce: 0.001999, loss_dice: 0.168534
[10:28:27.336] TRAIN: iteration 16778 : loss : 0.022530, loss_ce: 0.001265, loss_dice: 0.043796
[10:28:27.543] TRAIN: iteration 16779 : loss : 0.068969, loss_ce: 0.001835, loss_dice: 0.136102
[10:28:27.758] TRAIN: iteration 16780 : loss : 0.249041, loss_ce: 0.002983, loss_dice: 0.495099
[10:28:27.998] TRAIN: iteration 16781 : loss : 0.251186, loss_ce: 0.002252, loss_dice: 0.500120
[10:28:28.208] TRAIN: iteration 16782 : loss : 0.029287, loss_ce: 0.001415, loss_dice: 0.057159
[10:28:28.417] TRAIN: iteration 16783 : loss : 0.186213, loss_ce: 0.005148, loss_dice: 0.367278
[10:28:28.625] TRAIN: iteration 16784 : loss : 0.250912, loss_ce: 0.001728, loss_dice: 0.500095
[10:28:29.391] TRAIN: iteration 16785 : loss : 0.211450, loss_ce: 0.005204, loss_dice: 0.417696
[10:28:29.600] TRAIN: iteration 16786 : loss : 0.065667, loss_ce: 0.005703, loss_dice: 0.125631
[10:28:29.809] TRAIN: iteration 16787 : loss : 0.236675, loss_ce: 0.010100, loss_dice: 0.463249
[10:28:30.024] TRAIN: iteration 16788 : loss : 0.086548, loss_ce: 0.003587, loss_dice: 0.169509
[10:28:30.233] TRAIN: iteration 16789 : loss : 0.129495, loss_ce: 0.001927, loss_dice: 0.257062
[10:28:30.441] TRAIN: iteration 16790 : loss : 0.084077, loss_ce: 0.006641, loss_dice: 0.161513
[10:28:30.651] TRAIN: iteration 16791 : loss : 0.243405, loss_ce: 0.010781, loss_dice: 0.476028
[10:28:30.860] TRAIN: iteration 16792 : loss : 0.039168, loss_ce: 0.002074, loss_dice: 0.076262
[10:28:31.069] TRAIN: iteration 16793 : loss : 0.190811, loss_ce: 0.002670, loss_dice: 0.378951
[10:28:31.281] TRAIN: iteration 16794 : loss : 0.054865, loss_ce: 0.001269, loss_dice: 0.108462
[10:28:33.799] TRAIN: iteration 16795 : loss : 0.076765, loss_ce: 0.004782, loss_dice: 0.148747
[10:28:34.008] TRAIN: iteration 16796 : loss : 0.121963, loss_ce: 0.002119, loss_dice: 0.241808
[10:28:34.217] TRAIN: iteration 16797 : loss : 0.137359, loss_ce: 0.002955, loss_dice: 0.271764
[10:28:34.430] TRAIN: iteration 16798 : loss : 0.081824, loss_ce: 0.003228, loss_dice: 0.160420
[10:28:34.640] TRAIN: iteration 16799 : loss : 0.147192, loss_ce: 0.006328, loss_dice: 0.288056
[10:28:34.851] TRAIN: iteration 16800 : loss : 0.032531, loss_ce: 0.002713, loss_dice: 0.062350
[10:28:35.091] TRAIN: iteration 16801 : loss : 0.249415, loss_ce: 0.003035, loss_dice: 0.495795
[10:28:35.300] TRAIN: iteration 16802 : loss : 0.211672, loss_ce: 0.001810, loss_dice: 0.421534
[10:28:35.508] TRAIN: iteration 16803 : loss : 0.142588, loss_ce: 0.002672, loss_dice: 0.282504
[10:28:35.716] TRAIN: iteration 16804 : loss : 0.050813, loss_ce: 0.003948, loss_dice: 0.097679
[10:28:35.924] TRAIN: iteration 16805 : loss : 0.231547, loss_ce: 0.053089, loss_dice: 0.410005
[10:28:36.336] TRAIN: iteration 16806 : loss : 0.089622, loss_ce: 0.005997, loss_dice: 0.173248
[10:28:36.543] TRAIN: iteration 16807 : loss : 0.052221, loss_ce: 0.007107, loss_dice: 0.097335
[10:28:36.753] TRAIN: iteration 16808 : loss : 0.167615, loss_ce: 0.005461, loss_dice: 0.329769
[10:28:39.438] TRAIN: iteration 16809 : loss : 0.166654, loss_ce: 0.010277, loss_dice: 0.323030
[10:28:39.656] TRAIN: iteration 16810 : loss : 0.220213, loss_ce: 0.001312, loss_dice: 0.439114
[10:28:39.866] TRAIN: iteration 16811 : loss : 0.100539, loss_ce: 0.004344, loss_dice: 0.196733
[10:28:40.074] TRAIN: iteration 16812 : loss : 0.090563, loss_ce: 0.013744, loss_dice: 0.167382
[10:28:40.283] TRAIN: iteration 16813 : loss : 0.222301, loss_ce: 0.009421, loss_dice: 0.435180
[10:28:40.491] TRAIN: iteration 16814 : loss : 0.074079, loss_ce: 0.006882, loss_dice: 0.141276
[10:28:40.699] TRAIN: iteration 16815 : loss : 0.186673, loss_ce: 0.006819, loss_dice: 0.366528
[10:28:40.908] TRAIN: iteration 16816 : loss : 0.154805, loss_ce: 0.003672, loss_dice: 0.305937
[10:28:41.697] TRAIN: iteration 16817 : loss : 0.102274, loss_ce: 0.007336, loss_dice: 0.197212
[10:28:41.911] TRAIN: iteration 16818 : loss : 0.045314, loss_ce: 0.005244, loss_dice: 0.085384
[10:28:42.118] TRAIN: iteration 16819 : loss : 0.136984, loss_ce: 0.003263, loss_dice: 0.270705
[10:28:42.325] TRAIN: iteration 16820 : loss : 0.211879, loss_ce: 0.004174, loss_dice: 0.419583
[10:28:42.568] TRAIN: iteration 16821 : loss : 0.251458, loss_ce: 0.003001, loss_dice: 0.499915
[10:28:42.776] TRAIN: iteration 16822 : loss : 0.091074, loss_ce: 0.009910, loss_dice: 0.172238
[10:28:42.988] TRAIN: iteration 16823 : loss : 0.088303, loss_ce: 0.005385, loss_dice: 0.171221
[10:28:43.199] TRAIN: iteration 16824 : loss : 0.078227, loss_ce: 0.004397, loss_dice: 0.152057
[10:28:43.413] TRAIN: iteration 16825 : loss : 0.091217, loss_ce: 0.004604, loss_dice: 0.177829
[10:28:43.623] TRAIN: iteration 16826 : loss : 0.049032, loss_ce: 0.008736, loss_dice: 0.089329
[10:28:43.830] TRAIN: iteration 16827 : loss : 0.210697, loss_ce: 0.004199, loss_dice: 0.417195
[10:28:44.041] TRAIN: iteration 16828 : loss : 0.246764, loss_ce: 0.003263, loss_dice: 0.490265
[10:28:44.256] TRAIN: iteration 16829 : loss : 0.247510, loss_ce: 0.003902, loss_dice: 0.491117
[10:28:44.468] TRAIN: iteration 16830 : loss : 0.076384, loss_ce: 0.007433, loss_dice: 0.145335
[10:28:44.713] TRAIN: iteration 16831 : loss : 0.251862, loss_ce: 0.003498, loss_dice: 0.500225
[10:28:44.921] TRAIN: iteration 16832 : loss : 0.199110, loss_ce: 0.004053, loss_dice: 0.394167
[10:28:45.541] TRAIN: iteration 16833 : loss : 0.087120, loss_ce: 0.002932, loss_dice: 0.171308
[10:28:45.750] TRAIN: iteration 16834 : loss : 0.105230, loss_ce: 0.002154, loss_dice: 0.208306
[10:28:47.152] TRAIN: iteration 16835 : loss : 0.178707, loss_ce: 0.003074, loss_dice: 0.354340
[10:28:47.359] TRAIN: iteration 16836 : loss : 0.082249, loss_ce: 0.002748, loss_dice: 0.161750
[10:28:47.568] TRAIN: iteration 16837 : loss : 0.087984, loss_ce: 0.002470, loss_dice: 0.173499
[10:28:47.777] TRAIN: iteration 16838 : loss : 0.250907, loss_ce: 0.001724, loss_dice: 0.500089
[10:28:47.986] TRAIN: iteration 16839 : loss : 0.132562, loss_ce: 0.023511, loss_dice: 0.241612
[10:28:48.206] TRAIN: iteration 16840 : loss : 0.084447, loss_ce: 0.001186, loss_dice: 0.167708
[10:28:49.503] TRAIN: iteration 16841 : loss : 0.044557, loss_ce: 0.006126, loss_dice: 0.082987
[10:28:49.714] TRAIN: iteration 16842 : loss : 0.098244, loss_ce: 0.006131, loss_dice: 0.190356
[10:28:49.926] TRAIN: iteration 16843 : loss : 0.250851, loss_ce: 0.001636, loss_dice: 0.500066
[10:28:50.137] TRAIN: iteration 16844 : loss : 0.114878, loss_ce: 0.011537, loss_dice: 0.218218
[10:28:50.346] TRAIN: iteration 16845 : loss : 0.121781, loss_ce: 0.001930, loss_dice: 0.241631
[10:28:51.423] TRAIN: iteration 16846 : loss : 0.193160, loss_ce: 0.002817, loss_dice: 0.383502
[10:28:51.631] TRAIN: iteration 16847 : loss : 0.250254, loss_ce: 0.003957, loss_dice: 0.496551
[10:28:51.839] TRAIN: iteration 16848 : loss : 0.252164, loss_ce: 0.004708, loss_dice: 0.499619
[10:28:53.131] TRAIN: iteration 16849 : loss : 0.029428, loss_ce: 0.007428, loss_dice: 0.051427
[10:28:53.340] TRAIN: iteration 16850 : loss : 0.062158, loss_ce: 0.001493, loss_dice: 0.122823
[10:28:53.553] TRAIN: iteration 16851 : loss : 0.101295, loss_ce: 0.006415, loss_dice: 0.196176
[10:28:53.762] TRAIN: iteration 16852 : loss : 0.061063, loss_ce: 0.003051, loss_dice: 0.119074
[10:28:54.493] TRAIN: iteration 16853 : loss : 0.101097, loss_ce: 0.009516, loss_dice: 0.192678
[10:28:54.857] TRAIN: iteration 16854 : loss : 0.076375, loss_ce: 0.003528, loss_dice: 0.149223
[10:28:55.066] TRAIN: iteration 16855 : loss : 0.210299, loss_ce: 0.003219, loss_dice: 0.417378
[10:28:55.276] TRAIN: iteration 16856 : loss : 0.101914, loss_ce: 0.005875, loss_dice: 0.197954
[10:28:55.487] TRAIN: iteration 16857 : loss : 0.240914, loss_ce: 0.001602, loss_dice: 0.480226
[10:28:55.695] TRAIN: iteration 16858 : loss : 0.144874, loss_ce: 0.005091, loss_dice: 0.284658
[10:28:55.904] TRAIN: iteration 16859 : loss : 0.051966, loss_ce: 0.004855, loss_dice: 0.099077
[10:28:56.114] TRAIN: iteration 16860 : loss : 0.089560, loss_ce: 0.003488, loss_dice: 0.175632
[10:28:56.114] NaN or Inf found in input tensor.
[10:28:56.329] TRAIN: iteration 16861 : loss : 0.126759, loss_ce: 0.004566, loss_dice: 0.248952
[10:28:57.644] TRAIN: iteration 16862 : loss : 0.085697, loss_ce: 0.003640, loss_dice: 0.167753
[10:28:57.856] TRAIN: iteration 16863 : loss : 0.075420, loss_ce: 0.003860, loss_dice: 0.146979
[10:28:58.064] TRAIN: iteration 16864 : loss : 0.048475, loss_ce: 0.005010, loss_dice: 0.091941
[10:28:58.272] TRAIN: iteration 16865 : loss : 0.079488, loss_ce: 0.009224, loss_dice: 0.149752
[10:28:58.478] TRAIN: iteration 16866 : loss : 0.066004, loss_ce: 0.002765, loss_dice: 0.129243
[10:28:58.688] TRAIN: iteration 16867 : loss : 0.069256, loss_ce: 0.002870, loss_dice: 0.135642
[10:28:58.896] TRAIN: iteration 16868 : loss : 0.070960, loss_ce: 0.003500, loss_dice: 0.138421
[10:28:59.676] TRAIN: iteration 16869 : loss : 0.114791, loss_ce: 0.005130, loss_dice: 0.224453
[10:28:59.885] TRAIN: iteration 16870 : loss : 0.100635, loss_ce: 0.004767, loss_dice: 0.196504
[10:29:00.094] TRAIN: iteration 16871 : loss : 0.084642, loss_ce: 0.003218, loss_dice: 0.166066
[10:29:00.995] TRAIN: iteration 16872 : loss : 0.102853, loss_ce: 0.004222, loss_dice: 0.201484
[10:29:01.208] TRAIN: iteration 16873 : loss : 0.092031, loss_ce: 0.008532, loss_dice: 0.175531
[10:29:01.417] TRAIN: iteration 16874 : loss : 0.073044, loss_ce: 0.002405, loss_dice: 0.143684
[10:29:01.626] TRAIN: iteration 16875 : loss : 0.250976, loss_ce: 0.001870, loss_dice: 0.500082
[10:29:01.966] TRAIN: iteration 16876 : loss : 0.159483, loss_ce: 0.005132, loss_dice: 0.313835
[10:29:02.174] TRAIN: iteration 16877 : loss : 0.084346, loss_ce: 0.003842, loss_dice: 0.164850
[10:29:02.383] TRAIN: iteration 16878 : loss : 0.076341, loss_ce: 0.001504, loss_dice: 0.151179
[10:29:02.599] TRAIN: iteration 16879 : loss : 0.148928, loss_ce: 0.006381, loss_dice: 0.291474
[10:29:06.819] TRAIN: iteration 16880 : loss : 0.098646, loss_ce: 0.001585, loss_dice: 0.195708
[10:29:06.820] NaN or Inf found in input tensor.
[10:29:07.035] TRAIN: iteration 16881 : loss : 0.123912, loss_ce: 0.007348, loss_dice: 0.240476
[10:29:07.246] TRAIN: iteration 16882 : loss : 0.202209, loss_ce: 0.001488, loss_dice: 0.402931
[10:29:07.456] TRAIN: iteration 16883 : loss : 0.250665, loss_ce: 0.001280, loss_dice: 0.500051
[10:29:07.666] TRAIN: iteration 16884 : loss : 0.121092, loss_ce: 0.006698, loss_dice: 0.235486
[10:29:07.876] TRAIN: iteration 16885 : loss : 0.251466, loss_ce: 0.004243, loss_dice: 0.498689
[10:29:08.086] TRAIN: iteration 16886 : loss : 0.165918, loss_ce: 0.012898, loss_dice: 0.318938
[10:29:08.297] TRAIN: iteration 16887 : loss : 0.099960, loss_ce: 0.005133, loss_dice: 0.194786
[10:29:10.550] TRAIN: iteration 16888 : loss : 0.071726, loss_ce: 0.001862, loss_dice: 0.141591
[10:29:10.763] TRAIN: iteration 16889 : loss : 0.202038, loss_ce: 0.001302, loss_dice: 0.402773
[10:29:10.975] TRAIN: iteration 16890 : loss : 0.066103, loss_ce: 0.002271, loss_dice: 0.129935
[10:29:11.186] TRAIN: iteration 16891 : loss : 0.238694, loss_ce: 0.002604, loss_dice: 0.474784
[10:29:11.401] TRAIN: iteration 16892 : loss : 0.041374, loss_ce: 0.004222, loss_dice: 0.078525
[10:29:11.613] TRAIN: iteration 16893 : loss : 0.147852, loss_ce: 0.002327, loss_dice: 0.293377
[10:29:11.822] TRAIN: iteration 16894 : loss : 0.104820, loss_ce: 0.002327, loss_dice: 0.207312
[10:29:12.034] TRAIN: iteration 16895 : loss : 0.065201, loss_ce: 0.002712, loss_dice: 0.127690
[10:29:14.109] TRAIN: iteration 16896 : loss : 0.068678, loss_ce: 0.003041, loss_dice: 0.134315
[10:29:14.321] TRAIN: iteration 16897 : loss : 0.118522, loss_ce: 0.003487, loss_dice: 0.233558
[10:29:14.540] TRAIN: iteration 16898 : loss : 0.189139, loss_ce: 0.003937, loss_dice: 0.374341
[10:29:14.748] TRAIN: iteration 16899 : loss : 0.194177, loss_ce: 0.006174, loss_dice: 0.382181
[10:29:14.974] TRAIN: iteration 16900 : loss : 0.064366, loss_ce: 0.006248, loss_dice: 0.122484
[10:29:14.975] NaN or Inf found in input tensor.
[10:29:15.190] TRAIN: iteration 16901 : loss : 0.131262, loss_ce: 0.007500, loss_dice: 0.255024
[10:29:15.399] TRAIN: iteration 16902 : loss : 0.149946, loss_ce: 0.006327, loss_dice: 0.293564
[10:29:15.609] TRAIN: iteration 16903 : loss : 0.108691, loss_ce: 0.004026, loss_dice: 0.213355
[10:29:16.726] TRAIN: iteration 16904 : loss : 0.038830, loss_ce: 0.004781, loss_dice: 0.072878
[10:29:16.933] TRAIN: iteration 16905 : loss : 0.065664, loss_ce: 0.004278, loss_dice: 0.127050
[10:29:17.141] TRAIN: iteration 16906 : loss : 0.249566, loss_ce: 0.003118, loss_dice: 0.496013
[10:29:17.350] TRAIN: iteration 16907 : loss : 0.141627, loss_ce: 0.003787, loss_dice: 0.279467
[10:29:17.559] TRAIN: iteration 16908 : loss : 0.251514, loss_ce: 0.002826, loss_dice: 0.500201
[10:29:17.768] TRAIN: iteration 16909 : loss : 0.047813, loss_ce: 0.006718, loss_dice: 0.088908
[10:29:17.983] TRAIN: iteration 16910 : loss : 0.167958, loss_ce: 0.005451, loss_dice: 0.330464
[10:29:18.191] TRAIN: iteration 16911 : loss : 0.116821, loss_ce: 0.011438, loss_dice: 0.222204
[10:29:19.507] TRAIN: iteration 16912 : loss : 0.148740, loss_ce: 0.003283, loss_dice: 0.294197
[10:29:19.714] TRAIN: iteration 16913 : loss : 0.053783, loss_ce: 0.008133, loss_dice: 0.099432
[10:29:19.922] TRAIN: iteration 16914 : loss : 0.251209, loss_ce: 0.002281, loss_dice: 0.500136
[10:29:20.133] TRAIN: iteration 16915 : loss : 0.152557, loss_ce: 0.003754, loss_dice: 0.301360
[10:29:20.345] TRAIN: iteration 16916 : loss : 0.054044, loss_ce: 0.005990, loss_dice: 0.102098
[10:29:20.555] TRAIN: iteration 16917 : loss : 0.093278, loss_ce: 0.006036, loss_dice: 0.180520
[10:29:20.820] TRAIN: iteration 16918 : loss : 0.114627, loss_ce: 0.009178, loss_dice: 0.220077
[10:29:21.029] TRAIN: iteration 16919 : loss : 0.251328, loss_ce: 0.002497, loss_dice: 0.500158
[10:29:23.686] TRAIN: iteration 16920 : loss : 0.093777, loss_ce: 0.003507, loss_dice: 0.184047
[10:29:23.927] TRAIN: iteration 16921 : loss : 0.067699, loss_ce: 0.006354, loss_dice: 0.129044
[10:29:24.137] TRAIN: iteration 16922 : loss : 0.251202, loss_ce: 0.002275, loss_dice: 0.500129
[10:29:24.345] TRAIN: iteration 16923 : loss : 0.137422, loss_ce: 0.001820, loss_dice: 0.273024
[10:29:24.555] TRAIN: iteration 16924 : loss : 0.087817, loss_ce: 0.006310, loss_dice: 0.169324
[10:29:24.765] TRAIN: iteration 16925 : loss : 0.060588, loss_ce: 0.002764, loss_dice: 0.118412
[10:29:27.321] TRAIN: iteration 16926 : loss : 0.149387, loss_ce: 0.007552, loss_dice: 0.291222
[10:29:27.531] TRAIN: iteration 16927 : loss : 0.250783, loss_ce: 0.001513, loss_dice: 0.500053
[10:29:27.738] TRAIN: iteration 16928 : loss : 0.144650, loss_ce: 0.002528, loss_dice: 0.286772
[10:29:27.946] TRAIN: iteration 16929 : loss : 0.049166, loss_ce: 0.002348, loss_dice: 0.095984
[10:29:28.179] TRAIN: iteration 16930 : loss : 0.099875, loss_ce: 0.001670, loss_dice: 0.198080
[10:29:28.388] TRAIN: iteration 16931 : loss : 0.230703, loss_ce: 0.002244, loss_dice: 0.459162
[10:29:28.595] TRAIN: iteration 16932 : loss : 0.100106, loss_ce: 0.003407, loss_dice: 0.196804
[10:29:28.807] TRAIN: iteration 16933 : loss : 0.226615, loss_ce: 0.001489, loss_dice: 0.451741
[10:29:29.017] TRAIN: iteration 16934 : loss : 0.043686, loss_ce: 0.001233, loss_dice: 0.086139
[10:29:29.226] TRAIN: iteration 16935 : loss : 0.204833, loss_ce: 0.004449, loss_dice: 0.405218
[10:29:32.229] TRAIN: iteration 16936 : loss : 0.171407, loss_ce: 0.000646, loss_dice: 0.342168
[10:29:32.444] TRAIN: iteration 16937 : loss : 0.075541, loss_ce: 0.001510, loss_dice: 0.149572
[10:29:32.654] TRAIN: iteration 16938 : loss : 0.246712, loss_ce: 0.003450, loss_dice: 0.489973
[10:29:32.870] TRAIN: iteration 16939 : loss : 0.171056, loss_ce: 0.007248, loss_dice: 0.334865
[10:29:33.088] TRAIN: iteration 16940 : loss : 0.230583, loss_ce: 0.000462, loss_dice: 0.460703
[10:29:33.319] TRAIN: iteration 16941 : loss : 0.107539, loss_ce: 0.006142, loss_dice: 0.208935
[10:29:33.528] TRAIN: iteration 16942 : loss : 0.082439, loss_ce: 0.016290, loss_dice: 0.148587
[10:29:33.737] TRAIN: iteration 16943 : loss : 0.228750, loss_ce: 0.004701, loss_dice: 0.452799
[10:29:35.892] TRAIN: iteration 16944 : loss : 0.136165, loss_ce: 0.010854, loss_dice: 0.261476
[10:29:36.110] TRAIN: iteration 16945 : loss : 0.222757, loss_ce: 0.006182, loss_dice: 0.439332
[10:29:36.318] TRAIN: iteration 16946 : loss : 0.076618, loss_ce: 0.001751, loss_dice: 0.151485
[10:29:36.527] TRAIN: iteration 16947 : loss : 0.083406, loss_ce: 0.009387, loss_dice: 0.157425
[10:29:36.735] TRAIN: iteration 16948 : loss : 0.148247, loss_ce: 0.002438, loss_dice: 0.294057
[10:29:36.944] TRAIN: iteration 16949 : loss : 0.161436, loss_ce: 0.005838, loss_dice: 0.317034
[10:29:37.151] TRAIN: iteration 16950 : loss : 0.250337, loss_ce: 0.000667, loss_dice: 0.500007
[10:29:37.361] TRAIN: iteration 16951 : loss : 0.214438, loss_ce: 0.007111, loss_dice: 0.421765
[10:29:40.338] TRAIN: iteration 16952 : loss : 0.250311, loss_ce: 0.001407, loss_dice: 0.499215
[10:29:40.549] TRAIN: iteration 16953 : loss : 0.107279, loss_ce: 0.002164, loss_dice: 0.212393
[10:29:40.758] TRAIN: iteration 16954 : loss : 0.145033, loss_ce: 0.003845, loss_dice: 0.286221
[10:29:40.966] TRAIN: iteration 16955 : loss : 0.060710, loss_ce: 0.001724, loss_dice: 0.119696
[10:29:41.176] TRAIN: iteration 16956 : loss : 0.063958, loss_ce: 0.002694, loss_dice: 0.125222
[10:29:41.385] TRAIN: iteration 16957 : loss : 0.252213, loss_ce: 0.004520, loss_dice: 0.499905
[10:29:42.005] TRAIN: iteration 16958 : loss : 0.093257, loss_ce: 0.005558, loss_dice: 0.180956
[10:29:42.213] TRAIN: iteration 16959 : loss : 0.096916, loss_ce: 0.006237, loss_dice: 0.187595
[10:29:44.232] TRAIN: iteration 16960 : loss : 0.041679, loss_ce: 0.007867, loss_dice: 0.075490
[10:29:44.471] TRAIN: iteration 16961 : loss : 0.136205, loss_ce: 0.004904, loss_dice: 0.267505
[10:29:44.679] TRAIN: iteration 16962 : loss : 0.199010, loss_ce: 0.006006, loss_dice: 0.392014
[10:29:44.889] TRAIN: iteration 16963 : loss : 0.182591, loss_ce: 0.005917, loss_dice: 0.359265
[10:29:45.100] TRAIN: iteration 16964 : loss : 0.057882, loss_ce: 0.008030, loss_dice: 0.107734
[10:29:45.309] TRAIN: iteration 16965 : loss : 0.056279, loss_ce: 0.002820, loss_dice: 0.109739
[10:29:46.715] TRAIN: iteration 16966 : loss : 0.079378, loss_ce: 0.005743, loss_dice: 0.153014
[10:29:46.924] TRAIN: iteration 16967 : loss : 0.062234, loss_ce: 0.001486, loss_dice: 0.122981
[10:29:48.310] TRAIN: iteration 16968 : loss : 0.127695, loss_ce: 0.004995, loss_dice: 0.250395
[10:29:48.519] TRAIN: iteration 16969 : loss : 0.198812, loss_ce: 0.004582, loss_dice: 0.393042
[10:29:48.727] TRAIN: iteration 16970 : loss : 0.088822, loss_ce: 0.002694, loss_dice: 0.174949
[10:29:48.934] TRAIN: iteration 16971 : loss : 0.141570, loss_ce: 0.008117, loss_dice: 0.275024
[10:29:49.143] TRAIN: iteration 16972 : loss : 0.068467, loss_ce: 0.001370, loss_dice: 0.135564
[10:29:49.356] TRAIN: iteration 16973 : loss : 0.064183, loss_ce: 0.003154, loss_dice: 0.125212
[10:29:49.566] TRAIN: iteration 16974 : loss : 0.047676, loss_ce: 0.003486, loss_dice: 0.091866
[10:29:50.474] TRAIN: iteration 16975 : loss : 0.073528, loss_ce: 0.004308, loss_dice: 0.142749
[10:29:54.373] TRAIN: iteration 16976 : loss : 0.225698, loss_ce: 0.001346, loss_dice: 0.450050
[10:29:54.581] TRAIN: iteration 16977 : loss : 0.124710, loss_ce: 0.004565, loss_dice: 0.244855
[10:29:54.788] TRAIN: iteration 16978 : loss : 0.250726, loss_ce: 0.001412, loss_dice: 0.500041
[10:29:55.004] TRAIN: iteration 16979 : loss : 0.250773, loss_ce: 0.001475, loss_dice: 0.500071
[10:29:55.213] TRAIN: iteration 16980 : loss : 0.241785, loss_ce: 0.004984, loss_dice: 0.478585
[10:29:55.450] TRAIN: iteration 16981 : loss : 0.016884, loss_ce: 0.001549, loss_dice: 0.032219
[10:29:55.659] TRAIN: iteration 16982 : loss : 0.009128, loss_ce: 0.001045, loss_dice: 0.017211
[10:29:55.870] TRAIN: iteration 16983 : loss : 0.075101, loss_ce: 0.004024, loss_dice: 0.146177
[10:29:56.449] TRAIN: iteration 16984 : loss : 0.085805, loss_ce: 0.005592, loss_dice: 0.166019
[10:29:56.658] TRAIN: iteration 16985 : loss : 0.250008, loss_ce: 0.004865, loss_dice: 0.495150
[10:29:56.873] TRAIN: iteration 16986 : loss : 0.167455, loss_ce: 0.002971, loss_dice: 0.331939
[10:29:57.083] TRAIN: iteration 16987 : loss : 0.251309, loss_ce: 0.002457, loss_dice: 0.500161
[10:29:57.296] TRAIN: iteration 16988 : loss : 0.251442, loss_ce: 0.002709, loss_dice: 0.500175
[10:29:57.504] TRAIN: iteration 16989 : loss : 0.100571, loss_ce: 0.005285, loss_dice: 0.195857
[10:29:57.711] TRAIN: iteration 16990 : loss : 0.043494, loss_ce: 0.004223, loss_dice: 0.082764
[10:29:57.919] TRAIN: iteration 16991 : loss : 0.251077, loss_ce: 0.002046, loss_dice: 0.500108
[10:30:00.122] TRAIN: iteration 16992 : loss : 0.126846, loss_ce: 0.006456, loss_dice: 0.247236
[10:30:00.367] TRAIN: iteration 16993 : loss : 0.189953, loss_ce: 0.002845, loss_dice: 0.377062
[10:30:00.575] TRAIN: iteration 16994 : loss : 0.094837, loss_ce: 0.005597, loss_dice: 0.184076
[10:30:00.782] TRAIN: iteration 16995 : loss : 0.030239, loss_ce: 0.002133, loss_dice: 0.058345
[10:30:00.996] TRAIN: iteration 16996 : loss : 0.225947, loss_ce: 0.003514, loss_dice: 0.448380
[10:30:01.207] TRAIN: iteration 16997 : loss : 0.122952, loss_ce: 0.003521, loss_dice: 0.242383
[10:30:01.416] TRAIN: iteration 16998 : loss : 0.250655, loss_ce: 0.001267, loss_dice: 0.500043
[10:30:01.627] TRAIN: iteration 16999 : loss : 0.251867, loss_ce: 0.003489, loss_dice: 0.500246
[10:30:02.539] TRAIN: iteration 17000 : loss : 0.134083, loss_ce: 0.004947, loss_dice: 0.263220
[10:30:02.777] TRAIN: iteration 17001 : loss : 0.244708, loss_ce: 0.002063, loss_dice: 0.487353
[10:30:02.985] TRAIN: iteration 17002 : loss : 0.177621, loss_ce: 0.003376, loss_dice: 0.351865
[10:30:03.947] TRAIN: iteration 17003 : loss : 0.039141, loss_ce: 0.003235, loss_dice: 0.075047
[10:30:04.281] TRAIN: iteration 17004 : loss : 0.067927, loss_ce: 0.005869, loss_dice: 0.129984
[10:30:04.489] TRAIN: iteration 17005 : loss : 0.102814, loss_ce: 0.007001, loss_dice: 0.198626
[10:30:04.697] TRAIN: iteration 17006 : loss : 0.178767, loss_ce: 0.002026, loss_dice: 0.355508
[10:30:04.905] TRAIN: iteration 17007 : loss : 0.214761, loss_ce: 0.002572, loss_dice: 0.426950
[10:30:07.551] TRAIN: iteration 17008 : loss : 0.038598, loss_ce: 0.004551, loss_dice: 0.072645
[10:30:07.766] TRAIN: iteration 17009 : loss : 0.108054, loss_ce: 0.002618, loss_dice: 0.213491
[10:30:07.975] TRAIN: iteration 17010 : loss : 0.142942, loss_ce: 0.004566, loss_dice: 0.281318
[10:30:08.184] TRAIN: iteration 17011 : loss : 0.050035, loss_ce: 0.001674, loss_dice: 0.098397
[10:30:08.392] TRAIN: iteration 17012 : loss : 0.073093, loss_ce: 0.003921, loss_dice: 0.142266
[10:30:08.600] TRAIN: iteration 17013 : loss : 0.086822, loss_ce: 0.004757, loss_dice: 0.168887
[10:30:08.809] TRAIN: iteration 17014 : loss : 0.111180, loss_ce: 0.004580, loss_dice: 0.217780
[10:30:09.414] TRAIN: iteration 17015 : loss : 0.052062, loss_ce: 0.001215, loss_dice: 0.102908
[10:30:10.132] TRAIN: iteration 17016 : loss : 0.039417, loss_ce: 0.001651, loss_dice: 0.077184
[10:30:10.341] TRAIN: iteration 17017 : loss : 0.186239, loss_ce: 0.021265, loss_dice: 0.351214
[10:30:10.550] TRAIN: iteration 17018 : loss : 0.116929, loss_ce: 0.003575, loss_dice: 0.230282
[10:30:10.759] TRAIN: iteration 17019 : loss : 0.115088, loss_ce: 0.002523, loss_dice: 0.227652
[10:30:10.969] TRAIN: iteration 17020 : loss : 0.061635, loss_ce: 0.002562, loss_dice: 0.120708
[10:30:11.209] TRAIN: iteration 17021 : loss : 0.097844, loss_ce: 0.002911, loss_dice: 0.192776
[10:30:11.419] TRAIN: iteration 17022 : loss : 0.064163, loss_ce: 0.007063, loss_dice: 0.121262
[10:30:19.199] TRAIN: iteration 17023 : loss : 0.094282, loss_ce: 0.002275, loss_dice: 0.186288
[10:30:19.412] TRAIN: iteration 17024 : loss : 0.115641, loss_ce: 0.006238, loss_dice: 0.225044
[10:30:19.621] TRAIN: iteration 17025 : loss : 0.168413, loss_ce: 0.045281, loss_dice: 0.291544
[10:30:19.830] TRAIN: iteration 17026 : loss : 0.103083, loss_ce: 0.002764, loss_dice: 0.203402
[10:30:20.040] TRAIN: iteration 17027 : loss : 0.125730, loss_ce: 0.005445, loss_dice: 0.246015
[10:30:20.252] TRAIN: iteration 17028 : loss : 0.111416, loss_ce: 0.002577, loss_dice: 0.220256
[10:30:20.461] TRAIN: iteration 17029 : loss : 0.250549, loss_ce: 0.001072, loss_dice: 0.500026
[10:30:20.669] TRAIN: iteration 17030 : loss : 0.095108, loss_ce: 0.007418, loss_dice: 0.182799
[10:30:23.364] TRAIN: iteration 17031 : loss : 0.127602, loss_ce: 0.002247, loss_dice: 0.252957
[10:30:23.572] TRAIN: iteration 17032 : loss : 0.219058, loss_ce: 0.002652, loss_dice: 0.435464
[10:30:23.780] TRAIN: iteration 17033 : loss : 0.095047, loss_ce: 0.003539, loss_dice: 0.186556
[10:30:23.989] TRAIN: iteration 17034 : loss : 0.087356, loss_ce: 0.002584, loss_dice: 0.172128
[10:30:24.200] TRAIN: iteration 17035 : loss : 0.151565, loss_ce: 0.009113, loss_dice: 0.294017
[10:30:24.411] TRAIN: iteration 17036 : loss : 0.250211, loss_ce: 0.001851, loss_dice: 0.498572
[10:30:24.619] TRAIN: iteration 17037 : loss : 0.174878, loss_ce: 0.005561, loss_dice: 0.344195
[10:30:24.832] TRAIN: iteration 17038 : loss : 0.212240, loss_ce: 0.002456, loss_dice: 0.422024
[10:30:28.757] TRAIN: iteration 17039 : loss : 0.166509, loss_ce: 0.010533, loss_dice: 0.322484
[10:30:28.965] TRAIN: iteration 17040 : loss : 0.064978, loss_ce: 0.002735, loss_dice: 0.127220
[10:30:29.203] TRAIN: iteration 17041 : loss : 0.247170, loss_ce: 0.001397, loss_dice: 0.492943
[10:30:29.410] TRAIN: iteration 17042 : loss : 0.072819, loss_ce: 0.003532, loss_dice: 0.142107
[10:30:29.618] TRAIN: iteration 17043 : loss : 0.105489, loss_ce: 0.003638, loss_dice: 0.207339
[10:30:29.833] TRAIN: iteration 17044 : loss : 0.163937, loss_ce: 0.004989, loss_dice: 0.322885
[10:30:30.132] TRAIN: iteration 17045 : loss : 0.228100, loss_ce: 0.003766, loss_dice: 0.452434
[10:30:30.340] TRAIN: iteration 17046 : loss : 0.037105, loss_ce: 0.003551, loss_dice: 0.070659
[10:30:35.575] TRAIN: iteration 17047 : loss : 0.058017, loss_ce: 0.005365, loss_dice: 0.110670
[10:30:35.783] TRAIN: iteration 17048 : loss : 0.215129, loss_ce: 0.023255, loss_dice: 0.407002
[10:30:35.992] TRAIN: iteration 17049 : loss : 0.113180, loss_ce: 0.016144, loss_dice: 0.210215
[10:30:36.199] TRAIN: iteration 17050 : loss : 0.057411, loss_ce: 0.008140, loss_dice: 0.106682
[10:30:36.407] TRAIN: iteration 17051 : loss : 0.180473, loss_ce: 0.009016, loss_dice: 0.351930
[10:30:36.622] TRAIN: iteration 17052 : loss : 0.215905, loss_ce: 0.007124, loss_dice: 0.424686
[10:30:36.830] TRAIN: iteration 17053 : loss : 0.226488, loss_ce: 0.002366, loss_dice: 0.450610
[10:30:37.037] TRAIN: iteration 17054 : loss : 0.190781, loss_ce: 0.007427, loss_dice: 0.374135
[10:30:41.735] TRAIN: iteration 17055 : loss : 0.026869, loss_ce: 0.002030, loss_dice: 0.051708
[10:30:41.951] TRAIN: iteration 17056 : loss : 0.251207, loss_ce: 0.002301, loss_dice: 0.500112
[10:30:42.159] TRAIN: iteration 17057 : loss : 0.100491, loss_ce: 0.018490, loss_dice: 0.182493
[10:30:42.370] TRAIN: iteration 17058 : loss : 0.082731, loss_ce: 0.007253, loss_dice: 0.158210
[10:30:42.579] TRAIN: iteration 17059 : loss : 0.251443, loss_ce: 0.002745, loss_dice: 0.500141
[10:30:42.792] TRAIN: iteration 17060 : loss : 0.079128, loss_ce: 0.004860, loss_dice: 0.153397
[10:30:43.031] TRAIN: iteration 17061 : loss : 0.083597, loss_ce: 0.002107, loss_dice: 0.165087
[10:30:43.242] TRAIN: iteration 17062 : loss : 0.144958, loss_ce: 0.006751, loss_dice: 0.283164
[10:30:45.947] TRAIN: iteration 17063 : loss : 0.210860, loss_ce: 0.008709, loss_dice: 0.413012
[10:30:46.162] TRAIN: iteration 17064 : loss : 0.112015, loss_ce: 0.007959, loss_dice: 0.216072
[10:30:46.370] TRAIN: iteration 17065 : loss : 0.228715, loss_ce: 0.004290, loss_dice: 0.453140
[10:30:46.579] TRAIN: iteration 17066 : loss : 0.114014, loss_ce: 0.003561, loss_dice: 0.224467
[10:30:46.787] TRAIN: iteration 17067 : loss : 0.097931, loss_ce: 0.008783, loss_dice: 0.187079
[10:30:46.998] TRAIN: iteration 17068 : loss : 0.251347, loss_ce: 0.002557, loss_dice: 0.500136
[10:30:47.207] TRAIN: iteration 17069 : loss : 0.090299, loss_ce: 0.002741, loss_dice: 0.177857
[10:30:47.416] TRAIN: iteration 17070 : loss : 0.051819, loss_ce: 0.007325, loss_dice: 0.096312
[10:30:49.392] TRAIN: iteration 17071 : loss : 0.096348, loss_ce: 0.002280, loss_dice: 0.190416
[10:30:49.606] TRAIN: iteration 17072 : loss : 0.251600, loss_ce: 0.003064, loss_dice: 0.500136
[10:30:49.817] TRAIN: iteration 17073 : loss : 0.072512, loss_ce: 0.007840, loss_dice: 0.137184
[10:30:50.025] TRAIN: iteration 17074 : loss : 0.249355, loss_ce: 0.003064, loss_dice: 0.495646
[10:30:50.234] TRAIN: iteration 17075 : loss : 0.102429, loss_ce: 0.002000, loss_dice: 0.202858
[10:30:50.760] TRAIN: iteration 17076 : loss : 0.197499, loss_ce: 0.002286, loss_dice: 0.392712
[10:30:50.969] TRAIN: iteration 17077 : loss : 0.124852, loss_ce: 0.010894, loss_dice: 0.238810
[10:30:51.177] TRAIN: iteration 17078 : loss : 0.093597, loss_ce: 0.005361, loss_dice: 0.181833
[10:30:51.858] TRAIN: iteration 17079 : loss : 0.248651, loss_ce: 0.001930, loss_dice: 0.495371
[10:30:52.066] TRAIN: iteration 17080 : loss : 0.031828, loss_ce: 0.004577, loss_dice: 0.059079
[10:30:52.308] TRAIN: iteration 17081 : loss : 0.122527, loss_ce: 0.003225, loss_dice: 0.241829
[10:30:52.666] TRAIN: iteration 17082 : loss : 0.211363, loss_ce: 0.005999, loss_dice: 0.416728
[10:30:56.193] TRAIN: iteration 17083 : loss : 0.094671, loss_ce: 0.005579, loss_dice: 0.183764
[10:30:58.064] TRAIN: iteration 17084 : loss : 0.220626, loss_ce: 0.001563, loss_dice: 0.439690
[10:30:58.273] TRAIN: iteration 17085 : loss : 0.090234, loss_ce: 0.003942, loss_dice: 0.176525
[10:30:58.481] TRAIN: iteration 17086 : loss : 0.026074, loss_ce: 0.003219, loss_dice: 0.048930
[10:30:58.689] TRAIN: iteration 17087 : loss : 0.026445, loss_ce: 0.002557, loss_dice: 0.050333
[10:30:58.899] TRAIN: iteration 17088 : loss : 0.168713, loss_ce: 0.004939, loss_dice: 0.332486
[10:30:59.108] TRAIN: iteration 17089 : loss : 0.251637, loss_ce: 0.003496, loss_dice: 0.499777
[10:30:59.317] TRAIN: iteration 17090 : loss : 0.108663, loss_ce: 0.002450, loss_dice: 0.214877
[10:31:01.346] TRAIN: iteration 17091 : loss : 0.250649, loss_ce: 0.001267, loss_dice: 0.500030
[10:31:03.474] TRAIN: iteration 17092 : loss : 0.187448, loss_ce: 0.002830, loss_dice: 0.372067
[10:31:03.688] TRAIN: iteration 17093 : loss : 0.079244, loss_ce: 0.006944, loss_dice: 0.151543
[10:31:03.898] TRAIN: iteration 17094 : loss : 0.250914, loss_ce: 0.001751, loss_dice: 0.500078
[10:31:04.106] TRAIN: iteration 17095 : loss : 0.250646, loss_ce: 0.001435, loss_dice: 0.499856
[10:31:04.315] TRAIN: iteration 17096 : loss : 0.063022, loss_ce: 0.003447, loss_dice: 0.122597
[10:31:04.529] TRAIN: iteration 17097 : loss : 0.034230, loss_ce: 0.001206, loss_dice: 0.067255
[10:31:04.737] TRAIN: iteration 17098 : loss : 0.080801, loss_ce: 0.002733, loss_dice: 0.158869
[10:31:07.987] TRAIN: iteration 17099 : loss : 0.198521, loss_ce: 0.004047, loss_dice: 0.392994
[10:31:09.351] TRAIN: iteration 17100 : loss : 0.066813, loss_ce: 0.004703, loss_dice: 0.128924
[10:31:09.590] TRAIN: iteration 17101 : loss : 0.199023, loss_ce: 0.007672, loss_dice: 0.390375
[10:31:09.800] TRAIN: iteration 17102 : loss : 0.131117, loss_ce: 0.008718, loss_dice: 0.253515
[10:31:10.011] TRAIN: iteration 17103 : loss : 0.057033, loss_ce: 0.002096, loss_dice: 0.111969
[10:31:10.220] TRAIN: iteration 17104 : loss : 0.093439, loss_ce: 0.002857, loss_dice: 0.184021
[10:31:10.429] TRAIN: iteration 17105 : loss : 0.049003, loss_ce: 0.004934, loss_dice: 0.093072
[10:31:10.638] TRAIN: iteration 17106 : loss : 0.212383, loss_ce: 0.028430, loss_dice: 0.396337
[10:31:12.420] TRAIN: iteration 17107 : loss : 0.238976, loss_ce: 0.007456, loss_dice: 0.470497
[10:31:15.378] TRAIN: iteration 17108 : loss : 0.089377, loss_ce: 0.005232, loss_dice: 0.173523
[10:31:15.592] TRAIN: iteration 17109 : loss : 0.133583, loss_ce: 0.006544, loss_dice: 0.260621
[10:31:15.803] TRAIN: iteration 17110 : loss : 0.044756, loss_ce: 0.001082, loss_dice: 0.088430
[10:31:16.014] TRAIN: iteration 17111 : loss : 0.094848, loss_ce: 0.005941, loss_dice: 0.183755
[10:31:16.222] TRAIN: iteration 17112 : loss : 0.040900, loss_ce: 0.000973, loss_dice: 0.080827
[10:31:16.430] TRAIN: iteration 17113 : loss : 0.211086, loss_ce: 0.004271, loss_dice: 0.417900
[10:31:16.639] TRAIN: iteration 17114 : loss : 0.251614, loss_ce: 0.003054, loss_dice: 0.500175
[10:31:18.613] TRAIN: iteration 17115 : loss : 0.089246, loss_ce: 0.006499, loss_dice: 0.171993
[10:31:20.703] TRAIN: iteration 17116 : loss : 0.145430, loss_ce: 0.006110, loss_dice: 0.284751
[10:31:20.912] TRAIN: iteration 17117 : loss : 0.215692, loss_ce: 0.022853, loss_dice: 0.408532
[10:31:21.119] TRAIN: iteration 17118 : loss : 0.166164, loss_ce: 0.001889, loss_dice: 0.330439
[10:31:21.328] TRAIN: iteration 17119 : loss : 0.238822, loss_ce: 0.005065, loss_dice: 0.472580
[10:31:21.537] TRAIN: iteration 17120 : loss : 0.241746, loss_ce: 0.005619, loss_dice: 0.477873
[10:31:21.776] TRAIN: iteration 17121 : loss : 0.114561, loss_ce: 0.005196, loss_dice: 0.223925
[10:31:21.993] TRAIN: iteration 17122 : loss : 0.136882, loss_ce: 0.015325, loss_dice: 0.258439
[10:31:24.100] TRAIN: iteration 17123 : loss : 0.109611, loss_ce: 0.003872, loss_dice: 0.215350
[10:31:25.255] TRAIN: iteration 17124 : loss : 0.251482, loss_ce: 0.002784, loss_dice: 0.500180
[10:31:25.464] TRAIN: iteration 17125 : loss : 0.251515, loss_ce: 0.002871, loss_dice: 0.500159
[10:31:25.672] TRAIN: iteration 17126 : loss : 0.245645, loss_ce: 0.027093, loss_dice: 0.464197
[10:31:25.880] TRAIN: iteration 17127 : loss : 0.065510, loss_ce: 0.003229, loss_dice: 0.127791
[10:31:26.088] TRAIN: iteration 17128 : loss : 0.051321, loss_ce: 0.003898, loss_dice: 0.098744
[10:31:26.297] TRAIN: iteration 17129 : loss : 0.037616, loss_ce: 0.001327, loss_dice: 0.073906
[10:31:26.506] TRAIN: iteration 17130 : loss : 0.162580, loss_ce: 0.002844, loss_dice: 0.322316
[10:31:28.218] TRAIN: iteration 17131 : loss : 0.200542, loss_ce: 0.012461, loss_dice: 0.388623
[10:31:35.012] TRAIN: iteration 17132 : loss : 0.030983, loss_ce: 0.000946, loss_dice: 0.061020
[10:31:35.220] TRAIN: iteration 17133 : loss : 0.130057, loss_ce: 0.007164, loss_dice: 0.252949
[10:31:35.429] TRAIN: iteration 17134 : loss : 0.058414, loss_ce: 0.007042, loss_dice: 0.109786
[10:31:35.638] TRAIN: iteration 17135 : loss : 0.251068, loss_ce: 0.001993, loss_dice: 0.500143
[10:31:35.849] TRAIN: iteration 17136 : loss : 0.071574, loss_ce: 0.006089, loss_dice: 0.137059
[10:31:36.058] TRAIN: iteration 17137 : loss : 0.194137, loss_ce: 0.003529, loss_dice: 0.384745
[10:31:36.268] TRAIN: iteration 17138 : loss : 0.068403, loss_ce: 0.002497, loss_dice: 0.134309
[10:31:36.476] TRAIN: iteration 17139 : loss : 0.139665, loss_ce: 0.007831, loss_dice: 0.271498
[10:31:39.740] TRAIN: iteration 17140 : loss : 0.041241, loss_ce: 0.001988, loss_dice: 0.080493
[10:31:39.979] TRAIN: iteration 17141 : loss : 0.042649, loss_ce: 0.009597, loss_dice: 0.075701
[10:31:40.188] TRAIN: iteration 17142 : loss : 0.067641, loss_ce: 0.017090, loss_dice: 0.118192
[10:31:40.395] TRAIN: iteration 17143 : loss : 0.025294, loss_ce: 0.001679, loss_dice: 0.048908
[10:31:40.604] TRAIN: iteration 17144 : loss : 0.103971, loss_ce: 0.007195, loss_dice: 0.200747
[10:31:40.813] TRAIN: iteration 17145 : loss : 0.085155, loss_ce: 0.007836, loss_dice: 0.162473
[10:31:41.023] TRAIN: iteration 17146 : loss : 0.155153, loss_ce: 0.004405, loss_dice: 0.305901
[10:31:41.233] TRAIN: iteration 17147 : loss : 0.226715, loss_ce: 0.015048, loss_dice: 0.438383
[10:31:47.712] TRAIN: iteration 17148 : loss : 0.050828, loss_ce: 0.005166, loss_dice: 0.096490
[10:31:47.919] TRAIN: iteration 17149 : loss : 0.168452, loss_ce: 0.010501, loss_dice: 0.326402
[10:31:48.129] TRAIN: iteration 17150 : loss : 0.116294, loss_ce: 0.003779, loss_dice: 0.228809
[10:31:48.339] TRAIN: iteration 17151 : loss : 0.051759, loss_ce: 0.005509, loss_dice: 0.098010
[10:31:48.548] TRAIN: iteration 17152 : loss : 0.182229, loss_ce: 0.004462, loss_dice: 0.359995
[10:31:48.756] TRAIN: iteration 17153 : loss : 0.080250, loss_ce: 0.003654, loss_dice: 0.156846
[10:31:48.964] TRAIN: iteration 17154 : loss : 0.251724, loss_ce: 0.003199, loss_dice: 0.500248
[10:31:49.174] TRAIN: iteration 17155 : loss : 0.251956, loss_ce: 0.003622, loss_dice: 0.500291
[10:31:53.141] TRAIN: iteration 17156 : loss : 0.251254, loss_ce: 0.002343, loss_dice: 0.500164
[10:31:53.349] TRAIN: iteration 17157 : loss : 0.250962, loss_ce: 0.001806, loss_dice: 0.500117
[10:31:53.568] TRAIN: iteration 17158 : loss : 0.060714, loss_ce: 0.001470, loss_dice: 0.119958
[10:31:53.776] TRAIN: iteration 17159 : loss : 0.133461, loss_ce: 0.010495, loss_dice: 0.256427
[10:31:53.984] TRAIN: iteration 17160 : loss : 0.080835, loss_ce: 0.005094, loss_dice: 0.156576
[10:31:54.219] TRAIN: iteration 17161 : loss : 0.032409, loss_ce: 0.000967, loss_dice: 0.063851
[10:31:54.427] TRAIN: iteration 17162 : loss : 0.142904, loss_ce: 0.001810, loss_dice: 0.283997
[10:31:54.636] TRAIN: iteration 17163 : loss : 0.191749, loss_ce: 0.005128, loss_dice: 0.378370
[10:32:00.958] TRAIN: iteration 17164 : loss : 0.197670, loss_ce: 0.002030, loss_dice: 0.393310
[10:32:01.166] TRAIN: iteration 17165 : loss : 0.260005, loss_ce: 0.019741, loss_dice: 0.500270
[10:32:01.377] TRAIN: iteration 17166 : loss : 0.093371, loss_ce: 0.004532, loss_dice: 0.182210
[10:32:01.587] TRAIN: iteration 17167 : loss : 0.125078, loss_ce: 0.015967, loss_dice: 0.234189
[10:32:01.798] TRAIN: iteration 17168 : loss : 0.205461, loss_ce: 0.001907, loss_dice: 0.409016
[10:32:02.011] TRAIN: iteration 17169 : loss : 0.235931, loss_ce: 0.001136, loss_dice: 0.470726
[10:32:02.219] TRAIN: iteration 17170 : loss : 0.102021, loss_ce: 0.001815, loss_dice: 0.202228
[10:32:02.428] TRAIN: iteration 17171 : loss : 0.230966, loss_ce: 0.001643, loss_dice: 0.460289
[10:32:08.199] TRAIN: iteration 17172 : loss : 0.092620, loss_ce: 0.008123, loss_dice: 0.177117
[10:32:08.406] TRAIN: iteration 17173 : loss : 0.051249, loss_ce: 0.004502, loss_dice: 0.097997
[10:32:08.616] TRAIN: iteration 17174 : loss : 0.083234, loss_ce: 0.003206, loss_dice: 0.163261
[10:32:08.824] TRAIN: iteration 17175 : loss : 0.083920, loss_ce: 0.001793, loss_dice: 0.166046
[10:32:09.033] TRAIN: iteration 17176 : loss : 0.027669, loss_ce: 0.002112, loss_dice: 0.053225
[10:32:09.240] TRAIN: iteration 17177 : loss : 0.210510, loss_ce: 0.004234, loss_dice: 0.416787
[10:32:09.450] TRAIN: iteration 17178 : loss : 0.097831, loss_ce: 0.004587, loss_dice: 0.191075
[10:32:09.658] TRAIN: iteration 17179 : loss : 0.250933, loss_ce: 0.003641, loss_dice: 0.498225
[10:32:13.104] TRAIN: iteration 17180 : loss : 0.086785, loss_ce: 0.004249, loss_dice: 0.169321
[10:32:13.344] TRAIN: iteration 17181 : loss : 0.250890, loss_ce: 0.002904, loss_dice: 0.498876
[10:32:13.554] TRAIN: iteration 17182 : loss : 0.183335, loss_ce: 0.003096, loss_dice: 0.363575
[10:32:13.767] TRAIN: iteration 17183 : loss : 0.117566, loss_ce: 0.006349, loss_dice: 0.228783
[10:32:13.974] TRAIN: iteration 17184 : loss : 0.152776, loss_ce: 0.003254, loss_dice: 0.302297
[10:32:14.182] TRAIN: iteration 17185 : loss : 0.099523, loss_ce: 0.003869, loss_dice: 0.195176
[10:32:14.391] TRAIN: iteration 17186 : loss : 0.132503, loss_ce: 0.006796, loss_dice: 0.258210
[10:32:14.598] TRAIN: iteration 17187 : loss : 0.106940, loss_ce: 0.002775, loss_dice: 0.211104
[10:32:17.478] TRAIN: iteration 17188 : loss : 0.167070, loss_ce: 0.003614, loss_dice: 0.330527
[10:32:18.203] TRAIN: iteration 17189 : loss : 0.206247, loss_ce: 0.003610, loss_dice: 0.408885
[10:32:18.411] TRAIN: iteration 17190 : loss : 0.095395, loss_ce: 0.003095, loss_dice: 0.187695
[10:32:18.621] TRAIN: iteration 17191 : loss : 0.078576, loss_ce: 0.002532, loss_dice: 0.154620
[10:32:18.829] TRAIN: iteration 17192 : loss : 0.251230, loss_ce: 0.002357, loss_dice: 0.500103
[10:32:19.037] TRAIN: iteration 17193 : loss : 0.186908, loss_ce: 0.003539, loss_dice: 0.370278
[10:32:19.246] TRAIN: iteration 17194 : loss : 0.035732, loss_ce: 0.001837, loss_dice: 0.069627
[10:32:19.454] TRAIN: iteration 17195 : loss : 0.098436, loss_ce: 0.002515, loss_dice: 0.194357
[10:32:26.490] TRAIN: iteration 17196 : loss : 0.051204, loss_ce: 0.006038, loss_dice: 0.096369
[10:32:26.698] TRAIN: iteration 17197 : loss : 0.220331, loss_ce: 0.007054, loss_dice: 0.433609
[10:32:26.909] TRAIN: iteration 17198 : loss : 0.168830, loss_ce: 0.008945, loss_dice: 0.328716
[10:32:27.118] TRAIN: iteration 17199 : loss : 0.078826, loss_ce: 0.008330, loss_dice: 0.149321
[10:32:27.326] TRAIN: iteration 17200 : loss : 0.096681, loss_ce: 0.003682, loss_dice: 0.189680
[10:32:27.563] TRAIN: iteration 17201 : loss : 0.097045, loss_ce: 0.003753, loss_dice: 0.190336
[10:32:27.771] TRAIN: iteration 17202 : loss : 0.101121, loss_ce: 0.001802, loss_dice: 0.200440
[10:32:27.979] TRAIN: iteration 17203 : loss : 0.144267, loss_ce: 0.009183, loss_dice: 0.279351
[10:32:34.093] TRAIN: iteration 17204 : loss : 0.248458, loss_ce: 0.004409, loss_dice: 0.492508
[10:32:34.300] TRAIN: iteration 17205 : loss : 0.122254, loss_ce: 0.004567, loss_dice: 0.239941
[10:32:34.509] TRAIN: iteration 17206 : loss : 0.156479, loss_ce: 0.010485, loss_dice: 0.302473
[10:32:34.720] TRAIN: iteration 17207 : loss : 0.251786, loss_ce: 0.003361, loss_dice: 0.500211
[10:32:34.928] TRAIN: iteration 17208 : loss : 0.089335, loss_ce: 0.001963, loss_dice: 0.176708
[10:32:35.137] TRAIN: iteration 17209 : loss : 0.054494, loss_ce: 0.003209, loss_dice: 0.105778
[10:32:35.344] TRAIN: iteration 17210 : loss : 0.118838, loss_ce: 0.005539, loss_dice: 0.232136
[10:32:35.553] TRAIN: iteration 17211 : loss : 0.110224, loss_ce: 0.006159, loss_dice: 0.214289
[10:32:41.336] TRAIN: iteration 17212 : loss : 0.096236, loss_ce: 0.005119, loss_dice: 0.187354
[10:32:41.543] TRAIN: iteration 17213 : loss : 0.068063, loss_ce: 0.003344, loss_dice: 0.132782
[10:32:41.753] TRAIN: iteration 17214 : loss : 0.189579, loss_ce: 0.009158, loss_dice: 0.370001
[10:32:41.962] TRAIN: iteration 17215 : loss : 0.249764, loss_ce: 0.002804, loss_dice: 0.496723
[10:32:42.172] TRAIN: iteration 17216 : loss : 0.077186, loss_ce: 0.002882, loss_dice: 0.151490
[10:32:42.381] TRAIN: iteration 17217 : loss : 0.077624, loss_ce: 0.002761, loss_dice: 0.152487
[10:32:42.590] TRAIN: iteration 17218 : loss : 0.206221, loss_ce: 0.003529, loss_dice: 0.408913
[10:32:42.799] TRAIN: iteration 17219 : loss : 0.136601, loss_ce: 0.004300, loss_dice: 0.268902
[10:32:50.082] TRAIN: iteration 17220 : loss : 0.181563, loss_ce: 0.021944, loss_dice: 0.341182
[10:32:50.325] TRAIN: iteration 17221 : loss : 0.251086, loss_ce: 0.002038, loss_dice: 0.500134
[10:32:50.533] TRAIN: iteration 17222 : loss : 0.047047, loss_ce: 0.004502, loss_dice: 0.089592
[10:32:50.741] TRAIN: iteration 17223 : loss : 0.209821, loss_ce: 0.004312, loss_dice: 0.415330
[10:32:50.948] TRAIN: iteration 17224 : loss : 0.055815, loss_ce: 0.005858, loss_dice: 0.105771
[10:32:51.156] TRAIN: iteration 17225 : loss : 0.046212, loss_ce: 0.002963, loss_dice: 0.089461
[10:32:51.367] TRAIN: iteration 17226 : loss : 0.207019, loss_ce: 0.019224, loss_dice: 0.394813
[10:32:51.577] TRAIN: iteration 17227 : loss : 0.202524, loss_ce: 0.003549, loss_dice: 0.401500
[10:32:56.191] TRAIN: iteration 17228 : loss : 0.139595, loss_ce: 0.001769, loss_dice: 0.277420
[10:32:56.399] TRAIN: iteration 17229 : loss : 0.081682, loss_ce: 0.003152, loss_dice: 0.160212
[10:32:56.608] TRAIN: iteration 17230 : loss : 0.250433, loss_ce: 0.000845, loss_dice: 0.500020
[10:32:56.816] TRAIN: iteration 17231 : loss : 0.250966, loss_ce: 0.002604, loss_dice: 0.499329
[10:32:57.024] TRAIN: iteration 17232 : loss : 0.181360, loss_ce: 0.001349, loss_dice: 0.361370
[10:32:57.233] TRAIN: iteration 17233 : loss : 0.097551, loss_ce: 0.002876, loss_dice: 0.192226
[10:32:57.442] TRAIN: iteration 17234 : loss : 0.086843, loss_ce: 0.004632, loss_dice: 0.169054
[10:32:57.650] TRAIN: iteration 17235 : loss : 0.082580, loss_ce: 0.003957, loss_dice: 0.161204
[10:33:03.931] TRAIN: iteration 17236 : loss : 0.095368, loss_ce: 0.004487, loss_dice: 0.186250
[10:33:04.250] TRAIN: iteration 17237 : loss : 0.250645, loss_ce: 0.001225, loss_dice: 0.500064
[10:33:04.535] TRAIN: iteration 17238 : loss : 0.081763, loss_ce: 0.004818, loss_dice: 0.158708
[10:33:04.745] TRAIN: iteration 17239 : loss : 0.122646, loss_ce: 0.006817, loss_dice: 0.238474
[10:33:04.954] TRAIN: iteration 17240 : loss : 0.066930, loss_ce: 0.003775, loss_dice: 0.130084
[10:33:05.191] TRAIN: iteration 17241 : loss : 0.051721, loss_ce: 0.002103, loss_dice: 0.101339
[10:33:05.402] TRAIN: iteration 17242 : loss : 0.250683, loss_ce: 0.001317, loss_dice: 0.500049
[10:33:05.610] TRAIN: iteration 17243 : loss : 0.106273, loss_ce: 0.003092, loss_dice: 0.209454
[10:33:12.097] TRAIN: iteration 17244 : loss : 0.073813, loss_ce: 0.004737, loss_dice: 0.142890
[10:33:12.305] TRAIN: iteration 17245 : loss : 0.235862, loss_ce: 0.005003, loss_dice: 0.466722
[10:33:12.512] TRAIN: iteration 17246 : loss : 0.231225, loss_ce: 0.007028, loss_dice: 0.455422
[10:33:12.720] TRAIN: iteration 17247 : loss : 0.057171, loss_ce: 0.001195, loss_dice: 0.113148
[10:33:12.930] TRAIN: iteration 17248 : loss : 0.097807, loss_ce: 0.001389, loss_dice: 0.194225
[10:33:13.140] TRAIN: iteration 17249 : loss : 0.226652, loss_ce: 0.003664, loss_dice: 0.449639
[10:33:13.350] TRAIN: iteration 17250 : loss : 0.073138, loss_ce: 0.003426, loss_dice: 0.142851
[10:33:13.560] TRAIN: iteration 17251 : loss : 0.082949, loss_ce: 0.003717, loss_dice: 0.162182
[10:33:19.120] TRAIN: iteration 17252 : loss : 0.058505, loss_ce: 0.002869, loss_dice: 0.114141
[10:33:19.763] TRAIN: iteration 17253 : loss : 0.047554, loss_ce: 0.002678, loss_dice: 0.092430
[10:33:19.971] TRAIN: iteration 17254 : loss : 0.152305, loss_ce: 0.007450, loss_dice: 0.297160
[10:33:20.183] TRAIN: iteration 17255 : loss : 0.223106, loss_ce: 0.004113, loss_dice: 0.442099
[10:33:20.392] TRAIN: iteration 17256 : loss : 0.236186, loss_ce: 0.005892, loss_dice: 0.466480
[10:33:20.604] TRAIN: iteration 17257 : loss : 0.158947, loss_ce: 0.003215, loss_dice: 0.314678
[10:33:20.811] TRAIN: iteration 17258 : loss : 0.160551, loss_ce: 0.003785, loss_dice: 0.317318
[10:33:21.292] TRAIN: iteration 17259 : loss : 0.188727, loss_ce: 0.014686, loss_dice: 0.362768
[10:33:29.816] TRAIN: iteration 17260 : loss : 0.038028, loss_ce: 0.004719, loss_dice: 0.071336
[10:33:30.050] TRAIN: iteration 17261 : loss : 0.180780, loss_ce: 0.002550, loss_dice: 0.359009
[10:33:30.259] TRAIN: iteration 17262 : loss : 0.251196, loss_ce: 0.002274, loss_dice: 0.500118
[10:33:30.468] TRAIN: iteration 17263 : loss : 0.073160, loss_ce: 0.009152, loss_dice: 0.137169
[10:33:30.676] TRAIN: iteration 17264 : loss : 0.176799, loss_ce: 0.006982, loss_dice: 0.346615
[10:33:30.883] TRAIN: iteration 17265 : loss : 0.047496, loss_ce: 0.002488, loss_dice: 0.092505
[10:33:31.095] TRAIN: iteration 17266 : loss : 0.209955, loss_ce: 0.002581, loss_dice: 0.417330
[10:33:31.699] TRAIN: iteration 17267 : loss : 0.077068, loss_ce: 0.009273, loss_dice: 0.144863
[10:33:39.392] TRAIN: iteration 17268 : loss : 0.250917, loss_ce: 0.001770, loss_dice: 0.500063
[10:33:39.600] TRAIN: iteration 17269 : loss : 0.251532, loss_ce: 0.002875, loss_dice: 0.500189
[10:33:39.808] TRAIN: iteration 17270 : loss : 0.079160, loss_ce: 0.002496, loss_dice: 0.155824
[10:33:40.018] TRAIN: iteration 17271 : loss : 0.093611, loss_ce: 0.011597, loss_dice: 0.175624
[10:33:40.227] TRAIN: iteration 17272 : loss : 0.080555, loss_ce: 0.002456, loss_dice: 0.158654
[10:33:40.435] TRAIN: iteration 17273 : loss : 0.033455, loss_ce: 0.001576, loss_dice: 0.065334
[10:33:40.643] TRAIN: iteration 17274 : loss : 0.248621, loss_ce: 0.003508, loss_dice: 0.493733
[10:33:40.852] TRAIN: iteration 17275 : loss : 0.073525, loss_ce: 0.006986, loss_dice: 0.140064
[10:33:49.716] TRAIN: iteration 17276 : loss : 0.191668, loss_ce: 0.007598, loss_dice: 0.375738
[10:33:49.927] TRAIN: iteration 17277 : loss : 0.251679, loss_ce: 0.003268, loss_dice: 0.500089
[10:33:50.141] TRAIN: iteration 17278 : loss : 0.120842, loss_ce: 0.013872, loss_dice: 0.227812
[10:33:50.349] TRAIN: iteration 17279 : loss : 0.251654, loss_ce: 0.003095, loss_dice: 0.500212
[10:33:50.556] TRAIN: iteration 17280 : loss : 0.056151, loss_ce: 0.002290, loss_dice: 0.110012
[10:33:50.792] TRAIN: iteration 17281 : loss : 0.252260, loss_ce: 0.004215, loss_dice: 0.500304
[10:33:51.000] TRAIN: iteration 17282 : loss : 0.175090, loss_ce: 0.009167, loss_dice: 0.341014
[10:33:51.209] TRAIN: iteration 17283 : loss : 0.091024, loss_ce: 0.004584, loss_dice: 0.177463
[10:33:58.509] TRAIN: iteration 17284 : loss : 0.161949, loss_ce: 0.005075, loss_dice: 0.318822
[10:33:58.717] TRAIN: iteration 17285 : loss : 0.095219, loss_ce: 0.005823, loss_dice: 0.184614
[10:33:58.928] TRAIN: iteration 17286 : loss : 0.160747, loss_ce: 0.028246, loss_dice: 0.293249
[10:33:59.138] TRAIN: iteration 17287 : loss : 0.099823, loss_ce: 0.005095, loss_dice: 0.194551
[10:33:59.355] TRAIN: iteration 17288 : loss : 0.056709, loss_ce: 0.002500, loss_dice: 0.110918
[10:33:59.565] TRAIN: iteration 17289 : loss : 0.220929, loss_ce: 0.007011, loss_dice: 0.434846
[10:33:59.774] TRAIN: iteration 17290 : loss : 0.251778, loss_ce: 0.003367, loss_dice: 0.500189
[10:33:59.983] TRAIN: iteration 17291 : loss : 0.155640, loss_ce: 0.006114, loss_dice: 0.305166
[10:34:08.206] TRAIN: iteration 17292 : loss : 0.145579, loss_ce: 0.009867, loss_dice: 0.281292
[10:34:08.415] TRAIN: iteration 17293 : loss : 0.251377, loss_ce: 0.002632, loss_dice: 0.500122
[10:34:08.626] TRAIN: iteration 17294 : loss : 0.242802, loss_ce: 0.002759, loss_dice: 0.482845
[10:34:08.835] TRAIN: iteration 17295 : loss : 0.039610, loss_ce: 0.003521, loss_dice: 0.075700
[10:34:09.044] TRAIN: iteration 17296 : loss : 0.197830, loss_ce: 0.003098, loss_dice: 0.392561
[10:34:09.253] TRAIN: iteration 17297 : loss : 0.038851, loss_ce: 0.001849, loss_dice: 0.075853
[10:34:09.461] TRAIN: iteration 17298 : loss : 0.251054, loss_ce: 0.002045, loss_dice: 0.500063
[10:34:09.670] TRAIN: iteration 17299 : loss : 0.102118, loss_ce: 0.005545, loss_dice: 0.198691
[10:34:17.563] TRAIN: iteration 17300 : loss : 0.251132, loss_ce: 0.002205, loss_dice: 0.500060
[10:34:17.803] TRAIN: iteration 17301 : loss : 0.250818, loss_ce: 0.001589, loss_dice: 0.500047
[10:34:18.018] TRAIN: iteration 17302 : loss : 0.244363, loss_ce: 0.004929, loss_dice: 0.483797
[10:34:18.227] TRAIN: iteration 17303 : loss : 0.241459, loss_ce: 0.003479, loss_dice: 0.479439
[10:34:18.435] TRAIN: iteration 17304 : loss : 0.127098, loss_ce: 0.003985, loss_dice: 0.250210
[10:34:18.644] TRAIN: iteration 17305 : loss : 0.145336, loss_ce: 0.003143, loss_dice: 0.287530
[10:34:18.852] TRAIN: iteration 17306 : loss : 0.014938, loss_ce: 0.001520, loss_dice: 0.028355
[10:34:19.060] TRAIN: iteration 17307 : loss : 0.127258, loss_ce: 0.009571, loss_dice: 0.244946
[10:34:27.451] TRAIN: iteration 17308 : loss : 0.067208, loss_ce: 0.006552, loss_dice: 0.127863
[10:34:27.660] TRAIN: iteration 17309 : loss : 0.114365, loss_ce: 0.006819, loss_dice: 0.221911
[10:34:27.869] TRAIN: iteration 17310 : loss : 0.129670, loss_ce: 0.002117, loss_dice: 0.257222
[10:34:28.078] TRAIN: iteration 17311 : loss : 0.153711, loss_ce: 0.009223, loss_dice: 0.298198
[10:34:28.287] TRAIN: iteration 17312 : loss : 0.094280, loss_ce: 0.008211, loss_dice: 0.180349
[10:34:28.495] TRAIN: iteration 17313 : loss : 0.135731, loss_ce: 0.002328, loss_dice: 0.269133
[10:34:28.703] TRAIN: iteration 17314 : loss : 0.247270, loss_ce: 0.008854, loss_dice: 0.485686
[10:34:28.913] TRAIN: iteration 17315 : loss : 0.073968, loss_ce: 0.007472, loss_dice: 0.140463
[10:34:36.824] TRAIN: iteration 17316 : loss : 0.251570, loss_ce: 0.002950, loss_dice: 0.500190
[10:34:37.032] TRAIN: iteration 17317 : loss : 0.158991, loss_ce: 0.007863, loss_dice: 0.310119
[10:34:37.241] TRAIN: iteration 17318 : loss : 0.075250, loss_ce: 0.003868, loss_dice: 0.146633
[10:34:37.449] TRAIN: iteration 17319 : loss : 0.056514, loss_ce: 0.003092, loss_dice: 0.109936
[10:34:37.657] TRAIN: iteration 17320 : loss : 0.130882, loss_ce: 0.007691, loss_dice: 0.254072
[10:34:37.898] TRAIN: iteration 17321 : loss : 0.157951, loss_ce: 0.007739, loss_dice: 0.308162
[10:34:38.108] TRAIN: iteration 17322 : loss : 0.189329, loss_ce: 0.003210, loss_dice: 0.375448
[10:34:38.318] TRAIN: iteration 17323 : loss : 0.087729, loss_ce: 0.004601, loss_dice: 0.170856
[10:34:45.807] TRAIN: iteration 17324 : loss : 0.250431, loss_ce: 0.000850, loss_dice: 0.500011
[10:34:46.016] TRAIN: iteration 17325 : loss : 0.251782, loss_ce: 0.003337, loss_dice: 0.500227
[10:34:46.226] TRAIN: iteration 17326 : loss : 0.250683, loss_ce: 0.001318, loss_dice: 0.500048
[10:34:46.436] TRAIN: iteration 17327 : loss : 0.104375, loss_ce: 0.004790, loss_dice: 0.203961
[10:34:46.644] TRAIN: iteration 17328 : loss : 0.086701, loss_ce: 0.002198, loss_dice: 0.171205
[10:34:46.853] TRAIN: iteration 17329 : loss : 0.053364, loss_ce: 0.005737, loss_dice: 0.100992
[10:34:47.060] TRAIN: iteration 17330 : loss : 0.110373, loss_ce: 0.005270, loss_dice: 0.215476
[10:34:47.273] TRAIN: iteration 17331 : loss : 0.053318, loss_ce: 0.002164, loss_dice: 0.104472
[10:34:55.538] TRAIN: iteration 17332 : loss : 0.120605, loss_ce: 0.002823, loss_dice: 0.238388
[10:34:55.746] TRAIN: iteration 17333 : loss : 0.051991, loss_ce: 0.003134, loss_dice: 0.100849
[10:34:55.957] TRAIN: iteration 17334 : loss : 0.229298, loss_ce: 0.002317, loss_dice: 0.456279
[10:34:56.164] TRAIN: iteration 17335 : loss : 0.058439, loss_ce: 0.004732, loss_dice: 0.112146
[10:34:56.372] TRAIN: iteration 17336 : loss : 0.236937, loss_ce: 0.002441, loss_dice: 0.471434
[10:34:56.580] TRAIN: iteration 17337 : loss : 0.112932, loss_ce: 0.003233, loss_dice: 0.222630
[10:34:56.788] TRAIN: iteration 17338 : loss : 0.099431, loss_ce: 0.002235, loss_dice: 0.196628
[10:34:56.996] TRAIN: iteration 17339 : loss : 0.117021, loss_ce: 0.010546, loss_dice: 0.223495
[10:35:04.086] TRAIN: iteration 17340 : loss : 0.193441, loss_ce: 0.005513, loss_dice: 0.381369
[10:35:04.323] TRAIN: iteration 17341 : loss : 0.243530, loss_ce: 0.001711, loss_dice: 0.485348
[10:35:04.531] TRAIN: iteration 17342 : loss : 0.083354, loss_ce: 0.004334, loss_dice: 0.162374
[10:35:04.739] TRAIN: iteration 17343 : loss : 0.242432, loss_ce: 0.008269, loss_dice: 0.476595
[10:35:04.946] TRAIN: iteration 17344 : loss : 0.091813, loss_ce: 0.002648, loss_dice: 0.180978
[10:35:05.156] TRAIN: iteration 17345 : loss : 0.236629, loss_ce: 0.001215, loss_dice: 0.472044
[10:35:05.366] TRAIN: iteration 17346 : loss : 0.093037, loss_ce: 0.001926, loss_dice: 0.184148
[10:35:05.574] TRAIN: iteration 17347 : loss : 0.024655, loss_ce: 0.001575, loss_dice: 0.047736
[10:35:13.557] TRAIN: iteration 17348 : loss : 0.188753, loss_ce: 0.005375, loss_dice: 0.372131
[10:35:13.766] TRAIN: iteration 17349 : loss : 0.180448, loss_ce: 0.001857, loss_dice: 0.359038
[10:35:13.863] TRAIN: iteration 17350 : loss : 0.251222, loss_ce: 0.002342, loss_dice: 0.500102
[10:40:47.486] VALIDATION: iteration 9 : loss : 0.130784, loss_ce: 0.004794, loss_dice: 0.256774
[10:40:48.289] TRAIN: iteration 17351 : loss : 0.120133, loss_ce: 0.006577, loss_dice: 0.233690
[10:40:48.504] TRAIN: iteration 17352 : loss : 0.250943, loss_ce: 0.001808, loss_dice: 0.500078
[10:40:48.715] TRAIN: iteration 17353 : loss : 0.241455, loss_ce: 0.002456, loss_dice: 0.480453
[10:40:49.015] TRAIN: iteration 17354 : loss : 0.053080, loss_ce: 0.002683, loss_dice: 0.103476
[10:40:49.226] TRAIN: iteration 17355 : loss : 0.136403, loss_ce: 0.004664, loss_dice: 0.268142
[10:40:49.435] TRAIN: iteration 17356 : loss : 0.121571, loss_ce: 0.016531, loss_dice: 0.226612
[10:40:49.644] TRAIN: iteration 17357 : loss : 0.118102, loss_ce: 0.004743, loss_dice: 0.231460
[10:40:49.855] TRAIN: iteration 17358 : loss : 0.034275, loss_ce: 0.002342, loss_dice: 0.066208
[10:40:50.096] TRAIN: iteration 17359 : loss : 0.250599, loss_ce: 0.002131, loss_dice: 0.499067
[10:40:50.304] TRAIN: iteration 17360 : loss : 0.250960, loss_ce: 0.001852, loss_dice: 0.500068
[10:40:50.606] TRAIN: iteration 17361 : loss : 0.110978, loss_ce: 0.009816, loss_dice: 0.212140
[10:40:50.815] TRAIN: iteration 17362 : loss : 0.066576, loss_ce: 0.004448, loss_dice: 0.128705
[10:40:51.026] TRAIN: iteration 17363 : loss : 0.093734, loss_ce: 0.003227, loss_dice: 0.184242
[10:40:51.236] TRAIN: iteration 17364 : loss : 0.060807, loss_ce: 0.006444, loss_dice: 0.115171
[10:40:51.446] TRAIN: iteration 17365 : loss : 0.251787, loss_ce: 0.003397, loss_dice: 0.500178
[10:40:51.656] TRAIN: iteration 17366 : loss : 0.255030, loss_ce: 0.013435, loss_dice: 0.496625
[10:40:52.378] TRAIN: iteration 17367 : loss : 0.082960, loss_ce: 0.004711, loss_dice: 0.161208
[10:40:52.592] TRAIN: iteration 17368 : loss : 0.252263, loss_ce: 0.004240, loss_dice: 0.500285
[10:40:52.801] TRAIN: iteration 17369 : loss : 0.251763, loss_ce: 0.003326, loss_dice: 0.500200
[10:40:53.010] TRAIN: iteration 17370 : loss : 0.104618, loss_ce: 0.005684, loss_dice: 0.203553
[10:40:53.218] TRAIN: iteration 17371 : loss : 0.253227, loss_ce: 0.006017, loss_dice: 0.500436
[10:40:53.428] TRAIN: iteration 17372 : loss : 0.045286, loss_ce: 0.012791, loss_dice: 0.077781
[10:40:53.639] TRAIN: iteration 17373 : loss : 0.055928, loss_ce: 0.003148, loss_dice: 0.108707
[10:40:53.849] TRAIN: iteration 17374 : loss : 0.208444, loss_ce: 0.004135, loss_dice: 0.412753
[10:40:54.061] TRAIN: iteration 17375 : loss : 0.154396, loss_ce: 0.006659, loss_dice: 0.302133
[10:40:54.271] TRAIN: iteration 17376 : loss : 0.119144, loss_ce: 0.005972, loss_dice: 0.232315
[10:40:55.995] TRAIN: iteration 17377 : loss : 0.051854, loss_ce: 0.002746, loss_dice: 0.100962
[10:40:56.207] TRAIN: iteration 17378 : loss : 0.086754, loss_ce: 0.001935, loss_dice: 0.171573
[10:40:56.417] TRAIN: iteration 17379 : loss : 0.251605, loss_ce: 0.003025, loss_dice: 0.500186
[10:40:56.632] TRAIN: iteration 17380 : loss : 0.251571, loss_ce: 0.002970, loss_dice: 0.500172
[10:40:56.869] TRAIN: iteration 17381 : loss : 0.134397, loss_ce: 0.005017, loss_dice: 0.263777
[10:40:57.081] TRAIN: iteration 17382 : loss : 0.246264, loss_ce: 0.004223, loss_dice: 0.488305
[10:40:57.289] TRAIN: iteration 17383 : loss : 0.089689, loss_ce: 0.006942, loss_dice: 0.172437
[10:40:57.497] TRAIN: iteration 17384 : loss : 0.070528, loss_ce: 0.014958, loss_dice: 0.126098
[10:40:57.824] TRAIN: iteration 17385 : loss : 0.137237, loss_ce: 0.017819, loss_dice: 0.256655
[10:40:58.036] TRAIN: iteration 17386 : loss : 0.035469, loss_ce: 0.004000, loss_dice: 0.066937
[10:40:58.246] TRAIN: iteration 17387 : loss : 0.251575, loss_ce: 0.002965, loss_dice: 0.500185
[10:40:58.458] TRAIN: iteration 17388 : loss : 0.202572, loss_ce: 0.003499, loss_dice: 0.401644
[10:40:58.668] TRAIN: iteration 17389 : loss : 0.250996, loss_ce: 0.001906, loss_dice: 0.500086
[10:40:58.880] TRAIN: iteration 17390 : loss : 0.035027, loss_ce: 0.003735, loss_dice: 0.066319
[10:40:59.096] TRAIN: iteration 17391 : loss : 0.046277, loss_ce: 0.009664, loss_dice: 0.082891
[10:40:59.312] TRAIN: iteration 17392 : loss : 0.066574, loss_ce: 0.002801, loss_dice: 0.130346
[10:41:00.181] TRAIN: iteration 17393 : loss : 0.080333, loss_ce: 0.002904, loss_dice: 0.157762
[10:41:00.390] TRAIN: iteration 17394 : loss : 0.211681, loss_ce: 0.003682, loss_dice: 0.419679
[10:41:00.600] TRAIN: iteration 17395 : loss : 0.122022, loss_ce: 0.003715, loss_dice: 0.240329
[10:41:00.882] TRAIN: iteration 17396 : loss : 0.251827, loss_ce: 0.004209, loss_dice: 0.499445
[10:41:01.093] TRAIN: iteration 17397 : loss : 0.098037, loss_ce: 0.002425, loss_dice: 0.193648
[10:41:01.305] TRAIN: iteration 17398 : loss : 0.134814, loss_ce: 0.031209, loss_dice: 0.238418
[10:41:01.515] TRAIN: iteration 17399 : loss : 0.252171, loss_ce: 0.004040, loss_dice: 0.500302
[10:41:01.727] TRAIN: iteration 17400 : loss : 0.178965, loss_ce: 0.013565, loss_dice: 0.344365
[10:41:01.961] TRAIN: iteration 17401 : loss : 0.154855, loss_ce: 0.003277, loss_dice: 0.306432
[10:41:02.169] TRAIN: iteration 17402 : loss : 0.077357, loss_ce: 0.004255, loss_dice: 0.150459
[10:41:02.377] TRAIN: iteration 17403 : loss : 0.081973, loss_ce: 0.002868, loss_dice: 0.161079
[10:41:02.588] TRAIN: iteration 17404 : loss : 0.194488, loss_ce: 0.003353, loss_dice: 0.385623
[10:41:02.797] TRAIN: iteration 17405 : loss : 0.054562, loss_ce: 0.002320, loss_dice: 0.106803
[10:41:03.005] TRAIN: iteration 17406 : loss : 0.170594, loss_ce: 0.001560, loss_dice: 0.339627
[10:41:03.214] TRAIN: iteration 17407 : loss : 0.140928, loss_ce: 0.002328, loss_dice: 0.279527
[10:41:03.422] TRAIN: iteration 17408 : loss : 0.252266, loss_ce: 0.004200, loss_dice: 0.500331
[10:41:03.631] TRAIN: iteration 17409 : loss : 0.158507, loss_ce: 0.012545, loss_dice: 0.304468
[10:41:03.839] TRAIN: iteration 17410 : loss : 0.093323, loss_ce: 0.006171, loss_dice: 0.180476
[10:41:04.052] TRAIN: iteration 17411 : loss : 0.023798, loss_ce: 0.001632, loss_dice: 0.045964
[10:41:04.263] TRAIN: iteration 17412 : loss : 0.133435, loss_ce: 0.001607, loss_dice: 0.265264
[10:41:04.471] TRAIN: iteration 17413 : loss : 0.250720, loss_ce: 0.001382, loss_dice: 0.500057
[10:41:04.679] TRAIN: iteration 17414 : loss : 0.065997, loss_ce: 0.013019, loss_dice: 0.118975
[10:41:04.886] TRAIN: iteration 17415 : loss : 0.080040, loss_ce: 0.001435, loss_dice: 0.158644
[10:41:05.097] TRAIN: iteration 17416 : loss : 0.121170, loss_ce: 0.006989, loss_dice: 0.235350
[10:41:05.399] TRAIN: iteration 17417 : loss : 0.092200, loss_ce: 0.004080, loss_dice: 0.180319
[10:41:05.609] TRAIN: iteration 17418 : loss : 0.250673, loss_ce: 0.001312, loss_dice: 0.500034
[10:41:05.818] TRAIN: iteration 17419 : loss : 0.182996, loss_ce: 0.005695, loss_dice: 0.360297
[10:41:06.027] TRAIN: iteration 17420 : loss : 0.137889, loss_ce: 0.006134, loss_dice: 0.269644
[10:41:06.273] TRAIN: iteration 17421 : loss : 0.233450, loss_ce: 0.002632, loss_dice: 0.464269
[10:41:06.481] TRAIN: iteration 17422 : loss : 0.078260, loss_ce: 0.003078, loss_dice: 0.153442
[10:41:06.691] TRAIN: iteration 17423 : loss : 0.251643, loss_ce: 0.003099, loss_dice: 0.500187
[10:41:06.906] TRAIN: iteration 17424 : loss : 0.147163, loss_ce: 0.005981, loss_dice: 0.288345
[10:41:07.116] TRAIN: iteration 17425 : loss : 0.134920, loss_ce: 0.002653, loss_dice: 0.267187
[10:41:07.609] TRAIN: iteration 17426 : loss : 0.140871, loss_ce: 0.006800, loss_dice: 0.274942
[10:41:07.822] TRAIN: iteration 17427 : loss : 0.139146, loss_ce: 0.011464, loss_dice: 0.266827
[10:41:08.032] TRAIN: iteration 17428 : loss : 0.106587, loss_ce: 0.004589, loss_dice: 0.208585
[10:41:08.243] TRAIN: iteration 17429 : loss : 0.067463, loss_ce: 0.002303, loss_dice: 0.132623
[10:41:08.451] TRAIN: iteration 17430 : loss : 0.070502, loss_ce: 0.004237, loss_dice: 0.136767
[10:41:08.664] TRAIN: iteration 17431 : loss : 0.047273, loss_ce: 0.002215, loss_dice: 0.092331
[10:41:08.872] TRAIN: iteration 17432 : loss : 0.064827, loss_ce: 0.003480, loss_dice: 0.126174
[10:41:09.081] TRAIN: iteration 17433 : loss : 0.248448, loss_ce: 0.008304, loss_dice: 0.488592
[10:41:09.294] TRAIN: iteration 17434 : loss : 0.241338, loss_ce: 0.004063, loss_dice: 0.478613
[10:41:09.502] TRAIN: iteration 17435 : loss : 0.101207, loss_ce: 0.002364, loss_dice: 0.200051
[10:41:09.714] TRAIN: iteration 17436 : loss : 0.251338, loss_ce: 0.002531, loss_dice: 0.500144
[10:41:09.923] TRAIN: iteration 17437 : loss : 0.156277, loss_ce: 0.002760, loss_dice: 0.309793
[10:41:10.134] TRAIN: iteration 17438 : loss : 0.244149, loss_ce: 0.002447, loss_dice: 0.485850
[10:41:10.342] TRAIN: iteration 17439 : loss : 0.045107, loss_ce: 0.004714, loss_dice: 0.085501
[10:41:10.553] TRAIN: iteration 17440 : loss : 0.215481, loss_ce: 0.006193, loss_dice: 0.424768
[10:41:10.793] TRAIN: iteration 17441 : loss : 0.081816, loss_ce: 0.002808, loss_dice: 0.160823
[10:41:11.000] TRAIN: iteration 17442 : loss : 0.051101, loss_ce: 0.006985, loss_dice: 0.095217
[10:41:11.216] TRAIN: iteration 17443 : loss : 0.058297, loss_ce: 0.003487, loss_dice: 0.113107
[10:41:11.424] TRAIN: iteration 17444 : loss : 0.224405, loss_ce: 0.007799, loss_dice: 0.441012
[10:41:11.632] TRAIN: iteration 17445 : loss : 0.205153, loss_ce: 0.007748, loss_dice: 0.402557
[10:41:11.843] TRAIN: iteration 17446 : loss : 0.067090, loss_ce: 0.003620, loss_dice: 0.130560
[10:41:12.053] TRAIN: iteration 17447 : loss : 0.149720, loss_ce: 0.002958, loss_dice: 0.296481
[10:41:12.268] TRAIN: iteration 17448 : loss : 0.118462, loss_ce: 0.005723, loss_dice: 0.231201
[10:41:12.478] TRAIN: iteration 17449 : loss : 0.059860, loss_ce: 0.007342, loss_dice: 0.112378
[10:41:12.688] TRAIN: iteration 17450 : loss : 0.150733, loss_ce: 0.003244, loss_dice: 0.298221
[10:41:12.900] TRAIN: iteration 17451 : loss : 0.164785, loss_ce: 0.002231, loss_dice: 0.327339
[10:41:13.111] TRAIN: iteration 17452 : loss : 0.054565, loss_ce: 0.007454, loss_dice: 0.101676
[10:41:13.323] TRAIN: iteration 17453 : loss : 0.072664, loss_ce: 0.002396, loss_dice: 0.142932
[10:41:13.531] TRAIN: iteration 17454 : loss : 0.111066, loss_ce: 0.004299, loss_dice: 0.217833
[10:41:13.741] TRAIN: iteration 17455 : loss : 0.251351, loss_ce: 0.002563, loss_dice: 0.500139
[10:41:13.948] TRAIN: iteration 17456 : loss : 0.230135, loss_ce: 0.005234, loss_dice: 0.455035
[10:41:14.162] TRAIN: iteration 17457 : loss : 0.213466, loss_ce: 0.003645, loss_dice: 0.423287
[10:41:14.376] TRAIN: iteration 17458 : loss : 0.138995, loss_ce: 0.005583, loss_dice: 0.272407
[10:41:14.592] TRAIN: iteration 17459 : loss : 0.251003, loss_ce: 0.001909, loss_dice: 0.500098
[10:41:14.800] TRAIN: iteration 17460 : loss : 0.129370, loss_ce: 0.011145, loss_dice: 0.247595
[10:41:15.043] TRAIN: iteration 17461 : loss : 0.091511, loss_ce: 0.002291, loss_dice: 0.180731
[10:41:15.254] TRAIN: iteration 17462 : loss : 0.066474, loss_ce: 0.002278, loss_dice: 0.130670
[10:41:15.462] TRAIN: iteration 17463 : loss : 0.069915, loss_ce: 0.003625, loss_dice: 0.136204
[10:41:15.670] TRAIN: iteration 17464 : loss : 0.038555, loss_ce: 0.007310, loss_dice: 0.069799
[10:41:15.878] TRAIN: iteration 17465 : loss : 0.179785, loss_ce: 0.036981, loss_dice: 0.322588
[10:41:16.088] TRAIN: iteration 17466 : loss : 0.071067, loss_ce: 0.006339, loss_dice: 0.135795
[10:41:16.297] TRAIN: iteration 17467 : loss : 0.046410, loss_ce: 0.007089, loss_dice: 0.085730
[10:41:16.507] TRAIN: iteration 17468 : loss : 0.085951, loss_ce: 0.002880, loss_dice: 0.169022
[10:41:16.716] TRAIN: iteration 17469 : loss : 0.191785, loss_ce: 0.002595, loss_dice: 0.380975
[10:41:16.928] TRAIN: iteration 17470 : loss : 0.069459, loss_ce: 0.002736, loss_dice: 0.136182
[10:41:17.136] TRAIN: iteration 17471 : loss : 0.108298, loss_ce: 0.002448, loss_dice: 0.214147
[10:41:17.344] TRAIN: iteration 17472 : loss : 0.046063, loss_ce: 0.005720, loss_dice: 0.086405
[10:41:17.580] TRAIN: iteration 17473 : loss : 0.249611, loss_ce: 0.002718, loss_dice: 0.496503
[10:41:17.798] TRAIN: iteration 17474 : loss : 0.185618, loss_ce: 0.007324, loss_dice: 0.363912
[10:41:18.010] TRAIN: iteration 17475 : loss : 0.096479, loss_ce: 0.008945, loss_dice: 0.184012
[10:41:18.221] TRAIN: iteration 17476 : loss : 0.097048, loss_ce: 0.006802, loss_dice: 0.187294
[10:41:18.432] TRAIN: iteration 17477 : loss : 0.053920, loss_ce: 0.007209, loss_dice: 0.100632
[10:41:18.644] TRAIN: iteration 17478 : loss : 0.183681, loss_ce: 0.009025, loss_dice: 0.358336
[10:41:18.853] TRAIN: iteration 17479 : loss : 0.251614, loss_ce: 0.003018, loss_dice: 0.500210
[10:41:19.069] TRAIN: iteration 17480 : loss : 0.058162, loss_ce: 0.004576, loss_dice: 0.111747
[10:41:19.308] TRAIN: iteration 17481 : loss : 0.251420, loss_ce: 0.002672, loss_dice: 0.500167
[10:41:19.516] TRAIN: iteration 17482 : loss : 0.250974, loss_ce: 0.001850, loss_dice: 0.500097
[10:41:19.727] TRAIN: iteration 17483 : loss : 0.037074, loss_ce: 0.002219, loss_dice: 0.071930
[10:41:19.937] TRAIN: iteration 17484 : loss : 0.250991, loss_ce: 0.001883, loss_dice: 0.500098
[10:41:20.153] TRAIN: iteration 17485 : loss : 0.251337, loss_ce: 0.002511, loss_dice: 0.500163
[10:41:20.364] TRAIN: iteration 17486 : loss : 0.145568, loss_ce: 0.014701, loss_dice: 0.276436
[10:41:20.574] TRAIN: iteration 17487 : loss : 0.078722, loss_ce: 0.002284, loss_dice: 0.155161
[10:41:20.847] TRAIN: iteration 17488 : loss : 0.223129, loss_ce: 0.009206, loss_dice: 0.437051
[10:41:21.063] TRAIN: iteration 17489 : loss : 0.222379, loss_ce: 0.002665, loss_dice: 0.442092
[10:41:21.274] TRAIN: iteration 17490 : loss : 0.133332, loss_ce: 0.003245, loss_dice: 0.263419
[10:41:21.482] TRAIN: iteration 17491 : loss : 0.093403, loss_ce: 0.005235, loss_dice: 0.181572
[10:41:21.701] TRAIN: iteration 17492 : loss : 0.061326, loss_ce: 0.007064, loss_dice: 0.115588
[10:41:21.910] TRAIN: iteration 17493 : loss : 0.078698, loss_ce: 0.002805, loss_dice: 0.154590
[10:41:22.118] TRAIN: iteration 17494 : loss : 0.169905, loss_ce: 0.001849, loss_dice: 0.337960
[10:41:22.326] TRAIN: iteration 17495 : loss : 0.083530, loss_ce: 0.009416, loss_dice: 0.157643
[10:41:22.538] TRAIN: iteration 17496 : loss : 0.080577, loss_ce: 0.002566, loss_dice: 0.158589
[10:41:22.749] TRAIN: iteration 17497 : loss : 0.082330, loss_ce: 0.003323, loss_dice: 0.161337
[10:41:22.956] TRAIN: iteration 17498 : loss : 0.053043, loss_ce: 0.005351, loss_dice: 0.100735
[10:41:23.165] TRAIN: iteration 17499 : loss : 0.078956, loss_ce: 0.003635, loss_dice: 0.154276
[10:41:23.382] TRAIN: iteration 17500 : loss : 0.060912, loss_ce: 0.007126, loss_dice: 0.114699
[10:41:23.631] TRAIN: iteration 17501 : loss : 0.251382, loss_ce: 0.002626, loss_dice: 0.500138
[10:41:23.839] TRAIN: iteration 17502 : loss : 0.041054, loss_ce: 0.003981, loss_dice: 0.078127
[10:41:24.048] TRAIN: iteration 17503 : loss : 0.082109, loss_ce: 0.005748, loss_dice: 0.158470
[10:41:24.257] TRAIN: iteration 17504 : loss : 0.198867, loss_ce: 0.004103, loss_dice: 0.393630
[10:41:24.465] TRAIN: iteration 17505 : loss : 0.026128, loss_ce: 0.001949, loss_dice: 0.050306
[10:41:24.675] TRAIN: iteration 17506 : loss : 0.250943, loss_ce: 0.001803, loss_dice: 0.500083
[10:41:24.885] TRAIN: iteration 17507 : loss : 0.103200, loss_ce: 0.006114, loss_dice: 0.200287
[10:41:25.094] TRAIN: iteration 17508 : loss : 0.076893, loss_ce: 0.004598, loss_dice: 0.149189
[10:41:25.301] TRAIN: iteration 17509 : loss : 0.029186, loss_ce: 0.002134, loss_dice: 0.056238
[10:41:25.508] TRAIN: iteration 17510 : loss : 0.070650, loss_ce: 0.004871, loss_dice: 0.136429
[10:41:25.716] TRAIN: iteration 17511 : loss : 0.112610, loss_ce: 0.009773, loss_dice: 0.215446
[10:41:25.930] TRAIN: iteration 17512 : loss : 0.117207, loss_ce: 0.004848, loss_dice: 0.229566
[10:41:26.141] TRAIN: iteration 17513 : loss : 0.109706, loss_ce: 0.003982, loss_dice: 0.215430
[10:41:26.350] TRAIN: iteration 17514 : loss : 0.062586, loss_ce: 0.002221, loss_dice: 0.122952
[10:41:26.558] TRAIN: iteration 17515 : loss : 0.103786, loss_ce: 0.002021, loss_dice: 0.205550
[10:41:26.766] TRAIN: iteration 17516 : loss : 0.195779, loss_ce: 0.004833, loss_dice: 0.386724
[10:41:26.974] TRAIN: iteration 17517 : loss : 0.250939, loss_ce: 0.001805, loss_dice: 0.500072
[10:41:27.184] TRAIN: iteration 17518 : loss : 0.126214, loss_ce: 0.004075, loss_dice: 0.248352
[10:41:27.391] TRAIN: iteration 17519 : loss : 0.133080, loss_ce: 0.009696, loss_dice: 0.256464
[10:41:27.599] TRAIN: iteration 17520 : loss : 0.105832, loss_ce: 0.004258, loss_dice: 0.207407
[10:41:27.836] TRAIN: iteration 17521 : loss : 0.251247, loss_ce: 0.002355, loss_dice: 0.500140
[10:41:28.043] TRAIN: iteration 17522 : loss : 0.251175, loss_ce: 0.002221, loss_dice: 0.500128
[10:41:28.253] TRAIN: iteration 17523 : loss : 0.077253, loss_ce: 0.002185, loss_dice: 0.152322
[10:41:28.460] TRAIN: iteration 17524 : loss : 0.084286, loss_ce: 0.005883, loss_dice: 0.162688
[10:41:28.668] TRAIN: iteration 17525 : loss : 0.066131, loss_ce: 0.004686, loss_dice: 0.127575
[10:41:28.875] TRAIN: iteration 17526 : loss : 0.149726, loss_ce: 0.004527, loss_dice: 0.294924
[10:41:29.083] TRAIN: iteration 17527 : loss : 0.245064, loss_ce: 0.002092, loss_dice: 0.488036
[10:41:29.291] TRAIN: iteration 17528 : loss : 0.158461, loss_ce: 0.003151, loss_dice: 0.313772
[10:41:29.499] TRAIN: iteration 17529 : loss : 0.059157, loss_ce: 0.002466, loss_dice: 0.115847
[10:41:29.707] TRAIN: iteration 17530 : loss : 0.112723, loss_ce: 0.004819, loss_dice: 0.220626
[10:41:29.915] TRAIN: iteration 17531 : loss : 0.096254, loss_ce: 0.010777, loss_dice: 0.181731
[10:41:30.123] TRAIN: iteration 17532 : loss : 0.186269, loss_ce: 0.003838, loss_dice: 0.368700
[10:41:30.339] TRAIN: iteration 17533 : loss : 0.119121, loss_ce: 0.002018, loss_dice: 0.236225
[10:41:30.554] TRAIN: iteration 17534 : loss : 0.102651, loss_ce: 0.007797, loss_dice: 0.197504
[10:41:30.769] TRAIN: iteration 17535 : loss : 0.220976, loss_ce: 0.003460, loss_dice: 0.438492
[10:41:30.978] TRAIN: iteration 17536 : loss : 0.059990, loss_ce: 0.004556, loss_dice: 0.115424
[10:41:31.187] TRAIN: iteration 17537 : loss : 0.148357, loss_ce: 0.005813, loss_dice: 0.290901
[10:41:31.395] TRAIN: iteration 17538 : loss : 0.088335, loss_ce: 0.003224, loss_dice: 0.173445
[10:41:31.603] TRAIN: iteration 17539 : loss : 0.208314, loss_ce: 0.002390, loss_dice: 0.414238
[10:41:31.816] TRAIN: iteration 17540 : loss : 0.244199, loss_ce: 0.003114, loss_dice: 0.485284
[10:41:32.057] TRAIN: iteration 17541 : loss : 0.116281, loss_ce: 0.004827, loss_dice: 0.227734
[10:41:32.277] TRAIN: iteration 17542 : loss : 0.042803, loss_ce: 0.002228, loss_dice: 0.083378
[10:41:32.489] TRAIN: iteration 17543 : loss : 0.149759, loss_ce: 0.007653, loss_dice: 0.291865
[10:41:32.699] TRAIN: iteration 17544 : loss : 0.061517, loss_ce: 0.005847, loss_dice: 0.117188
[10:41:32.907] TRAIN: iteration 17545 : loss : 0.075908, loss_ce: 0.008210, loss_dice: 0.143606
[10:41:33.122] TRAIN: iteration 17546 : loss : 0.251057, loss_ce: 0.002012, loss_dice: 0.500102
[10:41:33.330] TRAIN: iteration 17547 : loss : 0.250720, loss_ce: 0.005508, loss_dice: 0.495932
[10:41:33.538] TRAIN: iteration 17548 : loss : 0.137229, loss_ce: 0.005118, loss_dice: 0.269340
[10:41:33.747] TRAIN: iteration 17549 : loss : 0.058564, loss_ce: 0.005120, loss_dice: 0.112008
[10:41:33.960] TRAIN: iteration 17550 : loss : 0.241551, loss_ce: 0.003632, loss_dice: 0.479471
[10:41:34.168] TRAIN: iteration 17551 : loss : 0.071557, loss_ce: 0.002933, loss_dice: 0.140182
[10:41:34.378] TRAIN: iteration 17552 : loss : 0.169782, loss_ce: 0.004094, loss_dice: 0.335471
[10:41:34.587] TRAIN: iteration 17553 : loss : 0.175318, loss_ce: 0.006218, loss_dice: 0.344419
[10:41:34.796] TRAIN: iteration 17554 : loss : 0.112868, loss_ce: 0.007096, loss_dice: 0.218640
[10:41:35.004] TRAIN: iteration 17555 : loss : 0.111740, loss_ce: 0.007754, loss_dice: 0.215726
[10:41:35.212] TRAIN: iteration 17556 : loss : 0.062379, loss_ce: 0.006167, loss_dice: 0.118591
[10:41:35.421] TRAIN: iteration 17557 : loss : 0.050380, loss_ce: 0.003234, loss_dice: 0.097526
[10:41:35.632] TRAIN: iteration 17558 : loss : 0.041400, loss_ce: 0.004157, loss_dice: 0.078642
[10:41:35.840] TRAIN: iteration 17559 : loss : 0.132210, loss_ce: 0.006931, loss_dice: 0.257489
[10:41:36.048] TRAIN: iteration 17560 : loss : 0.092997, loss_ce: 0.002820, loss_dice: 0.183173
[10:41:36.289] TRAIN: iteration 17561 : loss : 0.196626, loss_ce: 0.002257, loss_dice: 0.390995
[10:41:36.539] TRAIN: iteration 17562 : loss : 0.229479, loss_ce: 0.002364, loss_dice: 0.456593
[10:41:36.754] TRAIN: iteration 17563 : loss : 0.085842, loss_ce: 0.002518, loss_dice: 0.169166
[10:41:36.964] TRAIN: iteration 17564 : loss : 0.193611, loss_ce: 0.001631, loss_dice: 0.385592
[10:41:37.175] TRAIN: iteration 17565 : loss : 0.070090, loss_ce: 0.003391, loss_dice: 0.136789
[10:41:37.384] TRAIN: iteration 17566 : loss : 0.250661, loss_ce: 0.001246, loss_dice: 0.500075
[10:41:37.592] TRAIN: iteration 17567 : loss : 0.140045, loss_ce: 0.008289, loss_dice: 0.271801
[10:41:37.807] TRAIN: iteration 17568 : loss : 0.176407, loss_ce: 0.002246, loss_dice: 0.350568
[10:41:38.016] TRAIN: iteration 17569 : loss : 0.233082, loss_ce: 0.011110, loss_dice: 0.455053
[10:41:38.226] TRAIN: iteration 17570 : loss : 0.250740, loss_ce: 0.001438, loss_dice: 0.500043
[10:41:38.434] TRAIN: iteration 17571 : loss : 0.249030, loss_ce: 0.002095, loss_dice: 0.495966
[10:41:38.645] TRAIN: iteration 17572 : loss : 0.114765, loss_ce: 0.009750, loss_dice: 0.219781
[10:41:38.854] TRAIN: iteration 17573 : loss : 0.150069, loss_ce: 0.018670, loss_dice: 0.281467
[10:41:39.067] TRAIN: iteration 17574 : loss : 0.071105, loss_ce: 0.005331, loss_dice: 0.136880
[10:41:39.275] TRAIN: iteration 17575 : loss : 0.181852, loss_ce: 0.006351, loss_dice: 0.357353
[10:41:39.486] TRAIN: iteration 17576 : loss : 0.078238, loss_ce: 0.003127, loss_dice: 0.153350
[10:41:39.696] TRAIN: iteration 17577 : loss : 0.128296, loss_ce: 0.007636, loss_dice: 0.248957
[10:41:39.906] TRAIN: iteration 17578 : loss : 0.148937, loss_ce: 0.002691, loss_dice: 0.295184
[10:41:40.116] TRAIN: iteration 17579 : loss : 0.188982, loss_ce: 0.004295, loss_dice: 0.373668
[10:41:40.327] TRAIN: iteration 17580 : loss : 0.082193, loss_ce: 0.004412, loss_dice: 0.159974
[10:41:40.567] TRAIN: iteration 17581 : loss : 0.193516, loss_ce: 0.002790, loss_dice: 0.384242
[10:41:40.776] TRAIN: iteration 17582 : loss : 0.092404, loss_ce: 0.013760, loss_dice: 0.171049
[10:41:40.984] TRAIN: iteration 17583 : loss : 0.137125, loss_ce: 0.009847, loss_dice: 0.264402
[10:41:41.194] TRAIN: iteration 17584 : loss : 0.090591, loss_ce: 0.002641, loss_dice: 0.178540
[10:41:41.403] TRAIN: iteration 17585 : loss : 0.227916, loss_ce: 0.004857, loss_dice: 0.450975
[10:41:41.613] TRAIN: iteration 17586 : loss : 0.203806, loss_ce: 0.004101, loss_dice: 0.403510
[10:41:41.822] TRAIN: iteration 17587 : loss : 0.110468, loss_ce: 0.008227, loss_dice: 0.212710
[10:41:42.033] TRAIN: iteration 17588 : loss : 0.156863, loss_ce: 0.015985, loss_dice: 0.297741
[10:41:42.247] TRAIN: iteration 17589 : loss : 0.152542, loss_ce: 0.004149, loss_dice: 0.300936
[10:41:42.462] TRAIN: iteration 17590 : loss : 0.111225, loss_ce: 0.002497, loss_dice: 0.219954
[10:41:42.672] TRAIN: iteration 17591 : loss : 0.090581, loss_ce: 0.004887, loss_dice: 0.176275
[10:41:42.880] TRAIN: iteration 17592 : loss : 0.138584, loss_ce: 0.003670, loss_dice: 0.273497
[10:41:43.090] TRAIN: iteration 17593 : loss : 0.076598, loss_ce: 0.010653, loss_dice: 0.142544
[10:41:43.305] TRAIN: iteration 17594 : loss : 0.098302, loss_ce: 0.001634, loss_dice: 0.194970
[10:41:43.514] TRAIN: iteration 17595 : loss : 0.065576, loss_ce: 0.009595, loss_dice: 0.121557
[10:41:43.729] TRAIN: iteration 17596 : loss : 0.174733, loss_ce: 0.002270, loss_dice: 0.347196
[10:41:43.936] TRAIN: iteration 17597 : loss : 0.251505, loss_ce: 0.002813, loss_dice: 0.500198
[10:41:44.147] TRAIN: iteration 17598 : loss : 0.066250, loss_ce: 0.005180, loss_dice: 0.127321
[10:41:44.363] TRAIN: iteration 17599 : loss : 0.139445, loss_ce: 0.004754, loss_dice: 0.274136
[10:41:44.571] TRAIN: iteration 17600 : loss : 0.251303, loss_ce: 0.002440, loss_dice: 0.500166
[10:41:45.064] TRAIN: iteration 17601 : loss : 0.055763, loss_ce: 0.002651, loss_dice: 0.108876
[10:41:45.273] TRAIN: iteration 17602 : loss : 0.252204, loss_ce: 0.004436, loss_dice: 0.499971
[10:41:45.482] TRAIN: iteration 17603 : loss : 0.223544, loss_ce: 0.011837, loss_dice: 0.435251
[10:41:45.690] TRAIN: iteration 17604 : loss : 0.034725, loss_ce: 0.004844, loss_dice: 0.064606
[10:41:45.905] TRAIN: iteration 17605 : loss : 0.065012, loss_ce: 0.005462, loss_dice: 0.124563
[10:41:46.114] TRAIN: iteration 17606 : loss : 0.251666, loss_ce: 0.003110, loss_dice: 0.500222
[10:41:46.326] TRAIN: iteration 17607 : loss : 0.242765, loss_ce: 0.002775, loss_dice: 0.482755
[10:41:46.539] TRAIN: iteration 17608 : loss : 0.090689, loss_ce: 0.008295, loss_dice: 0.173083
[10:41:46.748] TRAIN: iteration 17609 : loss : 0.216122, loss_ce: 0.006592, loss_dice: 0.425652
[10:41:46.959] TRAIN: iteration 17610 : loss : 0.151726, loss_ce: 0.005027, loss_dice: 0.298425
[10:41:47.168] TRAIN: iteration 17611 : loss : 0.059126, loss_ce: 0.005183, loss_dice: 0.113069
[10:41:47.375] TRAIN: iteration 17612 : loss : 0.081985, loss_ce: 0.006519, loss_dice: 0.157452
[10:41:47.584] TRAIN: iteration 17613 : loss : 0.164991, loss_ce: 0.002748, loss_dice: 0.327234
[10:41:47.792] TRAIN: iteration 17614 : loss : 0.250864, loss_ce: 0.001658, loss_dice: 0.500069
[10:41:48.002] TRAIN: iteration 17615 : loss : 0.194334, loss_ce: 0.008402, loss_dice: 0.380265
[10:41:48.213] TRAIN: iteration 17616 : loss : 0.105506, loss_ce: 0.010154, loss_dice: 0.200857
[10:41:48.426] TRAIN: iteration 17617 : loss : 0.116879, loss_ce: 0.010387, loss_dice: 0.223371
[10:41:48.635] TRAIN: iteration 17618 : loss : 0.111741, loss_ce: 0.005039, loss_dice: 0.218442
[10:41:48.846] TRAIN: iteration 17619 : loss : 0.115860, loss_ce: 0.007093, loss_dice: 0.224627
[10:41:49.056] TRAIN: iteration 17620 : loss : 0.125226, loss_ce: 0.007935, loss_dice: 0.242518
[10:41:49.294] TRAIN: iteration 17621 : loss : 0.059885, loss_ce: 0.002656, loss_dice: 0.117113
[10:41:49.504] TRAIN: iteration 17622 : loss : 0.155053, loss_ce: 0.008219, loss_dice: 0.301888
[10:41:49.715] TRAIN: iteration 17623 : loss : 0.101118, loss_ce: 0.005284, loss_dice: 0.196952
[10:41:49.925] TRAIN: iteration 17624 : loss : 0.257602, loss_ce: 0.014140, loss_dice: 0.501063
[10:41:50.134] TRAIN: iteration 17625 : loss : 0.226441, loss_ce: 0.002507, loss_dice: 0.450376
[10:41:50.342] TRAIN: iteration 17626 : loss : 0.031881, loss_ce: 0.001427, loss_dice: 0.062335
[10:41:50.552] TRAIN: iteration 17627 : loss : 0.160016, loss_ce: 0.004167, loss_dice: 0.315864
[10:41:50.763] TRAIN: iteration 17628 : loss : 0.041258, loss_ce: 0.003392, loss_dice: 0.079124
[10:41:50.973] TRAIN: iteration 17629 : loss : 0.055484, loss_ce: 0.006500, loss_dice: 0.104467
[10:41:51.182] TRAIN: iteration 17630 : loss : 0.082406, loss_ce: 0.006308, loss_dice: 0.158503
[10:41:51.391] TRAIN: iteration 17631 : loss : 0.250707, loss_ce: 0.001356, loss_dice: 0.500058
[10:41:51.603] TRAIN: iteration 17632 : loss : 0.074297, loss_ce: 0.003200, loss_dice: 0.145393
[10:41:51.813] TRAIN: iteration 17633 : loss : 0.166454, loss_ce: 0.006314, loss_dice: 0.326593
[10:41:52.024] TRAIN: iteration 17634 : loss : 0.053842, loss_ce: 0.001828, loss_dice: 0.105856
[10:41:52.235] TRAIN: iteration 17635 : loss : 0.121290, loss_ce: 0.001787, loss_dice: 0.240794
[10:41:52.446] TRAIN: iteration 17636 : loss : 0.117054, loss_ce: 0.002856, loss_dice: 0.231252
[10:41:52.656] TRAIN: iteration 17637 : loss : 0.129567, loss_ce: 0.006439, loss_dice: 0.252694
[10:41:52.868] TRAIN: iteration 17638 : loss : 0.249273, loss_ce: 0.001923, loss_dice: 0.496622
[10:41:53.081] TRAIN: iteration 17639 : loss : 0.028216, loss_ce: 0.004774, loss_dice: 0.051657
[10:41:53.292] TRAIN: iteration 17640 : loss : 0.038982, loss_ce: 0.003792, loss_dice: 0.074172
[10:41:53.535] TRAIN: iteration 17641 : loss : 0.156362, loss_ce: 0.002549, loss_dice: 0.310176
[10:41:53.745] TRAIN: iteration 17642 : loss : 0.250611, loss_ce: 0.001180, loss_dice: 0.500041
[10:41:53.957] TRAIN: iteration 17643 : loss : 0.089943, loss_ce: 0.004646, loss_dice: 0.175239
[10:41:54.167] TRAIN: iteration 17644 : loss : 0.093509, loss_ce: 0.002204, loss_dice: 0.184815
[10:41:54.375] TRAIN: iteration 17645 : loss : 0.134951, loss_ce: 0.006023, loss_dice: 0.263879
[10:41:54.586] TRAIN: iteration 17646 : loss : 0.059962, loss_ce: 0.006561, loss_dice: 0.113363
[10:41:54.794] TRAIN: iteration 17647 : loss : 0.079309, loss_ce: 0.009600, loss_dice: 0.149018
[10:41:55.005] TRAIN: iteration 17648 : loss : 0.163009, loss_ce: 0.002355, loss_dice: 0.323664
[10:41:55.218] TRAIN: iteration 17649 : loss : 0.133627, loss_ce: 0.004608, loss_dice: 0.262646
[10:41:55.437] TRAIN: iteration 17650 : loss : 0.175096, loss_ce: 0.001747, loss_dice: 0.348445
[10:41:55.654] TRAIN: iteration 17651 : loss : 0.239055, loss_ce: 0.004602, loss_dice: 0.473508
[10:41:55.866] TRAIN: iteration 17652 : loss : 0.099811, loss_ce: 0.003671, loss_dice: 0.195951
[10:41:56.074] TRAIN: iteration 17653 : loss : 0.131561, loss_ce: 0.003164, loss_dice: 0.259957
[10:41:56.284] TRAIN: iteration 17654 : loss : 0.136531, loss_ce: 0.004149, loss_dice: 0.268913
[10:41:56.493] TRAIN: iteration 17655 : loss : 0.212258, loss_ce: 0.004272, loss_dice: 0.420244
[10:41:56.704] TRAIN: iteration 17656 : loss : 0.122148, loss_ce: 0.002621, loss_dice: 0.241675
[10:41:56.921] TRAIN: iteration 17657 : loss : 0.249170, loss_ce: 0.003373, loss_dice: 0.494968
[10:41:57.132] TRAIN: iteration 17658 : loss : 0.082466, loss_ce: 0.001508, loss_dice: 0.163424
[10:41:57.348] TRAIN: iteration 17659 : loss : 0.116004, loss_ce: 0.007980, loss_dice: 0.224029
[10:41:57.557] TRAIN: iteration 17660 : loss : 0.048029, loss_ce: 0.006395, loss_dice: 0.089664
[10:41:57.794] TRAIN: iteration 17661 : loss : 0.247385, loss_ce: 0.002389, loss_dice: 0.492381
[10:41:58.003] TRAIN: iteration 17662 : loss : 0.241276, loss_ce: 0.009823, loss_dice: 0.472729
[10:41:58.211] TRAIN: iteration 17663 : loss : 0.180604, loss_ce: 0.004091, loss_dice: 0.357117
[10:41:58.420] TRAIN: iteration 17664 : loss : 0.042376, loss_ce: 0.001417, loss_dice: 0.083334
[10:41:58.636] TRAIN: iteration 17665 : loss : 0.161990, loss_ce: 0.003170, loss_dice: 0.320809
[10:41:58.844] TRAIN: iteration 17666 : loss : 0.071846, loss_ce: 0.005076, loss_dice: 0.138615
[10:41:59.055] TRAIN: iteration 17667 : loss : 0.083828, loss_ce: 0.004588, loss_dice: 0.163069
[10:41:59.264] TRAIN: iteration 17668 : loss : 0.250935, loss_ce: 0.001775, loss_dice: 0.500095
[10:41:59.473] TRAIN: iteration 17669 : loss : 0.047601, loss_ce: 0.008265, loss_dice: 0.086936
[10:41:59.682] TRAIN: iteration 17670 : loss : 0.147910, loss_ce: 0.002143, loss_dice: 0.293676
[10:41:59.944] TRAIN: iteration 17671 : loss : 0.075792, loss_ce: 0.002515, loss_dice: 0.149068
[10:42:00.160] TRAIN: iteration 17672 : loss : 0.089736, loss_ce: 0.002662, loss_dice: 0.176809
[10:42:00.369] TRAIN: iteration 17673 : loss : 0.077627, loss_ce: 0.004266, loss_dice: 0.150987
[10:42:00.580] TRAIN: iteration 17674 : loss : 0.033932, loss_ce: 0.003854, loss_dice: 0.064010
[10:42:00.791] TRAIN: iteration 17675 : loss : 0.036047, loss_ce: 0.001820, loss_dice: 0.070274
[10:42:01.001] TRAIN: iteration 17676 : loss : 0.202005, loss_ce: 0.000931, loss_dice: 0.403078
[10:42:01.210] TRAIN: iteration 17677 : loss : 0.082183, loss_ce: 0.004504, loss_dice: 0.159862
[10:42:01.420] TRAIN: iteration 17678 : loss : 0.157744, loss_ce: 0.005907, loss_dice: 0.309582
[10:42:02.378] TRAIN: iteration 17679 : loss : 0.052663, loss_ce: 0.001705, loss_dice: 0.103621
[10:42:02.587] TRAIN: iteration 17680 : loss : 0.036147, loss_ce: 0.002291, loss_dice: 0.070003
[10:42:02.823] TRAIN: iteration 17681 : loss : 0.220230, loss_ce: 0.003095, loss_dice: 0.437365
[10:42:03.032] TRAIN: iteration 17682 : loss : 0.091765, loss_ce: 0.002241, loss_dice: 0.181290
[10:42:03.243] TRAIN: iteration 17683 : loss : 0.186003, loss_ce: 0.011308, loss_dice: 0.360697
[10:42:03.457] TRAIN: iteration 17684 : loss : 0.099760, loss_ce: 0.003234, loss_dice: 0.196286
[10:42:03.666] TRAIN: iteration 17685 : loss : 0.083954, loss_ce: 0.003279, loss_dice: 0.164630
[10:42:03.874] TRAIN: iteration 17686 : loss : 0.083134, loss_ce: 0.015863, loss_dice: 0.150405
[10:42:04.083] TRAIN: iteration 17687 : loss : 0.098403, loss_ce: 0.004438, loss_dice: 0.192368
[10:42:04.291] TRAIN: iteration 17688 : loss : 0.172928, loss_ce: 0.029716, loss_dice: 0.316141
[10:42:04.499] TRAIN: iteration 17689 : loss : 0.084270, loss_ce: 0.002135, loss_dice: 0.166404
[10:42:04.707] TRAIN: iteration 17690 : loss : 0.082137, loss_ce: 0.002263, loss_dice: 0.162011
[10:42:04.916] TRAIN: iteration 17691 : loss : 0.089766, loss_ce: 0.003041, loss_dice: 0.176492
[10:42:05.130] TRAIN: iteration 17692 : loss : 0.067805, loss_ce: 0.004305, loss_dice: 0.131305
[10:42:05.338] TRAIN: iteration 17693 : loss : 0.078158, loss_ce: 0.007534, loss_dice: 0.148782
[10:42:05.547] TRAIN: iteration 17694 : loss : 0.164188, loss_ce: 0.003982, loss_dice: 0.324395
[10:42:05.764] TRAIN: iteration 17695 : loss : 0.091007, loss_ce: 0.004565, loss_dice: 0.177449
[10:42:05.972] TRAIN: iteration 17696 : loss : 0.141506, loss_ce: 0.004959, loss_dice: 0.278053
[10:42:06.182] TRAIN: iteration 17697 : loss : 0.252703, loss_ce: 0.005516, loss_dice: 0.499891
[10:42:06.398] TRAIN: iteration 17698 : loss : 0.078031, loss_ce: 0.020974, loss_dice: 0.135089
[10:42:06.606] TRAIN: iteration 17699 : loss : 0.070033, loss_ce: 0.003728, loss_dice: 0.136338
[10:42:06.813] TRAIN: iteration 17700 : loss : 0.127599, loss_ce: 0.008702, loss_dice: 0.246496
[10:42:06.814] NaN or Inf found in input tensor.
[10:42:07.031] TRAIN: iteration 17701 : loss : 0.116626, loss_ce: 0.002729, loss_dice: 0.230523
[10:42:07.240] TRAIN: iteration 17702 : loss : 0.140642, loss_ce: 0.001852, loss_dice: 0.279433
[10:42:07.447] TRAIN: iteration 17703 : loss : 0.072538, loss_ce: 0.004655, loss_dice: 0.140420
[10:42:07.659] TRAIN: iteration 17704 : loss : 0.082648, loss_ce: 0.003183, loss_dice: 0.162113
[10:42:07.867] TRAIN: iteration 17705 : loss : 0.251484, loss_ce: 0.002792, loss_dice: 0.500176
[10:42:08.076] TRAIN: iteration 17706 : loss : 0.083250, loss_ce: 0.002445, loss_dice: 0.164056
[10:42:08.284] TRAIN: iteration 17707 : loss : 0.092264, loss_ce: 0.003035, loss_dice: 0.181492
[10:42:08.503] TRAIN: iteration 17708 : loss : 0.178176, loss_ce: 0.003196, loss_dice: 0.353156
[10:42:08.716] TRAIN: iteration 17709 : loss : 0.034433, loss_ce: 0.001371, loss_dice: 0.067496
[10:42:08.928] TRAIN: iteration 17710 : loss : 0.105735, loss_ce: 0.007669, loss_dice: 0.203802
[10:42:09.139] TRAIN: iteration 17711 : loss : 0.137341, loss_ce: 0.003247, loss_dice: 0.271435
[10:42:09.348] TRAIN: iteration 17712 : loss : 0.122901, loss_ce: 0.003347, loss_dice: 0.242455
[10:42:09.556] TRAIN: iteration 17713 : loss : 0.103419, loss_ce: 0.005806, loss_dice: 0.201032
[10:42:09.766] TRAIN: iteration 17714 : loss : 0.160361, loss_ce: 0.006009, loss_dice: 0.314713
[10:42:09.974] TRAIN: iteration 17715 : loss : 0.136455, loss_ce: 0.002428, loss_dice: 0.270483
[10:42:10.183] TRAIN: iteration 17716 : loss : 0.155204, loss_ce: 0.006574, loss_dice: 0.303833
[10:42:10.397] TRAIN: iteration 17717 : loss : 0.115528, loss_ce: 0.006335, loss_dice: 0.224721
[10:42:10.609] TRAIN: iteration 17718 : loss : 0.078241, loss_ce: 0.001560, loss_dice: 0.154921
[10:42:10.817] TRAIN: iteration 17719 : loss : 0.125154, loss_ce: 0.001620, loss_dice: 0.248688
[10:42:11.029] TRAIN: iteration 17720 : loss : 0.237757, loss_ce: 0.038912, loss_dice: 0.436603
[10:42:11.267] TRAIN: iteration 17721 : loss : 0.059785, loss_ce: 0.002985, loss_dice: 0.116585
[10:42:11.475] TRAIN: iteration 17722 : loss : 0.034920, loss_ce: 0.000556, loss_dice: 0.069284
[10:42:11.684] TRAIN: iteration 17723 : loss : 0.232759, loss_ce: 0.001141, loss_dice: 0.464377
[10:42:11.893] TRAIN: iteration 17724 : loss : 0.225278, loss_ce: 0.001256, loss_dice: 0.449299
[10:42:12.104] TRAIN: iteration 17725 : loss : 0.113515, loss_ce: 0.003064, loss_dice: 0.223966
[10:42:12.311] TRAIN: iteration 17726 : loss : 0.101761, loss_ce: 0.002106, loss_dice: 0.201415
[10:42:12.519] TRAIN: iteration 17727 : loss : 0.066366, loss_ce: 0.001769, loss_dice: 0.130964
[10:42:12.727] TRAIN: iteration 17728 : loss : 0.038960, loss_ce: 0.002663, loss_dice: 0.075257
[10:42:12.935] TRAIN: iteration 17729 : loss : 0.227873, loss_ce: 0.002739, loss_dice: 0.453006
[10:42:13.145] TRAIN: iteration 17730 : loss : 0.251602, loss_ce: 0.002989, loss_dice: 0.500215
[10:42:13.355] TRAIN: iteration 17731 : loss : 0.251162, loss_ce: 0.002174, loss_dice: 0.500150
[10:42:13.564] TRAIN: iteration 17732 : loss : 0.132669, loss_ce: 0.003894, loss_dice: 0.261443
[10:42:13.773] TRAIN: iteration 17733 : loss : 0.162755, loss_ce: 0.011119, loss_dice: 0.314391
[10:42:13.988] TRAIN: iteration 17734 : loss : 0.221991, loss_ce: 0.003957, loss_dice: 0.440025
[10:42:14.196] TRAIN: iteration 17735 : loss : 0.054509, loss_ce: 0.004828, loss_dice: 0.104189
[10:42:14.403] TRAIN: iteration 17736 : loss : 0.041341, loss_ce: 0.002720, loss_dice: 0.079962
[10:42:14.611] TRAIN: iteration 17737 : loss : 0.111274, loss_ce: 0.002804, loss_dice: 0.219744
[10:42:14.820] TRAIN: iteration 17738 : loss : 0.066563, loss_ce: 0.002479, loss_dice: 0.130648
[10:42:15.032] TRAIN: iteration 17739 : loss : 0.240041, loss_ce: 0.001527, loss_dice: 0.478556
[10:42:15.241] TRAIN: iteration 17740 : loss : 0.100973, loss_ce: 0.002842, loss_dice: 0.199104
[10:42:15.482] TRAIN: iteration 17741 : loss : 0.077718, loss_ce: 0.005236, loss_dice: 0.150200
[10:42:15.693] TRAIN: iteration 17742 : loss : 0.091779, loss_ce: 0.003939, loss_dice: 0.179618
[10:42:15.906] TRAIN: iteration 17743 : loss : 0.112002, loss_ce: 0.003637, loss_dice: 0.220367
[10:42:16.116] TRAIN: iteration 17744 : loss : 0.104240, loss_ce: 0.001710, loss_dice: 0.206769
[10:42:16.326] TRAIN: iteration 17745 : loss : 0.133368, loss_ce: 0.003466, loss_dice: 0.263271
[10:42:16.535] TRAIN: iteration 17746 : loss : 0.122824, loss_ce: 0.006636, loss_dice: 0.239012
[10:42:16.745] TRAIN: iteration 17747 : loss : 0.158081, loss_ce: 0.004680, loss_dice: 0.311481
[10:42:16.954] TRAIN: iteration 17748 : loss : 0.078007, loss_ce: 0.006049, loss_dice: 0.149964
[10:42:17.162] TRAIN: iteration 17749 : loss : 0.250944, loss_ce: 0.001800, loss_dice: 0.500087
[10:42:17.371] TRAIN: iteration 17750 : loss : 0.234668, loss_ce: 0.001871, loss_dice: 0.467464
[10:42:17.578] TRAIN: iteration 17751 : loss : 0.128197, loss_ce: 0.005118, loss_dice: 0.251277
[10:42:17.787] TRAIN: iteration 17752 : loss : 0.056616, loss_ce: 0.002759, loss_dice: 0.110474
[10:42:17.994] TRAIN: iteration 17753 : loss : 0.043264, loss_ce: 0.001601, loss_dice: 0.084928
[10:42:18.204] TRAIN: iteration 17754 : loss : 0.250873, loss_ce: 0.001679, loss_dice: 0.500067
[10:42:18.413] TRAIN: iteration 17755 : loss : 0.040209, loss_ce: 0.003609, loss_dice: 0.076809
[10:42:18.621] TRAIN: iteration 17756 : loss : 0.039524, loss_ce: 0.001049, loss_dice: 0.078000
[10:42:18.828] TRAIN: iteration 17757 : loss : 0.047869, loss_ce: 0.003033, loss_dice: 0.092706
[10:42:19.038] TRAIN: iteration 17758 : loss : 0.109939, loss_ce: 0.010424, loss_dice: 0.209454
[10:42:19.250] TRAIN: iteration 17759 : loss : 0.251212, loss_ce: 0.002315, loss_dice: 0.500108
[10:42:19.460] TRAIN: iteration 17760 : loss : 0.185701, loss_ce: 0.002442, loss_dice: 0.368959
[10:42:19.707] TRAIN: iteration 17761 : loss : 0.251592, loss_ce: 0.003880, loss_dice: 0.499304
[10:42:19.916] TRAIN: iteration 17762 : loss : 0.033509, loss_ce: 0.002859, loss_dice: 0.064158
[10:42:20.126] TRAIN: iteration 17763 : loss : 0.060257, loss_ce: 0.006579, loss_dice: 0.113934
[10:42:20.341] TRAIN: iteration 17764 : loss : 0.077653, loss_ce: 0.001977, loss_dice: 0.153329
[10:42:20.549] TRAIN: iteration 17765 : loss : 0.106495, loss_ce: 0.002520, loss_dice: 0.210471
[10:42:20.764] TRAIN: iteration 17766 : loss : 0.252715, loss_ce: 0.005030, loss_dice: 0.500399
[10:42:20.982] TRAIN: iteration 17767 : loss : 0.079458, loss_ce: 0.004884, loss_dice: 0.154032
[10:42:21.193] TRAIN: iteration 17768 : loss : 0.051509, loss_ce: 0.004466, loss_dice: 0.098553
[10:42:21.406] TRAIN: iteration 17769 : loss : 0.044208, loss_ce: 0.006377, loss_dice: 0.082039
[10:42:21.615] TRAIN: iteration 17770 : loss : 0.041688, loss_ce: 0.001247, loss_dice: 0.082130
[10:42:21.823] TRAIN: iteration 17771 : loss : 0.133052, loss_ce: 0.007730, loss_dice: 0.258374
[10:42:22.038] TRAIN: iteration 17772 : loss : 0.087968, loss_ce: 0.001583, loss_dice: 0.174352
[10:42:22.246] TRAIN: iteration 17773 : loss : 0.054312, loss_ce: 0.005317, loss_dice: 0.103307
[10:42:22.454] TRAIN: iteration 17774 : loss : 0.251408, loss_ce: 0.002662, loss_dice: 0.500154
[10:42:22.662] TRAIN: iteration 17775 : loss : 0.078599, loss_ce: 0.002005, loss_dice: 0.155192
[10:42:22.876] TRAIN: iteration 17776 : loss : 0.104843, loss_ce: 0.003467, loss_dice: 0.206218
[10:42:23.085] TRAIN: iteration 17777 : loss : 0.089394, loss_ce: 0.003849, loss_dice: 0.174938
[10:42:23.293] TRAIN: iteration 17778 : loss : 0.156172, loss_ce: 0.013607, loss_dice: 0.298738
[10:42:23.501] TRAIN: iteration 17779 : loss : 0.049819, loss_ce: 0.002100, loss_dice: 0.097537
[10:42:23.709] TRAIN: iteration 17780 : loss : 0.194354, loss_ce: 0.002319, loss_dice: 0.386389
[10:42:23.946] TRAIN: iteration 17781 : loss : 0.029693, loss_ce: 0.000892, loss_dice: 0.058493
[10:42:24.154] TRAIN: iteration 17782 : loss : 0.080985, loss_ce: 0.010851, loss_dice: 0.151119
[10:42:24.362] TRAIN: iteration 17783 : loss : 0.196001, loss_ce: 0.001968, loss_dice: 0.390034
[10:42:24.569] TRAIN: iteration 17784 : loss : 0.048475, loss_ce: 0.001242, loss_dice: 0.095708
[10:42:24.777] TRAIN: iteration 17785 : loss : 0.168582, loss_ce: 0.002113, loss_dice: 0.335051
[10:42:24.986] TRAIN: iteration 17786 : loss : 0.099280, loss_ce: 0.002172, loss_dice: 0.196388
[10:42:25.194] TRAIN: iteration 17787 : loss : 0.033670, loss_ce: 0.001886, loss_dice: 0.065454
[10:42:25.402] TRAIN: iteration 17788 : loss : 0.158649, loss_ce: 0.003065, loss_dice: 0.314233
[10:42:26.275] TRAIN: iteration 17789 : loss : 0.168542, loss_ce: 0.002680, loss_dice: 0.334404
[10:42:26.485] TRAIN: iteration 17790 : loss : 0.059574, loss_ce: 0.002460, loss_dice: 0.116689
[10:42:26.692] TRAIN: iteration 17791 : loss : 0.237990, loss_ce: 0.003418, loss_dice: 0.472563
[10:42:26.907] TRAIN: iteration 17792 : loss : 0.066917, loss_ce: 0.007395, loss_dice: 0.126438
[10:42:27.117] TRAIN: iteration 17793 : loss : 0.112765, loss_ce: 0.002714, loss_dice: 0.222816
[10:42:27.327] TRAIN: iteration 17794 : loss : 0.025655, loss_ce: 0.001235, loss_dice: 0.050075
[10:42:27.535] TRAIN: iteration 17795 : loss : 0.102430, loss_ce: 0.003593, loss_dice: 0.201268
[10:42:27.774] TRAIN: iteration 17796 : loss : 0.038738, loss_ce: 0.005094, loss_dice: 0.072383
[10:42:27.982] TRAIN: iteration 17797 : loss : 0.100084, loss_ce: 0.004637, loss_dice: 0.195531
[10:42:28.197] TRAIN: iteration 17798 : loss : 0.255539, loss_ce: 0.010751, loss_dice: 0.500328
[10:42:28.404] TRAIN: iteration 17799 : loss : 0.129730, loss_ce: 0.007464, loss_dice: 0.251996
[10:42:28.613] TRAIN: iteration 17800 : loss : 0.104865, loss_ce: 0.006702, loss_dice: 0.203028
[10:42:28.614] NaN or Inf found in input tensor.
[10:42:28.828] TRAIN: iteration 17801 : loss : 0.121614, loss_ce: 0.006860, loss_dice: 0.236368
[10:42:29.037] TRAIN: iteration 17802 : loss : 0.042336, loss_ce: 0.003347, loss_dice: 0.081325
[10:42:29.246] TRAIN: iteration 17803 : loss : 0.050996, loss_ce: 0.004744, loss_dice: 0.097248
[10:42:29.454] TRAIN: iteration 17804 : loss : 0.113111, loss_ce: 0.006494, loss_dice: 0.219728
[10:42:29.662] TRAIN: iteration 17805 : loss : 0.250632, loss_ce: 0.001230, loss_dice: 0.500033
[10:42:29.878] TRAIN: iteration 17806 : loss : 0.251144, loss_ce: 0.002152, loss_dice: 0.500136
[10:42:32.674] TRAIN: iteration 17807 : loss : 0.101593, loss_ce: 0.003379, loss_dice: 0.199806
[10:42:32.885] TRAIN: iteration 17808 : loss : 0.034335, loss_ce: 0.001826, loss_dice: 0.066844
[10:42:33.096] TRAIN: iteration 17809 : loss : 0.141557, loss_ce: 0.005996, loss_dice: 0.277118
[10:42:33.304] TRAIN: iteration 17810 : loss : 0.124234, loss_ce: 0.005751, loss_dice: 0.242718
[10:42:33.512] TRAIN: iteration 17811 : loss : 0.061581, loss_ce: 0.004042, loss_dice: 0.119120
[10:42:33.719] TRAIN: iteration 17812 : loss : 0.247903, loss_ce: 0.002936, loss_dice: 0.492869
[10:42:33.927] TRAIN: iteration 17813 : loss : 0.115191, loss_ce: 0.008268, loss_dice: 0.222114
[10:42:34.136] TRAIN: iteration 17814 : loss : 0.251961, loss_ce: 0.003661, loss_dice: 0.500260
[10:42:34.344] TRAIN: iteration 17815 : loss : 0.220485, loss_ce: 0.022778, loss_dice: 0.418191
[10:42:34.552] TRAIN: iteration 17816 : loss : 0.143117, loss_ce: 0.014590, loss_dice: 0.271643
[10:42:34.762] TRAIN: iteration 17817 : loss : 0.049630, loss_ce: 0.003413, loss_dice: 0.095848
[10:42:34.971] TRAIN: iteration 17818 : loss : 0.250940, loss_ce: 0.001784, loss_dice: 0.500096
[10:42:35.181] TRAIN: iteration 17819 : loss : 0.248820, loss_ce: 0.002705, loss_dice: 0.494935
[10:42:35.388] TRAIN: iteration 17820 : loss : 0.059350, loss_ce: 0.001423, loss_dice: 0.117276
[10:42:35.627] TRAIN: iteration 17821 : loss : 0.250500, loss_ce: 0.000974, loss_dice: 0.500027
[10:42:35.834] TRAIN: iteration 17822 : loss : 0.071394, loss_ce: 0.001253, loss_dice: 0.141535
[10:42:36.041] TRAIN: iteration 17823 : loss : 0.117798, loss_ce: 0.003586, loss_dice: 0.232010
[10:42:36.255] TRAIN: iteration 17824 : loss : 0.045118, loss_ce: 0.003605, loss_dice: 0.086631
[10:42:36.466] TRAIN: iteration 17825 : loss : 0.179780, loss_ce: 0.002092, loss_dice: 0.357467
[10:42:36.678] TRAIN: iteration 17826 : loss : 0.083630, loss_ce: 0.004544, loss_dice: 0.162717
[10:42:36.887] TRAIN: iteration 17827 : loss : 0.120217, loss_ce: 0.006133, loss_dice: 0.234301
[10:42:37.096] TRAIN: iteration 17828 : loss : 0.197672, loss_ce: 0.002728, loss_dice: 0.392616
[10:42:37.311] TRAIN: iteration 17829 : loss : 0.223150, loss_ce: 0.003998, loss_dice: 0.442302
[10:42:37.518] TRAIN: iteration 17830 : loss : 0.117403, loss_ce: 0.003106, loss_dice: 0.231700
[10:42:37.726] TRAIN: iteration 17831 : loss : 0.161438, loss_ce: 0.002102, loss_dice: 0.320775
[10:42:37.938] TRAIN: iteration 17832 : loss : 0.032079, loss_ce: 0.001472, loss_dice: 0.062685
[10:42:38.152] TRAIN: iteration 17833 : loss : 0.250657, loss_ce: 0.002503, loss_dice: 0.498811
[10:42:38.360] TRAIN: iteration 17834 : loss : 0.075298, loss_ce: 0.005332, loss_dice: 0.145263
[10:42:38.640] TRAIN: iteration 17835 : loss : 0.251082, loss_ce: 0.002253, loss_dice: 0.499910
[10:42:38.848] TRAIN: iteration 17836 : loss : 0.148329, loss_ce: 0.001568, loss_dice: 0.295090
[10:42:39.062] TRAIN: iteration 17837 : loss : 0.160258, loss_ce: 0.005573, loss_dice: 0.314943
[10:42:39.270] TRAIN: iteration 17838 : loss : 0.230730, loss_ce: 0.009850, loss_dice: 0.451609
[10:42:39.497] TRAIN: iteration 17839 : loss : 0.077533, loss_ce: 0.005318, loss_dice: 0.149749
[10:42:39.706] TRAIN: iteration 17840 : loss : 0.022242, loss_ce: 0.001373, loss_dice: 0.043111
[10:42:39.946] TRAIN: iteration 17841 : loss : 0.105122, loss_ce: 0.003577, loss_dice: 0.206668
[10:42:40.154] TRAIN: iteration 17842 : loss : 0.245818, loss_ce: 0.002421, loss_dice: 0.489216
[10:42:40.362] TRAIN: iteration 17843 : loss : 0.070650, loss_ce: 0.005470, loss_dice: 0.135831
[10:42:40.569] TRAIN: iteration 17844 : loss : 0.089644, loss_ce: 0.002396, loss_dice: 0.176892
[10:42:40.777] TRAIN: iteration 17845 : loss : 0.159405, loss_ce: 0.004618, loss_dice: 0.314193
[10:42:40.984] TRAIN: iteration 17846 : loss : 0.050380, loss_ce: 0.003677, loss_dice: 0.097082
[10:42:41.191] TRAIN: iteration 17847 : loss : 0.169338, loss_ce: 0.002158, loss_dice: 0.336519
[10:42:41.399] TRAIN: iteration 17848 : loss : 0.224551, loss_ce: 0.004305, loss_dice: 0.444796
[10:42:41.607] TRAIN: iteration 17849 : loss : 0.225907, loss_ce: 0.006626, loss_dice: 0.445188
[10:42:41.814] TRAIN: iteration 17850 : loss : 0.075319, loss_ce: 0.002057, loss_dice: 0.148581
[10:42:42.022] TRAIN: iteration 17851 : loss : 0.035273, loss_ce: 0.001807, loss_dice: 0.068740
[10:42:42.237] TRAIN: iteration 17852 : loss : 0.157728, loss_ce: 0.002349, loss_dice: 0.313107
[10:42:42.445] TRAIN: iteration 17853 : loss : 0.024371, loss_ce: 0.001106, loss_dice: 0.047636
[10:42:42.655] TRAIN: iteration 17854 : loss : 0.163165, loss_ce: 0.002503, loss_dice: 0.323826
[10:42:42.865] TRAIN: iteration 17855 : loss : 0.061101, loss_ce: 0.002830, loss_dice: 0.119372
[10:42:43.090] TRAIN: iteration 17856 : loss : 0.046099, loss_ce: 0.002659, loss_dice: 0.089539
[10:42:43.297] TRAIN: iteration 17857 : loss : 0.200792, loss_ce: 0.005279, loss_dice: 0.396306
[10:42:43.507] TRAIN: iteration 17858 : loss : 0.038716, loss_ce: 0.004575, loss_dice: 0.072857
[10:42:43.716] TRAIN: iteration 17859 : loss : 0.056213, loss_ce: 0.002335, loss_dice: 0.110091
[10:42:43.927] TRAIN: iteration 17860 : loss : 0.017142, loss_ce: 0.001273, loss_dice: 0.033010
[10:42:44.183] TRAIN: iteration 17861 : loss : 0.086164, loss_ce: 0.004107, loss_dice: 0.168220
[10:42:44.391] TRAIN: iteration 17862 : loss : 0.086052, loss_ce: 0.002326, loss_dice: 0.169778
[10:42:44.602] TRAIN: iteration 17863 : loss : 0.029606, loss_ce: 0.001295, loss_dice: 0.057917
[10:42:44.815] TRAIN: iteration 17864 : loss : 0.251178, loss_ce: 0.002220, loss_dice: 0.500137
[10:42:45.024] TRAIN: iteration 17865 : loss : 0.046214, loss_ce: 0.000989, loss_dice: 0.091439
[10:42:45.234] TRAIN: iteration 17866 : loss : 0.044819, loss_ce: 0.001884, loss_dice: 0.087755
[10:42:45.448] TRAIN: iteration 17867 : loss : 0.073269, loss_ce: 0.004586, loss_dice: 0.141952
[10:42:45.665] TRAIN: iteration 17868 : loss : 0.051469, loss_ce: 0.001752, loss_dice: 0.101186
[10:42:45.874] TRAIN: iteration 17869 : loss : 0.227704, loss_ce: 0.005216, loss_dice: 0.450193
[10:42:46.090] TRAIN: iteration 17870 : loss : 0.093543, loss_ce: 0.004310, loss_dice: 0.182776
[10:42:46.302] TRAIN: iteration 17871 : loss : 0.095286, loss_ce: 0.007877, loss_dice: 0.182695
[10:42:46.518] TRAIN: iteration 17872 : loss : 0.235455, loss_ce: 0.002464, loss_dice: 0.468447
[10:42:46.734] TRAIN: iteration 17873 : loss : 0.051835, loss_ce: 0.002454, loss_dice: 0.101216
[10:42:46.947] TRAIN: iteration 17874 : loss : 0.073338, loss_ce: 0.008875, loss_dice: 0.137802
[10:42:47.155] TRAIN: iteration 17875 : loss : 0.250802, loss_ce: 0.001520, loss_dice: 0.500083
[10:42:47.371] TRAIN: iteration 17876 : loss : 0.116648, loss_ce: 0.002558, loss_dice: 0.230739
[10:42:47.582] TRAIN: iteration 17877 : loss : 0.051878, loss_ce: 0.001771, loss_dice: 0.101985
[10:42:47.790] TRAIN: iteration 17878 : loss : 0.058039, loss_ce: 0.001723, loss_dice: 0.114356
[10:42:47.998] TRAIN: iteration 17879 : loss : 0.156031, loss_ce: 0.007097, loss_dice: 0.304965
[10:42:48.207] TRAIN: iteration 17880 : loss : 0.250424, loss_ce: 0.005230, loss_dice: 0.495619
[10:42:48.465] TRAIN: iteration 17881 : loss : 0.071315, loss_ce: 0.006373, loss_dice: 0.136257
[10:42:48.672] TRAIN: iteration 17882 : loss : 0.105394, loss_ce: 0.013665, loss_dice: 0.197123
[10:42:48.880] TRAIN: iteration 17883 : loss : 0.250477, loss_ce: 0.000924, loss_dice: 0.500030
[10:42:49.089] TRAIN: iteration 17884 : loss : 0.088537, loss_ce: 0.006234, loss_dice: 0.170841
[10:42:49.300] TRAIN: iteration 17885 : loss : 0.231658, loss_ce: 0.001809, loss_dice: 0.461508
[10:42:49.516] TRAIN: iteration 17886 : loss : 0.040253, loss_ce: 0.004473, loss_dice: 0.076032
[10:42:49.724] TRAIN: iteration 17887 : loss : 0.250290, loss_ce: 0.000573, loss_dice: 0.500007
[10:42:49.935] TRAIN: iteration 17888 : loss : 0.097635, loss_ce: 0.001464, loss_dice: 0.193807
[10:42:50.145] TRAIN: iteration 17889 : loss : 0.125739, loss_ce: 0.003343, loss_dice: 0.248135
[10:42:50.355] TRAIN: iteration 17890 : loss : 0.251542, loss_ce: 0.002902, loss_dice: 0.500182
[10:42:50.564] TRAIN: iteration 17891 : loss : 0.159207, loss_ce: 0.015008, loss_dice: 0.303406
[10:42:50.774] TRAIN: iteration 17892 : loss : 0.232589, loss_ce: 0.002746, loss_dice: 0.462433
[10:42:50.986] TRAIN: iteration 17893 : loss : 0.127866, loss_ce: 0.004208, loss_dice: 0.251524
[10:42:51.195] TRAIN: iteration 17894 : loss : 0.076506, loss_ce: 0.010510, loss_dice: 0.142503
[10:42:51.403] TRAIN: iteration 17895 : loss : 0.226113, loss_ce: 0.011877, loss_dice: 0.440350
[10:42:51.611] TRAIN: iteration 17896 : loss : 0.237680, loss_ce: 0.003324, loss_dice: 0.472036
[10:42:51.819] TRAIN: iteration 17897 : loss : 0.064743, loss_ce: 0.002904, loss_dice: 0.126582
[10:42:52.032] TRAIN: iteration 17898 : loss : 0.043966, loss_ce: 0.003986, loss_dice: 0.083946
[10:42:52.262] TRAIN: iteration 17899 : loss : 0.067457, loss_ce: 0.003802, loss_dice: 0.131111
[10:42:52.475] TRAIN: iteration 17900 : loss : 0.199534, loss_ce: 0.004164, loss_dice: 0.394905
[10:42:52.721] TRAIN: iteration 17901 : loss : 0.046409, loss_ce: 0.007678, loss_dice: 0.085139
[10:42:52.929] TRAIN: iteration 17902 : loss : 0.252145, loss_ce: 0.004039, loss_dice: 0.500251
[10:42:53.137] TRAIN: iteration 17903 : loss : 0.252013, loss_ce: 0.003761, loss_dice: 0.500265
[10:42:53.346] TRAIN: iteration 17904 : loss : 0.135826, loss_ce: 0.003161, loss_dice: 0.268492
[10:42:53.579] TRAIN: iteration 17905 : loss : 0.251182, loss_ce: 0.002272, loss_dice: 0.500091
[10:42:53.793] TRAIN: iteration 17906 : loss : 0.219245, loss_ce: 0.002516, loss_dice: 0.435975
[10:42:54.001] TRAIN: iteration 17907 : loss : 0.251491, loss_ce: 0.002849, loss_dice: 0.500132
[10:42:54.207] TRAIN: iteration 17908 : loss : 0.216246, loss_ce: 0.001769, loss_dice: 0.430722
[10:42:54.416] TRAIN: iteration 17909 : loss : 0.244937, loss_ce: 0.005330, loss_dice: 0.484544
[10:42:54.627] TRAIN: iteration 17910 : loss : 0.243250, loss_ce: 0.004444, loss_dice: 0.482055
[10:42:54.837] TRAIN: iteration 17911 : loss : 0.210607, loss_ce: 0.007983, loss_dice: 0.413231
[10:42:55.045] TRAIN: iteration 17912 : loss : 0.056162, loss_ce: 0.004515, loss_dice: 0.107810
[10:42:55.254] TRAIN: iteration 17913 : loss : 0.067552, loss_ce: 0.007707, loss_dice: 0.127397
[10:42:55.462] TRAIN: iteration 17914 : loss : 0.060080, loss_ce: 0.001562, loss_dice: 0.118598
[10:42:56.674] TRAIN: iteration 17915 : loss : 0.250951, loss_ce: 0.001819, loss_dice: 0.500084
[10:42:56.883] TRAIN: iteration 17916 : loss : 0.054460, loss_ce: 0.002547, loss_dice: 0.106374
[10:42:57.101] TRAIN: iteration 17917 : loss : 0.104995, loss_ce: 0.007453, loss_dice: 0.202537
[10:42:57.310] TRAIN: iteration 17918 : loss : 0.025996, loss_ce: 0.001927, loss_dice: 0.050064
[10:42:57.519] TRAIN: iteration 17919 : loss : 0.251623, loss_ce: 0.003043, loss_dice: 0.500203
[10:42:57.728] TRAIN: iteration 17920 : loss : 0.088227, loss_ce: 0.005511, loss_dice: 0.170942
[10:42:57.965] TRAIN: iteration 17921 : loss : 0.090622, loss_ce: 0.003929, loss_dice: 0.177315
[10:42:58.178] TRAIN: iteration 17922 : loss : 0.078793, loss_ce: 0.002026, loss_dice: 0.155561
[10:42:58.392] TRAIN: iteration 17923 : loss : 0.204201, loss_ce: 0.003042, loss_dice: 0.405360
[10:42:58.600] TRAIN: iteration 17924 : loss : 0.107207, loss_ce: 0.003068, loss_dice: 0.211347
[10:42:58.814] TRAIN: iteration 17925 : loss : 0.145091, loss_ce: 0.003929, loss_dice: 0.286252
[10:42:59.024] TRAIN: iteration 17926 : loss : 0.057405, loss_ce: 0.003605, loss_dice: 0.111204
[10:42:59.239] TRAIN: iteration 17927 : loss : 0.250983, loss_ce: 0.001863, loss_dice: 0.500104
[10:42:59.447] TRAIN: iteration 17928 : loss : 0.154652, loss_ce: 0.003060, loss_dice: 0.306245
[10:42:59.656] TRAIN: iteration 17929 : loss : 0.056348, loss_ce: 0.001467, loss_dice: 0.111228
[10:42:59.867] TRAIN: iteration 17930 : loss : 0.152691, loss_ce: 0.011073, loss_dice: 0.294309
[10:43:00.077] TRAIN: iteration 17931 : loss : 0.167504, loss_ce: 0.002655, loss_dice: 0.332354
[10:43:00.287] TRAIN: iteration 17932 : loss : 0.222056, loss_ce: 0.016033, loss_dice: 0.428079
[10:43:00.501] TRAIN: iteration 17933 : loss : 0.168666, loss_ce: 0.005174, loss_dice: 0.332159
[10:43:00.711] TRAIN: iteration 17934 : loss : 0.099548, loss_ce: 0.001750, loss_dice: 0.197345
[10:43:00.920] TRAIN: iteration 17935 : loss : 0.143272, loss_ce: 0.004125, loss_dice: 0.282420
[10:43:01.128] TRAIN: iteration 17936 : loss : 0.250472, loss_ce: 0.000914, loss_dice: 0.500031
[10:43:01.337] TRAIN: iteration 17937 : loss : 0.098037, loss_ce: 0.009459, loss_dice: 0.186614
[10:43:01.546] TRAIN: iteration 17938 : loss : 0.250614, loss_ce: 0.001172, loss_dice: 0.500057
[10:43:01.763] TRAIN: iteration 17939 : loss : 0.250307, loss_ce: 0.000591, loss_dice: 0.500022
[10:43:01.976] TRAIN: iteration 17940 : loss : 0.212281, loss_ce: 0.009937, loss_dice: 0.414624
[10:43:02.213] TRAIN: iteration 17941 : loss : 0.225823, loss_ce: 0.005452, loss_dice: 0.446194
[10:43:02.424] TRAIN: iteration 17942 : loss : 0.137344, loss_ce: 0.002183, loss_dice: 0.272506
[10:43:02.635] TRAIN: iteration 17943 : loss : 0.141488, loss_ce: 0.002200, loss_dice: 0.280777
[10:43:02.844] TRAIN: iteration 17944 : loss : 0.150083, loss_ce: 0.007021, loss_dice: 0.293144
[10:43:03.076] TRAIN: iteration 17945 : loss : 0.202478, loss_ce: 0.003437, loss_dice: 0.401519
[10:43:03.289] TRAIN: iteration 17946 : loss : 0.084880, loss_ce: 0.007567, loss_dice: 0.162192
[10:43:03.496] TRAIN: iteration 17947 : loss : 0.251983, loss_ce: 0.004668, loss_dice: 0.499298
[10:43:03.703] TRAIN: iteration 17948 : loss : 0.104887, loss_ce: 0.004563, loss_dice: 0.205211
[10:43:04.079] TRAIN: iteration 17949 : loss : 0.156585, loss_ce: 0.002320, loss_dice: 0.310850
[10:43:04.288] TRAIN: iteration 17950 : loss : 0.249520, loss_ce: 0.002972, loss_dice: 0.496068
[10:43:04.496] TRAIN: iteration 17951 : loss : 0.251012, loss_ce: 0.001939, loss_dice: 0.500085
[10:43:04.704] TRAIN: iteration 17952 : loss : 0.262216, loss_ce: 0.023829, loss_dice: 0.500603
[10:43:04.917] TRAIN: iteration 17953 : loss : 0.169538, loss_ce: 0.002177, loss_dice: 0.336899
[10:43:05.124] TRAIN: iteration 17954 : loss : 0.136048, loss_ce: 0.003107, loss_dice: 0.268989
[10:43:05.334] TRAIN: iteration 17955 : loss : 0.181482, loss_ce: 0.005763, loss_dice: 0.357201
[10:43:05.542] TRAIN: iteration 17956 : loss : 0.064508, loss_ce: 0.001491, loss_dice: 0.127526
[10:43:05.752] TRAIN: iteration 17957 : loss : 0.100258, loss_ce: 0.005226, loss_dice: 0.195290
[10:43:05.963] TRAIN: iteration 17958 : loss : 0.049533, loss_ce: 0.005371, loss_dice: 0.093695
[10:43:06.175] TRAIN: iteration 17959 : loss : 0.098627, loss_ce: 0.002072, loss_dice: 0.195183
[10:43:06.384] TRAIN: iteration 17960 : loss : 0.152833, loss_ce: 0.004345, loss_dice: 0.301321
[10:43:06.631] TRAIN: iteration 17961 : loss : 0.074572, loss_ce: 0.003702, loss_dice: 0.145442
[10:43:06.838] TRAIN: iteration 17962 : loss : 0.027452, loss_ce: 0.001485, loss_dice: 0.053418
[10:43:07.047] TRAIN: iteration 17963 : loss : 0.173172, loss_ce: 0.005946, loss_dice: 0.340399
[10:43:07.255] TRAIN: iteration 17964 : loss : 0.052621, loss_ce: 0.003381, loss_dice: 0.101861
[10:43:07.462] TRAIN: iteration 17965 : loss : 0.205684, loss_ce: 0.002224, loss_dice: 0.409145
[10:43:07.673] TRAIN: iteration 17966 : loss : 0.219287, loss_ce: 0.003135, loss_dice: 0.435440
[10:43:07.887] TRAIN: iteration 17967 : loss : 0.104248, loss_ce: 0.004162, loss_dice: 0.204334
[10:43:08.097] TRAIN: iteration 17968 : loss : 0.251183, loss_ce: 0.002220, loss_dice: 0.500146
[10:43:08.309] TRAIN: iteration 17969 : loss : 0.065397, loss_ce: 0.001071, loss_dice: 0.129723
[10:43:08.518] TRAIN: iteration 17970 : loss : 0.058120, loss_ce: 0.004389, loss_dice: 0.111851
[10:43:08.726] TRAIN: iteration 17971 : loss : 0.213661, loss_ce: 0.007220, loss_dice: 0.420103
[10:43:08.939] TRAIN: iteration 17972 : loss : 0.031551, loss_ce: 0.002730, loss_dice: 0.060372
[10:43:09.147] TRAIN: iteration 17973 : loss : 0.131936, loss_ce: 0.002287, loss_dice: 0.261586
[10:43:09.357] TRAIN: iteration 17974 : loss : 0.145780, loss_ce: 0.002393, loss_dice: 0.289167
[10:43:09.568] TRAIN: iteration 17975 : loss : 0.250495, loss_ce: 0.000949, loss_dice: 0.500041
[10:43:09.777] TRAIN: iteration 17976 : loss : 0.131644, loss_ce: 0.009290, loss_dice: 0.253998
[10:43:09.986] TRAIN: iteration 17977 : loss : 0.137294, loss_ce: 0.012748, loss_dice: 0.261840
[10:43:10.196] TRAIN: iteration 17978 : loss : 0.069835, loss_ce: 0.004468, loss_dice: 0.135202
[10:43:10.901] TRAIN: iteration 17979 : loss : 0.147060, loss_ce: 0.003579, loss_dice: 0.290540
[10:43:11.115] TRAIN: iteration 17980 : loss : 0.117282, loss_ce: 0.007803, loss_dice: 0.226762
[10:43:11.354] TRAIN: iteration 17981 : loss : 0.250808, loss_ce: 0.001529, loss_dice: 0.500087
[10:43:11.562] TRAIN: iteration 17982 : loss : 0.096917, loss_ce: 0.008444, loss_dice: 0.185390
[10:43:11.776] TRAIN: iteration 17983 : loss : 0.262055, loss_ce: 0.023542, loss_dice: 0.500567
[10:43:12.051] TRAIN: iteration 17984 : loss : 0.139612, loss_ce: 0.005325, loss_dice: 0.273898
[10:43:12.259] TRAIN: iteration 17985 : loss : 0.202442, loss_ce: 0.013458, loss_dice: 0.391425
[10:43:12.469] TRAIN: iteration 17986 : loss : 0.169906, loss_ce: 0.003733, loss_dice: 0.336080
[10:43:12.676] TRAIN: iteration 17987 : loss : 0.228515, loss_ce: 0.004215, loss_dice: 0.452815
[10:43:12.884] TRAIN: iteration 17988 : loss : 0.082983, loss_ce: 0.006364, loss_dice: 0.159603
[10:43:13.095] TRAIN: iteration 17989 : loss : 0.167746, loss_ce: 0.003711, loss_dice: 0.331780
[10:43:13.303] TRAIN: iteration 17990 : loss : 0.069881, loss_ce: 0.008321, loss_dice: 0.131441
[10:43:13.515] TRAIN: iteration 17991 : loss : 0.067962, loss_ce: 0.002878, loss_dice: 0.133045
[10:43:14.540] TRAIN: iteration 17992 : loss : 0.052851, loss_ce: 0.005392, loss_dice: 0.100310
[10:43:14.748] TRAIN: iteration 17993 : loss : 0.052093, loss_ce: 0.006845, loss_dice: 0.097341
[10:43:14.962] TRAIN: iteration 17994 : loss : 0.240329, loss_ce: 0.010638, loss_dice: 0.470020
[10:43:15.175] TRAIN: iteration 17995 : loss : 0.177366, loss_ce: 0.002293, loss_dice: 0.352438
[10:43:15.382] TRAIN: iteration 17996 : loss : 0.251001, loss_ce: 0.001904, loss_dice: 0.500098
[10:43:15.590] TRAIN: iteration 17997 : loss : 0.098419, loss_ce: 0.007479, loss_dice: 0.189359
[10:43:15.799] TRAIN: iteration 17998 : loss : 0.250867, loss_ce: 0.001640, loss_dice: 0.500095
[10:43:16.007] TRAIN: iteration 17999 : loss : 0.236724, loss_ce: 0.003455, loss_dice: 0.469992
[10:43:16.216] TRAIN: iteration 18000 : loss : 0.124831, loss_ce: 0.004669, loss_dice: 0.244993
[10:43:16.455] TRAIN: iteration 18001 : loss : 0.115902, loss_ce: 0.001630, loss_dice: 0.230175
[10:43:16.665] TRAIN: iteration 18002 : loss : 0.080333, loss_ce: 0.005313, loss_dice: 0.155352
[10:43:16.873] TRAIN: iteration 18003 : loss : 0.172590, loss_ce: 0.024890, loss_dice: 0.320290
[10:43:17.082] TRAIN: iteration 18004 : loss : 0.066937, loss_ce: 0.001842, loss_dice: 0.132032
[10:43:17.292] TRAIN: iteration 18005 : loss : 0.134717, loss_ce: 0.006062, loss_dice: 0.263372
[10:43:17.499] TRAIN: iteration 18006 : loss : 0.250361, loss_ce: 0.000709, loss_dice: 0.500013
[10:43:17.708] TRAIN: iteration 18007 : loss : 0.132070, loss_ce: 0.002619, loss_dice: 0.261520
[10:43:17.919] TRAIN: iteration 18008 : loss : 0.139798, loss_ce: 0.001424, loss_dice: 0.278172
[10:43:18.129] TRAIN: iteration 18009 : loss : 0.050630, loss_ce: 0.004386, loss_dice: 0.096874
[10:43:18.337] TRAIN: iteration 18010 : loss : 0.218505, loss_ce: 0.003594, loss_dice: 0.433416
[10:43:19.351] TRAIN: iteration 18011 : loss : 0.162839, loss_ce: 0.002642, loss_dice: 0.323037
[10:43:19.558] TRAIN: iteration 18012 : loss : 0.063338, loss_ce: 0.002868, loss_dice: 0.123809
[10:43:19.765] TRAIN: iteration 18013 : loss : 0.106324, loss_ce: 0.002828, loss_dice: 0.209821
[10:43:19.974] TRAIN: iteration 18014 : loss : 0.051225, loss_ce: 0.004026, loss_dice: 0.098424
[10:43:20.182] TRAIN: iteration 18015 : loss : 0.250991, loss_ce: 0.001876, loss_dice: 0.500106
[10:43:20.389] TRAIN: iteration 18016 : loss : 0.233884, loss_ce: 0.001192, loss_dice: 0.466575
[10:43:20.600] TRAIN: iteration 18017 : loss : 0.102502, loss_ce: 0.001378, loss_dice: 0.203626
[10:43:20.808] TRAIN: iteration 18018 : loss : 0.241420, loss_ce: 0.009104, loss_dice: 0.473736
[10:43:21.016] TRAIN: iteration 18019 : loss : 0.134075, loss_ce: 0.001426, loss_dice: 0.266725
[10:43:21.223] TRAIN: iteration 18020 : loss : 0.110348, loss_ce: 0.004011, loss_dice: 0.216686
[10:43:21.470] TRAIN: iteration 18021 : loss : 0.244572, loss_ce: 0.004069, loss_dice: 0.485076
[10:43:21.678] TRAIN: iteration 18022 : loss : 0.077627, loss_ce: 0.007064, loss_dice: 0.148189
[10:43:21.886] TRAIN: iteration 18023 : loss : 0.039601, loss_ce: 0.005129, loss_dice: 0.074073
[10:43:22.094] TRAIN: iteration 18024 : loss : 0.101856, loss_ce: 0.007702, loss_dice: 0.196009
[10:43:22.302] TRAIN: iteration 18025 : loss : 0.251699, loss_ce: 0.004765, loss_dice: 0.498634
[10:43:22.510] TRAIN: iteration 18026 : loss : 0.199160, loss_ce: 0.002964, loss_dice: 0.395357
[10:43:22.718] TRAIN: iteration 18027 : loss : 0.136088, loss_ce: 0.004259, loss_dice: 0.267918
[10:43:22.928] TRAIN: iteration 18028 : loss : 0.058739, loss_ce: 0.002640, loss_dice: 0.114837
[10:43:23.137] TRAIN: iteration 18029 : loss : 0.251296, loss_ce: 0.002417, loss_dice: 0.500175
[10:43:23.347] TRAIN: iteration 18030 : loss : 0.060125, loss_ce: 0.003216, loss_dice: 0.117034
[10:43:23.558] TRAIN: iteration 18031 : loss : 0.250674, loss_ce: 0.001290, loss_dice: 0.500059
[10:43:23.815] TRAIN: iteration 18032 : loss : 0.176409, loss_ce: 0.001625, loss_dice: 0.351192
[10:43:24.026] TRAIN: iteration 18033 : loss : 0.125076, loss_ce: 0.009829, loss_dice: 0.240324
[10:43:24.243] TRAIN: iteration 18034 : loss : 0.250798, loss_ce: 0.001518, loss_dice: 0.500079
[10:43:24.450] TRAIN: iteration 18035 : loss : 0.237517, loss_ce: 0.016609, loss_dice: 0.458426
[10:43:24.658] TRAIN: iteration 18036 : loss : 0.171314, loss_ce: 0.000940, loss_dice: 0.341688
[10:43:24.867] TRAIN: iteration 18037 : loss : 0.209277, loss_ce: 0.003155, loss_dice: 0.415400
[10:43:25.333] TRAIN: iteration 18038 : loss : 0.097513, loss_ce: 0.005123, loss_dice: 0.189904
[10:43:25.540] TRAIN: iteration 18039 : loss : 0.064486, loss_ce: 0.001381, loss_dice: 0.127592
[10:43:25.748] TRAIN: iteration 18040 : loss : 0.032012, loss_ce: 0.003423, loss_dice: 0.060602
[10:43:25.987] TRAIN: iteration 18041 : loss : 0.250211, loss_ce: 0.000416, loss_dice: 0.500006
[10:43:26.195] TRAIN: iteration 18042 : loss : 0.220559, loss_ce: 0.001192, loss_dice: 0.439926
[10:43:26.411] TRAIN: iteration 18043 : loss : 0.203618, loss_ce: 0.001311, loss_dice: 0.405926
[10:43:26.618] TRAIN: iteration 18044 : loss : 0.094726, loss_ce: 0.006643, loss_dice: 0.182809
[10:43:26.826] TRAIN: iteration 18045 : loss : 0.250224, loss_ce: 0.000442, loss_dice: 0.500007
[10:43:27.034] TRAIN: iteration 18046 : loss : 0.064790, loss_ce: 0.003240, loss_dice: 0.126340
[10:43:27.242] TRAIN: iteration 18047 : loss : 0.168297, loss_ce: 0.016208, loss_dice: 0.320387
[10:43:27.449] TRAIN: iteration 18048 : loss : 0.202126, loss_ce: 0.002498, loss_dice: 0.401754
[10:43:28.546] TRAIN: iteration 18049 : loss : 0.047359, loss_ce: 0.001944, loss_dice: 0.092774
[10:43:28.755] TRAIN: iteration 18050 : loss : 0.215444, loss_ce: 0.001503, loss_dice: 0.429384
[10:43:28.966] TRAIN: iteration 18051 : loss : 0.097559, loss_ce: 0.003880, loss_dice: 0.191239
[10:43:29.202] TRAIN: iteration 18052 : loss : 0.102891, loss_ce: 0.002803, loss_dice: 0.202979
[10:43:29.410] TRAIN: iteration 18053 : loss : 0.123219, loss_ce: 0.003619, loss_dice: 0.242818
[10:43:29.617] TRAIN: iteration 18054 : loss : 0.230127, loss_ce: 0.001769, loss_dice: 0.458485
[10:43:29.829] TRAIN: iteration 18055 : loss : 0.193194, loss_ce: 0.003491, loss_dice: 0.382896
[10:43:30.046] TRAIN: iteration 18056 : loss : 0.064730, loss_ce: 0.002422, loss_dice: 0.127037
[10:43:30.261] TRAIN: iteration 18057 : loss : 0.253018, loss_ce: 0.005628, loss_dice: 0.500409
[10:43:30.468] TRAIN: iteration 18058 : loss : 0.096441, loss_ce: 0.004045, loss_dice: 0.188837
[10:43:30.678] TRAIN: iteration 18059 : loss : 0.075808, loss_ce: 0.003888, loss_dice: 0.147728
[10:43:30.886] TRAIN: iteration 18060 : loss : 0.077350, loss_ce: 0.002122, loss_dice: 0.152578
[10:43:31.869] TRAIN: iteration 18061 : loss : 0.068935, loss_ce: 0.005646, loss_dice: 0.132224
[10:43:32.077] TRAIN: iteration 18062 : loss : 0.162183, loss_ce: 0.003479, loss_dice: 0.320888
[10:43:32.286] TRAIN: iteration 18063 : loss : 0.080117, loss_ce: 0.004115, loss_dice: 0.156119
[10:43:32.495] TRAIN: iteration 18064 : loss : 0.062775, loss_ce: 0.011126, loss_dice: 0.114423
[10:43:32.702] TRAIN: iteration 18065 : loss : 0.250853, loss_ce: 0.001637, loss_dice: 0.500069
[10:43:32.911] TRAIN: iteration 18066 : loss : 0.240279, loss_ce: 0.003746, loss_dice: 0.476813
[10:43:33.119] TRAIN: iteration 18067 : loss : 0.182135, loss_ce: 0.004155, loss_dice: 0.360116
[10:43:33.328] TRAIN: iteration 18068 : loss : 0.035953, loss_ce: 0.004833, loss_dice: 0.067074
[10:43:33.538] TRAIN: iteration 18069 : loss : 0.130322, loss_ce: 0.003484, loss_dice: 0.257159
[10:43:33.748] TRAIN: iteration 18070 : loss : 0.187801, loss_ce: 0.007517, loss_dice: 0.368084
[10:43:33.956] TRAIN: iteration 18071 : loss : 0.228945, loss_ce: 0.001888, loss_dice: 0.456001
[10:43:34.165] TRAIN: iteration 18072 : loss : 0.251702, loss_ce: 0.003188, loss_dice: 0.500217
[10:43:34.375] TRAIN: iteration 18073 : loss : 0.177642, loss_ce: 0.004872, loss_dice: 0.350411
[10:43:34.585] TRAIN: iteration 18074 : loss : 0.242968, loss_ce: 0.003700, loss_dice: 0.482236
[10:43:34.795] TRAIN: iteration 18075 : loss : 0.061788, loss_ce: 0.003184, loss_dice: 0.120393
[10:43:35.004] TRAIN: iteration 18076 : loss : 0.071159, loss_ce: 0.003002, loss_dice: 0.139316
[10:43:35.215] TRAIN: iteration 18077 : loss : 0.071709, loss_ce: 0.003098, loss_dice: 0.140320
[10:43:35.424] TRAIN: iteration 18078 : loss : 0.069029, loss_ce: 0.003565, loss_dice: 0.134493
[10:43:35.632] TRAIN: iteration 18079 : loss : 0.047561, loss_ce: 0.003858, loss_dice: 0.091263
[10:43:35.840] TRAIN: iteration 18080 : loss : 0.168117, loss_ce: 0.005406, loss_dice: 0.330827
[10:43:36.411] TRAIN: iteration 18081 : loss : 0.239024, loss_ce: 0.003319, loss_dice: 0.474730
[10:43:36.620] TRAIN: iteration 18082 : loss : 0.113986, loss_ce: 0.003509, loss_dice: 0.224463
[10:43:36.830] TRAIN: iteration 18083 : loss : 0.109060, loss_ce: 0.003175, loss_dice: 0.214945
[10:43:37.039] TRAIN: iteration 18084 : loss : 0.220568, loss_ce: 0.003061, loss_dice: 0.438075
[10:43:37.250] TRAIN: iteration 18085 : loss : 0.084818, loss_ce: 0.002823, loss_dice: 0.166813
[10:43:37.462] TRAIN: iteration 18086 : loss : 0.071011, loss_ce: 0.007420, loss_dice: 0.134603
[10:43:37.670] TRAIN: iteration 18087 : loss : 0.095723, loss_ce: 0.005964, loss_dice: 0.185482
[10:43:37.878] TRAIN: iteration 18088 : loss : 0.095342, loss_ce: 0.002926, loss_dice: 0.187758
[10:43:38.087] TRAIN: iteration 18089 : loss : 0.054341, loss_ce: 0.002606, loss_dice: 0.106076
[10:43:38.295] TRAIN: iteration 18090 : loss : 0.056999, loss_ce: 0.004953, loss_dice: 0.109044
[10:43:38.505] TRAIN: iteration 18091 : loss : 0.085252, loss_ce: 0.004207, loss_dice: 0.166297
[10:43:38.715] TRAIN: iteration 18092 : loss : 0.206242, loss_ce: 0.003208, loss_dice: 0.409276
[10:43:39.168] TRAIN: iteration 18093 : loss : 0.043084, loss_ce: 0.003671, loss_dice: 0.082496
[10:43:39.399] TRAIN: iteration 18094 : loss : 0.049389, loss_ce: 0.004497, loss_dice: 0.094280
[10:43:39.613] TRAIN: iteration 18095 : loss : 0.147236, loss_ce: 0.003396, loss_dice: 0.291075
[10:43:39.823] TRAIN: iteration 18096 : loss : 0.049848, loss_ce: 0.002765, loss_dice: 0.096931
[10:43:40.031] TRAIN: iteration 18097 : loss : 0.244926, loss_ce: 0.002367, loss_dice: 0.487486
[10:43:40.242] TRAIN: iteration 18098 : loss : 0.160241, loss_ce: 0.003614, loss_dice: 0.316867
[10:43:40.449] TRAIN: iteration 18099 : loss : 0.047067, loss_ce: 0.001170, loss_dice: 0.092964
[10:43:40.659] TRAIN: iteration 18100 : loss : 0.145681, loss_ce: 0.004056, loss_dice: 0.287306
[10:43:40.898] TRAIN: iteration 18101 : loss : 0.054941, loss_ce: 0.002066, loss_dice: 0.107815
[10:43:41.106] TRAIN: iteration 18102 : loss : 0.047780, loss_ce: 0.003152, loss_dice: 0.092408
[10:43:41.314] TRAIN: iteration 18103 : loss : 0.025333, loss_ce: 0.001384, loss_dice: 0.049282
[10:43:41.521] TRAIN: iteration 18104 : loss : 0.102881, loss_ce: 0.009599, loss_dice: 0.196162
[10:43:41.732] TRAIN: iteration 18105 : loss : 0.188013, loss_ce: 0.004050, loss_dice: 0.371976
[10:43:41.940] TRAIN: iteration 18106 : loss : 0.052135, loss_ce: 0.001881, loss_dice: 0.102388
[10:43:42.147] TRAIN: iteration 18107 : loss : 0.091042, loss_ce: 0.003198, loss_dice: 0.178885
[10:43:42.358] TRAIN: iteration 18108 : loss : 0.095666, loss_ce: 0.003516, loss_dice: 0.187816
[10:43:42.567] TRAIN: iteration 18109 : loss : 0.125167, loss_ce: 0.002241, loss_dice: 0.248094
[10:43:42.775] TRAIN: iteration 18110 : loss : 0.104921, loss_ce: 0.003848, loss_dice: 0.205993
[10:43:42.984] TRAIN: iteration 18111 : loss : 0.251748, loss_ce: 0.003294, loss_dice: 0.500203
[10:43:43.198] TRAIN: iteration 18112 : loss : 0.252286, loss_ce: 0.004262, loss_dice: 0.500310
[10:43:43.405] TRAIN: iteration 18113 : loss : 0.062765, loss_ce: 0.004616, loss_dice: 0.120914
[10:43:43.614] TRAIN: iteration 18114 : loss : 0.165086, loss_ce: 0.003641, loss_dice: 0.326532
[10:43:43.822] TRAIN: iteration 18115 : loss : 0.079940, loss_ce: 0.003049, loss_dice: 0.156832
[10:43:44.031] TRAIN: iteration 18116 : loss : 0.243627, loss_ce: 0.005000, loss_dice: 0.482254
[10:43:44.239] TRAIN: iteration 18117 : loss : 0.151527, loss_ce: 0.029948, loss_dice: 0.273107
[10:43:44.450] TRAIN: iteration 18118 : loss : 0.180016, loss_ce: 0.002503, loss_dice: 0.357530
[10:43:44.659] TRAIN: iteration 18119 : loss : 0.230657, loss_ce: 0.008855, loss_dice: 0.452458
[10:43:44.868] TRAIN: iteration 18120 : loss : 0.123886, loss_ce: 0.003294, loss_dice: 0.244479
[10:43:45.107] TRAIN: iteration 18121 : loss : 0.251656, loss_ce: 0.003108, loss_dice: 0.500204
[10:43:45.315] TRAIN: iteration 18122 : loss : 0.251163, loss_ce: 0.002213, loss_dice: 0.500112
[10:43:45.529] TRAIN: iteration 18123 : loss : 0.027110, loss_ce: 0.001646, loss_dice: 0.052574
[10:43:45.739] TRAIN: iteration 18124 : loss : 0.047432, loss_ce: 0.009536, loss_dice: 0.085328
[10:43:45.955] TRAIN: iteration 18125 : loss : 0.248627, loss_ce: 0.001774, loss_dice: 0.495480
[10:43:46.165] TRAIN: iteration 18126 : loss : 0.129995, loss_ce: 0.002607, loss_dice: 0.257383
[10:43:46.378] TRAIN: iteration 18127 : loss : 0.137093, loss_ce: 0.009404, loss_dice: 0.264782
[10:43:46.589] TRAIN: iteration 18128 : loss : 0.148531, loss_ce: 0.006929, loss_dice: 0.290134
[10:43:46.804] TRAIN: iteration 18129 : loss : 0.060429, loss_ce: 0.009711, loss_dice: 0.111147
[10:43:47.015] TRAIN: iteration 18130 : loss : 0.094885, loss_ce: 0.002191, loss_dice: 0.187580
[10:43:47.225] TRAIN: iteration 18131 : loss : 0.165952, loss_ce: 0.004213, loss_dice: 0.327690
[10:43:47.432] TRAIN: iteration 18132 : loss : 0.119122, loss_ce: 0.003229, loss_dice: 0.235014
[10:43:47.640] TRAIN: iteration 18133 : loss : 0.046434, loss_ce: 0.003789, loss_dice: 0.089078
[10:43:47.847] TRAIN: iteration 18134 : loss : 0.221622, loss_ce: 0.002400, loss_dice: 0.440844
[10:43:48.058] TRAIN: iteration 18135 : loss : 0.188234, loss_ce: 0.009076, loss_dice: 0.367391
[10:43:48.265] TRAIN: iteration 18136 : loss : 0.139923, loss_ce: 0.005441, loss_dice: 0.274405
[10:43:48.473] TRAIN: iteration 18137 : loss : 0.086335, loss_ce: 0.005981, loss_dice: 0.166689
[10:43:48.680] TRAIN: iteration 18138 : loss : 0.087535, loss_ce: 0.006050, loss_dice: 0.169020
[10:43:48.888] TRAIN: iteration 18139 : loss : 0.066729, loss_ce: 0.008874, loss_dice: 0.124584
[10:43:49.101] TRAIN: iteration 18140 : loss : 0.033881, loss_ce: 0.002149, loss_dice: 0.065614
[10:43:49.338] TRAIN: iteration 18141 : loss : 0.045317, loss_ce: 0.004433, loss_dice: 0.086201
[10:43:49.546] TRAIN: iteration 18142 : loss : 0.091153, loss_ce: 0.003397, loss_dice: 0.178909
[10:43:49.753] TRAIN: iteration 18143 : loss : 0.251366, loss_ce: 0.002594, loss_dice: 0.500137
[10:43:49.961] TRAIN: iteration 18144 : loss : 0.232063, loss_ce: 0.002395, loss_dice: 0.461731
[10:43:50.169] TRAIN: iteration 18145 : loss : 0.153762, loss_ce: 0.002111, loss_dice: 0.305412
[10:43:50.376] TRAIN: iteration 18146 : loss : 0.076481, loss_ce: 0.002105, loss_dice: 0.150857
[10:43:50.584] TRAIN: iteration 18147 : loss : 0.060472, loss_ce: 0.008616, loss_dice: 0.112328
[10:43:50.792] TRAIN: iteration 18148 : loss : 0.251002, loss_ce: 0.001915, loss_dice: 0.500088
[10:43:51.373] TRAIN: iteration 18149 : loss : 0.125748, loss_ce: 0.006099, loss_dice: 0.245397
[10:43:51.580] TRAIN: iteration 18150 : loss : 0.043214, loss_ce: 0.001518, loss_dice: 0.084909
[10:43:51.788] TRAIN: iteration 18151 : loss : 0.026977, loss_ce: 0.004064, loss_dice: 0.049890
[10:43:51.997] TRAIN: iteration 18152 : loss : 0.085152, loss_ce: 0.009698, loss_dice: 0.160605
[10:43:52.205] TRAIN: iteration 18153 : loss : 0.051730, loss_ce: 0.004539, loss_dice: 0.098920
[10:43:52.414] TRAIN: iteration 18154 : loss : 0.069555, loss_ce: 0.006200, loss_dice: 0.132910
[10:43:52.622] TRAIN: iteration 18155 : loss : 0.251292, loss_ce: 0.002443, loss_dice: 0.500140
[10:43:53.822] TRAIN: iteration 18156 : loss : 0.061607, loss_ce: 0.004404, loss_dice: 0.118809
[10:43:54.045] TRAIN: iteration 18157 : loss : 0.173384, loss_ce: 0.006297, loss_dice: 0.340471
[10:43:54.255] TRAIN: iteration 18158 : loss : 0.101649, loss_ce: 0.001619, loss_dice: 0.201679
[10:43:54.464] TRAIN: iteration 18159 : loss : 0.047118, loss_ce: 0.004903, loss_dice: 0.089333
[10:43:54.673] TRAIN: iteration 18160 : loss : 0.054890, loss_ce: 0.006900, loss_dice: 0.102879
[10:43:54.673] NaN or Inf found in input tensor.
[10:43:54.891] TRAIN: iteration 18161 : loss : 0.174262, loss_ce: 0.003528, loss_dice: 0.344995
[10:43:55.100] TRAIN: iteration 18162 : loss : 0.253055, loss_ce: 0.005974, loss_dice: 0.500136
[10:43:55.308] TRAIN: iteration 18163 : loss : 0.247852, loss_ce: 0.002440, loss_dice: 0.493263
[10:43:56.018] TRAIN: iteration 18164 : loss : 0.157875, loss_ce: 0.004077, loss_dice: 0.311674
[10:43:56.226] TRAIN: iteration 18165 : loss : 0.216678, loss_ce: 0.003116, loss_dice: 0.430241
[10:43:56.434] TRAIN: iteration 18166 : loss : 0.201949, loss_ce: 0.002201, loss_dice: 0.401697
[10:43:56.642] TRAIN: iteration 18167 : loss : 0.152762, loss_ce: 0.006181, loss_dice: 0.299342
[10:43:56.856] TRAIN: iteration 18168 : loss : 0.236510, loss_ce: 0.001797, loss_dice: 0.471224
[10:43:57.070] TRAIN: iteration 18169 : loss : 0.181921, loss_ce: 0.003812, loss_dice: 0.360029
[10:43:57.279] TRAIN: iteration 18170 : loss : 0.053963, loss_ce: 0.003767, loss_dice: 0.104160
[10:43:57.488] TRAIN: iteration 18171 : loss : 0.250689, loss_ce: 0.001340, loss_dice: 0.500038
[10:43:58.705] TRAIN: iteration 18172 : loss : 0.189542, loss_ce: 0.009233, loss_dice: 0.369851
[10:43:58.913] TRAIN: iteration 18173 : loss : 0.075320, loss_ce: 0.001707, loss_dice: 0.148934
[10:43:59.121] TRAIN: iteration 18174 : loss : 0.103916, loss_ce: 0.002844, loss_dice: 0.204988
[10:43:59.329] TRAIN: iteration 18175 : loss : 0.056693, loss_ce: 0.002950, loss_dice: 0.110436
[10:43:59.537] TRAIN: iteration 18176 : loss : 0.250552, loss_ce: 0.002797, loss_dice: 0.498306
[10:43:59.744] TRAIN: iteration 18177 : loss : 0.170952, loss_ce: 0.015171, loss_dice: 0.326733
[10:43:59.952] TRAIN: iteration 18178 : loss : 0.064445, loss_ce: 0.003275, loss_dice: 0.125615
[10:44:00.161] TRAIN: iteration 18179 : loss : 0.063664, loss_ce: 0.002019, loss_dice: 0.125308
[10:44:01.112] TRAIN: iteration 18180 : loss : 0.084793, loss_ce: 0.006159, loss_dice: 0.163428
[10:44:01.352] TRAIN: iteration 18181 : loss : 0.032714, loss_ce: 0.002405, loss_dice: 0.063023
[10:44:01.561] TRAIN: iteration 18182 : loss : 0.248337, loss_ce: 0.004892, loss_dice: 0.491782
[10:44:01.768] TRAIN: iteration 18183 : loss : 0.070704, loss_ce: 0.001835, loss_dice: 0.139573
[10:44:01.975] TRAIN: iteration 18184 : loss : 0.182073, loss_ce: 0.008109, loss_dice: 0.356037
[10:44:02.185] TRAIN: iteration 18185 : loss : 0.138032, loss_ce: 0.011941, loss_dice: 0.264123
[10:44:02.396] TRAIN: iteration 18186 : loss : 0.231316, loss_ce: 0.004798, loss_dice: 0.457835
[10:44:02.604] TRAIN: iteration 18187 : loss : 0.249641, loss_ce: 0.004278, loss_dice: 0.495005
[10:44:02.811] TRAIN: iteration 18188 : loss : 0.251632, loss_ce: 0.003133, loss_dice: 0.500131
[10:44:03.022] TRAIN: iteration 18189 : loss : 0.138350, loss_ce: 0.011365, loss_dice: 0.265335
[10:44:03.231] TRAIN: iteration 18190 : loss : 0.208643, loss_ce: 0.003550, loss_dice: 0.413737
[10:44:03.438] TRAIN: iteration 18191 : loss : 0.059115, loss_ce: 0.006559, loss_dice: 0.111671
[10:44:03.646] TRAIN: iteration 18192 : loss : 0.039597, loss_ce: 0.006717, loss_dice: 0.072477
[10:44:03.857] TRAIN: iteration 18193 : loss : 0.251817, loss_ce: 0.003442, loss_dice: 0.500191
[10:44:04.067] TRAIN: iteration 18194 : loss : 0.246172, loss_ce: 0.007259, loss_dice: 0.485086
[10:44:04.320] TRAIN: iteration 18195 : loss : 0.247568, loss_ce: 0.003345, loss_dice: 0.491790
[10:44:04.528] TRAIN: iteration 18196 : loss : 0.105173, loss_ce: 0.013165, loss_dice: 0.197181
[10:44:04.736] TRAIN: iteration 18197 : loss : 0.090844, loss_ce: 0.009306, loss_dice: 0.172382
[10:44:04.946] TRAIN: iteration 18198 : loss : 0.131120, loss_ce: 0.005009, loss_dice: 0.257231
[10:44:05.157] TRAIN: iteration 18199 : loss : 0.050948, loss_ce: 0.005307, loss_dice: 0.096590
[10:44:05.371] TRAIN: iteration 18200 : loss : 0.227147, loss_ce: 0.005300, loss_dice: 0.448995
[10:44:05.613] TRAIN: iteration 18201 : loss : 0.142610, loss_ce: 0.003515, loss_dice: 0.281706
[10:44:05.820] TRAIN: iteration 18202 : loss : 0.079345, loss_ce: 0.003013, loss_dice: 0.155677
[10:44:06.029] TRAIN: iteration 18203 : loss : 0.200241, loss_ce: 0.005477, loss_dice: 0.395005
[10:44:06.238] TRAIN: iteration 18204 : loss : 0.187342, loss_ce: 0.005007, loss_dice: 0.369677
[10:44:06.445] TRAIN: iteration 18205 : loss : 0.184837, loss_ce: 0.004236, loss_dice: 0.365438
[10:44:06.654] TRAIN: iteration 18206 : loss : 0.215667, loss_ce: 0.002675, loss_dice: 0.428660
[10:44:06.863] TRAIN: iteration 18207 : loss : 0.084978, loss_ce: 0.007438, loss_dice: 0.162518
[10:44:07.072] TRAIN: iteration 18208 : loss : 0.251232, loss_ce: 0.002384, loss_dice: 0.500079
[10:44:07.280] TRAIN: iteration 18209 : loss : 0.058183, loss_ce: 0.007246, loss_dice: 0.109120
[10:44:07.488] TRAIN: iteration 18210 : loss : 0.078109, loss_ce: 0.003178, loss_dice: 0.153041
[10:44:07.696] TRAIN: iteration 18211 : loss : 0.126612, loss_ce: 0.004261, loss_dice: 0.248963
[10:44:07.912] TRAIN: iteration 18212 : loss : 0.042007, loss_ce: 0.001022, loss_dice: 0.082993
[10:44:08.130] TRAIN: iteration 18213 : loss : 0.244819, loss_ce: 0.001734, loss_dice: 0.487905
[10:44:08.762] TRAIN: iteration 18214 : loss : 0.158314, loss_ce: 0.005250, loss_dice: 0.311377
[10:44:08.970] TRAIN: iteration 18215 : loss : 0.138850, loss_ce: 0.018487, loss_dice: 0.259214
[10:44:09.178] TRAIN: iteration 18216 : loss : 0.099442, loss_ce: 0.003335, loss_dice: 0.195549
[10:44:09.386] TRAIN: iteration 18217 : loss : 0.149977, loss_ce: 0.002832, loss_dice: 0.297123
[10:44:09.595] TRAIN: iteration 18218 : loss : 0.231423, loss_ce: 0.002390, loss_dice: 0.460455
[10:44:09.802] TRAIN: iteration 18219 : loss : 0.196019, loss_ce: 0.001596, loss_dice: 0.390441
[10:44:10.010] TRAIN: iteration 18220 : loss : 0.087153, loss_ce: 0.001988, loss_dice: 0.172317
[10:44:10.256] TRAIN: iteration 18221 : loss : 0.210585, loss_ce: 0.004788, loss_dice: 0.416382
[10:44:13.242] TRAIN: iteration 18222 : loss : 0.076091, loss_ce: 0.002023, loss_dice: 0.150158
[10:44:13.450] TRAIN: iteration 18223 : loss : 0.242012, loss_ce: 0.007831, loss_dice: 0.476194
[10:44:13.661] TRAIN: iteration 18224 : loss : 0.026636, loss_ce: 0.001003, loss_dice: 0.052269
[10:44:13.868] TRAIN: iteration 18225 : loss : 0.091181, loss_ce: 0.006641, loss_dice: 0.175721
[10:44:14.082] TRAIN: iteration 18226 : loss : 0.161003, loss_ce: 0.005701, loss_dice: 0.316304
[10:44:14.290] TRAIN: iteration 18227 : loss : 0.111902, loss_ce: 0.006173, loss_dice: 0.217631
[10:44:14.499] TRAIN: iteration 18228 : loss : 0.254091, loss_ce: 0.008910, loss_dice: 0.499271
[10:44:14.707] TRAIN: iteration 18229 : loss : 0.088818, loss_ce: 0.008405, loss_dice: 0.169231
[10:44:14.915] TRAIN: iteration 18230 : loss : 0.150821, loss_ce: 0.019980, loss_dice: 0.281662
[10:44:15.124] TRAIN: iteration 18231 : loss : 0.098706, loss_ce: 0.003213, loss_dice: 0.194199
[10:44:15.332] TRAIN: iteration 18232 : loss : 0.089436, loss_ce: 0.005049, loss_dice: 0.173822
[10:44:15.539] TRAIN: iteration 18233 : loss : 0.122557, loss_ce: 0.002810, loss_dice: 0.242305
[10:44:15.746] TRAIN: iteration 18234 : loss : 0.045681, loss_ce: 0.003517, loss_dice: 0.087844
[10:44:15.954] TRAIN: iteration 18235 : loss : 0.072224, loss_ce: 0.006429, loss_dice: 0.138018
[10:44:16.172] TRAIN: iteration 18236 : loss : 0.096746, loss_ce: 0.011586, loss_dice: 0.181905
[10:44:16.390] TRAIN: iteration 18237 : loss : 0.059045, loss_ce: 0.008881, loss_dice: 0.109208
[10:44:16.812] TRAIN: iteration 18238 : loss : 0.141733, loss_ce: 0.006051, loss_dice: 0.277415
[10:44:17.020] TRAIN: iteration 18239 : loss : 0.053521, loss_ce: 0.002360, loss_dice: 0.104682
[10:44:17.227] TRAIN: iteration 18240 : loss : 0.032413, loss_ce: 0.001553, loss_dice: 0.063272
[10:44:17.468] TRAIN: iteration 18241 : loss : 0.151365, loss_ce: 0.007536, loss_dice: 0.295193
[10:44:17.676] TRAIN: iteration 18242 : loss : 0.048503, loss_ce: 0.001872, loss_dice: 0.095135
[10:44:17.910] TRAIN: iteration 18243 : loss : 0.025607, loss_ce: 0.003599, loss_dice: 0.047615
[10:44:18.119] TRAIN: iteration 18244 : loss : 0.060812, loss_ce: 0.003032, loss_dice: 0.118592
[10:44:18.327] TRAIN: iteration 18245 : loss : 0.233879, loss_ce: 0.002746, loss_dice: 0.465013
[10:44:18.551] TRAIN: iteration 18246 : loss : 0.092597, loss_ce: 0.005101, loss_dice: 0.180094
[10:44:18.759] TRAIN: iteration 18247 : loss : 0.148648, loss_ce: 0.006218, loss_dice: 0.291079
[10:44:18.966] TRAIN: iteration 18248 : loss : 0.065867, loss_ce: 0.002731, loss_dice: 0.129003
[10:44:19.174] TRAIN: iteration 18249 : loss : 0.072003, loss_ce: 0.001564, loss_dice: 0.142441
[10:44:19.387] TRAIN: iteration 18250 : loss : 0.072385, loss_ce: 0.001564, loss_dice: 0.143206
[10:44:19.597] TRAIN: iteration 18251 : loss : 0.078669, loss_ce: 0.001697, loss_dice: 0.155640
[10:44:19.805] TRAIN: iteration 18252 : loss : 0.230029, loss_ce: 0.002551, loss_dice: 0.457507
[10:44:20.365] TRAIN: iteration 18253 : loss : 0.085920, loss_ce: 0.001924, loss_dice: 0.169916
[10:44:20.573] TRAIN: iteration 18254 : loss : 0.032013, loss_ce: 0.002030, loss_dice: 0.061996
[10:44:20.787] TRAIN: iteration 18255 : loss : 0.148192, loss_ce: 0.009129, loss_dice: 0.287255
[10:44:20.999] TRAIN: iteration 18256 : loss : 0.088135, loss_ce: 0.009929, loss_dice: 0.166341
[10:44:21.209] TRAIN: iteration 18257 : loss : 0.167594, loss_ce: 0.003590, loss_dice: 0.331598
[10:44:21.695] TRAIN: iteration 18258 : loss : 0.147371, loss_ce: 0.003204, loss_dice: 0.291539
[10:44:21.908] TRAIN: iteration 18259 : loss : 0.206104, loss_ce: 0.008550, loss_dice: 0.403658
[10:44:22.117] TRAIN: iteration 18260 : loss : 0.250593, loss_ce: 0.001116, loss_dice: 0.500069
[10:44:22.995] TRAIN: iteration 18261 : loss : 0.116412, loss_ce: 0.012629, loss_dice: 0.220195
[10:44:23.205] TRAIN: iteration 18262 : loss : 0.061233, loss_ce: 0.001768, loss_dice: 0.120698
[10:44:23.417] TRAIN: iteration 18263 : loss : 0.219966, loss_ce: 0.002264, loss_dice: 0.437669
[10:44:23.630] TRAIN: iteration 18264 : loss : 0.093383, loss_ce: 0.001234, loss_dice: 0.185532
[10:44:23.838] TRAIN: iteration 18265 : loss : 0.251034, loss_ce: 0.001969, loss_dice: 0.500098
[10:44:24.052] TRAIN: iteration 18266 : loss : 0.081460, loss_ce: 0.001668, loss_dice: 0.161253
[10:44:24.260] TRAIN: iteration 18267 : loss : 0.251120, loss_ce: 0.002095, loss_dice: 0.500145
[10:44:24.474] TRAIN: iteration 18268 : loss : 0.040594, loss_ce: 0.004838, loss_dice: 0.076350
[10:44:27.198] TRAIN: iteration 18269 : loss : 0.059037, loss_ce: 0.004506, loss_dice: 0.113568
[10:44:27.408] TRAIN: iteration 18270 : loss : 0.078585, loss_ce: 0.004209, loss_dice: 0.152960
[10:44:27.615] TRAIN: iteration 18271 : loss : 0.040557, loss_ce: 0.002405, loss_dice: 0.078709
[10:44:27.827] TRAIN: iteration 18272 : loss : 0.063290, loss_ce: 0.002335, loss_dice: 0.124245
[10:44:28.037] TRAIN: iteration 18273 : loss : 0.206737, loss_ce: 0.002543, loss_dice: 0.410931
[10:44:28.245] TRAIN: iteration 18274 : loss : 0.092647, loss_ce: 0.002398, loss_dice: 0.182895
[10:44:28.453] TRAIN: iteration 18275 : loss : 0.078955, loss_ce: 0.001580, loss_dice: 0.156330
[10:44:28.660] TRAIN: iteration 18276 : loss : 0.250262, loss_ce: 0.000519, loss_dice: 0.500005
[10:44:28.868] TRAIN: iteration 18277 : loss : 0.211118, loss_ce: 0.004778, loss_dice: 0.417458
[10:44:29.076] TRAIN: iteration 18278 : loss : 0.247004, loss_ce: 0.003405, loss_dice: 0.490604
[10:44:29.284] TRAIN: iteration 18279 : loss : 0.041684, loss_ce: 0.004897, loss_dice: 0.078470
[10:44:29.493] TRAIN: iteration 18280 : loss : 0.033859, loss_ce: 0.004335, loss_dice: 0.063382
[10:44:29.731] TRAIN: iteration 18281 : loss : 0.147004, loss_ce: 0.006001, loss_dice: 0.288006
[10:44:29.940] TRAIN: iteration 18282 : loss : 0.082756, loss_ce: 0.001720, loss_dice: 0.163792
[10:44:30.148] TRAIN: iteration 18283 : loss : 0.252378, loss_ce: 0.007050, loss_dice: 0.497706
[10:44:30.356] TRAIN: iteration 18284 : loss : 0.250696, loss_ce: 0.001309, loss_dice: 0.500084
[10:44:30.719] TRAIN: iteration 18285 : loss : 0.226108, loss_ce: 0.014390, loss_dice: 0.437825
[10:44:30.927] TRAIN: iteration 18286 : loss : 0.042999, loss_ce: 0.001300, loss_dice: 0.084698
[10:44:31.372] TRAIN: iteration 18287 : loss : 0.250632, loss_ce: 0.001223, loss_dice: 0.500041
[10:44:31.587] TRAIN: iteration 18288 : loss : 0.034680, loss_ce: 0.001942, loss_dice: 0.067419
[10:44:31.804] TRAIN: iteration 18289 : loss : 0.113604, loss_ce: 0.003536, loss_dice: 0.223671
[10:44:32.516] TRAIN: iteration 18290 : loss : 0.250863, loss_ce: 0.001635, loss_dice: 0.500091
[10:44:32.723] TRAIN: iteration 18291 : loss : 0.105638, loss_ce: 0.005593, loss_dice: 0.205684
[10:44:32.934] TRAIN: iteration 18292 : loss : 0.041317, loss_ce: 0.001638, loss_dice: 0.080995
[10:44:33.145] TRAIN: iteration 18293 : loss : 0.138933, loss_ce: 0.002048, loss_dice: 0.275817
[10:44:33.356] TRAIN: iteration 18294 : loss : 0.094016, loss_ce: 0.005079, loss_dice: 0.182952
[10:44:33.565] TRAIN: iteration 18295 : loss : 0.070889, loss_ce: 0.002135, loss_dice: 0.139642
[10:44:33.780] TRAIN: iteration 18296 : loss : 0.073602, loss_ce: 0.004763, loss_dice: 0.142442
[10:44:33.991] TRAIN: iteration 18297 : loss : 0.162176, loss_ce: 0.002722, loss_dice: 0.321630
[10:44:35.676] TRAIN: iteration 18298 : loss : 0.065003, loss_ce: 0.003420, loss_dice: 0.126586
[10:44:35.886] TRAIN: iteration 18299 : loss : 0.064523, loss_ce: 0.004161, loss_dice: 0.124886
[10:44:36.096] TRAIN: iteration 18300 : loss : 0.039805, loss_ce: 0.002102, loss_dice: 0.077508
[10:44:36.333] TRAIN: iteration 18301 : loss : 0.047603, loss_ce: 0.003521, loss_dice: 0.091685
[10:44:36.542] TRAIN: iteration 18302 : loss : 0.250761, loss_ce: 0.001439, loss_dice: 0.500083
[10:44:36.751] TRAIN: iteration 18303 : loss : 0.117142, loss_ce: 0.005279, loss_dice: 0.229005
[10:44:36.959] TRAIN: iteration 18304 : loss : 0.216133, loss_ce: 0.010483, loss_dice: 0.421782
[10:44:37.167] TRAIN: iteration 18305 : loss : 0.254427, loss_ce: 0.010228, loss_dice: 0.498627
[10:44:37.375] TRAIN: iteration 18306 : loss : 0.234976, loss_ce: 0.003135, loss_dice: 0.466817
[10:44:37.588] TRAIN: iteration 18307 : loss : 0.234732, loss_ce: 0.002292, loss_dice: 0.467171
[10:44:37.797] TRAIN: iteration 18308 : loss : 0.081674, loss_ce: 0.006788, loss_dice: 0.156559
[10:44:40.440] TRAIN: iteration 18309 : loss : 0.075397, loss_ce: 0.006325, loss_dice: 0.144470
[10:44:40.648] TRAIN: iteration 18310 : loss : 0.144915, loss_ce: 0.003291, loss_dice: 0.286539
[10:44:40.857] TRAIN: iteration 18311 : loss : 0.052653, loss_ce: 0.003817, loss_dice: 0.101490
[10:44:41.066] TRAIN: iteration 18312 : loss : 0.063976, loss_ce: 0.002677, loss_dice: 0.125275
[10:44:41.274] TRAIN: iteration 18313 : loss : 0.102262, loss_ce: 0.017402, loss_dice: 0.187122
[10:44:41.481] TRAIN: iteration 18314 : loss : 0.168110, loss_ce: 0.003470, loss_dice: 0.332751
[10:44:41.689] TRAIN: iteration 18315 : loss : 0.250834, loss_ce: 0.001606, loss_dice: 0.500061
[10:44:41.986] TRAIN: iteration 18316 : loss : 0.165885, loss_ce: 0.002883, loss_dice: 0.328887
[10:44:42.481] TRAIN: iteration 18317 : loss : 0.068420, loss_ce: 0.013603, loss_dice: 0.123237
[10:44:42.696] TRAIN: iteration 18318 : loss : 0.043665, loss_ce: 0.004000, loss_dice: 0.083330
[10:44:42.905] TRAIN: iteration 18319 : loss : 0.079760, loss_ce: 0.001960, loss_dice: 0.157560
[10:44:43.113] TRAIN: iteration 18320 : loss : 0.170315, loss_ce: 0.010199, loss_dice: 0.330432
[10:44:43.356] TRAIN: iteration 18321 : loss : 0.080829, loss_ce: 0.003274, loss_dice: 0.158383
[10:44:43.565] TRAIN: iteration 18322 : loss : 0.091018, loss_ce: 0.001850, loss_dice: 0.180187
[10:44:43.779] TRAIN: iteration 18323 : loss : 0.167858, loss_ce: 0.002213, loss_dice: 0.333503
[10:44:43.987] TRAIN: iteration 18324 : loss : 0.141830, loss_ce: 0.006618, loss_dice: 0.277042
[10:44:44.722] TRAIN: iteration 18325 : loss : 0.230024, loss_ce: 0.013240, loss_dice: 0.446807
[10:44:44.930] TRAIN: iteration 18326 : loss : 0.194918, loss_ce: 0.002369, loss_dice: 0.387467
[10:44:45.137] TRAIN: iteration 18327 : loss : 0.124469, loss_ce: 0.004725, loss_dice: 0.244213
[10:44:45.350] TRAIN: iteration 18328 : loss : 0.086356, loss_ce: 0.004489, loss_dice: 0.168223
[10:44:45.559] TRAIN: iteration 18329 : loss : 0.216124, loss_ce: 0.012156, loss_dice: 0.420093
[10:44:45.768] TRAIN: iteration 18330 : loss : 0.074464, loss_ce: 0.002990, loss_dice: 0.145938
[10:44:45.978] TRAIN: iteration 18331 : loss : 0.123581, loss_ce: 0.005747, loss_dice: 0.241414
[10:44:46.188] TRAIN: iteration 18332 : loss : 0.204467, loss_ce: 0.001514, loss_dice: 0.407420
[10:44:46.397] TRAIN: iteration 18333 : loss : 0.235775, loss_ce: 0.002925, loss_dice: 0.468624
[10:44:46.605] TRAIN: iteration 18334 : loss : 0.251013, loss_ce: 0.001930, loss_dice: 0.500096
[10:44:47.245] TRAIN: iteration 18335 : loss : 0.151190, loss_ce: 0.022904, loss_dice: 0.279476
[10:44:47.482] TRAIN: iteration 18336 : loss : 0.169602, loss_ce: 0.003398, loss_dice: 0.335805
[10:44:47.695] TRAIN: iteration 18337 : loss : 0.097433, loss_ce: 0.002977, loss_dice: 0.191889
[10:44:47.904] TRAIN: iteration 18338 : loss : 0.078420, loss_ce: 0.011273, loss_dice: 0.145567
[10:44:48.115] TRAIN: iteration 18339 : loss : 0.093367, loss_ce: 0.010161, loss_dice: 0.176572
[10:44:48.327] TRAIN: iteration 18340 : loss : 0.112883, loss_ce: 0.003885, loss_dice: 0.221880
[10:44:48.570] TRAIN: iteration 18341 : loss : 0.106157, loss_ce: 0.008201, loss_dice: 0.204113
[10:44:48.779] TRAIN: iteration 18342 : loss : 0.067534, loss_ce: 0.001832, loss_dice: 0.133237
[10:44:49.333] TRAIN: iteration 18343 : loss : 0.218217, loss_ce: 0.002461, loss_dice: 0.433972
[10:44:49.540] TRAIN: iteration 18344 : loss : 0.056388, loss_ce: 0.001954, loss_dice: 0.110822
[10:44:49.750] TRAIN: iteration 18345 : loss : 0.250513, loss_ce: 0.001004, loss_dice: 0.500023
[10:44:49.965] TRAIN: iteration 18346 : loss : 0.117506, loss_ce: 0.006342, loss_dice: 0.228670
[10:44:50.173] TRAIN: iteration 18347 : loss : 0.066018, loss_ce: 0.006292, loss_dice: 0.125743
[10:44:50.380] TRAIN: iteration 18348 : loss : 0.134264, loss_ce: 0.010149, loss_dice: 0.258378
[10:44:50.643] TRAIN: iteration 18349 : loss : 0.091788, loss_ce: 0.001648, loss_dice: 0.181927
[10:44:50.853] TRAIN: iteration 18350 : loss : 0.250929, loss_ce: 0.001761, loss_dice: 0.500097
[10:44:51.385] TRAIN: iteration 18351 : loss : 0.130007, loss_ce: 0.007059, loss_dice: 0.252954
[10:44:51.594] TRAIN: iteration 18352 : loss : 0.091897, loss_ce: 0.006196, loss_dice: 0.177598
[10:44:51.803] TRAIN: iteration 18353 : loss : 0.036900, loss_ce: 0.006995, loss_dice: 0.066805
[10:44:52.010] TRAIN: iteration 18354 : loss : 0.215980, loss_ce: 0.006592, loss_dice: 0.425367
[10:44:52.218] TRAIN: iteration 18355 : loss : 0.066087, loss_ce: 0.006233, loss_dice: 0.125940
[10:44:52.425] TRAIN: iteration 18356 : loss : 0.202128, loss_ce: 0.002581, loss_dice: 0.401675
[10:44:52.641] TRAIN: iteration 18357 : loss : 0.141377, loss_ce: 0.003979, loss_dice: 0.278776
[10:44:52.849] TRAIN: iteration 18358 : loss : 0.032845, loss_ce: 0.003304, loss_dice: 0.062385
[10:44:53.642] TRAIN: iteration 18359 : loss : 0.225156, loss_ce: 0.002041, loss_dice: 0.448271
[10:44:53.853] TRAIN: iteration 18360 : loss : 0.089789, loss_ce: 0.017540, loss_dice: 0.162038
[10:44:54.101] TRAIN: iteration 18361 : loss : 0.251275, loss_ce: 0.002407, loss_dice: 0.500142
[10:44:54.315] TRAIN: iteration 18362 : loss : 0.241810, loss_ce: 0.003199, loss_dice: 0.480421
[10:44:54.530] TRAIN: iteration 18363 : loss : 0.109015, loss_ce: 0.015299, loss_dice: 0.202732
[10:44:54.738] TRAIN: iteration 18364 : loss : 0.108586, loss_ce: 0.004901, loss_dice: 0.212271
[10:44:54.947] TRAIN: iteration 18365 : loss : 0.111801, loss_ce: 0.004141, loss_dice: 0.219461
[10:44:55.157] TRAIN: iteration 18366 : loss : 0.061190, loss_ce: 0.003032, loss_dice: 0.119348
[10:44:55.364] TRAIN: iteration 18367 : loss : 0.075780, loss_ce: 0.004950, loss_dice: 0.146609
[10:44:55.575] TRAIN: iteration 18368 : loss : 0.184076, loss_ce: 0.003428, loss_dice: 0.364724
[10:44:55.783] TRAIN: iteration 18369 : loss : 0.126466, loss_ce: 0.002778, loss_dice: 0.250154
[10:44:55.992] TRAIN: iteration 18370 : loss : 0.146836, loss_ce: 0.005174, loss_dice: 0.288497
[10:44:56.199] TRAIN: iteration 18371 : loss : 0.104010, loss_ce: 0.002499, loss_dice: 0.205521
[10:44:56.461] TRAIN: iteration 18372 : loss : 0.040303, loss_ce: 0.001318, loss_dice: 0.079287
[10:44:56.672] TRAIN: iteration 18373 : loss : 0.050039, loss_ce: 0.002649, loss_dice: 0.097428
[10:44:57.721] TRAIN: iteration 18374 : loss : 0.069100, loss_ce: 0.003732, loss_dice: 0.134468
[10:44:57.950] TRAIN: iteration 18375 : loss : 0.137405, loss_ce: 0.009432, loss_dice: 0.265379
[10:44:58.158] TRAIN: iteration 18376 : loss : 0.117049, loss_ce: 0.001774, loss_dice: 0.232325
[10:44:58.365] TRAIN: iteration 18377 : loss : 0.080447, loss_ce: 0.001377, loss_dice: 0.159517
[10:44:58.576] TRAIN: iteration 18378 : loss : 0.179369, loss_ce: 0.002361, loss_dice: 0.356377
[10:44:58.784] TRAIN: iteration 18379 : loss : 0.059501, loss_ce: 0.005394, loss_dice: 0.113608
[10:44:58.993] TRAIN: iteration 18380 : loss : 0.081408, loss_ce: 0.002942, loss_dice: 0.159874
[10:44:59.237] TRAIN: iteration 18381 : loss : 0.065294, loss_ce: 0.001558, loss_dice: 0.129031
[10:44:59.855] TRAIN: iteration 18382 : loss : 0.055673, loss_ce: 0.000947, loss_dice: 0.110399
[10:45:00.064] TRAIN: iteration 18383 : loss : 0.230256, loss_ce: 0.001124, loss_dice: 0.459388
[10:45:00.271] TRAIN: iteration 18384 : loss : 0.238287, loss_ce: 0.001943, loss_dice: 0.474630
[10:45:00.479] TRAIN: iteration 18385 : loss : 0.078920, loss_ce: 0.002417, loss_dice: 0.155424
[10:45:00.693] TRAIN: iteration 18386 : loss : 0.250998, loss_ce: 0.001868, loss_dice: 0.500128
[10:45:00.906] TRAIN: iteration 18387 : loss : 0.217058, loss_ce: 0.008389, loss_dice: 0.425726
[10:45:01.125] TRAIN: iteration 18388 : loss : 0.253947, loss_ce: 0.007321, loss_dice: 0.500573
[10:45:01.332] TRAIN: iteration 18389 : loss : 0.018873, loss_ce: 0.004031, loss_dice: 0.033714
[10:45:01.539] TRAIN: iteration 18390 : loss : 0.095612, loss_ce: 0.006261, loss_dice: 0.184963
[10:45:01.857] TRAIN: iteration 18391 : loss : 0.138199, loss_ce: 0.002947, loss_dice: 0.273450
[10:45:02.066] TRAIN: iteration 18392 : loss : 0.027829, loss_ce: 0.003163, loss_dice: 0.052494
[10:45:02.275] TRAIN: iteration 18393 : loss : 0.083038, loss_ce: 0.003761, loss_dice: 0.162315
[10:45:02.481] TRAIN: iteration 18394 : loss : 0.087058, loss_ce: 0.004772, loss_dice: 0.169345
[10:45:03.324] TRAIN: iteration 18395 : loss : 0.100094, loss_ce: 0.013007, loss_dice: 0.187182
[10:45:03.533] TRAIN: iteration 18396 : loss : 0.139074, loss_ce: 0.013487, loss_dice: 0.264661
[10:45:03.740] TRAIN: iteration 18397 : loss : 0.251056, loss_ce: 0.001967, loss_dice: 0.500145
[10:45:03.948] TRAIN: iteration 18398 : loss : 0.114910, loss_ce: 0.004658, loss_dice: 0.225162
[10:45:04.155] TRAIN: iteration 18399 : loss : 0.128352, loss_ce: 0.006980, loss_dice: 0.249725
[10:45:04.363] TRAIN: iteration 18400 : loss : 0.041937, loss_ce: 0.004858, loss_dice: 0.079015
[10:45:04.597] TRAIN: iteration 18401 : loss : 0.100207, loss_ce: 0.003823, loss_dice: 0.196591
[10:45:04.805] TRAIN: iteration 18402 : loss : 0.251500, loss_ce: 0.002789, loss_dice: 0.500211
[10:45:05.629] TRAIN: iteration 18403 : loss : 0.103867, loss_ce: 0.004012, loss_dice: 0.203722
[10:45:05.838] TRAIN: iteration 18404 : loss : 0.251101, loss_ce: 0.002459, loss_dice: 0.499743
[10:45:06.246] TRAIN: iteration 18405 : loss : 0.195963, loss_ce: 0.004132, loss_dice: 0.387793
[10:45:06.458] TRAIN: iteration 18406 : loss : 0.009271, loss_ce: 0.000968, loss_dice: 0.017575
[10:45:06.665] TRAIN: iteration 18407 : loss : 0.075509, loss_ce: 0.006132, loss_dice: 0.144885
[10:45:06.872] TRAIN: iteration 18408 : loss : 0.250673, loss_ce: 0.001767, loss_dice: 0.499578
[10:45:07.080] TRAIN: iteration 18409 : loss : 0.053772, loss_ce: 0.004062, loss_dice: 0.103483
[10:45:07.305] TRAIN: iteration 18410 : loss : 0.140233, loss_ce: 0.007715, loss_dice: 0.272751
[10:45:07.879] TRAIN: iteration 18411 : loss : 0.084105, loss_ce: 0.002202, loss_dice: 0.166008
[10:45:08.708] TRAIN: iteration 18412 : loss : 0.251435, loss_ce: 0.002682, loss_dice: 0.500189
[10:45:09.154] TRAIN: iteration 18413 : loss : 0.181970, loss_ce: 0.009386, loss_dice: 0.354553
[10:45:09.362] TRAIN: iteration 18414 : loss : 0.197776, loss_ce: 0.004031, loss_dice: 0.391522
[10:45:09.576] TRAIN: iteration 18415 : loss : 0.070854, loss_ce: 0.002722, loss_dice: 0.138985
[10:45:09.786] TRAIN: iteration 18416 : loss : 0.120345, loss_ce: 0.003636, loss_dice: 0.237054
[10:45:09.994] TRAIN: iteration 18417 : loss : 0.140409, loss_ce: 0.004977, loss_dice: 0.275840
[10:45:10.201] TRAIN: iteration 18418 : loss : 0.078807, loss_ce: 0.002445, loss_dice: 0.155168
[10:45:10.451] TRAIN: iteration 18419 : loss : 0.185737, loss_ce: 0.020882, loss_dice: 0.350592
[10:45:11.710] TRAIN: iteration 18420 : loss : 0.085445, loss_ce: 0.006595, loss_dice: 0.164295
[10:45:11.955] TRAIN: iteration 18421 : loss : 0.100385, loss_ce: 0.003992, loss_dice: 0.196778
[10:45:12.166] TRAIN: iteration 18422 : loss : 0.104369, loss_ce: 0.002883, loss_dice: 0.205854
[10:45:12.378] TRAIN: iteration 18423 : loss : 0.076774, loss_ce: 0.006419, loss_dice: 0.147128
[10:45:12.591] TRAIN: iteration 18424 : loss : 0.219797, loss_ce: 0.006744, loss_dice: 0.432850
[10:45:12.800] TRAIN: iteration 18425 : loss : 0.143597, loss_ce: 0.002214, loss_dice: 0.284980
[10:45:13.008] TRAIN: iteration 18426 : loss : 0.064998, loss_ce: 0.006088, loss_dice: 0.123908
[10:45:16.065] TRAIN: iteration 18427 : loss : 0.169173, loss_ce: 0.004621, loss_dice: 0.333726
[10:45:16.274] TRAIN: iteration 18428 : loss : 0.034626, loss_ce: 0.004358, loss_dice: 0.064894
[10:45:16.483] TRAIN: iteration 18429 : loss : 0.251209, loss_ce: 0.002269, loss_dice: 0.500150
[10:45:16.692] TRAIN: iteration 18430 : loss : 0.140622, loss_ce: 0.006984, loss_dice: 0.274260
[10:45:16.901] TRAIN: iteration 18431 : loss : 0.191971, loss_ce: 0.003042, loss_dice: 0.380899
[10:45:17.110] TRAIN: iteration 18432 : loss : 0.044960, loss_ce: 0.002780, loss_dice: 0.087140
[10:45:17.397] TRAIN: iteration 18433 : loss : 0.156005, loss_ce: 0.003451, loss_dice: 0.308559
[10:45:17.612] TRAIN: iteration 18434 : loss : 0.038951, loss_ce: 0.005383, loss_dice: 0.072519
[10:45:17.822] TRAIN: iteration 18435 : loss : 0.049433, loss_ce: 0.007467, loss_dice: 0.091400
[10:45:18.031] TRAIN: iteration 18436 : loss : 0.075916, loss_ce: 0.003402, loss_dice: 0.148430
[10:45:18.239] TRAIN: iteration 18437 : loss : 0.164912, loss_ce: 0.007272, loss_dice: 0.322551
[10:45:18.449] TRAIN: iteration 18438 : loss : 0.146204, loss_ce: 0.010839, loss_dice: 0.281570
[10:45:18.660] TRAIN: iteration 18439 : loss : 0.251093, loss_ce: 0.002066, loss_dice: 0.500121
[10:45:18.869] TRAIN: iteration 18440 : loss : 0.078186, loss_ce: 0.003780, loss_dice: 0.152593
[10:45:19.134] TRAIN: iteration 18441 : loss : 0.099858, loss_ce: 0.006469, loss_dice: 0.193246
[10:45:19.342] TRAIN: iteration 18442 : loss : 0.037255, loss_ce: 0.004665, loss_dice: 0.069844
[10:45:19.554] TRAIN: iteration 18443 : loss : 0.231242, loss_ce: 0.007297, loss_dice: 0.455188
[10:45:19.762] TRAIN: iteration 18444 : loss : 0.065297, loss_ce: 0.002327, loss_dice: 0.128267
[10:45:19.971] TRAIN: iteration 18445 : loss : 0.069956, loss_ce: 0.001682, loss_dice: 0.138231
[10:45:20.180] TRAIN: iteration 18446 : loss : 0.251726, loss_ce: 0.003232, loss_dice: 0.500220
[10:45:20.390] TRAIN: iteration 18447 : loss : 0.067739, loss_ce: 0.003959, loss_dice: 0.131519
[10:45:20.602] TRAIN: iteration 18448 : loss : 0.071935, loss_ce: 0.002179, loss_dice: 0.141692
[10:45:20.812] TRAIN: iteration 18449 : loss : 0.251081, loss_ce: 0.002065, loss_dice: 0.500098
[10:45:21.020] TRAIN: iteration 18450 : loss : 0.070307, loss_ce: 0.004195, loss_dice: 0.136418
[10:45:21.229] TRAIN: iteration 18451 : loss : 0.204178, loss_ce: 0.053288, loss_dice: 0.355067
[10:45:21.844] TRAIN: iteration 18452 : loss : 0.096588, loss_ce: 0.006046, loss_dice: 0.187130
[10:45:22.055] TRAIN: iteration 18453 : loss : 0.050326, loss_ce: 0.003402, loss_dice: 0.097251
[10:45:22.266] TRAIN: iteration 18454 : loss : 0.070104, loss_ce: 0.001183, loss_dice: 0.139026
[10:45:22.479] TRAIN: iteration 18455 : loss : 0.100336, loss_ce: 0.016155, loss_dice: 0.184518
[10:45:22.691] TRAIN: iteration 18456 : loss : 0.248823, loss_ce: 0.003171, loss_dice: 0.494475
[10:45:22.904] TRAIN: iteration 18457 : loss : 0.218451, loss_ce: 0.004585, loss_dice: 0.432317
[10:45:23.114] TRAIN: iteration 18458 : loss : 0.250110, loss_ce: 0.002823, loss_dice: 0.497398
[10:45:23.324] TRAIN: iteration 18459 : loss : 0.167451, loss_ce: 0.002587, loss_dice: 0.332314
[10:45:25.377] TRAIN: iteration 18460 : loss : 0.132228, loss_ce: 0.001618, loss_dice: 0.262838
[10:45:25.617] TRAIN: iteration 18461 : loss : 0.086472, loss_ce: 0.002991, loss_dice: 0.169954
[10:45:25.826] TRAIN: iteration 18462 : loss : 0.251818, loss_ce: 0.004803, loss_dice: 0.498832
[10:45:26.037] TRAIN: iteration 18463 : loss : 0.223645, loss_ce: 0.002714, loss_dice: 0.444576
[10:45:26.246] TRAIN: iteration 18464 : loss : 0.058854, loss_ce: 0.001859, loss_dice: 0.115848
[10:45:26.454] TRAIN: iteration 18465 : loss : 0.194893, loss_ce: 0.002128, loss_dice: 0.387657
[10:45:26.663] TRAIN: iteration 18466 : loss : 0.249351, loss_ce: 0.002382, loss_dice: 0.496320
[10:45:26.934] TRAIN: iteration 18467 : loss : 0.139139, loss_ce: 0.001973, loss_dice: 0.276305
[10:45:27.145] TRAIN: iteration 18468 : loss : 0.037871, loss_ce: 0.003835, loss_dice: 0.071907
[10:45:27.353] TRAIN: iteration 18469 : loss : 0.122552, loss_ce: 0.002156, loss_dice: 0.242949
[10:45:27.562] TRAIN: iteration 18470 : loss : 0.166581, loss_ce: 0.015544, loss_dice: 0.317618
[10:45:28.427] TRAIN: iteration 18471 : loss : 0.109846, loss_ce: 0.004499, loss_dice: 0.215194
[10:45:28.636] TRAIN: iteration 18472 : loss : 0.044251, loss_ce: 0.002158, loss_dice: 0.086344
[10:45:28.844] TRAIN: iteration 18473 : loss : 0.233916, loss_ce: 0.002250, loss_dice: 0.465582
[10:45:29.056] TRAIN: iteration 18474 : loss : 0.251462, loss_ce: 0.002748, loss_dice: 0.500176
[10:45:29.265] TRAIN: iteration 18475 : loss : 0.123228, loss_ce: 0.001707, loss_dice: 0.244748
[10:45:29.474] TRAIN: iteration 18476 : loss : 0.250488, loss_ce: 0.000959, loss_dice: 0.500017
[10:45:29.682] TRAIN: iteration 18477 : loss : 0.025806, loss_ce: 0.003148, loss_dice: 0.048464
[10:45:29.898] TRAIN: iteration 18478 : loss : 0.130898, loss_ce: 0.004057, loss_dice: 0.257739
[10:45:30.710] TRAIN: iteration 18479 : loss : 0.109550, loss_ce: 0.006851, loss_dice: 0.212249
[10:45:30.918] TRAIN: iteration 18480 : loss : 0.251400, loss_ce: 0.004625, loss_dice: 0.498175
[10:45:31.157] TRAIN: iteration 18481 : loss : 0.090151, loss_ce: 0.017161, loss_dice: 0.163140
[10:45:31.366] TRAIN: iteration 18482 : loss : 0.076166, loss_ce: 0.001494, loss_dice: 0.150838
[10:45:31.578] TRAIN: iteration 18483 : loss : 0.121941, loss_ce: 0.003370, loss_dice: 0.240512
[10:45:31.809] TRAIN: iteration 18484 : loss : 0.125135, loss_ce: 0.005541, loss_dice: 0.244730
[10:45:32.020] TRAIN: iteration 18485 : loss : 0.236424, loss_ce: 0.002432, loss_dice: 0.470416
[10:45:32.235] TRAIN: iteration 18486 : loss : 0.052630, loss_ce: 0.007258, loss_dice: 0.098003
[10:45:32.442] TRAIN: iteration 18487 : loss : 0.152429, loss_ce: 0.002351, loss_dice: 0.302506
[10:45:32.649] TRAIN: iteration 18488 : loss : 0.116319, loss_ce: 0.004042, loss_dice: 0.228596
[10:45:32.858] TRAIN: iteration 18489 : loss : 0.062031, loss_ce: 0.004607, loss_dice: 0.119455
[10:45:33.066] TRAIN: iteration 18490 : loss : 0.131903, loss_ce: 0.002566, loss_dice: 0.261240
[10:45:33.276] TRAIN: iteration 18491 : loss : 0.025476, loss_ce: 0.001568, loss_dice: 0.049383
[10:45:34.031] TRAIN: iteration 18492 : loss : 0.198246, loss_ce: 0.006102, loss_dice: 0.390389
[10:45:34.902] TRAIN: iteration 18493 : loss : 0.035422, loss_ce: 0.006464, loss_dice: 0.064381
[10:45:35.113] TRAIN: iteration 18494 : loss : 0.113334, loss_ce: 0.006595, loss_dice: 0.220073
[10:45:35.321] TRAIN: iteration 18495 : loss : 0.084697, loss_ce: 0.013247, loss_dice: 0.156146
[10:45:35.528] TRAIN: iteration 18496 : loss : 0.204346, loss_ce: 0.002658, loss_dice: 0.406034
[10:45:36.899] TRAIN: iteration 18497 : loss : 0.077618, loss_ce: 0.002716, loss_dice: 0.152520
[10:45:37.108] TRAIN: iteration 18498 : loss : 0.079405, loss_ce: 0.004251, loss_dice: 0.154559
[10:45:37.322] TRAIN: iteration 18499 : loss : 0.241563, loss_ce: 0.002634, loss_dice: 0.480492
[10:45:38.783] TRAIN: iteration 18500 : loss : 0.243012, loss_ce: 0.003965, loss_dice: 0.482059
[10:45:39.016] TRAIN: iteration 18501 : loss : 0.081843, loss_ce: 0.001631, loss_dice: 0.162054
[10:45:39.224] TRAIN: iteration 18502 : loss : 0.130101, loss_ce: 0.006883, loss_dice: 0.253320
[10:45:39.438] TRAIN: iteration 18503 : loss : 0.248964, loss_ce: 0.003460, loss_dice: 0.494468
[10:45:39.646] TRAIN: iteration 18504 : loss : 0.246618, loss_ce: 0.002367, loss_dice: 0.490869
[10:45:40.125] TRAIN: iteration 18505 : loss : 0.052306, loss_ce: 0.004693, loss_dice: 0.099920
[10:45:40.332] TRAIN: iteration 18506 : loss : 0.250394, loss_ce: 0.000766, loss_dice: 0.500023
[10:45:40.547] TRAIN: iteration 18507 : loss : 0.099549, loss_ce: 0.004035, loss_dice: 0.195063
[10:45:40.814] TRAIN: iteration 18508 : loss : 0.186711, loss_ce: 0.001421, loss_dice: 0.372002
[10:45:41.021] TRAIN: iteration 18509 : loss : 0.250653, loss_ce: 0.001269, loss_dice: 0.500036
[10:45:41.229] TRAIN: iteration 18510 : loss : 0.078207, loss_ce: 0.007666, loss_dice: 0.148748
[10:45:41.438] TRAIN: iteration 18511 : loss : 0.029880, loss_ce: 0.001970, loss_dice: 0.057790
[10:45:41.650] TRAIN: iteration 18512 : loss : 0.152632, loss_ce: 0.004544, loss_dice: 0.300720
[10:45:41.917] TRAIN: iteration 18513 : loss : 0.091533, loss_ce: 0.004696, loss_dice: 0.178370
[10:45:42.127] TRAIN: iteration 18514 : loss : 0.089442, loss_ce: 0.006476, loss_dice: 0.172408
[10:45:42.334] TRAIN: iteration 18515 : loss : 0.080954, loss_ce: 0.008901, loss_dice: 0.153006
[10:45:42.636] TRAIN: iteration 18516 : loss : 0.116305, loss_ce: 0.007395, loss_dice: 0.225216
[10:45:44.475] TRAIN: iteration 18517 : loss : 0.221691, loss_ce: 0.003042, loss_dice: 0.440341
[10:45:44.682] TRAIN: iteration 18518 : loss : 0.086257, loss_ce: 0.002659, loss_dice: 0.169855
[10:45:44.892] TRAIN: iteration 18519 : loss : 0.075989, loss_ce: 0.008876, loss_dice: 0.143102
[10:45:45.100] TRAIN: iteration 18520 : loss : 0.238785, loss_ce: 0.005832, loss_dice: 0.471738
[10:45:45.332] TRAIN: iteration 18521 : loss : 0.053992, loss_ce: 0.005456, loss_dice: 0.102528
[10:45:45.541] TRAIN: iteration 18522 : loss : 0.085039, loss_ce: 0.005596, loss_dice: 0.164481
[10:45:45.753] TRAIN: iteration 18523 : loss : 0.158136, loss_ce: 0.007349, loss_dice: 0.308922
[10:45:45.961] TRAIN: iteration 18524 : loss : 0.252744, loss_ce: 0.005128, loss_dice: 0.500360
[10:45:46.360] TRAIN: iteration 18525 : loss : 0.052620, loss_ce: 0.010560, loss_dice: 0.094680
[10:45:46.568] TRAIN: iteration 18526 : loss : 0.145939, loss_ce: 0.003995, loss_dice: 0.287883
[10:45:46.859] TRAIN: iteration 18527 : loss : 0.079324, loss_ce: 0.019050, loss_dice: 0.139599
[10:45:47.069] TRAIN: iteration 18528 : loss : 0.228538, loss_ce: 0.003313, loss_dice: 0.453764
[10:45:47.370] TRAIN: iteration 18529 : loss : 0.251489, loss_ce: 0.002794, loss_dice: 0.500185
[10:45:47.578] TRAIN: iteration 18530 : loss : 0.057124, loss_ce: 0.003203, loss_dice: 0.111044
[10:45:47.789] TRAIN: iteration 18531 : loss : 0.095497, loss_ce: 0.003870, loss_dice: 0.187124
[10:45:49.981] TRAIN: iteration 18532 : loss : 0.252855, loss_ce: 0.005368, loss_dice: 0.500341
[10:45:50.190] TRAIN: iteration 18533 : loss : 0.071335, loss_ce: 0.005892, loss_dice: 0.136778
[10:45:50.400] TRAIN: iteration 18534 : loss : 0.066331, loss_ce: 0.009562, loss_dice: 0.123101
[10:45:50.610] TRAIN: iteration 18535 : loss : 0.248929, loss_ce: 0.002381, loss_dice: 0.495476
[10:45:50.818] TRAIN: iteration 18536 : loss : 0.042807, loss_ce: 0.001424, loss_dice: 0.084191
[10:45:51.028] TRAIN: iteration 18537 : loss : 0.252444, loss_ce: 0.004667, loss_dice: 0.500220
[10:45:51.419] TRAIN: iteration 18538 : loss : 0.252386, loss_ce: 0.004974, loss_dice: 0.499798
[10:45:51.634] TRAIN: iteration 18539 : loss : 0.172037, loss_ce: 0.006807, loss_dice: 0.337267
[10:45:51.842] TRAIN: iteration 18540 : loss : 0.244725, loss_ce: 0.001412, loss_dice: 0.488038
[10:45:52.080] TRAIN: iteration 18541 : loss : 0.081218, loss_ce: 0.008781, loss_dice: 0.153655
[10:45:52.288] TRAIN: iteration 18542 : loss : 0.138658, loss_ce: 0.010213, loss_dice: 0.267103
[10:45:52.964] TRAIN: iteration 18543 : loss : 0.060134, loss_ce: 0.002001, loss_dice: 0.118268
[10:45:53.178] TRAIN: iteration 18544 : loss : 0.078487, loss_ce: 0.001879, loss_dice: 0.155096
[10:45:53.385] TRAIN: iteration 18545 : loss : 0.025358, loss_ce: 0.001393, loss_dice: 0.049323
[10:45:53.869] TRAIN: iteration 18546 : loss : 0.097950, loss_ce: 0.004797, loss_dice: 0.191104
[10:45:54.080] TRAIN: iteration 18547 : loss : 0.139577, loss_ce: 0.006722, loss_dice: 0.272431
[10:45:54.767] TRAIN: iteration 18548 : loss : 0.145774, loss_ce: 0.009515, loss_dice: 0.282033
[10:45:54.975] TRAIN: iteration 18549 : loss : 0.084741, loss_ce: 0.002404, loss_dice: 0.167077
[10:45:55.184] TRAIN: iteration 18550 : loss : 0.096732, loss_ce: 0.004928, loss_dice: 0.188535
[10:45:57.517] TRAIN: iteration 18551 : loss : 0.050690, loss_ce: 0.002886, loss_dice: 0.098495
[10:45:57.750] TRAIN: iteration 18552 : loss : 0.251406, loss_ce: 0.002678, loss_dice: 0.500135
[10:45:57.957] TRAIN: iteration 18553 : loss : 0.200511, loss_ce: 0.003697, loss_dice: 0.397325
[10:45:58.165] TRAIN: iteration 18554 : loss : 0.077630, loss_ce: 0.004361, loss_dice: 0.150900
[10:45:58.377] TRAIN: iteration 18555 : loss : 0.050006, loss_ce: 0.004511, loss_dice: 0.095501
[10:45:58.585] TRAIN: iteration 18556 : loss : 0.092139, loss_ce: 0.007526, loss_dice: 0.176752
[10:45:58.793] TRAIN: iteration 18557 : loss : 0.110483, loss_ce: 0.002080, loss_dice: 0.218885
[10:45:59.000] TRAIN: iteration 18558 : loss : 0.175495, loss_ce: 0.009039, loss_dice: 0.341952
[10:45:59.208] TRAIN: iteration 18559 : loss : 0.102611, loss_ce: 0.002684, loss_dice: 0.202539
[10:45:59.416] TRAIN: iteration 18560 : loss : 0.223680, loss_ce: 0.009458, loss_dice: 0.437902
[10:45:59.663] TRAIN: iteration 18561 : loss : 0.213274, loss_ce: 0.001781, loss_dice: 0.424767
[10:45:59.874] TRAIN: iteration 18562 : loss : 0.060986, loss_ce: 0.001995, loss_dice: 0.119978
[10:46:00.082] TRAIN: iteration 18563 : loss : 0.186143, loss_ce: 0.003956, loss_dice: 0.368331
[10:46:01.662] TRAIN: iteration 18564 : loss : 0.020646, loss_ce: 0.001990, loss_dice: 0.039301
[10:46:01.870] TRAIN: iteration 18565 : loss : 0.250142, loss_ce: 0.002104, loss_dice: 0.498179
[10:46:02.079] TRAIN: iteration 18566 : loss : 0.064083, loss_ce: 0.001641, loss_dice: 0.126524
[10:46:02.290] TRAIN: iteration 18567 : loss : 0.090665, loss_ce: 0.006102, loss_dice: 0.175228
[10:46:02.498] TRAIN: iteration 18568 : loss : 0.141567, loss_ce: 0.002442, loss_dice: 0.280692
[10:46:02.818] TRAIN: iteration 18569 : loss : 0.058596, loss_ce: 0.002376, loss_dice: 0.114817
[10:46:03.027] TRAIN: iteration 18570 : loss : 0.084342, loss_ce: 0.002688, loss_dice: 0.165996
[10:46:03.237] TRAIN: iteration 18571 : loss : 0.251643, loss_ce: 0.003364, loss_dice: 0.499923
[10:46:05.031] TRAIN: iteration 18572 : loss : 0.154492, loss_ce: 0.002784, loss_dice: 0.306199
[10:46:05.244] TRAIN: iteration 18573 : loss : 0.082081, loss_ce: 0.002792, loss_dice: 0.161370
[10:46:05.453] TRAIN: iteration 18574 : loss : 0.064197, loss_ce: 0.005892, loss_dice: 0.122502
[10:46:05.660] TRAIN: iteration 18575 : loss : 0.064968, loss_ce: 0.001998, loss_dice: 0.127938
[10:46:05.870] TRAIN: iteration 18576 : loss : 0.038534, loss_ce: 0.002351, loss_dice: 0.074718
[10:46:06.077] TRAIN: iteration 18577 : loss : 0.170889, loss_ce: 0.003477, loss_dice: 0.338300
[10:46:06.284] TRAIN: iteration 18578 : loss : 0.251080, loss_ce: 0.002047, loss_dice: 0.500114
[10:46:06.493] TRAIN: iteration 18579 : loss : 0.123918, loss_ce: 0.011371, loss_dice: 0.236466
[10:46:06.707] TRAIN: iteration 18580 : loss : 0.055418, loss_ce: 0.005162, loss_dice: 0.105673
[10:46:06.949] TRAIN: iteration 18581 : loss : 0.095900, loss_ce: 0.002573, loss_dice: 0.189227
[10:46:07.159] TRAIN: iteration 18582 : loss : 0.190560, loss_ce: 0.004926, loss_dice: 0.376195
[10:46:07.674] TRAIN: iteration 18583 : loss : 0.095717, loss_ce: 0.010983, loss_dice: 0.180451
[10:46:07.882] TRAIN: iteration 18584 : loss : 0.047390, loss_ce: 0.005019, loss_dice: 0.089761
[10:46:08.090] TRAIN: iteration 18585 : loss : 0.214088, loss_ce: 0.012503, loss_dice: 0.415673
[10:46:08.302] TRAIN: iteration 18586 : loss : 0.132138, loss_ce: 0.004498, loss_dice: 0.259779
[10:46:08.509] TRAIN: iteration 18587 : loss : 0.053890, loss_ce: 0.004292, loss_dice: 0.103487
[10:46:11.234] TRAIN: iteration 18588 : loss : 0.130847, loss_ce: 0.015724, loss_dice: 0.245969
[10:46:11.442] TRAIN: iteration 18589 : loss : 0.058404, loss_ce: 0.002684, loss_dice: 0.114123
[10:46:11.651] TRAIN: iteration 18590 : loss : 0.102372, loss_ce: 0.005277, loss_dice: 0.199468
[10:46:11.860] TRAIN: iteration 18591 : loss : 0.160067, loss_ce: 0.013929, loss_dice: 0.306204
[10:46:12.067] TRAIN: iteration 18592 : loss : 0.051689, loss_ce: 0.002164, loss_dice: 0.101214
[10:46:12.277] TRAIN: iteration 18593 : loss : 0.094452, loss_ce: 0.004422, loss_dice: 0.184483
[10:46:12.485] TRAIN: iteration 18594 : loss : 0.077115, loss_ce: 0.003580, loss_dice: 0.150650
[10:46:12.693] TRAIN: iteration 18595 : loss : 0.093058, loss_ce: 0.004182, loss_dice: 0.181934
[10:46:14.270] TRAIN: iteration 18596 : loss : 0.049251, loss_ce: 0.001914, loss_dice: 0.096587
[10:46:14.477] TRAIN: iteration 18597 : loss : 0.119392, loss_ce: 0.003892, loss_dice: 0.234892
[10:46:14.685] TRAIN: iteration 18598 : loss : 0.251513, loss_ce: 0.002842, loss_dice: 0.500183
[10:46:14.893] TRAIN: iteration 18599 : loss : 0.173327, loss_ce: 0.002365, loss_dice: 0.344290
[10:46:15.102] TRAIN: iteration 18600 : loss : 0.131342, loss_ce: 0.021105, loss_dice: 0.241579
[10:46:15.354] TRAIN: iteration 18601 : loss : 0.251546, loss_ce: 0.002899, loss_dice: 0.500194
[10:46:15.569] TRAIN: iteration 18602 : loss : 0.055938, loss_ce: 0.001605, loss_dice: 0.110271
[10:46:15.784] TRAIN: iteration 18603 : loss : 0.120937, loss_ce: 0.005023, loss_dice: 0.236852
[10:46:17.514] TRAIN: iteration 18604 : loss : 0.198533, loss_ce: 0.003278, loss_dice: 0.393788
[10:46:17.724] TRAIN: iteration 18605 : loss : 0.250409, loss_ce: 0.000800, loss_dice: 0.500017
[10:46:17.932] TRAIN: iteration 18606 : loss : 0.056799, loss_ce: 0.001200, loss_dice: 0.112397
[10:46:18.147] TRAIN: iteration 18607 : loss : 0.195393, loss_ce: 0.014560, loss_dice: 0.376227
[10:46:18.358] TRAIN: iteration 18608 : loss : 0.052094, loss_ce: 0.002113, loss_dice: 0.102076
[10:46:18.566] TRAIN: iteration 18609 : loss : 0.230917, loss_ce: 0.003546, loss_dice: 0.458288
[10:46:18.775] TRAIN: iteration 18610 : loss : 0.162650, loss_ce: 0.002844, loss_dice: 0.322455
[10:46:19.081] TRAIN: iteration 18611 : loss : 0.106139, loss_ce: 0.009958, loss_dice: 0.202321
[10:46:19.289] TRAIN: iteration 18612 : loss : 0.076042, loss_ce: 0.007262, loss_dice: 0.144823
[10:46:19.498] TRAIN: iteration 18613 : loss : 0.193203, loss_ce: 0.004771, loss_dice: 0.381636
[10:46:21.877] TRAIN: iteration 18614 : loss : 0.250962, loss_ce: 0.001815, loss_dice: 0.500108
[10:46:22.085] TRAIN: iteration 18615 : loss : 0.028294, loss_ce: 0.000696, loss_dice: 0.055892
[10:46:22.296] TRAIN: iteration 18616 : loss : 0.185457, loss_ce: 0.005028, loss_dice: 0.365886
[10:46:22.507] TRAIN: iteration 18617 : loss : 0.066885, loss_ce: 0.012061, loss_dice: 0.121710
[10:46:22.714] TRAIN: iteration 18618 : loss : 0.250638, loss_ce: 0.001235, loss_dice: 0.500041
[10:46:22.924] TRAIN: iteration 18619 : loss : 0.251753, loss_ce: 0.003283, loss_dice: 0.500224
[10:46:23.134] TRAIN: iteration 18620 : loss : 0.129492, loss_ce: 0.022260, loss_dice: 0.236723
[10:46:23.368] TRAIN: iteration 18621 : loss : 0.241078, loss_ce: 0.003172, loss_dice: 0.478984
[10:46:24.612] TRAIN: iteration 18622 : loss : 0.149220, loss_ce: 0.016015, loss_dice: 0.282426
[10:46:25.472] TRAIN: iteration 18623 : loss : 0.045087, loss_ce: 0.001637, loss_dice: 0.088537
[10:46:25.680] TRAIN: iteration 18624 : loss : 0.252037, loss_ce: 0.003822, loss_dice: 0.500251
[10:46:25.888] TRAIN: iteration 18625 : loss : 0.138620, loss_ce: 0.003120, loss_dice: 0.274120
[10:46:26.104] TRAIN: iteration 18626 : loss : 0.082969, loss_ce: 0.004428, loss_dice: 0.161510
[10:46:26.315] TRAIN: iteration 18627 : loss : 0.114891, loss_ce: 0.003179, loss_dice: 0.226603
[10:46:26.524] TRAIN: iteration 18628 : loss : 0.111203, loss_ce: 0.002883, loss_dice: 0.219523
[10:46:26.733] TRAIN: iteration 18629 : loss : 0.074157, loss_ce: 0.002274, loss_dice: 0.146040
[10:46:29.626] TRAIN: iteration 18630 : loss : 0.248210, loss_ce: 0.007363, loss_dice: 0.489056
[10:46:30.180] TRAIN: iteration 18631 : loss : 0.204923, loss_ce: 0.006988, loss_dice: 0.402858
[10:46:30.391] TRAIN: iteration 18632 : loss : 0.035781, loss_ce: 0.005367, loss_dice: 0.066194
[10:46:30.599] TRAIN: iteration 18633 : loss : 0.033122, loss_ce: 0.001415, loss_dice: 0.064829
[10:46:30.809] TRAIN: iteration 18634 : loss : 0.177870, loss_ce: 0.006191, loss_dice: 0.349550
[10:46:31.016] TRAIN: iteration 18635 : loss : 0.251563, loss_ce: 0.002944, loss_dice: 0.500183
[10:46:31.230] TRAIN: iteration 18636 : loss : 0.143374, loss_ce: 0.005181, loss_dice: 0.281566
[10:46:31.438] TRAIN: iteration 18637 : loss : 0.030298, loss_ce: 0.002367, loss_dice: 0.058229
[10:46:32.031] TRAIN: iteration 18638 : loss : 0.123402, loss_ce: 0.002499, loss_dice: 0.244305
[10:46:33.942] TRAIN: iteration 18639 : loss : 0.044044, loss_ce: 0.004539, loss_dice: 0.083548
[10:46:34.151] TRAIN: iteration 18640 : loss : 0.185177, loss_ce: 0.008235, loss_dice: 0.362120
[10:46:34.382] TRAIN: iteration 18641 : loss : 0.183029, loss_ce: 0.003459, loss_dice: 0.362599
[10:46:34.597] TRAIN: iteration 18642 : loss : 0.088722, loss_ce: 0.005173, loss_dice: 0.172271
[10:46:34.806] TRAIN: iteration 18643 : loss : 0.098968, loss_ce: 0.004012, loss_dice: 0.193925
[10:46:35.014] TRAIN: iteration 18644 : loss : 0.093871, loss_ce: 0.005254, loss_dice: 0.182487
[10:46:35.222] TRAIN: iteration 18645 : loss : 0.100601, loss_ce: 0.001552, loss_dice: 0.199649
[10:46:36.047] TRAIN: iteration 18646 : loss : 0.201094, loss_ce: 0.002341, loss_dice: 0.399847
[10:46:36.260] TRAIN: iteration 18647 : loss : 0.141328, loss_ce: 0.003064, loss_dice: 0.279592
[10:46:36.469] TRAIN: iteration 18648 : loss : 0.250695, loss_ce: 0.001338, loss_dice: 0.500053
[10:46:36.680] TRAIN: iteration 18649 : loss : 0.158771, loss_ce: 0.006372, loss_dice: 0.311169
[10:46:36.889] TRAIN: iteration 18650 : loss : 0.072651, loss_ce: 0.003800, loss_dice: 0.141503
[10:46:37.097] TRAIN: iteration 18651 : loss : 0.160408, loss_ce: 0.013071, loss_dice: 0.307745
[10:46:37.304] TRAIN: iteration 18652 : loss : 0.059175, loss_ce: 0.003817, loss_dice: 0.114533
[10:46:37.607] TRAIN: iteration 18653 : loss : 0.086446, loss_ce: 0.010418, loss_dice: 0.162474
[10:46:38.536] TRAIN: iteration 18654 : loss : 0.251619, loss_ce: 0.003030, loss_dice: 0.500208
[10:46:39.475] TRAIN: iteration 18655 : loss : 0.035751, loss_ce: 0.002944, loss_dice: 0.068558
[10:46:39.682] TRAIN: iteration 18656 : loss : 0.149052, loss_ce: 0.003426, loss_dice: 0.294679
[10:46:39.890] TRAIN: iteration 18657 : loss : 0.250938, loss_ce: 0.001773, loss_dice: 0.500103
[10:46:40.104] TRAIN: iteration 18658 : loss : 0.248915, loss_ce: 0.002773, loss_dice: 0.495057
[10:46:40.312] TRAIN: iteration 18659 : loss : 0.058500, loss_ce: 0.003057, loss_dice: 0.113944
[10:46:40.519] TRAIN: iteration 18660 : loss : 0.044142, loss_ce: 0.009713, loss_dice: 0.078571
[10:46:41.726] TRAIN: iteration 18661 : loss : 0.203224, loss_ce: 0.005658, loss_dice: 0.400790
[10:46:41.934] TRAIN: iteration 18662 : loss : 0.251583, loss_ce: 0.002970, loss_dice: 0.500197
[10:46:42.447] TRAIN: iteration 18663 : loss : 0.154398, loss_ce: 0.008002, loss_dice: 0.300794
[10:46:42.654] TRAIN: iteration 18664 : loss : 0.073717, loss_ce: 0.003334, loss_dice: 0.144101
[10:46:42.861] TRAIN: iteration 18665 : loss : 0.060190, loss_ce: 0.005657, loss_dice: 0.114723
[10:46:43.071] TRAIN: iteration 18666 : loss : 0.119775, loss_ce: 0.006736, loss_dice: 0.232813
[10:46:43.279] TRAIN: iteration 18667 : loss : 0.100339, loss_ce: 0.005045, loss_dice: 0.195633
[10:46:44.920] TRAIN: iteration 18668 : loss : 0.051082, loss_ce: 0.001539, loss_dice: 0.100625
[10:46:45.128] TRAIN: iteration 18669 : loss : 0.034526, loss_ce: 0.001546, loss_dice: 0.067506
[10:46:45.337] TRAIN: iteration 18670 : loss : 0.149175, loss_ce: 0.006351, loss_dice: 0.291998
[10:46:45.630] TRAIN: iteration 18671 : loss : 0.031678, loss_ce: 0.004280, loss_dice: 0.059076
[10:46:45.839] TRAIN: iteration 18672 : loss : 0.146115, loss_ce: 0.006558, loss_dice: 0.285671
[10:46:46.047] TRAIN: iteration 18673 : loss : 0.103915, loss_ce: 0.005598, loss_dice: 0.202231
[10:46:46.257] TRAIN: iteration 18674 : loss : 0.226777, loss_ce: 0.007955, loss_dice: 0.445600
[10:46:46.466] TRAIN: iteration 18675 : loss : 0.234545, loss_ce: 0.002930, loss_dice: 0.466160
[10:46:49.246] TRAIN: iteration 18676 : loss : 0.114914, loss_ce: 0.002296, loss_dice: 0.227533
[10:46:49.456] TRAIN: iteration 18677 : loss : 0.089688, loss_ce: 0.011500, loss_dice: 0.167876
[10:46:49.665] TRAIN: iteration 18678 : loss : 0.047134, loss_ce: 0.001536, loss_dice: 0.092732
[10:46:49.878] TRAIN: iteration 18679 : loss : 0.250715, loss_ce: 0.008990, loss_dice: 0.492439
[10:46:50.090] TRAIN: iteration 18680 : loss : 0.137501, loss_ce: 0.005309, loss_dice: 0.269693
[10:46:50.091] NaN or Inf found in input tensor.
[10:46:50.308] TRAIN: iteration 18681 : loss : 0.186610, loss_ce: 0.012358, loss_dice: 0.360862
[10:46:50.521] TRAIN: iteration 18682 : loss : 0.205531, loss_ce: 0.003078, loss_dice: 0.407983
[10:46:50.733] TRAIN: iteration 18683 : loss : 0.032299, loss_ce: 0.005049, loss_dice: 0.059549
[10:46:52.683] TRAIN: iteration 18684 : loss : 0.171921, loss_ce: 0.011171, loss_dice: 0.332671
[10:46:52.891] TRAIN: iteration 18685 : loss : 0.114233, loss_ce: 0.005649, loss_dice: 0.222817
[10:46:53.099] TRAIN: iteration 18686 : loss : 0.126750, loss_ce: 0.004900, loss_dice: 0.248599
[10:46:53.307] TRAIN: iteration 18687 : loss : 0.184763, loss_ce: 0.006745, loss_dice: 0.362781
[10:46:53.517] TRAIN: iteration 18688 : loss : 0.105936, loss_ce: 0.003180, loss_dice: 0.208691
[10:46:53.727] TRAIN: iteration 18689 : loss : 0.153782, loss_ce: 0.007391, loss_dice: 0.300173
[10:46:53.937] TRAIN: iteration 18690 : loss : 0.056859, loss_ce: 0.002344, loss_dice: 0.111373
[10:46:54.146] TRAIN: iteration 18691 : loss : 0.252118, loss_ce: 0.004486, loss_dice: 0.499750
[10:46:57.122] TRAIN: iteration 18692 : loss : 0.083797, loss_ce: 0.007264, loss_dice: 0.160331
[10:46:57.336] TRAIN: iteration 18693 : loss : 0.104670, loss_ce: 0.002874, loss_dice: 0.206466
[10:46:57.543] TRAIN: iteration 18694 : loss : 0.075772, loss_ce: 0.006047, loss_dice: 0.145498
[10:46:57.751] TRAIN: iteration 18695 : loss : 0.124735, loss_ce: 0.008119, loss_dice: 0.241352
[10:46:57.958] TRAIN: iteration 18696 : loss : 0.242564, loss_ce: 0.002558, loss_dice: 0.482571
[10:46:58.169] TRAIN: iteration 18697 : loss : 0.203817, loss_ce: 0.003921, loss_dice: 0.403712
[10:46:58.376] TRAIN: iteration 18698 : loss : 0.168706, loss_ce: 0.002464, loss_dice: 0.334949
[10:46:58.583] TRAIN: iteration 18699 : loss : 0.250720, loss_ce: 0.001408, loss_dice: 0.500033
[10:47:00.600] TRAIN: iteration 18700 : loss : 0.132659, loss_ce: 0.005898, loss_dice: 0.259420
[10:47:00.838] TRAIN: iteration 18701 : loss : 0.251019, loss_ce: 0.001973, loss_dice: 0.500065
[10:47:01.047] TRAIN: iteration 18702 : loss : 0.233128, loss_ce: 0.004478, loss_dice: 0.461778
[10:47:01.254] TRAIN: iteration 18703 : loss : 0.049270, loss_ce: 0.002116, loss_dice: 0.096425
[10:47:01.462] TRAIN: iteration 18704 : loss : 0.251471, loss_ce: 0.002915, loss_dice: 0.500026
[10:47:01.671] TRAIN: iteration 18705 : loss : 0.253368, loss_ce: 0.006293, loss_dice: 0.500443
[10:47:01.882] TRAIN: iteration 18706 : loss : 0.117408, loss_ce: 0.002918, loss_dice: 0.231897
[10:47:02.091] TRAIN: iteration 18707 : loss : 0.163768, loss_ce: 0.002240, loss_dice: 0.325295
[10:47:04.961] TRAIN: iteration 18708 : loss : 0.251433, loss_ce: 0.002708, loss_dice: 0.500157
[10:47:05.175] TRAIN: iteration 18709 : loss : 0.110063, loss_ce: 0.010653, loss_dice: 0.209474
[10:47:05.382] TRAIN: iteration 18710 : loss : 0.119978, loss_ce: 0.002994, loss_dice: 0.236961
[10:47:05.589] TRAIN: iteration 18711 : loss : 0.251112, loss_ce: 0.002106, loss_dice: 0.500118
[10:47:05.798] TRAIN: iteration 18712 : loss : 0.241331, loss_ce: 0.002954, loss_dice: 0.479708
[10:47:06.005] TRAIN: iteration 18713 : loss : 0.121406, loss_ce: 0.002842, loss_dice: 0.239971
[10:47:06.214] TRAIN: iteration 18714 : loss : 0.146153, loss_ce: 0.001964, loss_dice: 0.290341
[10:47:06.422] TRAIN: iteration 18715 : loss : 0.092412, loss_ce: 0.002226, loss_dice: 0.182598
[10:47:08.070] TRAIN: iteration 18716 : loss : 0.202641, loss_ce: 0.004437, loss_dice: 0.400846
[10:47:08.277] TRAIN: iteration 18717 : loss : 0.162557, loss_ce: 0.007089, loss_dice: 0.318025
[10:47:08.485] TRAIN: iteration 18718 : loss : 0.250862, loss_ce: 0.001646, loss_dice: 0.500078
[10:47:08.694] TRAIN: iteration 18719 : loss : 0.148356, loss_ce: 0.002197, loss_dice: 0.294516
[10:47:08.903] TRAIN: iteration 18720 : loss : 0.251870, loss_ce: 0.003588, loss_dice: 0.500151
[10:47:09.633] TRAIN: iteration 18721 : loss : 0.219490, loss_ce: 0.003971, loss_dice: 0.435008
[10:47:09.840] TRAIN: iteration 18722 : loss : 0.076274, loss_ce: 0.003430, loss_dice: 0.149119
[10:47:10.048] TRAIN: iteration 18723 : loss : 0.250592, loss_ce: 0.001161, loss_dice: 0.500023
[10:47:11.214] TRAIN: iteration 18724 : loss : 0.103930, loss_ce: 0.007708, loss_dice: 0.200152
[10:47:11.427] TRAIN: iteration 18725 : loss : 0.244601, loss_ce: 0.003186, loss_dice: 0.486015
[10:47:11.634] TRAIN: iteration 18726 : loss : 0.036330, loss_ce: 0.001900, loss_dice: 0.070759
[10:47:11.842] TRAIN: iteration 18727 : loss : 0.091863, loss_ce: 0.004181, loss_dice: 0.179545
[10:47:12.056] TRAIN: iteration 18728 : loss : 0.094301, loss_ce: 0.014032, loss_dice: 0.174571
[10:47:13.539] TRAIN: iteration 18729 : loss : 0.119803, loss_ce: 0.007708, loss_dice: 0.231898
[10:47:13.746] TRAIN: iteration 18730 : loss : 0.090845, loss_ce: 0.003518, loss_dice: 0.178173
[10:47:13.953] TRAIN: iteration 18731 : loss : 0.119309, loss_ce: 0.004013, loss_dice: 0.234605
[10:47:16.092] TRAIN: iteration 18732 : loss : 0.251634, loss_ce: 0.003083, loss_dice: 0.500186
[10:47:16.305] TRAIN: iteration 18733 : loss : 0.091314, loss_ce: 0.002961, loss_dice: 0.179667
[10:47:16.512] TRAIN: iteration 18734 : loss : 0.143020, loss_ce: 0.003438, loss_dice: 0.282603
[10:47:16.720] TRAIN: iteration 18735 : loss : 0.251277, loss_ce: 0.002407, loss_dice: 0.500147
[10:47:16.929] TRAIN: iteration 18736 : loss : 0.034401, loss_ce: 0.005501, loss_dice: 0.063302
[10:47:18.062] TRAIN: iteration 18737 : loss : 0.032744, loss_ce: 0.003224, loss_dice: 0.062264
[10:47:18.270] TRAIN: iteration 18738 : loss : 0.044663, loss_ce: 0.002643, loss_dice: 0.086683
[10:47:18.478] TRAIN: iteration 18739 : loss : 0.132141, loss_ce: 0.002083, loss_dice: 0.262198
[10:47:21.207] TRAIN: iteration 18740 : loss : 0.164919, loss_ce: 0.001956, loss_dice: 0.327881
[10:47:21.436] TRAIN: iteration 18741 : loss : 0.167924, loss_ce: 0.003675, loss_dice: 0.332172
[10:47:21.652] TRAIN: iteration 18742 : loss : 0.097186, loss_ce: 0.010248, loss_dice: 0.184124
[10:47:21.860] TRAIN: iteration 18743 : loss : 0.050752, loss_ce: 0.004220, loss_dice: 0.097284
[10:47:22.068] TRAIN: iteration 18744 : loss : 0.230762, loss_ce: 0.007782, loss_dice: 0.453743
[10:47:22.275] TRAIN: iteration 18745 : loss : 0.084837, loss_ce: 0.002520, loss_dice: 0.167153
[10:47:22.482] TRAIN: iteration 18746 : loss : 0.088356, loss_ce: 0.005050, loss_dice: 0.171662
[10:47:22.690] TRAIN: iteration 18747 : loss : 0.105536, loss_ce: 0.002632, loss_dice: 0.208440
[10:47:27.069] TRAIN: iteration 18748 : loss : 0.224185, loss_ce: 0.001027, loss_dice: 0.447343
[10:47:27.277] TRAIN: iteration 18749 : loss : 0.112519, loss_ce: 0.003444, loss_dice: 0.221594
[10:47:27.487] TRAIN: iteration 18750 : loss : 0.068354, loss_ce: 0.003074, loss_dice: 0.133634
[10:47:27.695] TRAIN: iteration 18751 : loss : 0.250971, loss_ce: 0.001829, loss_dice: 0.500114
[10:47:27.903] TRAIN: iteration 18752 : loss : 0.246601, loss_ce: 0.005631, loss_dice: 0.487572
[10:47:28.112] TRAIN: iteration 18753 : loss : 0.064015, loss_ce: 0.001183, loss_dice: 0.126847
[10:47:28.320] TRAIN: iteration 18754 : loss : 0.081246, loss_ce: 0.001259, loss_dice: 0.161234
[10:47:28.534] TRAIN: iteration 18755 : loss : 0.118498, loss_ce: 0.004186, loss_dice: 0.232811
[10:47:30.482] TRAIN: iteration 18756 : loss : 0.251598, loss_ce: 0.003018, loss_dice: 0.500179
[10:47:30.691] TRAIN: iteration 18757 : loss : 0.155343, loss_ce: 0.001285, loss_dice: 0.309400
[10:47:30.901] TRAIN: iteration 18758 : loss : 0.116663, loss_ce: 0.026990, loss_dice: 0.206337
[10:47:31.116] TRAIN: iteration 18759 : loss : 0.033383, loss_ce: 0.001613, loss_dice: 0.065154
[10:47:31.323] TRAIN: iteration 18760 : loss : 0.043409, loss_ce: 0.003406, loss_dice: 0.083413
[10:47:31.539] TRAIN: iteration 18761 : loss : 0.226500, loss_ce: 0.003585, loss_dice: 0.449415
[10:47:31.790] TRAIN: iteration 18762 : loss : 0.172217, loss_ce: 0.003625, loss_dice: 0.340809
[10:47:31.998] TRAIN: iteration 18763 : loss : 0.204202, loss_ce: 0.012653, loss_dice: 0.395751
[10:47:34.534] TRAIN: iteration 18764 : loss : 0.082855, loss_ce: 0.010961, loss_dice: 0.154749
[10:47:34.745] TRAIN: iteration 18765 : loss : 0.182772, loss_ce: 0.002169, loss_dice: 0.363375
[10:47:34.953] TRAIN: iteration 18766 : loss : 0.220594, loss_ce: 0.001175, loss_dice: 0.440013
[10:47:35.161] TRAIN: iteration 18767 : loss : 0.217177, loss_ce: 0.001889, loss_dice: 0.432464
[10:47:35.369] TRAIN: iteration 18768 : loss : 0.153439, loss_ce: 0.002689, loss_dice: 0.304190
[10:47:36.807] TRAIN: iteration 18769 : loss : 0.036097, loss_ce: 0.000876, loss_dice: 0.071317
[10:47:37.016] TRAIN: iteration 18770 : loss : 0.091006, loss_ce: 0.002675, loss_dice: 0.179337
[10:47:37.225] TRAIN: iteration 18771 : loss : 0.133754, loss_ce: 0.012908, loss_dice: 0.254600
[10:47:39.171] TRAIN: iteration 18772 : loss : 0.066464, loss_ce: 0.002323, loss_dice: 0.130606
[10:47:39.380] TRAIN: iteration 18773 : loss : 0.117322, loss_ce: 0.002389, loss_dice: 0.232255
[10:47:39.610] TRAIN: iteration 18774 : loss : 0.212202, loss_ce: 0.002909, loss_dice: 0.421495
[10:47:39.817] TRAIN: iteration 18775 : loss : 0.062080, loss_ce: 0.007105, loss_dice: 0.117055
[10:47:40.027] TRAIN: iteration 18776 : loss : 0.035532, loss_ce: 0.003893, loss_dice: 0.067170
[10:47:40.235] TRAIN: iteration 18777 : loss : 0.146617, loss_ce: 0.012285, loss_dice: 0.280949
[10:47:40.445] TRAIN: iteration 18778 : loss : 0.163495, loss_ce: 0.003414, loss_dice: 0.323576
[10:47:40.655] TRAIN: iteration 18779 : loss : 0.159142, loss_ce: 0.006933, loss_dice: 0.311351
[10:47:43.729] TRAIN: iteration 18780 : loss : 0.248809, loss_ce: 0.001682, loss_dice: 0.495937
[10:47:43.730] NaN or Inf found in input tensor.
[10:47:43.947] TRAIN: iteration 18781 : loss : 0.037539, loss_ce: 0.002502, loss_dice: 0.072576
[10:47:44.158] TRAIN: iteration 18782 : loss : 0.242274, loss_ce: 0.004721, loss_dice: 0.479828
[10:47:44.387] TRAIN: iteration 18783 : loss : 0.250563, loss_ce: 0.001104, loss_dice: 0.500021
[10:47:44.598] TRAIN: iteration 18784 : loss : 0.075042, loss_ce: 0.002631, loss_dice: 0.147452
[10:47:44.969] TRAIN: iteration 18785 : loss : 0.112417, loss_ce: 0.003670, loss_dice: 0.221163
[10:47:45.184] TRAIN: iteration 18786 : loss : 0.107222, loss_ce: 0.005291, loss_dice: 0.209154
[10:47:45.393] TRAIN: iteration 18787 : loss : 0.194390, loss_ce: 0.006028, loss_dice: 0.382751
[10:47:50.648] TRAIN: iteration 18788 : loss : 0.080108, loss_ce: 0.002252, loss_dice: 0.157964
[10:47:50.929] TRAIN: iteration 18789 : loss : 0.133928, loss_ce: 0.002276, loss_dice: 0.265581
[10:47:51.138] TRAIN: iteration 18790 : loss : 0.152959, loss_ce: 0.005489, loss_dice: 0.300429
[10:47:51.345] TRAIN: iteration 18791 : loss : 0.104133, loss_ce: 0.003241, loss_dice: 0.205024
[10:47:51.554] TRAIN: iteration 18792 : loss : 0.210457, loss_ce: 0.005505, loss_dice: 0.415409
[10:47:51.763] TRAIN: iteration 18793 : loss : 0.067504, loss_ce: 0.003340, loss_dice: 0.131668
[10:47:51.972] TRAIN: iteration 18794 : loss : 0.131998, loss_ce: 0.003169, loss_dice: 0.260828
[10:47:52.181] TRAIN: iteration 18795 : loss : 0.157510, loss_ce: 0.002877, loss_dice: 0.312142
[10:47:53.723] TRAIN: iteration 18796 : loss : 0.123980, loss_ce: 0.006366, loss_dice: 0.241595
[10:47:53.930] TRAIN: iteration 18797 : loss : 0.094307, loss_ce: 0.003520, loss_dice: 0.185093
[10:47:54.139] TRAIN: iteration 18798 : loss : 0.082211, loss_ce: 0.006057, loss_dice: 0.158364
[10:47:54.347] TRAIN: iteration 18799 : loss : 0.251500, loss_ce: 0.002843, loss_dice: 0.500157
[10:47:54.555] TRAIN: iteration 18800 : loss : 0.044787, loss_ce: 0.001924, loss_dice: 0.087651
[10:47:55.663] TRAIN: iteration 18801 : loss : 0.205682, loss_ce: 0.002558, loss_dice: 0.408807
[10:47:55.870] TRAIN: iteration 18802 : loss : 0.156098, loss_ce: 0.004630, loss_dice: 0.307566
[10:47:56.078] TRAIN: iteration 18803 : loss : 0.057401, loss_ce: 0.002651, loss_dice: 0.112151
[10:47:58.046] TRAIN: iteration 18804 : loss : 0.029212, loss_ce: 0.001263, loss_dice: 0.057161
[10:47:58.253] TRAIN: iteration 18805 : loss : 0.250647, loss_ce: 0.001262, loss_dice: 0.500031
[10:47:58.462] TRAIN: iteration 18806 : loss : 0.104779, loss_ce: 0.004533, loss_dice: 0.205025
[10:47:58.670] TRAIN: iteration 18807 : loss : 0.077719, loss_ce: 0.004013, loss_dice: 0.151425
[10:47:58.879] TRAIN: iteration 18808 : loss : 0.071417, loss_ce: 0.019017, loss_dice: 0.123816
[10:47:59.493] TRAIN: iteration 18809 : loss : 0.248693, loss_ce: 0.002866, loss_dice: 0.494521
[10:47:59.700] TRAIN: iteration 18810 : loss : 0.127589, loss_ce: 0.002470, loss_dice: 0.252707
[10:47:59.910] TRAIN: iteration 18811 : loss : 0.052023, loss_ce: 0.002076, loss_dice: 0.101970
[10:48:04.334] TRAIN: iteration 18812 : loss : 0.251369, loss_ce: 0.002574, loss_dice: 0.500165
[10:48:04.541] TRAIN: iteration 18813 : loss : 0.073376, loss_ce: 0.002975, loss_dice: 0.143777
[10:48:04.749] TRAIN: iteration 18814 : loss : 0.161345, loss_ce: 0.005620, loss_dice: 0.317070
[10:48:04.956] TRAIN: iteration 18815 : loss : 0.098933, loss_ce: 0.003789, loss_dice: 0.194077
[10:48:05.169] TRAIN: iteration 18816 : loss : 0.150870, loss_ce: 0.008269, loss_dice: 0.293471
[10:48:05.377] TRAIN: iteration 18817 : loss : 0.115037, loss_ce: 0.002408, loss_dice: 0.227667
[10:48:05.585] TRAIN: iteration 18818 : loss : 0.088695, loss_ce: 0.001071, loss_dice: 0.176319
[10:48:05.794] TRAIN: iteration 18819 : loss : 0.134423, loss_ce: 0.004155, loss_dice: 0.264691
[10:48:06.604] TRAIN: iteration 18820 : loss : 0.177030, loss_ce: 0.003112, loss_dice: 0.350949
[10:48:07.079] TRAIN: iteration 18821 : loss : 0.085118, loss_ce: 0.006946, loss_dice: 0.163290
[10:48:08.058] TRAIN: iteration 18822 : loss : 0.126838, loss_ce: 0.004116, loss_dice: 0.249560
[10:48:08.267] TRAIN: iteration 18823 : loss : 0.142920, loss_ce: 0.007541, loss_dice: 0.278298
[10:48:08.475] TRAIN: iteration 18824 : loss : 0.081349, loss_ce: 0.006727, loss_dice: 0.155971
[10:48:08.683] TRAIN: iteration 18825 : loss : 0.111311, loss_ce: 0.002337, loss_dice: 0.220284
[10:48:08.891] TRAIN: iteration 18826 : loss : 0.049871, loss_ce: 0.002124, loss_dice: 0.097617
[10:48:09.100] TRAIN: iteration 18827 : loss : 0.221177, loss_ce: 0.005793, loss_dice: 0.436562
[10:48:11.779] TRAIN: iteration 18828 : loss : 0.053101, loss_ce: 0.002754, loss_dice: 0.103448
[10:48:11.989] TRAIN: iteration 18829 : loss : 0.252034, loss_ce: 0.003768, loss_dice: 0.500300
[10:48:13.255] TRAIN: iteration 18830 : loss : 0.156916, loss_ce: 0.001094, loss_dice: 0.312738
[10:48:13.465] TRAIN: iteration 18831 : loss : 0.208881, loss_ce: 0.007108, loss_dice: 0.410655
[10:48:13.676] TRAIN: iteration 18832 : loss : 0.107087, loss_ce: 0.002325, loss_dice: 0.211848
[10:48:13.887] TRAIN: iteration 18833 : loss : 0.052514, loss_ce: 0.001838, loss_dice: 0.103189
[10:48:14.098] TRAIN: iteration 18834 : loss : 0.072350, loss_ce: 0.001936, loss_dice: 0.142763
[10:48:14.308] TRAIN: iteration 18835 : loss : 0.207696, loss_ce: 0.001942, loss_dice: 0.413449
[10:48:19.450] TRAIN: iteration 18836 : loss : 0.076456, loss_ce: 0.002500, loss_dice: 0.150413
[10:48:19.657] TRAIN: iteration 18837 : loss : 0.242932, loss_ce: 0.005625, loss_dice: 0.480240
[10:48:21.064] TRAIN: iteration 18838 : loss : 0.088990, loss_ce: 0.005279, loss_dice: 0.172702
[10:48:21.271] TRAIN: iteration 18839 : loss : 0.200657, loss_ce: 0.001122, loss_dice: 0.400192
[10:48:21.481] TRAIN: iteration 18840 : loss : 0.051004, loss_ce: 0.002299, loss_dice: 0.099709
[10:48:21.718] TRAIN: iteration 18841 : loss : 0.134501, loss_ce: 0.006479, loss_dice: 0.262522
[10:48:21.926] TRAIN: iteration 18842 : loss : 0.094024, loss_ce: 0.007329, loss_dice: 0.180720
[10:48:22.136] TRAIN: iteration 18843 : loss : 0.039830, loss_ce: 0.002562, loss_dice: 0.077097
[10:48:25.824] TRAIN: iteration 18844 : loss : 0.083106, loss_ce: 0.006610, loss_dice: 0.159602
[10:48:26.039] TRAIN: iteration 18845 : loss : 0.056164, loss_ce: 0.001432, loss_dice: 0.110895
[10:48:26.913] TRAIN: iteration 18846 : loss : 0.103039, loss_ce: 0.003820, loss_dice: 0.202259
[10:48:27.124] TRAIN: iteration 18847 : loss : 0.097157, loss_ce: 0.004364, loss_dice: 0.189950
[10:48:27.333] TRAIN: iteration 18848 : loss : 0.067899, loss_ce: 0.002638, loss_dice: 0.133161
[10:48:27.542] TRAIN: iteration 18849 : loss : 0.251801, loss_ce: 0.003372, loss_dice: 0.500230
[10:48:27.750] TRAIN: iteration 18850 : loss : 0.246023, loss_ce: 0.006128, loss_dice: 0.485919
[10:48:27.958] TRAIN: iteration 18851 : loss : 0.125650, loss_ce: 0.004342, loss_dice: 0.246959
[10:48:31.651] TRAIN: iteration 18852 : loss : 0.252230, loss_ce: 0.004153, loss_dice: 0.500307
[10:48:31.859] TRAIN: iteration 18853 : loss : 0.219966, loss_ce: 0.002416, loss_dice: 0.437517
[10:48:34.303] TRAIN: iteration 18854 : loss : 0.113983, loss_ce: 0.009431, loss_dice: 0.218536
[10:48:34.510] TRAIN: iteration 18855 : loss : 0.101577, loss_ce: 0.005409, loss_dice: 0.197745
[10:48:34.718] TRAIN: iteration 18856 : loss : 0.251331, loss_ce: 0.002513, loss_dice: 0.500149
[10:48:34.926] TRAIN: iteration 18857 : loss : 0.103068, loss_ce: 0.006486, loss_dice: 0.199650
[10:48:35.134] TRAIN: iteration 18858 : loss : 0.062848, loss_ce: 0.004324, loss_dice: 0.121372
[10:48:35.343] TRAIN: iteration 18859 : loss : 0.165880, loss_ce: 0.002276, loss_dice: 0.329485
[10:48:35.975] TRAIN: iteration 18860 : loss : 0.185838, loss_ce: 0.010998, loss_dice: 0.360678
[10:48:36.213] TRAIN: iteration 18861 : loss : 0.115475, loss_ce: 0.006052, loss_dice: 0.224897
[10:48:40.574] TRAIN: iteration 18862 : loss : 0.251105, loss_ce: 0.002094, loss_dice: 0.500116
[10:48:40.785] TRAIN: iteration 18863 : loss : 0.235303, loss_ce: 0.006672, loss_dice: 0.463933
[10:48:40.994] TRAIN: iteration 18864 : loss : 0.069076, loss_ce: 0.008928, loss_dice: 0.129223
[10:48:41.206] TRAIN: iteration 18865 : loss : 0.070463, loss_ce: 0.002883, loss_dice: 0.138043
[10:48:41.413] TRAIN: iteration 18866 : loss : 0.086082, loss_ce: 0.008972, loss_dice: 0.163193
[10:48:41.622] TRAIN: iteration 18867 : loss : 0.068664, loss_ce: 0.006772, loss_dice: 0.130556
[10:48:42.674] TRAIN: iteration 18868 : loss : 0.254170, loss_ce: 0.008815, loss_dice: 0.499525
[10:48:43.186] TRAIN: iteration 18869 : loss : 0.145363, loss_ce: 0.019440, loss_dice: 0.271286
[10:48:43.395] TRAIN: iteration 18870 : loss : 0.105810, loss_ce: 0.002067, loss_dice: 0.209553
[10:48:43.602] TRAIN: iteration 18871 : loss : 0.050943, loss_ce: 0.002848, loss_dice: 0.099038
[10:48:43.810] TRAIN: iteration 18872 : loss : 0.127495, loss_ce: 0.004268, loss_dice: 0.250722
[10:48:44.021] TRAIN: iteration 18873 : loss : 0.083769, loss_ce: 0.003663, loss_dice: 0.163876
[10:48:45.006] TRAIN: iteration 18874 : loss : 0.241495, loss_ce: 0.001853, loss_dice: 0.481137
[10:48:45.215] TRAIN: iteration 18875 : loss : 0.250981, loss_ce: 0.001852, loss_dice: 0.500111
[10:48:49.343] TRAIN: iteration 18876 : loss : 0.172110, loss_ce: 0.002508, loss_dice: 0.341713
[10:48:50.617] TRAIN: iteration 18877 : loss : 0.158938, loss_ce: 0.001870, loss_dice: 0.316005
[10:48:50.824] TRAIN: iteration 18878 : loss : 0.242759, loss_ce: 0.002511, loss_dice: 0.483008
[10:48:51.032] TRAIN: iteration 18879 : loss : 0.024959, loss_ce: 0.001539, loss_dice: 0.048380
[10:48:51.240] TRAIN: iteration 18880 : loss : 0.062502, loss_ce: 0.005400, loss_dice: 0.119605
[10:48:51.477] TRAIN: iteration 18881 : loss : 0.050524, loss_ce: 0.003110, loss_dice: 0.097938
[10:48:51.685] TRAIN: iteration 18882 : loss : 0.052754, loss_ce: 0.001359, loss_dice: 0.104150
[10:48:51.893] TRAIN: iteration 18883 : loss : 0.183477, loss_ce: 0.005359, loss_dice: 0.361596
[10:48:55.537] TRAIN: iteration 18884 : loss : 0.157154, loss_ce: 0.002752, loss_dice: 0.311556
[10:48:55.746] TRAIN: iteration 18885 : loss : 0.115246, loss_ce: 0.008753, loss_dice: 0.221740
[10:48:55.953] TRAIN: iteration 18886 : loss : 0.175111, loss_ce: 0.007378, loss_dice: 0.342845
[10:48:56.160] TRAIN: iteration 18887 : loss : 0.198677, loss_ce: 0.003601, loss_dice: 0.393753
[10:48:56.367] TRAIN: iteration 18888 : loss : 0.076007, loss_ce: 0.002112, loss_dice: 0.149902
[10:48:56.574] TRAIN: iteration 18889 : loss : 0.063191, loss_ce: 0.004219, loss_dice: 0.122163
[10:48:59.869] TRAIN: iteration 18890 : loss : 0.089469, loss_ce: 0.002405, loss_dice: 0.176532
[10:49:00.077] TRAIN: iteration 18891 : loss : 0.109098, loss_ce: 0.003377, loss_dice: 0.214819
[10:49:00.286] TRAIN: iteration 18892 : loss : 0.050122, loss_ce: 0.004526, loss_dice: 0.095717
[10:49:02.028] TRAIN: iteration 18893 : loss : 0.045177, loss_ce: 0.006123, loss_dice: 0.084231
[10:49:02.237] TRAIN: iteration 18894 : loss : 0.085916, loss_ce: 0.009487, loss_dice: 0.162345
[10:49:02.450] TRAIN: iteration 18895 : loss : 0.161304, loss_ce: 0.004050, loss_dice: 0.318557
[10:49:04.650] TRAIN: iteration 18896 : loss : 0.093513, loss_ce: 0.002508, loss_dice: 0.184517
[10:49:04.860] TRAIN: iteration 18897 : loss : 0.050301, loss_ce: 0.003429, loss_dice: 0.097173
[10:49:07.028] TRAIN: iteration 18898 : loss : 0.161585, loss_ce: 0.002283, loss_dice: 0.320887
[10:49:07.239] TRAIN: iteration 18899 : loss : 0.225547, loss_ce: 0.002060, loss_dice: 0.449033
[10:49:07.447] TRAIN: iteration 18900 : loss : 0.103257, loss_ce: 0.002372, loss_dice: 0.204142
[10:49:08.552] TRAIN: iteration 18901 : loss : 0.233467, loss_ce: 0.010652, loss_dice: 0.456282
[10:49:08.761] TRAIN: iteration 18902 : loss : 0.077735, loss_ce: 0.002528, loss_dice: 0.152943
[10:49:08.969] TRAIN: iteration 18903 : loss : 0.067581, loss_ce: 0.002158, loss_dice: 0.133004
[10:49:10.446] TRAIN: iteration 18904 : loss : 0.109111, loss_ce: 0.002103, loss_dice: 0.216119
[10:49:10.654] TRAIN: iteration 18905 : loss : 0.101449, loss_ce: 0.001246, loss_dice: 0.201652
[10:49:13.806] TRAIN: iteration 18906 : loss : 0.251759, loss_ce: 0.003275, loss_dice: 0.500244
[10:49:14.022] TRAIN: iteration 18907 : loss : 0.145972, loss_ce: 0.005799, loss_dice: 0.286145
[10:49:14.707] TRAIN: iteration 18908 : loss : 0.157461, loss_ce: 0.003836, loss_dice: 0.311087
[10:49:15.026] TRAIN: iteration 18909 : loss : 0.072209, loss_ce: 0.000800, loss_dice: 0.143617
[10:49:15.234] TRAIN: iteration 18910 : loss : 0.236151, loss_ce: 0.006343, loss_dice: 0.465960
[10:49:15.479] TRAIN: iteration 18911 : loss : 0.139761, loss_ce: 0.001678, loss_dice: 0.277844
[10:49:21.309] TRAIN: iteration 18912 : loss : 0.145484, loss_ce: 0.004663, loss_dice: 0.286304
[10:49:21.516] TRAIN: iteration 18913 : loss : 0.223677, loss_ce: 0.006252, loss_dice: 0.441101
[10:49:22.652] TRAIN: iteration 18914 : loss : 0.067804, loss_ce: 0.001110, loss_dice: 0.134497
[10:49:22.863] TRAIN: iteration 18915 : loss : 0.106745, loss_ce: 0.001023, loss_dice: 0.212468
[10:49:23.338] TRAIN: iteration 18916 : loss : 0.168407, loss_ce: 0.014203, loss_dice: 0.322611
[10:49:23.544] TRAIN: iteration 18917 : loss : 0.216309, loss_ce: 0.001625, loss_dice: 0.430992
[10:49:23.752] TRAIN: iteration 18918 : loss : 0.132868, loss_ce: 0.000891, loss_dice: 0.264845
[10:49:23.959] TRAIN: iteration 18919 : loss : 0.015515, loss_ce: 0.000751, loss_dice: 0.030280
[10:49:29.059] TRAIN: iteration 18920 : loss : 0.172016, loss_ce: 0.005554, loss_dice: 0.338478
[10:49:29.307] TRAIN: iteration 18921 : loss : 0.245456, loss_ce: 0.004336, loss_dice: 0.486575
[10:49:29.515] TRAIN: iteration 18922 : loss : 0.093004, loss_ce: 0.005595, loss_dice: 0.180413
[10:49:29.723] TRAIN: iteration 18923 : loss : 0.097817, loss_ce: 0.002290, loss_dice: 0.193345
[10:49:29.965] TRAIN: iteration 18924 : loss : 0.132832, loss_ce: 0.007469, loss_dice: 0.258195
[10:49:30.174] TRAIN: iteration 18925 : loss : 0.125335, loss_ce: 0.005406, loss_dice: 0.245263
[10:49:30.381] TRAIN: iteration 18926 : loss : 0.181050, loss_ce: 0.001182, loss_dice: 0.360918
[10:49:30.589] TRAIN: iteration 18927 : loss : 0.138167, loss_ce: 0.010423, loss_dice: 0.265910
[10:49:42.663] TRAIN: iteration 18928 : loss : 0.179391, loss_ce: 0.006573, loss_dice: 0.352208
[10:49:42.871] TRAIN: iteration 18929 : loss : 0.190756, loss_ce: 0.001685, loss_dice: 0.379827
[10:49:43.080] TRAIN: iteration 18930 : loss : 0.206863, loss_ce: 0.003724, loss_dice: 0.410002
[10:49:43.291] TRAIN: iteration 18931 : loss : 0.170912, loss_ce: 0.002184, loss_dice: 0.339639
[10:49:43.499] TRAIN: iteration 18932 : loss : 0.251074, loss_ce: 0.002052, loss_dice: 0.500096
[10:49:43.707] TRAIN: iteration 18933 : loss : 0.147827, loss_ce: 0.005457, loss_dice: 0.290196
[10:49:43.917] TRAIN: iteration 18934 : loss : 0.037961, loss_ce: 0.002217, loss_dice: 0.073705
[10:49:44.128] TRAIN: iteration 18935 : loss : 0.118662, loss_ce: 0.001967, loss_dice: 0.235356
[10:49:51.574] TRAIN: iteration 18936 : loss : 0.095775, loss_ce: 0.007752, loss_dice: 0.183799
[10:49:51.782] TRAIN: iteration 18937 : loss : 0.084088, loss_ce: 0.003052, loss_dice: 0.165125
[10:49:51.989] TRAIN: iteration 18938 : loss : 0.179728, loss_ce: 0.003530, loss_dice: 0.355927
[10:49:52.199] TRAIN: iteration 18939 : loss : 0.251834, loss_ce: 0.003424, loss_dice: 0.500243
[10:49:52.408] TRAIN: iteration 18940 : loss : 0.201935, loss_ce: 0.041996, loss_dice: 0.361873
[10:49:52.656] TRAIN: iteration 18941 : loss : 0.041755, loss_ce: 0.004259, loss_dice: 0.079251
[10:49:52.864] TRAIN: iteration 18942 : loss : 0.068445, loss_ce: 0.006243, loss_dice: 0.130647
[10:49:53.071] TRAIN: iteration 18943 : loss : 0.069455, loss_ce: 0.002157, loss_dice: 0.136753
[10:49:58.793] TRAIN: iteration 18944 : loss : 0.119862, loss_ce: 0.003542, loss_dice: 0.236182
[10:49:59.005] TRAIN: iteration 18945 : loss : 0.058272, loss_ce: 0.005624, loss_dice: 0.110920
[10:49:59.216] TRAIN: iteration 18946 : loss : 0.054798, loss_ce: 0.004789, loss_dice: 0.104807
[10:49:59.425] TRAIN: iteration 18947 : loss : 0.032925, loss_ce: 0.003775, loss_dice: 0.062075
[10:49:59.636] TRAIN: iteration 18948 : loss : 0.165200, loss_ce: 0.002571, loss_dice: 0.327828
[10:49:59.844] TRAIN: iteration 18949 : loss : 0.160777, loss_ce: 0.002046, loss_dice: 0.319508
[10:50:00.053] TRAIN: iteration 18950 : loss : 0.066035, loss_ce: 0.005645, loss_dice: 0.126425
[10:50:00.262] TRAIN: iteration 18951 : loss : 0.138602, loss_ce: 0.003562, loss_dice: 0.273642
[10:50:05.181] TRAIN: iteration 18952 : loss : 0.250450, loss_ce: 0.000889, loss_dice: 0.500010
[10:50:05.392] TRAIN: iteration 18953 : loss : 0.090873, loss_ce: 0.003123, loss_dice: 0.178623
[10:50:05.599] TRAIN: iteration 18954 : loss : 0.115798, loss_ce: 0.004478, loss_dice: 0.227117
[10:50:05.806] TRAIN: iteration 18955 : loss : 0.109835, loss_ce: 0.006353, loss_dice: 0.213317
[10:50:06.014] TRAIN: iteration 18956 : loss : 0.178635, loss_ce: 0.003059, loss_dice: 0.354212
[10:50:06.228] TRAIN: iteration 18957 : loss : 0.224505, loss_ce: 0.015473, loss_dice: 0.433538
[10:50:06.435] TRAIN: iteration 18958 : loss : 0.077890, loss_ce: 0.008760, loss_dice: 0.147019
[10:50:06.642] TRAIN: iteration 18959 : loss : 0.216351, loss_ce: 0.003513, loss_dice: 0.429189
[10:50:11.633] TRAIN: iteration 18960 : loss : 0.069277, loss_ce: 0.003686, loss_dice: 0.134868
[10:50:11.872] TRAIN: iteration 18961 : loss : 0.128296, loss_ce: 0.002995, loss_dice: 0.253597
[10:50:12.082] TRAIN: iteration 18962 : loss : 0.165634, loss_ce: 0.001375, loss_dice: 0.329893
[10:50:12.290] TRAIN: iteration 18963 : loss : 0.043521, loss_ce: 0.001736, loss_dice: 0.085306
[10:50:12.497] TRAIN: iteration 18964 : loss : 0.118260, loss_ce: 0.003270, loss_dice: 0.233251
[10:50:12.707] TRAIN: iteration 18965 : loss : 0.060078, loss_ce: 0.001134, loss_dice: 0.119022
[10:50:12.915] TRAIN: iteration 18966 : loss : 0.100148, loss_ce: 0.004324, loss_dice: 0.195972
[10:50:13.125] TRAIN: iteration 18967 : loss : 0.171376, loss_ce: 0.005878, loss_dice: 0.336874
[10:50:18.833] TRAIN: iteration 18968 : loss : 0.081526, loss_ce: 0.003053, loss_dice: 0.159999
[10:50:19.041] TRAIN: iteration 18969 : loss : 0.085377, loss_ce: 0.004147, loss_dice: 0.166607
[10:50:19.248] TRAIN: iteration 18970 : loss : 0.084384, loss_ce: 0.007112, loss_dice: 0.161656
[10:50:19.461] TRAIN: iteration 18971 : loss : 0.161270, loss_ce: 0.023406, loss_dice: 0.299135
[10:50:19.668] TRAIN: iteration 18972 : loss : 0.247751, loss_ce: 0.004902, loss_dice: 0.490599
[10:50:19.876] TRAIN: iteration 18973 : loss : 0.112075, loss_ce: 0.005876, loss_dice: 0.218274
[10:50:20.084] TRAIN: iteration 18974 : loss : 0.087607, loss_ce: 0.004968, loss_dice: 0.170246
[10:50:20.291] TRAIN: iteration 18975 : loss : 0.214848, loss_ce: 0.007387, loss_dice: 0.422310
[10:50:27.349] TRAIN: iteration 18976 : loss : 0.066725, loss_ce: 0.004877, loss_dice: 0.128573
[10:50:27.557] TRAIN: iteration 18977 : loss : 0.129326, loss_ce: 0.010797, loss_dice: 0.247856
[10:50:27.768] TRAIN: iteration 18978 : loss : 0.188978, loss_ce: 0.015757, loss_dice: 0.362200
[10:50:27.976] TRAIN: iteration 18979 : loss : 0.124072, loss_ce: 0.005218, loss_dice: 0.242926
[10:50:28.186] TRAIN: iteration 18980 : loss : 0.191877, loss_ce: 0.011148, loss_dice: 0.372606
[10:50:28.422] TRAIN: iteration 18981 : loss : 0.220218, loss_ce: 0.008088, loss_dice: 0.432349
[10:50:28.629] TRAIN: iteration 18982 : loss : 0.145776, loss_ce: 0.005955, loss_dice: 0.285597
[10:50:28.837] TRAIN: iteration 18983 : loss : 0.039167, loss_ce: 0.003910, loss_dice: 0.074424
[10:50:35.262] TRAIN: iteration 18984 : loss : 0.066186, loss_ce: 0.002912, loss_dice: 0.129461
[10:50:35.475] TRAIN: iteration 18985 : loss : 0.252487, loss_ce: 0.004654, loss_dice: 0.500320
[10:50:35.683] TRAIN: iteration 18986 : loss : 0.168317, loss_ce: 0.007608, loss_dice: 0.329026
[10:50:35.894] TRAIN: iteration 18987 : loss : 0.163503, loss_ce: 0.007683, loss_dice: 0.319324
[10:50:36.377] TRAIN: iteration 18988 : loss : 0.122866, loss_ce: 0.006223, loss_dice: 0.239509
[10:50:36.588] TRAIN: iteration 18989 : loss : 0.251097, loss_ce: 0.002096, loss_dice: 0.500098
[10:50:36.796] TRAIN: iteration 18990 : loss : 0.054303, loss_ce: 0.005184, loss_dice: 0.103423
[10:50:37.005] TRAIN: iteration 18991 : loss : 0.054340, loss_ce: 0.001601, loss_dice: 0.107080
[10:50:41.784] TRAIN: iteration 18992 : loss : 0.251222, loss_ce: 0.002320, loss_dice: 0.500124
[10:50:41.990] TRAIN: iteration 18993 : loss : 0.082850, loss_ce: 0.003798, loss_dice: 0.161903
[10:50:42.198] TRAIN: iteration 18994 : loss : 0.150176, loss_ce: 0.005450, loss_dice: 0.294903
[10:50:42.405] TRAIN: iteration 18995 : loss : 0.149519, loss_ce: 0.002656, loss_dice: 0.296381
[10:50:44.024] TRAIN: iteration 18996 : loss : 0.252170, loss_ce: 0.004045, loss_dice: 0.500294
[10:50:44.289] TRAIN: iteration 18997 : loss : 0.244337, loss_ce: 0.001897, loss_dice: 0.486778
[10:50:44.498] TRAIN: iteration 18998 : loss : 0.111468, loss_ce: 0.010361, loss_dice: 0.212574
[10:50:44.705] TRAIN: iteration 18999 : loss : 0.042201, loss_ce: 0.001018, loss_dice: 0.083383
[10:50:49.439] TRAIN: iteration 19000 : loss : 0.086010, loss_ce: 0.001679, loss_dice: 0.170340
[10:50:49.440] NaN or Inf found in input tensor.
[10:50:49.654] TRAIN: iteration 19001 : loss : 0.114159, loss_ce: 0.015435, loss_dice: 0.212884
[10:50:49.861] TRAIN: iteration 19002 : loss : 0.157603, loss_ce: 0.002698, loss_dice: 0.312508
[10:50:50.070] TRAIN: iteration 19003 : loss : 0.106007, loss_ce: 0.004780, loss_dice: 0.207234
[10:50:53.488] TRAIN: iteration 19004 : loss : 0.050696, loss_ce: 0.003612, loss_dice: 0.097779
[10:50:53.695] TRAIN: iteration 19005 : loss : 0.078539, loss_ce: 0.003017, loss_dice: 0.154062
[10:50:53.902] TRAIN: iteration 19006 : loss : 0.240885, loss_ce: 0.002438, loss_dice: 0.479332
[10:50:54.111] TRAIN: iteration 19007 : loss : 0.239170, loss_ce: 0.005028, loss_dice: 0.473312
[10:50:58.044] TRAIN: iteration 19008 : loss : 0.095259, loss_ce: 0.003129, loss_dice: 0.187389
[10:50:58.252] TRAIN: iteration 19009 : loss : 0.066674, loss_ce: 0.013627, loss_dice: 0.119721
[10:50:58.836] TRAIN: iteration 19010 : loss : 0.189553, loss_ce: 0.004716, loss_dice: 0.374391
[10:50:59.044] TRAIN: iteration 19011 : loss : 0.244897, loss_ce: 0.002725, loss_dice: 0.487069
[10:51:04.784] TRAIN: iteration 19012 : loss : 0.217363, loss_ce: 0.003588, loss_dice: 0.431138
[10:51:04.998] TRAIN: iteration 19013 : loss : 0.153874, loss_ce: 0.006589, loss_dice: 0.301160
[10:51:05.211] TRAIN: iteration 19014 : loss : 0.061560, loss_ce: 0.004802, loss_dice: 0.118317
[10:51:05.421] TRAIN: iteration 19015 : loss : 0.040074, loss_ce: 0.006452, loss_dice: 0.073697
[10:51:08.737] TRAIN: iteration 19016 : loss : 0.245781, loss_ce: 0.004919, loss_dice: 0.486643
[10:51:08.944] TRAIN: iteration 19017 : loss : 0.175420, loss_ce: 0.015488, loss_dice: 0.335353
[10:51:09.152] TRAIN: iteration 19018 : loss : 0.103604, loss_ce: 0.002737, loss_dice: 0.204471
[10:51:09.359] TRAIN: iteration 19019 : loss : 0.132619, loss_ce: 0.005231, loss_dice: 0.260007
[10:51:13.597] TRAIN: iteration 19020 : loss : 0.051189, loss_ce: 0.009611, loss_dice: 0.092767
[10:51:13.831] TRAIN: iteration 19021 : loss : 0.130371, loss_ce: 0.006901, loss_dice: 0.253840
[10:51:14.039] TRAIN: iteration 19022 : loss : 0.242925, loss_ce: 0.003082, loss_dice: 0.482768
[10:51:14.250] TRAIN: iteration 19023 : loss : 0.195637, loss_ce: 0.004467, loss_dice: 0.386807
[10:51:15.813] TRAIN: iteration 19024 : loss : 0.138146, loss_ce: 0.002909, loss_dice: 0.273382
[10:51:16.026] TRAIN: iteration 19025 : loss : 0.087208, loss_ce: 0.002686, loss_dice: 0.171730
[10:51:18.170] TRAIN: iteration 19026 : loss : 0.251768, loss_ce: 0.003329, loss_dice: 0.500208
[10:51:18.379] TRAIN: iteration 19027 : loss : 0.089555, loss_ce: 0.003930, loss_dice: 0.175180
[10:51:23.003] TRAIN: iteration 19028 : loss : 0.085618, loss_ce: 0.003424, loss_dice: 0.167812
[10:51:23.211] TRAIN: iteration 19029 : loss : 0.162377, loss_ce: 0.002039, loss_dice: 0.322715
[10:51:23.419] TRAIN: iteration 19030 : loss : 0.251269, loss_ce: 0.002395, loss_dice: 0.500143
[10:51:23.627] TRAIN: iteration 19031 : loss : 0.088237, loss_ce: 0.002052, loss_dice: 0.174423
[10:51:25.069] TRAIN: iteration 19032 : loss : 0.054976, loss_ce: 0.001292, loss_dice: 0.108660
[10:51:25.276] TRAIN: iteration 19033 : loss : 0.217533, loss_ce: 0.002886, loss_dice: 0.432181
[10:51:27.722] TRAIN: iteration 19034 : loss : 0.175039, loss_ce: 0.001906, loss_dice: 0.348172
[10:51:27.929] TRAIN: iteration 19035 : loss : 0.049026, loss_ce: 0.001070, loss_dice: 0.096981
[10:51:31.925] TRAIN: iteration 19036 : loss : 0.059761, loss_ce: 0.003603, loss_dice: 0.115918
[10:51:32.133] TRAIN: iteration 19037 : loss : 0.104856, loss_ce: 0.002883, loss_dice: 0.206830
[10:51:32.340] TRAIN: iteration 19038 : loss : 0.168223, loss_ce: 0.002676, loss_dice: 0.333769
[10:51:32.547] TRAIN: iteration 19039 : loss : 0.116682, loss_ce: 0.003355, loss_dice: 0.230008
[10:51:33.228] TRAIN: iteration 19040 : loss : 0.251058, loss_ce: 0.001984, loss_dice: 0.500133
[10:51:33.465] TRAIN: iteration 19041 : loss : 0.250800, loss_ce: 0.002121, loss_dice: 0.499480
[10:51:37.164] TRAIN: iteration 19042 : loss : 0.150258, loss_ce: 0.004351, loss_dice: 0.296166
[10:51:37.371] TRAIN: iteration 19043 : loss : 0.070115, loss_ce: 0.001791, loss_dice: 0.138439
[10:51:41.853] TRAIN: iteration 19044 : loss : 0.217770, loss_ce: 0.001560, loss_dice: 0.433980
[10:51:42.073] TRAIN: iteration 19045 : loss : 0.092972, loss_ce: 0.001824, loss_dice: 0.184119
[10:51:42.281] TRAIN: iteration 19046 : loss : 0.232977, loss_ce: 0.000788, loss_dice: 0.465166
[10:51:42.488] TRAIN: iteration 19047 : loss : 0.158618, loss_ce: 0.004262, loss_dice: 0.312975
[10:51:42.695] TRAIN: iteration 19048 : loss : 0.184267, loss_ce: 0.015624, loss_dice: 0.352911
[10:51:42.904] TRAIN: iteration 19049 : loss : 0.059210, loss_ce: 0.004161, loss_dice: 0.114258
[10:51:47.400] TRAIN: iteration 19050 : loss : 0.231749, loss_ce: 0.008326, loss_dice: 0.455172
[10:51:47.608] TRAIN: iteration 19051 : loss : 0.057861, loss_ce: 0.002044, loss_dice: 0.113678
[10:51:49.972] TRAIN: iteration 19052 : loss : 0.054849, loss_ce: 0.004936, loss_dice: 0.104763
[10:51:50.179] TRAIN: iteration 19053 : loss : 0.156871, loss_ce: 0.002842, loss_dice: 0.310901
[10:51:50.387] TRAIN: iteration 19054 : loss : 0.065636, loss_ce: 0.005888, loss_dice: 0.125384
[10:51:50.596] TRAIN: iteration 19055 : loss : 0.063569, loss_ce: 0.005913, loss_dice: 0.121225
[10:51:51.913] TRAIN: iteration 19056 : loss : 0.112560, loss_ce: 0.004730, loss_dice: 0.220391
[10:51:52.121] TRAIN: iteration 19057 : loss : 0.089923, loss_ce: 0.008108, loss_dice: 0.171737
[10:51:56.637] TRAIN: iteration 19058 : loss : 0.076925, loss_ce: 0.002660, loss_dice: 0.151190
[10:51:56.844] TRAIN: iteration 19059 : loss : 0.047529, loss_ce: 0.003030, loss_dice: 0.092028
[10:51:59.504] TRAIN: iteration 19060 : loss : 0.117733, loss_ce: 0.002184, loss_dice: 0.233282
[10:51:59.738] TRAIN: iteration 19061 : loss : 0.053975, loss_ce: 0.002189, loss_dice: 0.105761
[10:51:59.945] TRAIN: iteration 19062 : loss : 0.050765, loss_ce: 0.003310, loss_dice: 0.098220
[10:52:00.152] TRAIN: iteration 19063 : loss : 0.250913, loss_ce: 0.001736, loss_dice: 0.500089
[10:52:01.814] TRAIN: iteration 19064 : loss : 0.072315, loss_ce: 0.006163, loss_dice: 0.138468
[10:52:02.022] TRAIN: iteration 19065 : loss : 0.081938, loss_ce: 0.002244, loss_dice: 0.161632
[10:52:06.621] TRAIN: iteration 19066 : loss : 0.147485, loss_ce: 0.006647, loss_dice: 0.288324
[10:52:06.834] TRAIN: iteration 19067 : loss : 0.251152, loss_ce: 0.002167, loss_dice: 0.500137
[10:52:08.645] TRAIN: iteration 19068 : loss : 0.064446, loss_ce: 0.002549, loss_dice: 0.126342
[10:52:08.854] TRAIN: iteration 19069 : loss : 0.113548, loss_ce: 0.006303, loss_dice: 0.220792
[10:52:09.061] TRAIN: iteration 19070 : loss : 0.193873, loss_ce: 0.004453, loss_dice: 0.383293
[10:52:09.269] TRAIN: iteration 19071 : loss : 0.064666, loss_ce: 0.003482, loss_dice: 0.125851
[10:52:09.477] TRAIN: iteration 19072 : loss : 0.162696, loss_ce: 0.003378, loss_dice: 0.322013
[10:52:09.684] TRAIN: iteration 19073 : loss : 0.148021, loss_ce: 0.011443, loss_dice: 0.284599
[10:52:14.246] TRAIN: iteration 19074 : loss : 0.250879, loss_ce: 0.001680, loss_dice: 0.500077
[10:52:14.460] TRAIN: iteration 19075 : loss : 0.251224, loss_ce: 0.002307, loss_dice: 0.500141
[10:52:17.227] TRAIN: iteration 19076 : loss : 0.055235, loss_ce: 0.003081, loss_dice: 0.107388
[10:52:17.435] TRAIN: iteration 19077 : loss : 0.151793, loss_ce: 0.009321, loss_dice: 0.294264
[10:52:17.643] TRAIN: iteration 19078 : loss : 0.048417, loss_ce: 0.001635, loss_dice: 0.095198
[10:52:17.850] TRAIN: iteration 19079 : loss : 0.087558, loss_ce: 0.003892, loss_dice: 0.171224
[10:52:18.666] TRAIN: iteration 19080 : loss : 0.052197, loss_ce: 0.002037, loss_dice: 0.102357
[10:52:18.903] TRAIN: iteration 19081 : loss : 0.051471, loss_ce: 0.003768, loss_dice: 0.099175
[10:52:25.390] TRAIN: iteration 19082 : loss : 0.224914, loss_ce: 0.002788, loss_dice: 0.447040
[10:52:25.598] TRAIN: iteration 19083 : loss : 0.247983, loss_ce: 0.011425, loss_dice: 0.484541
[10:52:26.072] TRAIN: iteration 19084 : loss : 0.111618, loss_ce: 0.002406, loss_dice: 0.220830
[10:52:26.168] TRAIN: iteration 19085 : loss : 0.247488, loss_ce: 0.002741, loss_dice: 0.492235
[10:57:58.969] VALIDATION: iteration 10 : loss : 0.140044, loss_ce: 0.006125, loss_dice: 0.273963
[10:57:59.731] TRAIN: iteration 19086 : loss : 0.122704, loss_ce: 0.005176, loss_dice: 0.240231
[10:58:00.425] TRAIN: iteration 19087 : loss : 0.111505, loss_ce: 0.003809, loss_dice: 0.219201
[10:58:00.633] TRAIN: iteration 19088 : loss : 0.108475, loss_ce: 0.003117, loss_dice: 0.213833
[10:58:00.844] TRAIN: iteration 19089 : loss : 0.170291, loss_ce: 0.021058, loss_dice: 0.319523
[10:58:01.053] TRAIN: iteration 19090 : loss : 0.070138, loss_ce: 0.002509, loss_dice: 0.137767
[10:58:01.734] TRAIN: iteration 19091 : loss : 0.195804, loss_ce: 0.006726, loss_dice: 0.384881
[10:58:01.942] TRAIN: iteration 19092 : loss : 0.029619, loss_ce: 0.001482, loss_dice: 0.057756
[10:58:02.151] TRAIN: iteration 19093 : loss : 0.252127, loss_ce: 0.064488, loss_dice: 0.439765
[10:58:02.360] TRAIN: iteration 19094 : loss : 0.096302, loss_ce: 0.001952, loss_dice: 0.190651
[10:58:02.632] TRAIN: iteration 19095 : loss : 0.103071, loss_ce: 0.009459, loss_dice: 0.196683
[10:58:02.843] TRAIN: iteration 19096 : loss : 0.162786, loss_ce: 0.005346, loss_dice: 0.320226
[10:58:03.061] TRAIN: iteration 19097 : loss : 0.139075, loss_ce: 0.003508, loss_dice: 0.274641
[10:58:03.277] TRAIN: iteration 19098 : loss : 0.242946, loss_ce: 0.002929, loss_dice: 0.482963
[10:58:03.486] TRAIN: iteration 19099 : loss : 0.183688, loss_ce: 0.004578, loss_dice: 0.362797
[10:58:03.697] TRAIN: iteration 19100 : loss : 0.227207, loss_ce: 0.028613, loss_dice: 0.425800
[10:58:03.958] TRAIN: iteration 19101 : loss : 0.040360, loss_ce: 0.003337, loss_dice: 0.077383
[10:58:04.165] TRAIN: iteration 19102 : loss : 0.160420, loss_ce: 0.013323, loss_dice: 0.307517
[10:58:04.373] TRAIN: iteration 19103 : loss : 0.024425, loss_ce: 0.001636, loss_dice: 0.047213
[10:58:04.583] TRAIN: iteration 19104 : loss : 0.245336, loss_ce: 0.005473, loss_dice: 0.485200
[10:58:04.796] TRAIN: iteration 19105 : loss : 0.126572, loss_ce: 0.006011, loss_dice: 0.247133
[10:58:05.009] TRAIN: iteration 19106 : loss : 0.252029, loss_ce: 0.003873, loss_dice: 0.500186
[10:58:05.224] TRAIN: iteration 19107 : loss : 0.046154, loss_ce: 0.005425, loss_dice: 0.086883
[10:58:05.433] TRAIN: iteration 19108 : loss : 0.122597, loss_ce: 0.003703, loss_dice: 0.241491
[10:58:05.644] TRAIN: iteration 19109 : loss : 0.207777, loss_ce: 0.005554, loss_dice: 0.409999
[10:58:05.856] TRAIN: iteration 19110 : loss : 0.055265, loss_ce: 0.003374, loss_dice: 0.107156
[10:58:06.069] TRAIN: iteration 19111 : loss : 0.110642, loss_ce: 0.003151, loss_dice: 0.218133
[10:58:06.290] TRAIN: iteration 19112 : loss : 0.082501, loss_ce: 0.002929, loss_dice: 0.162073
[10:58:06.505] TRAIN: iteration 19113 : loss : 0.014581, loss_ce: 0.002028, loss_dice: 0.027134
[10:58:06.717] TRAIN: iteration 19114 : loss : 0.167431, loss_ce: 0.003132, loss_dice: 0.331730
[10:58:06.926] TRAIN: iteration 19115 : loss : 0.081268, loss_ce: 0.004608, loss_dice: 0.157927
[10:58:07.135] TRAIN: iteration 19116 : loss : 0.167838, loss_ce: 0.004642, loss_dice: 0.331033
[10:58:07.349] TRAIN: iteration 19117 : loss : 0.243074, loss_ce: 0.002148, loss_dice: 0.484001
[10:58:07.563] TRAIN: iteration 19118 : loss : 0.102053, loss_ce: 0.002043, loss_dice: 0.202063
[10:58:07.771] TRAIN: iteration 19119 : loss : 0.199216, loss_ce: 0.008028, loss_dice: 0.390404
[10:58:07.990] TRAIN: iteration 19120 : loss : 0.203092, loss_ce: 0.005895, loss_dice: 0.400289
[10:58:07.990] NaN or Inf found in input tensor.
[10:58:08.214] TRAIN: iteration 19121 : loss : 0.251646, loss_ce: 0.003067, loss_dice: 0.500224
[10:58:08.422] TRAIN: iteration 19122 : loss : 0.100103, loss_ce: 0.004243, loss_dice: 0.195963
[10:58:08.638] TRAIN: iteration 19123 : loss : 0.111958, loss_ce: 0.002599, loss_dice: 0.221318
[10:58:08.851] TRAIN: iteration 19124 : loss : 0.082758, loss_ce: 0.013105, loss_dice: 0.152411
[10:58:09.059] TRAIN: iteration 19125 : loss : 0.112076, loss_ce: 0.004477, loss_dice: 0.219675
[10:58:09.269] TRAIN: iteration 19126 : loss : 0.146681, loss_ce: 0.002321, loss_dice: 0.291041
[10:58:09.479] TRAIN: iteration 19127 : loss : 0.161288, loss_ce: 0.012021, loss_dice: 0.310555
[10:58:09.691] TRAIN: iteration 19128 : loss : 0.081020, loss_ce: 0.002798, loss_dice: 0.159241
[10:58:09.901] TRAIN: iteration 19129 : loss : 0.251217, loss_ce: 0.002305, loss_dice: 0.500129
[10:58:10.121] TRAIN: iteration 19130 : loss : 0.212096, loss_ce: 0.005874, loss_dice: 0.418318
[10:58:10.329] TRAIN: iteration 19131 : loss : 0.128616, loss_ce: 0.002268, loss_dice: 0.254964
[10:58:10.537] TRAIN: iteration 19132 : loss : 0.148076, loss_ce: 0.002687, loss_dice: 0.293466
[10:58:10.746] TRAIN: iteration 19133 : loss : 0.055729, loss_ce: 0.002272, loss_dice: 0.109186
[10:58:10.958] TRAIN: iteration 19134 : loss : 0.211974, loss_ce: 0.003136, loss_dice: 0.420811
[10:58:11.166] TRAIN: iteration 19135 : loss : 0.179431, loss_ce: 0.004409, loss_dice: 0.354454
[10:58:11.377] TRAIN: iteration 19136 : loss : 0.105485, loss_ce: 0.005625, loss_dice: 0.205345
[10:58:11.586] TRAIN: iteration 19137 : loss : 0.181992, loss_ce: 0.008417, loss_dice: 0.355566
[10:58:11.795] TRAIN: iteration 19138 : loss : 0.240447, loss_ce: 0.002513, loss_dice: 0.478381
[10:58:12.004] TRAIN: iteration 19139 : loss : 0.052016, loss_ce: 0.005166, loss_dice: 0.098866
[10:58:12.213] TRAIN: iteration 19140 : loss : 0.250607, loss_ce: 0.001196, loss_dice: 0.500019
[10:58:12.458] TRAIN: iteration 19141 : loss : 0.066206, loss_ce: 0.003649, loss_dice: 0.128764
[10:58:12.667] TRAIN: iteration 19142 : loss : 0.246909, loss_ce: 0.005485, loss_dice: 0.488333
[10:58:12.874] TRAIN: iteration 19143 : loss : 0.097919, loss_ce: 0.004199, loss_dice: 0.191639
[10:58:13.084] TRAIN: iteration 19144 : loss : 0.073900, loss_ce: 0.008071, loss_dice: 0.139728
[10:58:13.292] TRAIN: iteration 19145 : loss : 0.239514, loss_ce: 0.004849, loss_dice: 0.474178
[10:58:13.503] TRAIN: iteration 19146 : loss : 0.085971, loss_ce: 0.002782, loss_dice: 0.169161
[10:58:13.711] TRAIN: iteration 19147 : loss : 0.250917, loss_ce: 0.001751, loss_dice: 0.500084
[10:58:13.920] TRAIN: iteration 19148 : loss : 0.251651, loss_ce: 0.003104, loss_dice: 0.500197
[10:58:14.132] TRAIN: iteration 19149 : loss : 0.190402, loss_ce: 0.007494, loss_dice: 0.373309
[10:58:14.343] TRAIN: iteration 19150 : loss : 0.139273, loss_ce: 0.001896, loss_dice: 0.276650
[10:58:14.551] TRAIN: iteration 19151 : loss : 0.253968, loss_ce: 0.007961, loss_dice: 0.499975
[10:58:14.765] TRAIN: iteration 19152 : loss : 0.231495, loss_ce: 0.009527, loss_dice: 0.453462
[10:58:14.976] TRAIN: iteration 19153 : loss : 0.035950, loss_ce: 0.003085, loss_dice: 0.068815
[10:58:15.189] TRAIN: iteration 19154 : loss : 0.140529, loss_ce: 0.002581, loss_dice: 0.278477
[10:58:15.398] TRAIN: iteration 19155 : loss : 0.262653, loss_ce: 0.024718, loss_dice: 0.500588
[10:58:15.608] TRAIN: iteration 19156 : loss : 0.239071, loss_ce: 0.009232, loss_dice: 0.468910
[10:58:15.819] TRAIN: iteration 19157 : loss : 0.065349, loss_ce: 0.001822, loss_dice: 0.128876
[10:58:16.037] TRAIN: iteration 19158 : loss : 0.134241, loss_ce: 0.005608, loss_dice: 0.262874
[10:58:16.250] TRAIN: iteration 19159 : loss : 0.193441, loss_ce: 0.005366, loss_dice: 0.381516
[10:58:16.457] TRAIN: iteration 19160 : loss : 0.246366, loss_ce: 0.002439, loss_dice: 0.490293
[10:58:16.691] TRAIN: iteration 19161 : loss : 0.251597, loss_ce: 0.002999, loss_dice: 0.500194
[10:58:16.899] TRAIN: iteration 19162 : loss : 0.082361, loss_ce: 0.007950, loss_dice: 0.156772
[10:58:17.109] TRAIN: iteration 19163 : loss : 0.130164, loss_ce: 0.005305, loss_dice: 0.255023
[10:58:17.321] TRAIN: iteration 19164 : loss : 0.053264, loss_ce: 0.002063, loss_dice: 0.104466
[10:58:17.529] TRAIN: iteration 19165 : loss : 0.120127, loss_ce: 0.008153, loss_dice: 0.232100
[10:58:17.736] TRAIN: iteration 19166 : loss : 0.112837, loss_ce: 0.005505, loss_dice: 0.220170
[10:58:17.944] TRAIN: iteration 19167 : loss : 0.097802, loss_ce: 0.002689, loss_dice: 0.192915
[10:58:18.153] TRAIN: iteration 19168 : loss : 0.128148, loss_ce: 0.004791, loss_dice: 0.251506
[10:58:18.361] TRAIN: iteration 19169 : loss : 0.233690, loss_ce: 0.002917, loss_dice: 0.464464
[10:58:18.568] TRAIN: iteration 19170 : loss : 0.087075, loss_ce: 0.003411, loss_dice: 0.170738
[10:58:18.778] TRAIN: iteration 19171 : loss : 0.251122, loss_ce: 0.003740, loss_dice: 0.498505
[10:58:18.986] TRAIN: iteration 19172 : loss : 0.252018, loss_ce: 0.003806, loss_dice: 0.500229
[10:58:19.196] TRAIN: iteration 19173 : loss : 0.250732, loss_ce: 0.003579, loss_dice: 0.497885
[10:58:19.414] TRAIN: iteration 19174 : loss : 0.119480, loss_ce: 0.004484, loss_dice: 0.234476
[10:58:19.622] TRAIN: iteration 19175 : loss : 0.251778, loss_ce: 0.003360, loss_dice: 0.500197
[10:58:19.832] TRAIN: iteration 19176 : loss : 0.251793, loss_ce: 0.003378, loss_dice: 0.500209
[10:58:20.041] TRAIN: iteration 19177 : loss : 0.018218, loss_ce: 0.001617, loss_dice: 0.034820
[10:58:20.251] TRAIN: iteration 19178 : loss : 0.136586, loss_ce: 0.007821, loss_dice: 0.265351
[10:58:20.463] TRAIN: iteration 19179 : loss : 0.125505, loss_ce: 0.003662, loss_dice: 0.247349
[10:58:20.712] TRAIN: iteration 19180 : loss : 0.119748, loss_ce: 0.004744, loss_dice: 0.234752
[10:58:20.953] TRAIN: iteration 19181 : loss : 0.137040, loss_ce: 0.003950, loss_dice: 0.270129
[10:58:21.161] TRAIN: iteration 19182 : loss : 0.106800, loss_ce: 0.005365, loss_dice: 0.208235
[10:58:21.369] TRAIN: iteration 19183 : loss : 0.136873, loss_ce: 0.009098, loss_dice: 0.264649
[10:58:21.584] TRAIN: iteration 19184 : loss : 0.075171, loss_ce: 0.002806, loss_dice: 0.147536
[10:58:21.792] TRAIN: iteration 19185 : loss : 0.094165, loss_ce: 0.002973, loss_dice: 0.185356
[10:58:22.004] TRAIN: iteration 19186 : loss : 0.050600, loss_ce: 0.002279, loss_dice: 0.098920
[10:58:22.215] TRAIN: iteration 19187 : loss : 0.089511, loss_ce: 0.012139, loss_dice: 0.166884
[10:58:22.424] TRAIN: iteration 19188 : loss : 0.090656, loss_ce: 0.003697, loss_dice: 0.177616
[10:58:22.631] TRAIN: iteration 19189 : loss : 0.230741, loss_ce: 0.002930, loss_dice: 0.458551
[10:58:22.941] TRAIN: iteration 19190 : loss : 0.067725, loss_ce: 0.004010, loss_dice: 0.131439
[10:58:23.154] TRAIN: iteration 19191 : loss : 0.068202, loss_ce: 0.005050, loss_dice: 0.131354
[10:58:23.363] TRAIN: iteration 19192 : loss : 0.039085, loss_ce: 0.004288, loss_dice: 0.073881
[10:58:23.572] TRAIN: iteration 19193 : loss : 0.252090, loss_ce: 0.003918, loss_dice: 0.500261
[10:58:23.780] TRAIN: iteration 19194 : loss : 0.251332, loss_ce: 0.002523, loss_dice: 0.500141
[10:58:23.990] TRAIN: iteration 19195 : loss : 0.097647, loss_ce: 0.003445, loss_dice: 0.191849
[10:58:24.205] TRAIN: iteration 19196 : loss : 0.176854, loss_ce: 0.003565, loss_dice: 0.350143
[10:58:24.414] TRAIN: iteration 19197 : loss : 0.079374, loss_ce: 0.006178, loss_dice: 0.152570
[10:58:24.629] TRAIN: iteration 19198 : loss : 0.107698, loss_ce: 0.003438, loss_dice: 0.211957
[10:58:24.836] TRAIN: iteration 19199 : loss : 0.195451, loss_ce: 0.003836, loss_dice: 0.387065
[10:58:25.046] TRAIN: iteration 19200 : loss : 0.244784, loss_ce: 0.002966, loss_dice: 0.486603
[10:58:25.281] TRAIN: iteration 19201 : loss : 0.076290, loss_ce: 0.007229, loss_dice: 0.145350
[10:58:25.490] TRAIN: iteration 19202 : loss : 0.098794, loss_ce: 0.008292, loss_dice: 0.189296
[10:58:25.704] TRAIN: iteration 19203 : loss : 0.252033, loss_ce: 0.003810, loss_dice: 0.500256
[10:58:25.919] TRAIN: iteration 19204 : loss : 0.087827, loss_ce: 0.006329, loss_dice: 0.169324
[10:58:26.127] TRAIN: iteration 19205 : loss : 0.251744, loss_ce: 0.003300, loss_dice: 0.500188
[10:58:26.337] TRAIN: iteration 19206 : loss : 0.211982, loss_ce: 0.013178, loss_dice: 0.410786
[10:58:26.546] TRAIN: iteration 19207 : loss : 0.046469, loss_ce: 0.004597, loss_dice: 0.088342
[10:58:26.755] TRAIN: iteration 19208 : loss : 0.094244, loss_ce: 0.009206, loss_dice: 0.179281
[10:58:26.964] TRAIN: iteration 19209 : loss : 0.045572, loss_ce: 0.003886, loss_dice: 0.087259
[10:58:27.173] TRAIN: iteration 19210 : loss : 0.072715, loss_ce: 0.004268, loss_dice: 0.141162
[10:58:27.381] TRAIN: iteration 19211 : loss : 0.120227, loss_ce: 0.003477, loss_dice: 0.236976
[10:58:27.588] TRAIN: iteration 19212 : loss : 0.057780, loss_ce: 0.002463, loss_dice: 0.113096
[10:58:27.795] TRAIN: iteration 19213 : loss : 0.138290, loss_ce: 0.005894, loss_dice: 0.270685
[10:58:28.003] TRAIN: iteration 19214 : loss : 0.220980, loss_ce: 0.003170, loss_dice: 0.438791
[10:58:28.211] TRAIN: iteration 19215 : loss : 0.081906, loss_ce: 0.001972, loss_dice: 0.161839
[10:58:28.419] TRAIN: iteration 19216 : loss : 0.074104, loss_ce: 0.002005, loss_dice: 0.146204
[10:58:28.626] TRAIN: iteration 19217 : loss : 0.168221, loss_ce: 0.004022, loss_dice: 0.332419
[10:58:28.835] TRAIN: iteration 19218 : loss : 0.068732, loss_ce: 0.002874, loss_dice: 0.134590
[10:58:29.043] TRAIN: iteration 19219 : loss : 0.069377, loss_ce: 0.006169, loss_dice: 0.132586
[10:58:29.250] TRAIN: iteration 19220 : loss : 0.071665, loss_ce: 0.002619, loss_dice: 0.140711
[10:58:29.490] TRAIN: iteration 19221 : loss : 0.102066, loss_ce: 0.003841, loss_dice: 0.200290
[10:58:29.699] TRAIN: iteration 19222 : loss : 0.138721, loss_ce: 0.005978, loss_dice: 0.271464
[10:58:29.906] TRAIN: iteration 19223 : loss : 0.056298, loss_ce: 0.002638, loss_dice: 0.109958
[10:58:30.114] TRAIN: iteration 19224 : loss : 0.040832, loss_ce: 0.002223, loss_dice: 0.079442
[10:58:30.321] TRAIN: iteration 19225 : loss : 0.133996, loss_ce: 0.006597, loss_dice: 0.261396
[10:58:30.528] TRAIN: iteration 19226 : loss : 0.168640, loss_ce: 0.010297, loss_dice: 0.326983
[10:58:30.736] TRAIN: iteration 19227 : loss : 0.049016, loss_ce: 0.001967, loss_dice: 0.096065
[10:58:30.944] TRAIN: iteration 19228 : loss : 0.084311, loss_ce: 0.005102, loss_dice: 0.163520
[10:58:31.152] TRAIN: iteration 19229 : loss : 0.250448, loss_ce: 0.000873, loss_dice: 0.500023
[10:58:31.359] TRAIN: iteration 19230 : loss : 0.145615, loss_ce: 0.010249, loss_dice: 0.280981
[10:58:31.567] TRAIN: iteration 19231 : loss : 0.055784, loss_ce: 0.003644, loss_dice: 0.107924
[10:58:31.774] TRAIN: iteration 19232 : loss : 0.020584, loss_ce: 0.000995, loss_dice: 0.040173
[10:58:31.983] TRAIN: iteration 19233 : loss : 0.210036, loss_ce: 0.004944, loss_dice: 0.415128
[10:58:32.192] TRAIN: iteration 19234 : loss : 0.050706, loss_ce: 0.002347, loss_dice: 0.099065
[10:58:32.401] TRAIN: iteration 19235 : loss : 0.038603, loss_ce: 0.001476, loss_dice: 0.075729
[10:58:32.612] TRAIN: iteration 19236 : loss : 0.112594, loss_ce: 0.002848, loss_dice: 0.222341
[10:58:32.822] TRAIN: iteration 19237 : loss : 0.041658, loss_ce: 0.000801, loss_dice: 0.082516
[10:58:33.031] TRAIN: iteration 19238 : loss : 0.070647, loss_ce: 0.005076, loss_dice: 0.136219
[10:58:33.239] TRAIN: iteration 19239 : loss : 0.093689, loss_ce: 0.002759, loss_dice: 0.184619
[10:58:33.448] TRAIN: iteration 19240 : loss : 0.144759, loss_ce: 0.007376, loss_dice: 0.282142
[10:58:33.687] TRAIN: iteration 19241 : loss : 0.077050, loss_ce: 0.003471, loss_dice: 0.150630
[10:58:33.897] TRAIN: iteration 19242 : loss : 0.250932, loss_ce: 0.001772, loss_dice: 0.500092
[10:58:34.111] TRAIN: iteration 19243 : loss : 0.251050, loss_ce: 0.001987, loss_dice: 0.500113
[10:58:34.324] TRAIN: iteration 19244 : loss : 0.040899, loss_ce: 0.003178, loss_dice: 0.078620
[10:58:34.534] TRAIN: iteration 19245 : loss : 0.044738, loss_ce: 0.003040, loss_dice: 0.086435
[10:58:34.750] TRAIN: iteration 19246 : loss : 0.239275, loss_ce: 0.004203, loss_dice: 0.474347
[10:58:34.958] TRAIN: iteration 19247 : loss : 0.250483, loss_ce: 0.002884, loss_dice: 0.498081
[10:58:35.172] TRAIN: iteration 19248 : loss : 0.057064, loss_ce: 0.002800, loss_dice: 0.111328
[10:58:36.324] TRAIN: iteration 19249 : loss : 0.078856, loss_ce: 0.004093, loss_dice: 0.153619
[10:58:36.532] TRAIN: iteration 19250 : loss : 0.055054, loss_ce: 0.001762, loss_dice: 0.108347
[10:58:36.747] TRAIN: iteration 19251 : loss : 0.061274, loss_ce: 0.001754, loss_dice: 0.120795
[10:58:36.956] TRAIN: iteration 19252 : loss : 0.072720, loss_ce: 0.004590, loss_dice: 0.140851
[10:58:37.170] TRAIN: iteration 19253 : loss : 0.077561, loss_ce: 0.001514, loss_dice: 0.153607
[10:58:37.377] TRAIN: iteration 19254 : loss : 0.046201, loss_ce: 0.000847, loss_dice: 0.091554
[10:58:37.585] TRAIN: iteration 19255 : loss : 0.185770, loss_ce: 0.003111, loss_dice: 0.368429
[10:58:37.792] TRAIN: iteration 19256 : loss : 0.178237, loss_ce: 0.010350, loss_dice: 0.346123
[10:58:38.255] TRAIN: iteration 19257 : loss : 0.039141, loss_ce: 0.002527, loss_dice: 0.075754
[10:58:38.479] TRAIN: iteration 19258 : loss : 0.145194, loss_ce: 0.006944, loss_dice: 0.283443
[10:58:38.688] TRAIN: iteration 19259 : loss : 0.110978, loss_ce: 0.005594, loss_dice: 0.216363
[10:58:38.897] TRAIN: iteration 19260 : loss : 0.248779, loss_ce: 0.001330, loss_dice: 0.496228
[10:58:39.133] TRAIN: iteration 19261 : loss : 0.086965, loss_ce: 0.006770, loss_dice: 0.167160
[10:58:39.349] TRAIN: iteration 19262 : loss : 0.076543, loss_ce: 0.004151, loss_dice: 0.148934
[10:58:39.564] TRAIN: iteration 19263 : loss : 0.068363, loss_ce: 0.004560, loss_dice: 0.132167
[10:58:39.775] TRAIN: iteration 19264 : loss : 0.140177, loss_ce: 0.001134, loss_dice: 0.279221
[10:58:39.986] TRAIN: iteration 19265 : loss : 0.251210, loss_ce: 0.002267, loss_dice: 0.500154
[10:58:40.194] TRAIN: iteration 19266 : loss : 0.025111, loss_ce: 0.001425, loss_dice: 0.048797
[10:58:40.403] TRAIN: iteration 19267 : loss : 0.079979, loss_ce: 0.007818, loss_dice: 0.152139
[10:58:40.611] TRAIN: iteration 19268 : loss : 0.258128, loss_ce: 0.016597, loss_dice: 0.499659
[10:58:40.820] TRAIN: iteration 19269 : loss : 0.224642, loss_ce: 0.001793, loss_dice: 0.447490
[10:58:41.031] TRAIN: iteration 19270 : loss : 0.120266, loss_ce: 0.035168, loss_dice: 0.205364
[10:58:41.241] TRAIN: iteration 19271 : loss : 0.250779, loss_ce: 0.001490, loss_dice: 0.500069
[10:58:41.456] TRAIN: iteration 19272 : loss : 0.136154, loss_ce: 0.004796, loss_dice: 0.267511
[10:58:41.664] TRAIN: iteration 19273 : loss : 0.118191, loss_ce: 0.004329, loss_dice: 0.232053
[10:58:41.872] TRAIN: iteration 19274 : loss : 0.151508, loss_ce: 0.001626, loss_dice: 0.301389
[10:58:42.084] TRAIN: iteration 19275 : loss : 0.175227, loss_ce: 0.003597, loss_dice: 0.346857
[10:58:42.292] TRAIN: iteration 19276 : loss : 0.075576, loss_ce: 0.004194, loss_dice: 0.146958
[10:58:42.502] TRAIN: iteration 19277 : loss : 0.138537, loss_ce: 0.008933, loss_dice: 0.268141
[10:58:42.710] TRAIN: iteration 19278 : loss : 0.048654, loss_ce: 0.002668, loss_dice: 0.094639
[10:58:42.922] TRAIN: iteration 19279 : loss : 0.249683, loss_ce: 0.002479, loss_dice: 0.496887
[10:58:43.130] TRAIN: iteration 19280 : loss : 0.014206, loss_ce: 0.001156, loss_dice: 0.027255
[10:58:43.375] TRAIN: iteration 19281 : loss : 0.161927, loss_ce: 0.004945, loss_dice: 0.318909
[10:58:43.590] TRAIN: iteration 19282 : loss : 0.067756, loss_ce: 0.004022, loss_dice: 0.131490
[10:58:43.801] TRAIN: iteration 19283 : loss : 0.056161, loss_ce: 0.005245, loss_dice: 0.107077
[10:58:44.010] TRAIN: iteration 19284 : loss : 0.081102, loss_ce: 0.007687, loss_dice: 0.154516
[10:58:44.218] TRAIN: iteration 19285 : loss : 0.093355, loss_ce: 0.005029, loss_dice: 0.181681
[10:58:44.429] TRAIN: iteration 19286 : loss : 0.038445, loss_ce: 0.002162, loss_dice: 0.074728
[10:58:44.640] TRAIN: iteration 19287 : loss : 0.168723, loss_ce: 0.002387, loss_dice: 0.335059
[10:58:44.850] TRAIN: iteration 19288 : loss : 0.206423, loss_ce: 0.009050, loss_dice: 0.403797
[10:58:45.060] TRAIN: iteration 19289 : loss : 0.229509, loss_ce: 0.002536, loss_dice: 0.456481
[10:58:45.271] TRAIN: iteration 19290 : loss : 0.090711, loss_ce: 0.007072, loss_dice: 0.174350
[10:58:45.529] TRAIN: iteration 19291 : loss : 0.200389, loss_ce: 0.002995, loss_dice: 0.397783
[10:58:45.742] TRAIN: iteration 19292 : loss : 0.161332, loss_ce: 0.004474, loss_dice: 0.318191
[10:58:45.951] TRAIN: iteration 19293 : loss : 0.066926, loss_ce: 0.004875, loss_dice: 0.128977
[10:58:46.160] TRAIN: iteration 19294 : loss : 0.035013, loss_ce: 0.002191, loss_dice: 0.067834
[10:58:46.369] TRAIN: iteration 19295 : loss : 0.049715, loss_ce: 0.002518, loss_dice: 0.096913
[10:58:46.577] TRAIN: iteration 19296 : loss : 0.071949, loss_ce: 0.004134, loss_dice: 0.139764
[10:58:46.788] TRAIN: iteration 19297 : loss : 0.153320, loss_ce: 0.001481, loss_dice: 0.305158
[10:58:46.995] TRAIN: iteration 19298 : loss : 0.040156, loss_ce: 0.000878, loss_dice: 0.079435
[10:58:47.211] TRAIN: iteration 19299 : loss : 0.073224, loss_ce: 0.002654, loss_dice: 0.143794
[10:58:47.418] TRAIN: iteration 19300 : loss : 0.084971, loss_ce: 0.002922, loss_dice: 0.167020
[10:58:47.658] TRAIN: iteration 19301 : loss : 0.250388, loss_ce: 0.000765, loss_dice: 0.500010
[10:58:47.867] TRAIN: iteration 19302 : loss : 0.064855, loss_ce: 0.000906, loss_dice: 0.128804
[10:58:48.086] TRAIN: iteration 19303 : loss : 0.099549, loss_ce: 0.001240, loss_dice: 0.197857
[10:58:48.295] TRAIN: iteration 19304 : loss : 0.247856, loss_ce: 0.002440, loss_dice: 0.493271
[10:58:48.502] TRAIN: iteration 19305 : loss : 0.173455, loss_ce: 0.011775, loss_dice: 0.335134
[10:58:48.710] TRAIN: iteration 19306 : loss : 0.028765, loss_ce: 0.001368, loss_dice: 0.056162
[10:58:48.919] TRAIN: iteration 19307 : loss : 0.188358, loss_ce: 0.033509, loss_dice: 0.343208
[10:58:49.127] TRAIN: iteration 19308 : loss : 0.133577, loss_ce: 0.003455, loss_dice: 0.263698
[10:58:49.355] TRAIN: iteration 19309 : loss : 0.251060, loss_ce: 0.002357, loss_dice: 0.499764
[10:58:49.563] TRAIN: iteration 19310 : loss : 0.163442, loss_ce: 0.003895, loss_dice: 0.322989
[10:58:49.771] TRAIN: iteration 19311 : loss : 0.228335, loss_ce: 0.001359, loss_dice: 0.455311
[10:58:49.979] TRAIN: iteration 19312 : loss : 0.126000, loss_ce: 0.012803, loss_dice: 0.239197
[10:58:50.192] TRAIN: iteration 19313 : loss : 0.045798, loss_ce: 0.002606, loss_dice: 0.088991
[10:58:50.403] TRAIN: iteration 19314 : loss : 0.154029, loss_ce: 0.003627, loss_dice: 0.304432
[10:58:50.612] TRAIN: iteration 19315 : loss : 0.097809, loss_ce: 0.008161, loss_dice: 0.187458
[10:58:50.820] TRAIN: iteration 19316 : loss : 0.058752, loss_ce: 0.007295, loss_dice: 0.110208
[10:58:51.029] TRAIN: iteration 19317 : loss : 0.045770, loss_ce: 0.003898, loss_dice: 0.087642
[10:58:51.238] TRAIN: iteration 19318 : loss : 0.251785, loss_ce: 0.003364, loss_dice: 0.500207
[10:58:51.449] TRAIN: iteration 19319 : loss : 0.197477, loss_ce: 0.006158, loss_dice: 0.388796
[10:58:51.658] TRAIN: iteration 19320 : loss : 0.125825, loss_ce: 0.010731, loss_dice: 0.240919
[10:58:51.887] TRAIN: iteration 19321 : loss : 0.197572, loss_ce: 0.004180, loss_dice: 0.390963
[10:58:52.094] TRAIN: iteration 19322 : loss : 0.218190, loss_ce: 0.009704, loss_dice: 0.426676
[10:58:52.301] TRAIN: iteration 19323 : loss : 0.134293, loss_ce: 0.002788, loss_dice: 0.265799
[10:58:52.522] TRAIN: iteration 19324 : loss : 0.126911, loss_ce: 0.007563, loss_dice: 0.246258
[10:58:52.733] TRAIN: iteration 19325 : loss : 0.251394, loss_ce: 0.013958, loss_dice: 0.488830
[10:58:52.942] TRAIN: iteration 19326 : loss : 0.221306, loss_ce: 0.003880, loss_dice: 0.438733
[10:58:53.150] TRAIN: iteration 19327 : loss : 0.125068, loss_ce: 0.009856, loss_dice: 0.240280
[10:58:53.358] TRAIN: iteration 19328 : loss : 0.085029, loss_ce: 0.004294, loss_dice: 0.165763
[10:58:53.565] TRAIN: iteration 19329 : loss : 0.127805, loss_ce: 0.006092, loss_dice: 0.249519
[10:58:53.774] TRAIN: iteration 19330 : loss : 0.072118, loss_ce: 0.003595, loss_dice: 0.140641
[10:58:53.985] TRAIN: iteration 19331 : loss : 0.173438, loss_ce: 0.002615, loss_dice: 0.344261
[10:58:54.193] TRAIN: iteration 19332 : loss : 0.038867, loss_ce: 0.001068, loss_dice: 0.076666
[10:58:54.400] TRAIN: iteration 19333 : loss : 0.138768, loss_ce: 0.002803, loss_dice: 0.274733
[10:58:54.611] TRAIN: iteration 19334 : loss : 0.250435, loss_ce: 0.000871, loss_dice: 0.499998
[10:58:54.819] TRAIN: iteration 19335 : loss : 0.069224, loss_ce: 0.001595, loss_dice: 0.136853
[10:58:55.031] TRAIN: iteration 19336 : loss : 0.043851, loss_ce: 0.002062, loss_dice: 0.085640
[10:58:55.244] TRAIN: iteration 19337 : loss : 0.116187, loss_ce: 0.005934, loss_dice: 0.226440
[10:58:55.454] TRAIN: iteration 19338 : loss : 0.119575, loss_ce: 0.004216, loss_dice: 0.234935
[10:58:55.668] TRAIN: iteration 19339 : loss : 0.126289, loss_ce: 0.002150, loss_dice: 0.250427
[10:58:55.888] TRAIN: iteration 19340 : loss : 0.084323, loss_ce: 0.002460, loss_dice: 0.166186
[10:58:56.130] TRAIN: iteration 19341 : loss : 0.088056, loss_ce: 0.006212, loss_dice: 0.169901
[10:58:56.337] TRAIN: iteration 19342 : loss : 0.121460, loss_ce: 0.003022, loss_dice: 0.239897
[10:58:56.544] TRAIN: iteration 19343 : loss : 0.178152, loss_ce: 0.002461, loss_dice: 0.353844
[10:58:56.753] TRAIN: iteration 19344 : loss : 0.071395, loss_ce: 0.002209, loss_dice: 0.140580
[10:58:56.960] TRAIN: iteration 19345 : loss : 0.062845, loss_ce: 0.001266, loss_dice: 0.124425
[10:58:57.168] TRAIN: iteration 19346 : loss : 0.089048, loss_ce: 0.002146, loss_dice: 0.175950
[10:58:57.375] TRAIN: iteration 19347 : loss : 0.065792, loss_ce: 0.001479, loss_dice: 0.130105
[10:58:57.582] TRAIN: iteration 19348 : loss : 0.051880, loss_ce: 0.002615, loss_dice: 0.101145
[10:58:57.789] TRAIN: iteration 19349 : loss : 0.090275, loss_ce: 0.007616, loss_dice: 0.172934
[10:58:57.999] TRAIN: iteration 19350 : loss : 0.101673, loss_ce: 0.006350, loss_dice: 0.196997
[10:58:58.207] TRAIN: iteration 19351 : loss : 0.223561, loss_ce: 0.002891, loss_dice: 0.444230
[10:58:58.414] TRAIN: iteration 19352 : loss : 0.053907, loss_ce: 0.003941, loss_dice: 0.103873
[10:58:58.622] TRAIN: iteration 19353 : loss : 0.117801, loss_ce: 0.010194, loss_dice: 0.225409
[10:58:58.830] TRAIN: iteration 19354 : loss : 0.127323, loss_ce: 0.003022, loss_dice: 0.251624
[10:58:59.043] TRAIN: iteration 19355 : loss : 0.051975, loss_ce: 0.004733, loss_dice: 0.099216
[10:58:59.251] TRAIN: iteration 19356 : loss : 0.056843, loss_ce: 0.006609, loss_dice: 0.107078
[10:58:59.458] TRAIN: iteration 19357 : loss : 0.052728, loss_ce: 0.009086, loss_dice: 0.096370
[10:58:59.667] TRAIN: iteration 19358 : loss : 0.144468, loss_ce: 0.005964, loss_dice: 0.282972
[10:58:59.878] TRAIN: iteration 19359 : loss : 0.074010, loss_ce: 0.004092, loss_dice: 0.143927
[10:59:00.087] TRAIN: iteration 19360 : loss : 0.099777, loss_ce: 0.004712, loss_dice: 0.194842
[10:59:00.334] TRAIN: iteration 19361 : loss : 0.072991, loss_ce: 0.006009, loss_dice: 0.139972
[10:59:00.542] TRAIN: iteration 19362 : loss : 0.065340, loss_ce: 0.002428, loss_dice: 0.128251
[10:59:00.750] TRAIN: iteration 19363 : loss : 0.251468, loss_ce: 0.002753, loss_dice: 0.500183
[10:59:00.960] TRAIN: iteration 19364 : loss : 0.065507, loss_ce: 0.002685, loss_dice: 0.128329
[10:59:01.174] TRAIN: iteration 19365 : loss : 0.089244, loss_ce: 0.001064, loss_dice: 0.177425
[10:59:01.381] TRAIN: iteration 19366 : loss : 0.065628, loss_ce: 0.009075, loss_dice: 0.122182
[10:59:01.589] TRAIN: iteration 19367 : loss : 0.094724, loss_ce: 0.002194, loss_dice: 0.187255
[10:59:01.798] TRAIN: iteration 19368 : loss : 0.056113, loss_ce: 0.005345, loss_dice: 0.106881
[10:59:02.008] TRAIN: iteration 19369 : loss : 0.080452, loss_ce: 0.002942, loss_dice: 0.157963
[10:59:02.223] TRAIN: iteration 19370 : loss : 0.062574, loss_ce: 0.002298, loss_dice: 0.122851
[10:59:02.436] TRAIN: iteration 19371 : loss : 0.124671, loss_ce: 0.005974, loss_dice: 0.243368
[10:59:02.646] TRAIN: iteration 19372 : loss : 0.095959, loss_ce: 0.003441, loss_dice: 0.188477
[10:59:02.856] TRAIN: iteration 19373 : loss : 0.052488, loss_ce: 0.002856, loss_dice: 0.102119
[10:59:03.064] TRAIN: iteration 19374 : loss : 0.047457, loss_ce: 0.002715, loss_dice: 0.092200
[10:59:03.279] TRAIN: iteration 19375 : loss : 0.073650, loss_ce: 0.002265, loss_dice: 0.145034
[10:59:03.494] TRAIN: iteration 19376 : loss : 0.258661, loss_ce: 0.017122, loss_dice: 0.500199
[10:59:03.709] TRAIN: iteration 19377 : loss : 0.117523, loss_ce: 0.011949, loss_dice: 0.223098
[10:59:03.920] TRAIN: iteration 19378 : loss : 0.130080, loss_ce: 0.019670, loss_dice: 0.240489
[10:59:04.128] TRAIN: iteration 19379 : loss : 0.119398, loss_ce: 0.010387, loss_dice: 0.228409
[10:59:04.336] TRAIN: iteration 19380 : loss : 0.021897, loss_ce: 0.001196, loss_dice: 0.042598
[10:59:04.580] TRAIN: iteration 19381 : loss : 0.183117, loss_ce: 0.011738, loss_dice: 0.354497
[10:59:04.789] TRAIN: iteration 19382 : loss : 0.088774, loss_ce: 0.001456, loss_dice: 0.176093
[10:59:04.997] TRAIN: iteration 19383 : loss : 0.069401, loss_ce: 0.002053, loss_dice: 0.136749
[10:59:05.212] TRAIN: iteration 19384 : loss : 0.250887, loss_ce: 0.001669, loss_dice: 0.500106
[10:59:05.427] TRAIN: iteration 19385 : loss : 0.145065, loss_ce: 0.009825, loss_dice: 0.280306
[10:59:05.634] TRAIN: iteration 19386 : loss : 0.063149, loss_ce: 0.006992, loss_dice: 0.119307
[10:59:05.841] TRAIN: iteration 19387 : loss : 0.093916, loss_ce: 0.001944, loss_dice: 0.185887
[10:59:06.048] TRAIN: iteration 19388 : loss : 0.148769, loss_ce: 0.002138, loss_dice: 0.295401
[10:59:06.256] TRAIN: iteration 19389 : loss : 0.085868, loss_ce: 0.005890, loss_dice: 0.165847
[10:59:06.528] TRAIN: iteration 19390 : loss : 0.076926, loss_ce: 0.003302, loss_dice: 0.150551
[10:59:06.737] TRAIN: iteration 19391 : loss : 0.136718, loss_ce: 0.011218, loss_dice: 0.262219
[10:59:06.944] TRAIN: iteration 19392 : loss : 0.108867, loss_ce: 0.001277, loss_dice: 0.216457
[10:59:07.152] TRAIN: iteration 19393 : loss : 0.155022, loss_ce: 0.001865, loss_dice: 0.308179
[10:59:07.360] TRAIN: iteration 19394 : loss : 0.119037, loss_ce: 0.017093, loss_dice: 0.220980
[10:59:07.568] TRAIN: iteration 19395 : loss : 0.126950, loss_ce: 0.001806, loss_dice: 0.252094
[10:59:07.776] TRAIN: iteration 19396 : loss : 0.160064, loss_ce: 0.001027, loss_dice: 0.319101
[10:59:07.984] TRAIN: iteration 19397 : loss : 0.077088, loss_ce: 0.005095, loss_dice: 0.149081
[10:59:08.192] TRAIN: iteration 19398 : loss : 0.039060, loss_ce: 0.001643, loss_dice: 0.076478
[10:59:08.401] TRAIN: iteration 19399 : loss : 0.125570, loss_ce: 0.002189, loss_dice: 0.248951
[10:59:08.610] TRAIN: iteration 19400 : loss : 0.112746, loss_ce: 0.013907, loss_dice: 0.211584
[10:59:08.846] TRAIN: iteration 19401 : loss : 0.028529, loss_ce: 0.001826, loss_dice: 0.055232
[10:59:09.054] TRAIN: iteration 19402 : loss : 0.092583, loss_ce: 0.008949, loss_dice: 0.176216
[10:59:09.261] TRAIN: iteration 19403 : loss : 0.081237, loss_ce: 0.005126, loss_dice: 0.157348
[10:59:09.470] TRAIN: iteration 19404 : loss : 0.196635, loss_ce: 0.013755, loss_dice: 0.379515
[10:59:09.684] TRAIN: iteration 19405 : loss : 0.166616, loss_ce: 0.001783, loss_dice: 0.331449
[10:59:09.892] TRAIN: iteration 19406 : loss : 0.054815, loss_ce: 0.001302, loss_dice: 0.108328
[10:59:10.103] TRAIN: iteration 19407 : loss : 0.060975, loss_ce: 0.002378, loss_dice: 0.119572
[10:59:10.313] TRAIN: iteration 19408 : loss : 0.050481, loss_ce: 0.002045, loss_dice: 0.098917
[10:59:10.533] TRAIN: iteration 19409 : loss : 0.147566, loss_ce: 0.004563, loss_dice: 0.290570
[10:59:10.746] TRAIN: iteration 19410 : loss : 0.035906, loss_ce: 0.005310, loss_dice: 0.066503
[10:59:10.970] TRAIN: iteration 19411 : loss : 0.059233, loss_ce: 0.003280, loss_dice: 0.115185
[10:59:11.178] TRAIN: iteration 19412 : loss : 0.044325, loss_ce: 0.003498, loss_dice: 0.085153
[10:59:11.387] TRAIN: iteration 19413 : loss : 0.075813, loss_ce: 0.002194, loss_dice: 0.149432
[10:59:11.600] TRAIN: iteration 19414 : loss : 0.087479, loss_ce: 0.004412, loss_dice: 0.170545
[10:59:11.808] TRAIN: iteration 19415 : loss : 0.196342, loss_ce: 0.001792, loss_dice: 0.390893
[10:59:12.023] TRAIN: iteration 19416 : loss : 0.095488, loss_ce: 0.001908, loss_dice: 0.189068
[10:59:12.247] TRAIN: iteration 19417 : loss : 0.123707, loss_ce: 0.005835, loss_dice: 0.241579
[10:59:12.463] TRAIN: iteration 19418 : loss : 0.209246, loss_ce: 0.004750, loss_dice: 0.413741
[10:59:12.674] TRAIN: iteration 19419 : loss : 0.120138, loss_ce: 0.002706, loss_dice: 0.237570
[10:59:12.883] TRAIN: iteration 19420 : loss : 0.081044, loss_ce: 0.002595, loss_dice: 0.159493
[10:59:13.118] TRAIN: iteration 19421 : loss : 0.250432, loss_ce: 0.000851, loss_dice: 0.500013
[10:59:13.333] TRAIN: iteration 19422 : loss : 0.225184, loss_ce: 0.012398, loss_dice: 0.437970
[10:59:13.542] TRAIN: iteration 19423 : loss : 0.156048, loss_ce: 0.009527, loss_dice: 0.302569
[10:59:13.751] TRAIN: iteration 19424 : loss : 0.086658, loss_ce: 0.006636, loss_dice: 0.166680
[10:59:13.958] TRAIN: iteration 19425 : loss : 0.045490, loss_ce: 0.001730, loss_dice: 0.089250
[10:59:14.171] TRAIN: iteration 19426 : loss : 0.124680, loss_ce: 0.002334, loss_dice: 0.247026
[10:59:14.381] TRAIN: iteration 19427 : loss : 0.099981, loss_ce: 0.005645, loss_dice: 0.194318
[10:59:14.592] TRAIN: iteration 19428 : loss : 0.042132, loss_ce: 0.002939, loss_dice: 0.081324
[10:59:14.802] TRAIN: iteration 19429 : loss : 0.125884, loss_ce: 0.004034, loss_dice: 0.247733
[10:59:15.012] TRAIN: iteration 19430 : loss : 0.154772, loss_ce: 0.003810, loss_dice: 0.305734
[10:59:15.223] TRAIN: iteration 19431 : loss : 0.100929, loss_ce: 0.007016, loss_dice: 0.194842
[10:59:15.431] TRAIN: iteration 19432 : loss : 0.039330, loss_ce: 0.008733, loss_dice: 0.069927
[10:59:15.639] TRAIN: iteration 19433 : loss : 0.169251, loss_ce: 0.009379, loss_dice: 0.329123
[10:59:15.849] TRAIN: iteration 19434 : loss : 0.057608, loss_ce: 0.006888, loss_dice: 0.108329
[10:59:16.064] TRAIN: iteration 19435 : loss : 0.100685, loss_ce: 0.004171, loss_dice: 0.197199
[10:59:16.275] TRAIN: iteration 19436 : loss : 0.221506, loss_ce: 0.008688, loss_dice: 0.434324
[10:59:16.485] TRAIN: iteration 19437 : loss : 0.251362, loss_ce: 0.006285, loss_dice: 0.496439
[10:59:16.695] TRAIN: iteration 19438 : loss : 0.238708, loss_ce: 0.005489, loss_dice: 0.471927
[10:59:16.905] TRAIN: iteration 19439 : loss : 0.233233, loss_ce: 0.007270, loss_dice: 0.459196
[10:59:17.115] TRAIN: iteration 19440 : loss : 0.226557, loss_ce: 0.005561, loss_dice: 0.447552
[10:59:17.115] NaN or Inf found in input tensor.
[10:59:17.330] TRAIN: iteration 19441 : loss : 0.195821, loss_ce: 0.004783, loss_dice: 0.386859
[10:59:17.537] TRAIN: iteration 19442 : loss : 0.039301, loss_ce: 0.004185, loss_dice: 0.074417
[10:59:17.746] TRAIN: iteration 19443 : loss : 0.251295, loss_ce: 0.002452, loss_dice: 0.500139
[10:59:17.953] TRAIN: iteration 19444 : loss : 0.087755, loss_ce: 0.003212, loss_dice: 0.172299
[10:59:18.161] TRAIN: iteration 19445 : loss : 0.217660, loss_ce: 0.008694, loss_dice: 0.426625
[10:59:18.371] TRAIN: iteration 19446 : loss : 0.139512, loss_ce: 0.010226, loss_dice: 0.268798
[10:59:18.586] TRAIN: iteration 19447 : loss : 0.051502, loss_ce: 0.009711, loss_dice: 0.093292
[10:59:18.796] TRAIN: iteration 19448 : loss : 0.124679, loss_ce: 0.008821, loss_dice: 0.240536
[10:59:19.004] TRAIN: iteration 19449 : loss : 0.252253, loss_ce: 0.004192, loss_dice: 0.500314
[10:59:19.214] TRAIN: iteration 19450 : loss : 0.175183, loss_ce: 0.004440, loss_dice: 0.345925
[10:59:19.427] TRAIN: iteration 19451 : loss : 0.082310, loss_ce: 0.011959, loss_dice: 0.152661
[10:59:19.637] TRAIN: iteration 19452 : loss : 0.222714, loss_ce: 0.002909, loss_dice: 0.442520
[10:59:19.845] TRAIN: iteration 19453 : loss : 0.090000, loss_ce: 0.005253, loss_dice: 0.174747
[10:59:20.055] TRAIN: iteration 19454 : loss : 0.072679, loss_ce: 0.010783, loss_dice: 0.134575
[10:59:20.262] TRAIN: iteration 19455 : loss : 0.134116, loss_ce: 0.005393, loss_dice: 0.262839
[10:59:20.472] TRAIN: iteration 19456 : loss : 0.175148, loss_ce: 0.007276, loss_dice: 0.343019
[10:59:20.687] TRAIN: iteration 19457 : loss : 0.252242, loss_ce: 0.004203, loss_dice: 0.500282
[10:59:20.897] TRAIN: iteration 19458 : loss : 0.238847, loss_ce: 0.003084, loss_dice: 0.474609
[10:59:21.107] TRAIN: iteration 19459 : loss : 0.097511, loss_ce: 0.002791, loss_dice: 0.192231
[10:59:21.321] TRAIN: iteration 19460 : loss : 0.049090, loss_ce: 0.004821, loss_dice: 0.093359
[10:59:21.557] TRAIN: iteration 19461 : loss : 0.234582, loss_ce: 0.002561, loss_dice: 0.466602
[10:59:21.765] TRAIN: iteration 19462 : loss : 0.073470, loss_ce: 0.006323, loss_dice: 0.140617
[10:59:21.973] TRAIN: iteration 19463 : loss : 0.232814, loss_ce: 0.017359, loss_dice: 0.448270
[10:59:22.182] TRAIN: iteration 19464 : loss : 0.100593, loss_ce: 0.007704, loss_dice: 0.193482
[10:59:22.392] TRAIN: iteration 19465 : loss : 0.113116, loss_ce: 0.001974, loss_dice: 0.224259
[10:59:22.602] TRAIN: iteration 19466 : loss : 0.097064, loss_ce: 0.009367, loss_dice: 0.184760
[10:59:22.811] TRAIN: iteration 19467 : loss : 0.100322, loss_ce: 0.001730, loss_dice: 0.198915
[10:59:23.022] TRAIN: iteration 19468 : loss : 0.145398, loss_ce: 0.001993, loss_dice: 0.288803
[10:59:23.233] TRAIN: iteration 19469 : loss : 0.242579, loss_ce: 0.004037, loss_dice: 0.481122
[10:59:23.442] TRAIN: iteration 19470 : loss : 0.055063, loss_ce: 0.003119, loss_dice: 0.107006
[10:59:23.650] TRAIN: iteration 19471 : loss : 0.084154, loss_ce: 0.004746, loss_dice: 0.163563
[10:59:23.859] TRAIN: iteration 19472 : loss : 0.251289, loss_ce: 0.002419, loss_dice: 0.500159
[10:59:24.068] TRAIN: iteration 19473 : loss : 0.126941, loss_ce: 0.019032, loss_dice: 0.234850
[10:59:24.279] TRAIN: iteration 19474 : loss : 0.245769, loss_ce: 0.004227, loss_dice: 0.487311
[10:59:24.488] TRAIN: iteration 19475 : loss : 0.063995, loss_ce: 0.002514, loss_dice: 0.125476
[10:59:24.702] TRAIN: iteration 19476 : loss : 0.188202, loss_ce: 0.004768, loss_dice: 0.371636
[10:59:24.917] TRAIN: iteration 19477 : loss : 0.251660, loss_ce: 0.003114, loss_dice: 0.500207
[10:59:25.131] TRAIN: iteration 19478 : loss : 0.057833, loss_ce: 0.002380, loss_dice: 0.113287
[10:59:25.544] TRAIN: iteration 19479 : loss : 0.076517, loss_ce: 0.001459, loss_dice: 0.151575
[10:59:25.755] TRAIN: iteration 19480 : loss : 0.074588, loss_ce: 0.002905, loss_dice: 0.146270
[10:59:25.994] TRAIN: iteration 19481 : loss : 0.040525, loss_ce: 0.002763, loss_dice: 0.078287
[10:59:26.204] TRAIN: iteration 19482 : loss : 0.071398, loss_ce: 0.001904, loss_dice: 0.140892
[10:59:26.419] TRAIN: iteration 19483 : loss : 0.067159, loss_ce: 0.003909, loss_dice: 0.130410
[10:59:26.634] TRAIN: iteration 19484 : loss : 0.232435, loss_ce: 0.004946, loss_dice: 0.459925
[10:59:26.841] TRAIN: iteration 19485 : loss : 0.079023, loss_ce: 0.007163, loss_dice: 0.150883
[10:59:27.050] TRAIN: iteration 19486 : loss : 0.103840, loss_ce: 0.004045, loss_dice: 0.203634
[10:59:27.260] TRAIN: iteration 19487 : loss : 0.221752, loss_ce: 0.003689, loss_dice: 0.439815
[10:59:27.467] TRAIN: iteration 19488 : loss : 0.132089, loss_ce: 0.003162, loss_dice: 0.261015
[10:59:27.674] TRAIN: iteration 19489 : loss : 0.251437, loss_ce: 0.002690, loss_dice: 0.500183
[10:59:27.882] TRAIN: iteration 19490 : loss : 0.026078, loss_ce: 0.001767, loss_dice: 0.050390
[10:59:28.092] TRAIN: iteration 19491 : loss : 0.153326, loss_ce: 0.003926, loss_dice: 0.302727
[10:59:28.303] TRAIN: iteration 19492 : loss : 0.064034, loss_ce: 0.004043, loss_dice: 0.124025
[10:59:28.514] TRAIN: iteration 19493 : loss : 0.169156, loss_ce: 0.002755, loss_dice: 0.335556
[10:59:28.732] TRAIN: iteration 19494 : loss : 0.096655, loss_ce: 0.003147, loss_dice: 0.190163
[10:59:28.947] TRAIN: iteration 19495 : loss : 0.252453, loss_ce: 0.004595, loss_dice: 0.500310
[10:59:29.155] TRAIN: iteration 19496 : loss : 0.248082, loss_ce: 0.002228, loss_dice: 0.493935
[10:59:29.370] TRAIN: iteration 19497 : loss : 0.063549, loss_ce: 0.002810, loss_dice: 0.124288
[10:59:29.583] TRAIN: iteration 19498 : loss : 0.086160, loss_ce: 0.002488, loss_dice: 0.169832
[10:59:29.796] TRAIN: iteration 19499 : loss : 0.118532, loss_ce: 0.003048, loss_dice: 0.234016
[10:59:30.004] TRAIN: iteration 19500 : loss : 0.243411, loss_ce: 0.002094, loss_dice: 0.484728
[10:59:30.244] TRAIN: iteration 19501 : loss : 0.041704, loss_ce: 0.002021, loss_dice: 0.081387
[10:59:30.459] TRAIN: iteration 19502 : loss : 0.049512, loss_ce: 0.002978, loss_dice: 0.096046
[10:59:30.667] TRAIN: iteration 19503 : loss : 0.078623, loss_ce: 0.004972, loss_dice: 0.152274
[10:59:30.876] TRAIN: iteration 19504 : loss : 0.249318, loss_ce: 0.003774, loss_dice: 0.494861
[10:59:31.086] TRAIN: iteration 19505 : loss : 0.228951, loss_ce: 0.002905, loss_dice: 0.454998
[10:59:31.297] TRAIN: iteration 19506 : loss : 0.112973, loss_ce: 0.003827, loss_dice: 0.222120
[10:59:31.512] TRAIN: iteration 19507 : loss : 0.117766, loss_ce: 0.009174, loss_dice: 0.226357
[10:59:32.001] TRAIN: iteration 19508 : loss : 0.080833, loss_ce: 0.004279, loss_dice: 0.157386
[10:59:32.405] TRAIN: iteration 19509 : loss : 0.071298, loss_ce: 0.001973, loss_dice: 0.140624
[10:59:32.614] TRAIN: iteration 19510 : loss : 0.063818, loss_ce: 0.001842, loss_dice: 0.125794
[10:59:32.823] TRAIN: iteration 19511 : loss : 0.152671, loss_ce: 0.002434, loss_dice: 0.302908
[10:59:33.035] TRAIN: iteration 19512 : loss : 0.162038, loss_ce: 0.003702, loss_dice: 0.320373
[10:59:33.242] TRAIN: iteration 19513 : loss : 0.249117, loss_ce: 0.003561, loss_dice: 0.494673
[10:59:33.451] TRAIN: iteration 19514 : loss : 0.024766, loss_ce: 0.001248, loss_dice: 0.048284
[10:59:33.662] TRAIN: iteration 19515 : loss : 0.050788, loss_ce: 0.001778, loss_dice: 0.099798
[10:59:33.969] TRAIN: iteration 19516 : loss : 0.230414, loss_ce: 0.004341, loss_dice: 0.456487
[10:59:34.181] TRAIN: iteration 19517 : loss : 0.083309, loss_ce: 0.005270, loss_dice: 0.161349
[10:59:34.391] TRAIN: iteration 19518 : loss : 0.079185, loss_ce: 0.005886, loss_dice: 0.152485
[10:59:34.599] TRAIN: iteration 19519 : loss : 0.065816, loss_ce: 0.003646, loss_dice: 0.127987
[10:59:34.808] TRAIN: iteration 19520 : loss : 0.094573, loss_ce: 0.004877, loss_dice: 0.184269
[10:59:35.048] TRAIN: iteration 19521 : loss : 0.251990, loss_ce: 0.003715, loss_dice: 0.500264
[10:59:35.259] TRAIN: iteration 19522 : loss : 0.250999, loss_ce: 0.003913, loss_dice: 0.498085
[10:59:35.475] TRAIN: iteration 19523 : loss : 0.251564, loss_ce: 0.002938, loss_dice: 0.500191
[10:59:35.686] TRAIN: iteration 19524 : loss : 0.077764, loss_ce: 0.003281, loss_dice: 0.152247
[10:59:35.894] TRAIN: iteration 19525 : loss : 0.251054, loss_ce: 0.002015, loss_dice: 0.500093
[10:59:36.102] TRAIN: iteration 19526 : loss : 0.096872, loss_ce: 0.008218, loss_dice: 0.185525
[10:59:36.310] TRAIN: iteration 19527 : loss : 0.023951, loss_ce: 0.005396, loss_dice: 0.042506
[10:59:36.518] TRAIN: iteration 19528 : loss : 0.190231, loss_ce: 0.008648, loss_dice: 0.371814
[10:59:36.730] TRAIN: iteration 19529 : loss : 0.191486, loss_ce: 0.008880, loss_dice: 0.374092
[10:59:37.008] TRAIN: iteration 19530 : loss : 0.149507, loss_ce: 0.016478, loss_dice: 0.282537
[10:59:37.225] TRAIN: iteration 19531 : loss : 0.164203, loss_ce: 0.005921, loss_dice: 0.322485
[10:59:37.434] TRAIN: iteration 19532 : loss : 0.252540, loss_ce: 0.004747, loss_dice: 0.500333
[10:59:37.643] TRAIN: iteration 19533 : loss : 0.059731, loss_ce: 0.005718, loss_dice: 0.113743
[10:59:37.851] TRAIN: iteration 19534 : loss : 0.063903, loss_ce: 0.003799, loss_dice: 0.124007
[10:59:38.060] TRAIN: iteration 19535 : loss : 0.244985, loss_ce: 0.002077, loss_dice: 0.487893
[10:59:38.267] TRAIN: iteration 19536 : loss : 0.116417, loss_ce: 0.003576, loss_dice: 0.229259
[10:59:38.478] TRAIN: iteration 19537 : loss : 0.048216, loss_ce: 0.004087, loss_dice: 0.092345
[10:59:38.686] TRAIN: iteration 19538 : loss : 0.251688, loss_ce: 0.003157, loss_dice: 0.500220
[10:59:38.898] TRAIN: iteration 19539 : loss : 0.082425, loss_ce: 0.009276, loss_dice: 0.155574
[10:59:39.110] TRAIN: iteration 19540 : loss : 0.086908, loss_ce: 0.005639, loss_dice: 0.168178
[10:59:39.349] TRAIN: iteration 19541 : loss : 0.181367, loss_ce: 0.010537, loss_dice: 0.352198
[10:59:39.557] TRAIN: iteration 19542 : loss : 0.034677, loss_ce: 0.003927, loss_dice: 0.065428
[10:59:39.766] TRAIN: iteration 19543 : loss : 0.170085, loss_ce: 0.003726, loss_dice: 0.336444
[10:59:39.973] TRAIN: iteration 19544 : loss : 0.181423, loss_ce: 0.008141, loss_dice: 0.354704
[10:59:40.180] TRAIN: iteration 19545 : loss : 0.252255, loss_ce: 0.004201, loss_dice: 0.500309
[10:59:40.396] TRAIN: iteration 19546 : loss : 0.062983, loss_ce: 0.004271, loss_dice: 0.121695
[10:59:40.606] TRAIN: iteration 19547 : loss : 0.067007, loss_ce: 0.002919, loss_dice: 0.131095
[10:59:40.816] TRAIN: iteration 19548 : loss : 0.151846, loss_ce: 0.002331, loss_dice: 0.301361
[10:59:41.997] TRAIN: iteration 19549 : loss : 0.090122, loss_ce: 0.012328, loss_dice: 0.167915
[10:59:42.210] TRAIN: iteration 19550 : loss : 0.251309, loss_ce: 0.002470, loss_dice: 0.500147
[10:59:42.431] TRAIN: iteration 19551 : loss : 0.136662, loss_ce: 0.015371, loss_dice: 0.257952
[10:59:42.637] TRAIN: iteration 19552 : loss : 0.237993, loss_ce: 0.009492, loss_dice: 0.466495
[10:59:42.845] TRAIN: iteration 19553 : loss : 0.103476, loss_ce: 0.010974, loss_dice: 0.195978
[10:59:43.054] TRAIN: iteration 19554 : loss : 0.239798, loss_ce: 0.002773, loss_dice: 0.476822
[10:59:43.264] TRAIN: iteration 19555 : loss : 0.211992, loss_ce: 0.003545, loss_dice: 0.420438
[10:59:43.476] TRAIN: iteration 19556 : loss : 0.050724, loss_ce: 0.003500, loss_dice: 0.097948
[10:59:43.685] TRAIN: iteration 19557 : loss : 0.252576, loss_ce: 0.004822, loss_dice: 0.500330
[10:59:43.893] TRAIN: iteration 19558 : loss : 0.128782, loss_ce: 0.004336, loss_dice: 0.253228
[10:59:44.102] TRAIN: iteration 19559 : loss : 0.252160, loss_ce: 0.006791, loss_dice: 0.497528
[10:59:44.310] TRAIN: iteration 19560 : loss : 0.150340, loss_ce: 0.006573, loss_dice: 0.294108
[10:59:44.552] TRAIN: iteration 19561 : loss : 0.078299, loss_ce: 0.004205, loss_dice: 0.152394
[10:59:44.763] TRAIN: iteration 19562 : loss : 0.212357, loss_ce: 0.003927, loss_dice: 0.420786
[10:59:44.971] TRAIN: iteration 19563 : loss : 0.069596, loss_ce: 0.002949, loss_dice: 0.136243
[10:59:45.179] TRAIN: iteration 19564 : loss : 0.155081, loss_ce: 0.005412, loss_dice: 0.304749
[10:59:45.387] TRAIN: iteration 19565 : loss : 0.236489, loss_ce: 0.003633, loss_dice: 0.469345
[10:59:45.603] TRAIN: iteration 19566 : loss : 0.072291, loss_ce: 0.007037, loss_dice: 0.137544
[10:59:45.812] TRAIN: iteration 19567 : loss : 0.126719, loss_ce: 0.002314, loss_dice: 0.251123
[10:59:46.410] TRAIN: iteration 19568 : loss : 0.033130, loss_ce: 0.004431, loss_dice: 0.061829
[10:59:46.618] TRAIN: iteration 19569 : loss : 0.078121, loss_ce: 0.006402, loss_dice: 0.149840
[10:59:46.825] TRAIN: iteration 19570 : loss : 0.050424, loss_ce: 0.004664, loss_dice: 0.096184
[10:59:47.033] TRAIN: iteration 19571 : loss : 0.065454, loss_ce: 0.001891, loss_dice: 0.129017
[10:59:47.243] TRAIN: iteration 19572 : loss : 0.087049, loss_ce: 0.005139, loss_dice: 0.168959
[10:59:47.455] TRAIN: iteration 19573 : loss : 0.080311, loss_ce: 0.005361, loss_dice: 0.155260
[10:59:47.671] TRAIN: iteration 19574 : loss : 0.250280, loss_ce: 0.005946, loss_dice: 0.494613
[10:59:47.886] TRAIN: iteration 19575 : loss : 0.085655, loss_ce: 0.002991, loss_dice: 0.168319
[10:59:48.097] TRAIN: iteration 19576 : loss : 0.050903, loss_ce: 0.008500, loss_dice: 0.093305
[10:59:48.308] TRAIN: iteration 19577 : loss : 0.037945, loss_ce: 0.001570, loss_dice: 0.074320
[10:59:48.525] TRAIN: iteration 19578 : loss : 0.206975, loss_ce: 0.002194, loss_dice: 0.411757
[10:59:48.737] TRAIN: iteration 19579 : loss : 0.251137, loss_ce: 0.002197, loss_dice: 0.500077
[10:59:48.951] TRAIN: iteration 19580 : loss : 0.212975, loss_ce: 0.004137, loss_dice: 0.421814
[10:59:49.188] TRAIN: iteration 19581 : loss : 0.070844, loss_ce: 0.001893, loss_dice: 0.139795
[10:59:49.400] TRAIN: iteration 19582 : loss : 0.251751, loss_ce: 0.003266, loss_dice: 0.500236
[10:59:49.615] TRAIN: iteration 19583 : loss : 0.061015, loss_ce: 0.002706, loss_dice: 0.119324
[10:59:49.823] TRAIN: iteration 19584 : loss : 0.077934, loss_ce: 0.004821, loss_dice: 0.151046
[10:59:50.031] TRAIN: iteration 19585 : loss : 0.251198, loss_ce: 0.002295, loss_dice: 0.500102
[10:59:50.240] TRAIN: iteration 19586 : loss : 0.210199, loss_ce: 0.004681, loss_dice: 0.415717
[10:59:50.448] TRAIN: iteration 19587 : loss : 0.085995, loss_ce: 0.003977, loss_dice: 0.168013
[10:59:50.655] TRAIN: iteration 19588 : loss : 0.115591, loss_ce: 0.002442, loss_dice: 0.228739
[10:59:50.864] TRAIN: iteration 19589 : loss : 0.251901, loss_ce: 0.003539, loss_dice: 0.500264
[10:59:51.076] TRAIN: iteration 19590 : loss : 0.099864, loss_ce: 0.010769, loss_dice: 0.188959
[10:59:51.283] TRAIN: iteration 19591 : loss : 0.090066, loss_ce: 0.014911, loss_dice: 0.165220
[10:59:51.499] TRAIN: iteration 19592 : loss : 0.204349, loss_ce: 0.003190, loss_dice: 0.405509
[10:59:51.706] TRAIN: iteration 19593 : loss : 0.211786, loss_ce: 0.003546, loss_dice: 0.420027
[10:59:51.914] TRAIN: iteration 19594 : loss : 0.251032, loss_ce: 0.001952, loss_dice: 0.500112
[10:59:52.123] TRAIN: iteration 19595 : loss : 0.142296, loss_ce: 0.003264, loss_dice: 0.281329
[10:59:52.338] TRAIN: iteration 19596 : loss : 0.168476, loss_ce: 0.010714, loss_dice: 0.326239
[10:59:52.548] TRAIN: iteration 19597 : loss : 0.238743, loss_ce: 0.001097, loss_dice: 0.476390
[10:59:52.757] TRAIN: iteration 19598 : loss : 0.094743, loss_ce: 0.002660, loss_dice: 0.186825
[10:59:52.966] TRAIN: iteration 19599 : loss : 0.238724, loss_ce: 0.003873, loss_dice: 0.473574
[10:59:53.174] TRAIN: iteration 19600 : loss : 0.070059, loss_ce: 0.002897, loss_dice: 0.137221
[10:59:53.414] TRAIN: iteration 19601 : loss : 0.071845, loss_ce: 0.002351, loss_dice: 0.141340
[10:59:53.621] TRAIN: iteration 19602 : loss : 0.256243, loss_ce: 0.012095, loss_dice: 0.500391
[10:59:53.829] TRAIN: iteration 19603 : loss : 0.251333, loss_ce: 0.002493, loss_dice: 0.500174
[10:59:54.039] TRAIN: iteration 19604 : loss : 0.102906, loss_ce: 0.007378, loss_dice: 0.198434
[10:59:54.246] TRAIN: iteration 19605 : loss : 0.090635, loss_ce: 0.007184, loss_dice: 0.174086
[10:59:54.453] TRAIN: iteration 19606 : loss : 0.078964, loss_ce: 0.015882, loss_dice: 0.142045
[10:59:54.664] TRAIN: iteration 19607 : loss : 0.237344, loss_ce: 0.003373, loss_dice: 0.471316
[10:59:54.872] TRAIN: iteration 19608 : loss : 0.125407, loss_ce: 0.001367, loss_dice: 0.249447
[10:59:55.085] TRAIN: iteration 19609 : loss : 0.198711, loss_ce: 0.004197, loss_dice: 0.393225
[10:59:55.300] TRAIN: iteration 19610 : loss : 0.187227, loss_ce: 0.004288, loss_dice: 0.370165
[10:59:55.514] TRAIN: iteration 19611 : loss : 0.151233, loss_ce: 0.001817, loss_dice: 0.300650
[10:59:55.722] TRAIN: iteration 19612 : loss : 0.090524, loss_ce: 0.005844, loss_dice: 0.175204
[10:59:55.929] TRAIN: iteration 19613 : loss : 0.111990, loss_ce: 0.004445, loss_dice: 0.219535
[10:59:56.137] TRAIN: iteration 19614 : loss : 0.073205, loss_ce: 0.005851, loss_dice: 0.140560
[10:59:56.357] TRAIN: iteration 19615 : loss : 0.202317, loss_ce: 0.007640, loss_dice: 0.396994
[10:59:56.564] TRAIN: iteration 19616 : loss : 0.250475, loss_ce: 0.000934, loss_dice: 0.500017
[10:59:56.772] TRAIN: iteration 19617 : loss : 0.247301, loss_ce: 0.003922, loss_dice: 0.490681
[10:59:56.990] TRAIN: iteration 19618 : loss : 0.124231, loss_ce: 0.002175, loss_dice: 0.246287
[10:59:57.200] TRAIN: iteration 19619 : loss : 0.080245, loss_ce: 0.003040, loss_dice: 0.157450
[10:59:57.409] TRAIN: iteration 19620 : loss : 0.089708, loss_ce: 0.007899, loss_dice: 0.171518
[10:59:57.647] TRAIN: iteration 19621 : loss : 0.239433, loss_ce: 0.008410, loss_dice: 0.470456
[10:59:57.855] TRAIN: iteration 19622 : loss : 0.037964, loss_ce: 0.003577, loss_dice: 0.072351
[10:59:58.062] TRAIN: iteration 19623 : loss : 0.026602, loss_ce: 0.002258, loss_dice: 0.050946
[10:59:58.271] TRAIN: iteration 19624 : loss : 0.247253, loss_ce: 0.001405, loss_dice: 0.493101
[10:59:58.480] TRAIN: iteration 19625 : loss : 0.253282, loss_ce: 0.007550, loss_dice: 0.499013
[10:59:58.689] TRAIN: iteration 19626 : loss : 0.078549, loss_ce: 0.001974, loss_dice: 0.155124
[10:59:58.897] TRAIN: iteration 19627 : loss : 0.188861, loss_ce: 0.003301, loss_dice: 0.374421
[10:59:59.109] TRAIN: iteration 19628 : loss : 0.250701, loss_ce: 0.001356, loss_dice: 0.500046
[10:59:59.319] TRAIN: iteration 19629 : loss : 0.080060, loss_ce: 0.003240, loss_dice: 0.156879
[10:59:59.528] TRAIN: iteration 19630 : loss : 0.257816, loss_ce: 0.021409, loss_dice: 0.494223
[10:59:59.736] TRAIN: iteration 19631 : loss : 0.154574, loss_ce: 0.004353, loss_dice: 0.304795
[10:59:59.944] TRAIN: iteration 19632 : loss : 0.025646, loss_ce: 0.003556, loss_dice: 0.047736
[11:00:00.157] TRAIN: iteration 19633 : loss : 0.250731, loss_ce: 0.001420, loss_dice: 0.500042
[11:00:00.368] TRAIN: iteration 19634 : loss : 0.251032, loss_ce: 0.001965, loss_dice: 0.500099
[11:00:00.576] TRAIN: iteration 19635 : loss : 0.071889, loss_ce: 0.003101, loss_dice: 0.140677
[11:00:00.789] TRAIN: iteration 19636 : loss : 0.043432, loss_ce: 0.003107, loss_dice: 0.083756
[11:00:00.998] TRAIN: iteration 19637 : loss : 0.038505, loss_ce: 0.001232, loss_dice: 0.075778
[11:00:01.210] TRAIN: iteration 19638 : loss : 0.140483, loss_ce: 0.001938, loss_dice: 0.279028
[11:00:01.418] TRAIN: iteration 19639 : loss : 0.053838, loss_ce: 0.002316, loss_dice: 0.105361
[11:00:01.628] TRAIN: iteration 19640 : loss : 0.083139, loss_ce: 0.003128, loss_dice: 0.163149
[11:00:01.629] NaN or Inf found in input tensor.
[11:00:01.844] TRAIN: iteration 19641 : loss : 0.231107, loss_ce: 0.001353, loss_dice: 0.460860
[11:00:02.051] TRAIN: iteration 19642 : loss : 0.152024, loss_ce: 0.002174, loss_dice: 0.301874
[11:00:02.260] TRAIN: iteration 19643 : loss : 0.120259, loss_ce: 0.003733, loss_dice: 0.236785
[11:00:03.507] TRAIN: iteration 19644 : loss : 0.250809, loss_ce: 0.004805, loss_dice: 0.496814
[11:00:03.716] TRAIN: iteration 19645 : loss : 0.115869, loss_ce: 0.006423, loss_dice: 0.225316
[11:00:03.924] TRAIN: iteration 19646 : loss : 0.104510, loss_ce: 0.002961, loss_dice: 0.206058
[11:00:04.133] TRAIN: iteration 19647 : loss : 0.119898, loss_ce: 0.002308, loss_dice: 0.237487
[11:00:04.341] TRAIN: iteration 19648 : loss : 0.041867, loss_ce: 0.004174, loss_dice: 0.079559
[11:00:04.548] TRAIN: iteration 19649 : loss : 0.214663, loss_ce: 0.001950, loss_dice: 0.427377
[11:00:04.762] TRAIN: iteration 19650 : loss : 0.251079, loss_ce: 0.002049, loss_dice: 0.500109
[11:00:04.977] TRAIN: iteration 19651 : loss : 0.185999, loss_ce: 0.001839, loss_dice: 0.370159
[11:00:06.103] TRAIN: iteration 19652 : loss : 0.102080, loss_ce: 0.003836, loss_dice: 0.200324
[11:00:06.313] TRAIN: iteration 19653 : loss : 0.116482, loss_ce: 0.009630, loss_dice: 0.223333
[11:00:06.521] TRAIN: iteration 19654 : loss : 0.149922, loss_ce: 0.008977, loss_dice: 0.290868
[11:00:06.731] TRAIN: iteration 19655 : loss : 0.251177, loss_ce: 0.002207, loss_dice: 0.500147
[11:00:06.943] TRAIN: iteration 19656 : loss : 0.103657, loss_ce: 0.004231, loss_dice: 0.203083
[11:00:07.152] TRAIN: iteration 19657 : loss : 0.093905, loss_ce: 0.003634, loss_dice: 0.184176
[11:00:07.420] TRAIN: iteration 19658 : loss : 0.102687, loss_ce: 0.001592, loss_dice: 0.203782
[11:00:07.628] TRAIN: iteration 19659 : loss : 0.076692, loss_ce: 0.005419, loss_dice: 0.147964
[11:00:07.836] TRAIN: iteration 19660 : loss : 0.129457, loss_ce: 0.004045, loss_dice: 0.254868
[11:00:08.079] TRAIN: iteration 19661 : loss : 0.133038, loss_ce: 0.012316, loss_dice: 0.253760
[11:00:08.286] TRAIN: iteration 19662 : loss : 0.169696, loss_ce: 0.008886, loss_dice: 0.330507
[11:00:08.500] TRAIN: iteration 19663 : loss : 0.095233, loss_ce: 0.004359, loss_dice: 0.186108
[11:00:08.708] TRAIN: iteration 19664 : loss : 0.151433, loss_ce: 0.002318, loss_dice: 0.300549
[11:00:08.923] TRAIN: iteration 19665 : loss : 0.195341, loss_ce: 0.004981, loss_dice: 0.385700
[11:00:09.137] TRAIN: iteration 19666 : loss : 0.251082, loss_ce: 0.002447, loss_dice: 0.499717
[11:00:09.348] TRAIN: iteration 19667 : loss : 0.239829, loss_ce: 0.000845, loss_dice: 0.478814
[11:00:09.556] TRAIN: iteration 19668 : loss : 0.251541, loss_ce: 0.002881, loss_dice: 0.500200
[11:00:09.765] TRAIN: iteration 19669 : loss : 0.056027, loss_ce: 0.003566, loss_dice: 0.108488
[11:00:09.977] TRAIN: iteration 19670 : loss : 0.149498, loss_ce: 0.003804, loss_dice: 0.295193
[11:00:10.188] TRAIN: iteration 19671 : loss : 0.035289, loss_ce: 0.006046, loss_dice: 0.064532
[11:00:10.397] TRAIN: iteration 19672 : loss : 0.223261, loss_ce: 0.003002, loss_dice: 0.443520
[11:00:10.608] TRAIN: iteration 19673 : loss : 0.056496, loss_ce: 0.002481, loss_dice: 0.110512
[11:00:10.820] TRAIN: iteration 19674 : loss : 0.066026, loss_ce: 0.002008, loss_dice: 0.130045
[11:00:11.033] TRAIN: iteration 19675 : loss : 0.091512, loss_ce: 0.001599, loss_dice: 0.181424
[11:00:11.251] TRAIN: iteration 19676 : loss : 0.251076, loss_ce: 0.002036, loss_dice: 0.500115
[11:00:11.466] TRAIN: iteration 19677 : loss : 0.052344, loss_ce: 0.002480, loss_dice: 0.102207
[11:00:11.682] TRAIN: iteration 19678 : loss : 0.060998, loss_ce: 0.010021, loss_dice: 0.111976
[11:00:12.137] TRAIN: iteration 19679 : loss : 0.110285, loss_ce: 0.001998, loss_dice: 0.218572
[11:00:12.346] TRAIN: iteration 19680 : loss : 0.155331, loss_ce: 0.004003, loss_dice: 0.306660
[11:00:12.637] TRAIN: iteration 19681 : loss : 0.155182, loss_ce: 0.010189, loss_dice: 0.300175
[11:00:12.852] TRAIN: iteration 19682 : loss : 0.247988, loss_ce: 0.002806, loss_dice: 0.493171
[11:00:13.713] TRAIN: iteration 19683 : loss : 0.071438, loss_ce: 0.008677, loss_dice: 0.134198
[11:00:13.921] TRAIN: iteration 19684 : loss : 0.070817, loss_ce: 0.004610, loss_dice: 0.137024
[11:00:14.130] TRAIN: iteration 19685 : loss : 0.070544, loss_ce: 0.001743, loss_dice: 0.139346
[11:00:14.337] TRAIN: iteration 19686 : loss : 0.161100, loss_ce: 0.005547, loss_dice: 0.316653
[11:00:17.141] TRAIN: iteration 19687 : loss : 0.251774, loss_ce: 0.003339, loss_dice: 0.500210
[11:00:17.349] TRAIN: iteration 19688 : loss : 0.246977, loss_ce: 0.004887, loss_dice: 0.489067
[11:00:17.558] TRAIN: iteration 19689 : loss : 0.236109, loss_ce: 0.002993, loss_dice: 0.469224
[11:00:17.766] TRAIN: iteration 19690 : loss : 0.028745, loss_ce: 0.002483, loss_dice: 0.055008
[11:00:17.974] TRAIN: iteration 19691 : loss : 0.128616, loss_ce: 0.004185, loss_dice: 0.253048
[11:00:18.185] TRAIN: iteration 19692 : loss : 0.225280, loss_ce: 0.025592, loss_dice: 0.424968
[11:00:18.460] TRAIN: iteration 19693 : loss : 0.047516, loss_ce: 0.009189, loss_dice: 0.085844
[11:00:18.674] TRAIN: iteration 19694 : loss : 0.049601, loss_ce: 0.002322, loss_dice: 0.096880
[11:00:18.883] TRAIN: iteration 19695 : loss : 0.047745, loss_ce: 0.004117, loss_dice: 0.091374
[11:00:19.097] TRAIN: iteration 19696 : loss : 0.230825, loss_ce: 0.001129, loss_dice: 0.460520
[11:00:19.348] TRAIN: iteration 19697 : loss : 0.077044, loss_ce: 0.001377, loss_dice: 0.152711
[11:00:19.555] TRAIN: iteration 19698 : loss : 0.250880, loss_ce: 0.001668, loss_dice: 0.500092
[11:00:19.763] TRAIN: iteration 19699 : loss : 0.099014, loss_ce: 0.002551, loss_dice: 0.195478
[11:00:19.978] TRAIN: iteration 19700 : loss : 0.251807, loss_ce: 0.003362, loss_dice: 0.500253
[11:00:20.219] TRAIN: iteration 19701 : loss : 0.075381, loss_ce: 0.010168, loss_dice: 0.140594
[11:00:20.428] TRAIN: iteration 19702 : loss : 0.105392, loss_ce: 0.005062, loss_dice: 0.205722
[11:00:20.638] TRAIN: iteration 19703 : loss : 0.082703, loss_ce: 0.002616, loss_dice: 0.162790
[11:00:20.846] TRAIN: iteration 19704 : loss : 0.251066, loss_ce: 0.004298, loss_dice: 0.497834
[11:00:21.056] TRAIN: iteration 19705 : loss : 0.083304, loss_ce: 0.004208, loss_dice: 0.162399
[11:00:21.265] TRAIN: iteration 19706 : loss : 0.048694, loss_ce: 0.003163, loss_dice: 0.094225
[11:00:21.475] TRAIN: iteration 19707 : loss : 0.068101, loss_ce: 0.010404, loss_dice: 0.125797
[11:00:21.682] TRAIN: iteration 19708 : loss : 0.085561, loss_ce: 0.004737, loss_dice: 0.166385
[11:00:21.890] TRAIN: iteration 19709 : loss : 0.094129, loss_ce: 0.002830, loss_dice: 0.185429
[11:00:22.100] TRAIN: iteration 19710 : loss : 0.029793, loss_ce: 0.002710, loss_dice: 0.056876
[11:00:22.314] TRAIN: iteration 19711 : loss : 0.053963, loss_ce: 0.001981, loss_dice: 0.105945
[11:00:22.522] TRAIN: iteration 19712 : loss : 0.053996, loss_ce: 0.002099, loss_dice: 0.105893
[11:00:22.729] TRAIN: iteration 19713 : loss : 0.045736, loss_ce: 0.002309, loss_dice: 0.089164
[11:00:22.944] TRAIN: iteration 19714 : loss : 0.107530, loss_ce: 0.002328, loss_dice: 0.212731
[11:00:23.216] TRAIN: iteration 19715 : loss : 0.138261, loss_ce: 0.004564, loss_dice: 0.271959
[11:00:23.425] TRAIN: iteration 19716 : loss : 0.082991, loss_ce: 0.001037, loss_dice: 0.164946
[11:00:23.633] TRAIN: iteration 19717 : loss : 0.053145, loss_ce: 0.002903, loss_dice: 0.103386
[11:00:23.847] TRAIN: iteration 19718 : loss : 0.048183, loss_ce: 0.011487, loss_dice: 0.084878
[11:00:24.058] TRAIN: iteration 19719 : loss : 0.032954, loss_ce: 0.001229, loss_dice: 0.064680
[11:00:24.295] TRAIN: iteration 19720 : loss : 0.259813, loss_ce: 0.020276, loss_dice: 0.499349
[11:00:24.530] TRAIN: iteration 19721 : loss : 0.145009, loss_ce: 0.006950, loss_dice: 0.283067
[11:00:24.743] TRAIN: iteration 19722 : loss : 0.133947, loss_ce: 0.006837, loss_dice: 0.261057
[11:00:25.126] TRAIN: iteration 19723 : loss : 0.022543, loss_ce: 0.001390, loss_dice: 0.043697
[11:00:25.336] TRAIN: iteration 19724 : loss : 0.251909, loss_ce: 0.003546, loss_dice: 0.500271
[11:00:25.577] TRAIN: iteration 19725 : loss : 0.238784, loss_ce: 0.002462, loss_dice: 0.475105
[11:00:25.786] TRAIN: iteration 19726 : loss : 0.179919, loss_ce: 0.009733, loss_dice: 0.350104
[11:00:26.002] TRAIN: iteration 19727 : loss : 0.090888, loss_ce: 0.003475, loss_dice: 0.178300
[11:00:26.214] TRAIN: iteration 19728 : loss : 0.229194, loss_ce: 0.005301, loss_dice: 0.453088
[11:00:26.425] TRAIN: iteration 19729 : loss : 0.037546, loss_ce: 0.005237, loss_dice: 0.069855
[11:00:26.638] TRAIN: iteration 19730 : loss : 0.042158, loss_ce: 0.002883, loss_dice: 0.081433
[11:00:27.026] TRAIN: iteration 19731 : loss : 0.213827, loss_ce: 0.007343, loss_dice: 0.420311
[11:00:27.235] TRAIN: iteration 19732 : loss : 0.146803, loss_ce: 0.004994, loss_dice: 0.288612
[11:00:27.444] TRAIN: iteration 19733 : loss : 0.052021, loss_ce: 0.001464, loss_dice: 0.102578
[11:00:27.652] TRAIN: iteration 19734 : loss : 0.047437, loss_ce: 0.001862, loss_dice: 0.093011
[11:00:27.860] TRAIN: iteration 19735 : loss : 0.246111, loss_ce: 0.001617, loss_dice: 0.490604
[11:00:28.071] TRAIN: iteration 19736 : loss : 0.260868, loss_ce: 0.021380, loss_dice: 0.500355
[11:00:28.284] TRAIN: iteration 19737 : loss : 0.174927, loss_ce: 0.008921, loss_dice: 0.340934
[11:00:28.493] TRAIN: iteration 19738 : loss : 0.119580, loss_ce: 0.004992, loss_dice: 0.234168
[11:00:28.701] TRAIN: iteration 19739 : loss : 0.157315, loss_ce: 0.002472, loss_dice: 0.312159
[11:00:28.912] TRAIN: iteration 19740 : loss : 0.095713, loss_ce: 0.002146, loss_dice: 0.189280
[11:00:28.913] NaN or Inf found in input tensor.
[11:00:29.133] TRAIN: iteration 19741 : loss : 0.173928, loss_ce: 0.003777, loss_dice: 0.344079
[11:00:29.340] TRAIN: iteration 19742 : loss : 0.240645, loss_ce: 0.002278, loss_dice: 0.479013
[11:00:30.016] TRAIN: iteration 19743 : loss : 0.230768, loss_ce: 0.001264, loss_dice: 0.460272
[11:00:30.224] TRAIN: iteration 19744 : loss : 0.051712, loss_ce: 0.002325, loss_dice: 0.101099
[11:00:30.435] TRAIN: iteration 19745 : loss : 0.067614, loss_ce: 0.002147, loss_dice: 0.133081
[11:00:30.643] TRAIN: iteration 19746 : loss : 0.049318, loss_ce: 0.002903, loss_dice: 0.095732
[11:00:30.852] TRAIN: iteration 19747 : loss : 0.226398, loss_ce: 0.007552, loss_dice: 0.445244
[11:00:31.059] TRAIN: iteration 19748 : loss : 0.250728, loss_ce: 0.001406, loss_dice: 0.500050
[11:00:31.270] TRAIN: iteration 19749 : loss : 0.172851, loss_ce: 0.026351, loss_dice: 0.319351
[11:00:31.483] TRAIN: iteration 19750 : loss : 0.121295, loss_ce: 0.003981, loss_dice: 0.238608
[11:00:31.864] TRAIN: iteration 19751 : loss : 0.210709, loss_ce: 0.003776, loss_dice: 0.417642
[11:00:32.073] TRAIN: iteration 19752 : loss : 0.250928, loss_ce: 0.001770, loss_dice: 0.500085
[11:00:32.331] TRAIN: iteration 19753 : loss : 0.251548, loss_ce: 0.002898, loss_dice: 0.500199
[11:00:32.538] TRAIN: iteration 19754 : loss : 0.046273, loss_ce: 0.002380, loss_dice: 0.090166
[11:00:32.746] TRAIN: iteration 19755 : loss : 0.185368, loss_ce: 0.006479, loss_dice: 0.364257
[11:00:32.953] TRAIN: iteration 19756 : loss : 0.199579, loss_ce: 0.005024, loss_dice: 0.394133
[11:00:33.161] TRAIN: iteration 19757 : loss : 0.109206, loss_ce: 0.003097, loss_dice: 0.215315
[11:00:33.369] TRAIN: iteration 19758 : loss : 0.146060, loss_ce: 0.005924, loss_dice: 0.286197
[11:00:33.707] TRAIN: iteration 19759 : loss : 0.104237, loss_ce: 0.004911, loss_dice: 0.203564
[11:00:33.916] TRAIN: iteration 19760 : loss : 0.082326, loss_ce: 0.006845, loss_dice: 0.157806
[11:00:34.156] TRAIN: iteration 19761 : loss : 0.130850, loss_ce: 0.003793, loss_dice: 0.257906
[11:00:34.364] TRAIN: iteration 19762 : loss : 0.059333, loss_ce: 0.009285, loss_dice: 0.109382
[11:00:34.572] TRAIN: iteration 19763 : loss : 0.100618, loss_ce: 0.007243, loss_dice: 0.193992
[11:00:34.786] TRAIN: iteration 19764 : loss : 0.099267, loss_ce: 0.002914, loss_dice: 0.195620
[11:00:34.995] TRAIN: iteration 19765 : loss : 0.118214, loss_ce: 0.017817, loss_dice: 0.218610
[11:00:35.214] TRAIN: iteration 19766 : loss : 0.101121, loss_ce: 0.002731, loss_dice: 0.199510
[11:00:35.423] TRAIN: iteration 19767 : loss : 0.094620, loss_ce: 0.003416, loss_dice: 0.185824
[11:00:35.634] TRAIN: iteration 19768 : loss : 0.117129, loss_ce: 0.006016, loss_dice: 0.228243
[11:00:35.845] TRAIN: iteration 19769 : loss : 0.081152, loss_ce: 0.001924, loss_dice: 0.160380
[11:00:36.054] TRAIN: iteration 19770 : loss : 0.250707, loss_ce: 0.001376, loss_dice: 0.500037
[11:00:36.265] TRAIN: iteration 19771 : loss : 0.078300, loss_ce: 0.002645, loss_dice: 0.153955
[11:00:36.479] TRAIN: iteration 19772 : loss : 0.073846, loss_ce: 0.002282, loss_dice: 0.145410
[11:00:36.687] TRAIN: iteration 19773 : loss : 0.090092, loss_ce: 0.001305, loss_dice: 0.178879
[11:00:36.895] TRAIN: iteration 19774 : loss : 0.202830, loss_ce: 0.005341, loss_dice: 0.400318
[11:00:37.104] TRAIN: iteration 19775 : loss : 0.045814, loss_ce: 0.002456, loss_dice: 0.089172
[11:00:37.314] TRAIN: iteration 19776 : loss : 0.251108, loss_ce: 0.002086, loss_dice: 0.500130
[11:00:37.524] TRAIN: iteration 19777 : loss : 0.151845, loss_ce: 0.002829, loss_dice: 0.300861
[11:00:37.786] TRAIN: iteration 19778 : loss : 0.133825, loss_ce: 0.005827, loss_dice: 0.261823
[11:00:37.996] TRAIN: iteration 19779 : loss : 0.154533, loss_ce: 0.000980, loss_dice: 0.308087
[11:00:38.215] TRAIN: iteration 19780 : loss : 0.250680, loss_ce: 0.001304, loss_dice: 0.500055
[11:00:38.461] TRAIN: iteration 19781 : loss : 0.170450, loss_ce: 0.012967, loss_dice: 0.327932
[11:00:38.668] TRAIN: iteration 19782 : loss : 0.244131, loss_ce: 0.003349, loss_dice: 0.484913
[11:00:38.880] TRAIN: iteration 19783 : loss : 0.156852, loss_ce: 0.005480, loss_dice: 0.308225
[11:00:39.089] TRAIN: iteration 19784 : loss : 0.149845, loss_ce: 0.004131, loss_dice: 0.295560
[11:00:39.296] TRAIN: iteration 19785 : loss : 0.196682, loss_ce: 0.021880, loss_dice: 0.371484
[11:00:39.696] TRAIN: iteration 19786 : loss : 0.078424, loss_ce: 0.001635, loss_dice: 0.155214
[11:00:39.904] TRAIN: iteration 19787 : loss : 0.108427, loss_ce: 0.007594, loss_dice: 0.209260
[11:00:40.112] TRAIN: iteration 19788 : loss : 0.121336, loss_ce: 0.010871, loss_dice: 0.231802
[11:00:40.320] TRAIN: iteration 19789 : loss : 0.218677, loss_ce: 0.002584, loss_dice: 0.434771
[11:00:40.530] TRAIN: iteration 19790 : loss : 0.037366, loss_ce: 0.004033, loss_dice: 0.070700
[11:00:40.739] TRAIN: iteration 19791 : loss : 0.093089, loss_ce: 0.001216, loss_dice: 0.184962
[11:00:40.950] TRAIN: iteration 19792 : loss : 0.110069, loss_ce: 0.002814, loss_dice: 0.217324
[11:00:41.158] TRAIN: iteration 19793 : loss : 0.074666, loss_ce: 0.001832, loss_dice: 0.147500
[11:00:41.366] TRAIN: iteration 19794 : loss : 0.100160, loss_ce: 0.002481, loss_dice: 0.197839
[11:00:41.572] TRAIN: iteration 19795 : loss : 0.093569, loss_ce: 0.013962, loss_dice: 0.173176
[11:00:41.781] TRAIN: iteration 19796 : loss : 0.078438, loss_ce: 0.007684, loss_dice: 0.149192
[11:00:41.989] TRAIN: iteration 19797 : loss : 0.226700, loss_ce: 0.002938, loss_dice: 0.450461
[11:00:42.197] TRAIN: iteration 19798 : loss : 0.195996, loss_ce: 0.004123, loss_dice: 0.387870
[11:00:42.404] TRAIN: iteration 19799 : loss : 0.129264, loss_ce: 0.004646, loss_dice: 0.253881
[11:00:42.618] TRAIN: iteration 19800 : loss : 0.068631, loss_ce: 0.004513, loss_dice: 0.132749
[11:00:42.619] NaN or Inf found in input tensor.
[11:00:42.842] TRAIN: iteration 19801 : loss : 0.113060, loss_ce: 0.003400, loss_dice: 0.222720
[11:00:43.050] TRAIN: iteration 19802 : loss : 0.169712, loss_ce: 0.003154, loss_dice: 0.336271
[11:00:43.257] TRAIN: iteration 19803 : loss : 0.248963, loss_ce: 0.003106, loss_dice: 0.494819
[11:00:44.332] TRAIN: iteration 19804 : loss : 0.045128, loss_ce: 0.008007, loss_dice: 0.082249
[11:00:44.546] TRAIN: iteration 19805 : loss : 0.111486, loss_ce: 0.002984, loss_dice: 0.219989
[11:00:44.760] TRAIN: iteration 19806 : loss : 0.123087, loss_ce: 0.007652, loss_dice: 0.238522
[11:00:44.967] TRAIN: iteration 19807 : loss : 0.185225, loss_ce: 0.003566, loss_dice: 0.366884
[11:00:45.175] TRAIN: iteration 19808 : loss : 0.095675, loss_ce: 0.006544, loss_dice: 0.184805
[11:00:45.386] TRAIN: iteration 19809 : loss : 0.158600, loss_ce: 0.007216, loss_dice: 0.309984
[11:00:45.594] TRAIN: iteration 19810 : loss : 0.166761, loss_ce: 0.003914, loss_dice: 0.329609
[11:00:45.801] TRAIN: iteration 19811 : loss : 0.234522, loss_ce: 0.002150, loss_dice: 0.466894
[11:00:46.009] TRAIN: iteration 19812 : loss : 0.057665, loss_ce: 0.009452, loss_dice: 0.105878
[11:00:46.220] TRAIN: iteration 19813 : loss : 0.083666, loss_ce: 0.003915, loss_dice: 0.163417
[11:00:46.428] TRAIN: iteration 19814 : loss : 0.115401, loss_ce: 0.010460, loss_dice: 0.220341
[11:00:47.231] TRAIN: iteration 19815 : loss : 0.191757, loss_ce: 0.004078, loss_dice: 0.379437
[11:00:47.440] TRAIN: iteration 19816 : loss : 0.154086, loss_ce: 0.008022, loss_dice: 0.300150
[11:00:47.648] TRAIN: iteration 19817 : loss : 0.047783, loss_ce: 0.006815, loss_dice: 0.088752
[11:00:47.859] TRAIN: iteration 19818 : loss : 0.035355, loss_ce: 0.001766, loss_dice: 0.068944
[11:00:48.067] TRAIN: iteration 19819 : loss : 0.252442, loss_ce: 0.004654, loss_dice: 0.500231
[11:00:48.283] TRAIN: iteration 19820 : loss : 0.176664, loss_ce: 0.002194, loss_dice: 0.351135
[11:00:48.521] TRAIN: iteration 19821 : loss : 0.251936, loss_ce: 0.003639, loss_dice: 0.500234
[11:00:48.728] TRAIN: iteration 19822 : loss : 0.091412, loss_ce: 0.006380, loss_dice: 0.176444
[11:00:49.096] TRAIN: iteration 19823 : loss : 0.047844, loss_ce: 0.003328, loss_dice: 0.092360
[11:00:49.314] TRAIN: iteration 19824 : loss : 0.076837, loss_ce: 0.003691, loss_dice: 0.149983
[11:00:49.530] TRAIN: iteration 19825 : loss : 0.145220, loss_ce: 0.005343, loss_dice: 0.285098
[11:00:49.739] TRAIN: iteration 19826 : loss : 0.121819, loss_ce: 0.002079, loss_dice: 0.241559
[11:00:49.957] TRAIN: iteration 19827 : loss : 0.037286, loss_ce: 0.001475, loss_dice: 0.073096
[11:00:50.166] TRAIN: iteration 19828 : loss : 0.243654, loss_ce: 0.002297, loss_dice: 0.485011
[11:00:50.374] TRAIN: iteration 19829 : loss : 0.051970, loss_ce: 0.003230, loss_dice: 0.100711
[11:00:50.590] TRAIN: iteration 19830 : loss : 0.110214, loss_ce: 0.003215, loss_dice: 0.217213
[11:00:50.798] TRAIN: iteration 19831 : loss : 0.250997, loss_ce: 0.001906, loss_dice: 0.500088
[11:00:51.006] TRAIN: iteration 19832 : loss : 0.250699, loss_ce: 0.001350, loss_dice: 0.500047
[11:00:51.218] TRAIN: iteration 19833 : loss : 0.170261, loss_ce: 0.005768, loss_dice: 0.334753
[11:00:51.430] TRAIN: iteration 19834 : loss : 0.055076, loss_ce: 0.008707, loss_dice: 0.101445
[11:00:51.637] TRAIN: iteration 19835 : loss : 0.119349, loss_ce: 0.003170, loss_dice: 0.235528
[11:00:51.845] TRAIN: iteration 19836 : loss : 0.084385, loss_ce: 0.003999, loss_dice: 0.164772
[11:00:52.060] TRAIN: iteration 19837 : loss : 0.024446, loss_ce: 0.001049, loss_dice: 0.047842
[11:00:52.268] TRAIN: iteration 19838 : loss : 0.139626, loss_ce: 0.005711, loss_dice: 0.273542
[11:00:53.430] TRAIN: iteration 19839 : loss : 0.086219, loss_ce: 0.002047, loss_dice: 0.170391
[11:00:53.637] TRAIN: iteration 19840 : loss : 0.042407, loss_ce: 0.006670, loss_dice: 0.078145
[11:00:53.875] TRAIN: iteration 19841 : loss : 0.059132, loss_ce: 0.002154, loss_dice: 0.116110
[11:00:54.083] TRAIN: iteration 19842 : loss : 0.249253, loss_ce: 0.001215, loss_dice: 0.497291
[11:00:54.292] TRAIN: iteration 19843 : loss : 0.059742, loss_ce: 0.003560, loss_dice: 0.115924
[11:00:54.499] TRAIN: iteration 19844 : loss : 0.187587, loss_ce: 0.005557, loss_dice: 0.369616
[11:00:54.707] TRAIN: iteration 19845 : loss : 0.136498, loss_ce: 0.025906, loss_dice: 0.247091
[11:00:54.916] TRAIN: iteration 19846 : loss : 0.232737, loss_ce: 0.004991, loss_dice: 0.460483
[11:00:55.124] TRAIN: iteration 19847 : loss : 0.249183, loss_ce: 0.005004, loss_dice: 0.493362
[11:00:55.334] TRAIN: iteration 19848 : loss : 0.130509, loss_ce: 0.003333, loss_dice: 0.257686
[11:00:55.544] TRAIN: iteration 19849 : loss : 0.250476, loss_ce: 0.000914, loss_dice: 0.500037
[11:00:55.754] TRAIN: iteration 19850 : loss : 0.238702, loss_ce: 0.002918, loss_dice: 0.474486
[11:00:55.962] TRAIN: iteration 19851 : loss : 0.127969, loss_ce: 0.001225, loss_dice: 0.254713
[11:00:56.172] TRAIN: iteration 19852 : loss : 0.239466, loss_ce: 0.007006, loss_dice: 0.471927
[11:00:56.380] TRAIN: iteration 19853 : loss : 0.196932, loss_ce: 0.001398, loss_dice: 0.392467
[11:00:56.588] TRAIN: iteration 19854 : loss : 0.163641, loss_ce: 0.002027, loss_dice: 0.325256
[11:00:56.796] TRAIN: iteration 19855 : loss : 0.129221, loss_ce: 0.002230, loss_dice: 0.256213
[11:00:57.005] TRAIN: iteration 19856 : loss : 0.094069, loss_ce: 0.003041, loss_dice: 0.185097
[11:00:57.214] TRAIN: iteration 19857 : loss : 0.045086, loss_ce: 0.002668, loss_dice: 0.087504
[11:00:57.423] TRAIN: iteration 19858 : loss : 0.250818, loss_ce: 0.001565, loss_dice: 0.500071
[11:00:57.637] TRAIN: iteration 19859 : loss : 0.132937, loss_ce: 0.006315, loss_dice: 0.259559
[11:00:57.847] TRAIN: iteration 19860 : loss : 0.049709, loss_ce: 0.008553, loss_dice: 0.090865
[11:00:58.085] TRAIN: iteration 19861 : loss : 0.075243, loss_ce: 0.001655, loss_dice: 0.148830
[11:00:58.298] TRAIN: iteration 19862 : loss : 0.097227, loss_ce: 0.002119, loss_dice: 0.192334
[11:00:58.508] TRAIN: iteration 19863 : loss : 0.122117, loss_ce: 0.010847, loss_dice: 0.233388
[11:00:58.718] TRAIN: iteration 19864 : loss : 0.175078, loss_ce: 0.017044, loss_dice: 0.333113
[11:00:58.931] TRAIN: iteration 19865 : loss : 0.253455, loss_ce: 0.008288, loss_dice: 0.498621
[11:00:59.141] TRAIN: iteration 19866 : loss : 0.176041, loss_ce: 0.002879, loss_dice: 0.349204
[11:00:59.349] TRAIN: iteration 19867 : loss : 0.253741, loss_ce: 0.007390, loss_dice: 0.500092
[11:00:59.557] TRAIN: iteration 19868 : loss : 0.250695, loss_ce: 0.001353, loss_dice: 0.500037
[11:00:59.765] TRAIN: iteration 19869 : loss : 0.143025, loss_ce: 0.002876, loss_dice: 0.283173
[11:00:59.972] TRAIN: iteration 19870 : loss : 0.093774, loss_ce: 0.003940, loss_dice: 0.183607
[11:01:00.181] TRAIN: iteration 19871 : loss : 0.113715, loss_ce: 0.013697, loss_dice: 0.213733
[11:01:01.221] TRAIN: iteration 19872 : loss : 0.032581, loss_ce: 0.001355, loss_dice: 0.063807
[11:01:01.430] TRAIN: iteration 19873 : loss : 0.025544, loss_ce: 0.001215, loss_dice: 0.049872
[11:01:01.644] TRAIN: iteration 19874 : loss : 0.040672, loss_ce: 0.007851, loss_dice: 0.073494
[11:01:01.856] TRAIN: iteration 19875 : loss : 0.056012, loss_ce: 0.001736, loss_dice: 0.110289
[11:01:02.066] TRAIN: iteration 19876 : loss : 0.254281, loss_ce: 0.015291, loss_dice: 0.493272
[11:01:02.279] TRAIN: iteration 19877 : loss : 0.093169, loss_ce: 0.002068, loss_dice: 0.184270
[11:01:02.489] TRAIN: iteration 19878 : loss : 0.022834, loss_ce: 0.001192, loss_dice: 0.044477
[11:01:02.699] TRAIN: iteration 19879 : loss : 0.171287, loss_ce: 0.011235, loss_dice: 0.331338
[11:01:02.908] TRAIN: iteration 19880 : loss : 0.069620, loss_ce: 0.008140, loss_dice: 0.131100
[11:01:03.149] TRAIN: iteration 19881 : loss : 0.082574, loss_ce: 0.001735, loss_dice: 0.163413
[11:01:03.436] TRAIN: iteration 19882 : loss : 0.036994, loss_ce: 0.000935, loss_dice: 0.073054
[11:01:03.715] TRAIN: iteration 19883 : loss : 0.188379, loss_ce: 0.002780, loss_dice: 0.373978
[11:01:03.924] TRAIN: iteration 19884 : loss : 0.228089, loss_ce: 0.006465, loss_dice: 0.449713
[11:01:04.134] TRAIN: iteration 19885 : loss : 0.197205, loss_ce: 0.001684, loss_dice: 0.392726
[11:01:04.341] TRAIN: iteration 19886 : loss : 0.087443, loss_ce: 0.005277, loss_dice: 0.169608
[11:01:04.548] TRAIN: iteration 19887 : loss : 0.047990, loss_ce: 0.002271, loss_dice: 0.093709
[11:01:04.756] TRAIN: iteration 19888 : loss : 0.137309, loss_ce: 0.011077, loss_dice: 0.263541
[11:01:04.963] TRAIN: iteration 19889 : loss : 0.113650, loss_ce: 0.005165, loss_dice: 0.222135
[11:01:05.170] TRAIN: iteration 19890 : loss : 0.085048, loss_ce: 0.003179, loss_dice: 0.166916
[11:01:05.378] TRAIN: iteration 19891 : loss : 0.198585, loss_ce: 0.007623, loss_dice: 0.389546
[11:01:06.699] TRAIN: iteration 19892 : loss : 0.190327, loss_ce: 0.002242, loss_dice: 0.378413
[11:01:06.907] TRAIN: iteration 19893 : loss : 0.013088, loss_ce: 0.001421, loss_dice: 0.024755
[11:01:07.117] TRAIN: iteration 19894 : loss : 0.087809, loss_ce: 0.004220, loss_dice: 0.171398
[11:01:07.324] TRAIN: iteration 19895 : loss : 0.155386, loss_ce: 0.003330, loss_dice: 0.307442
[11:01:07.532] TRAIN: iteration 19896 : loss : 0.121849, loss_ce: 0.002877, loss_dice: 0.240821
[11:01:07.739] TRAIN: iteration 19897 : loss : 0.188827, loss_ce: 0.001786, loss_dice: 0.375868
[11:01:07.948] TRAIN: iteration 19898 : loss : 0.226839, loss_ce: 0.008963, loss_dice: 0.444715
[11:01:08.156] TRAIN: iteration 19899 : loss : 0.248723, loss_ce: 0.002252, loss_dice: 0.495194
[11:01:09.829] TRAIN: iteration 19900 : loss : 0.066350, loss_ce: 0.008805, loss_dice: 0.123894
[11:01:10.066] TRAIN: iteration 19901 : loss : 0.228799, loss_ce: 0.003687, loss_dice: 0.453912
[11:01:10.281] TRAIN: iteration 19902 : loss : 0.239636, loss_ce: 0.003440, loss_dice: 0.475831
[11:01:10.488] TRAIN: iteration 19903 : loss : 0.192962, loss_ce: 0.008598, loss_dice: 0.377327
[11:01:10.696] TRAIN: iteration 19904 : loss : 0.216083, loss_ce: 0.007295, loss_dice: 0.424871
[11:01:10.905] TRAIN: iteration 19905 : loss : 0.251666, loss_ce: 0.003131, loss_dice: 0.500201
[11:01:11.114] TRAIN: iteration 19906 : loss : 0.135728, loss_ce: 0.005171, loss_dice: 0.266285
[11:01:11.321] TRAIN: iteration 19907 : loss : 0.060214, loss_ce: 0.003625, loss_dice: 0.116804
[11:01:13.239] TRAIN: iteration 19908 : loss : 0.082090, loss_ce: 0.002611, loss_dice: 0.161570
[11:01:13.454] TRAIN: iteration 19909 : loss : 0.095556, loss_ce: 0.002551, loss_dice: 0.188560
[11:01:13.663] TRAIN: iteration 19910 : loss : 0.105159, loss_ce: 0.009593, loss_dice: 0.200726
[11:01:13.872] TRAIN: iteration 19911 : loss : 0.187789, loss_ce: 0.007222, loss_dice: 0.368357
[11:01:14.084] TRAIN: iteration 19912 : loss : 0.053327, loss_ce: 0.002693, loss_dice: 0.103962
[11:01:14.292] TRAIN: iteration 19913 : loss : 0.028098, loss_ce: 0.003376, loss_dice: 0.052819
[11:01:14.502] TRAIN: iteration 19914 : loss : 0.072635, loss_ce: 0.003981, loss_dice: 0.141290
[11:01:14.716] TRAIN: iteration 19915 : loss : 0.073031, loss_ce: 0.002746, loss_dice: 0.143316
[11:01:15.050] TRAIN: iteration 19916 : loss : 0.249601, loss_ce: 0.003025, loss_dice: 0.496177
[11:01:15.257] TRAIN: iteration 19917 : loss : 0.029595, loss_ce: 0.001370, loss_dice: 0.057821
[11:01:15.466] TRAIN: iteration 19918 : loss : 0.111655, loss_ce: 0.006746, loss_dice: 0.216564
[11:01:15.675] TRAIN: iteration 19919 : loss : 0.043658, loss_ce: 0.001727, loss_dice: 0.085590
[11:01:15.884] TRAIN: iteration 19920 : loss : 0.111074, loss_ce: 0.004601, loss_dice: 0.217547
[11:01:16.139] TRAIN: iteration 19921 : loss : 0.082339, loss_ce: 0.006609, loss_dice: 0.158069
[11:01:16.348] TRAIN: iteration 19922 : loss : 0.057799, loss_ce: 0.003590, loss_dice: 0.112009
[11:01:16.557] TRAIN: iteration 19923 : loss : 0.222386, loss_ce: 0.004981, loss_dice: 0.439791
[11:01:17.554] TRAIN: iteration 19924 : loss : 0.146496, loss_ce: 0.009082, loss_dice: 0.283910
[11:01:17.764] TRAIN: iteration 19925 : loss : 0.092629, loss_ce: 0.002640, loss_dice: 0.182619
[11:01:17.971] TRAIN: iteration 19926 : loss : 0.238114, loss_ce: 0.002586, loss_dice: 0.473641
[11:01:18.180] TRAIN: iteration 19927 : loss : 0.033817, loss_ce: 0.004777, loss_dice: 0.062856
[11:01:18.388] TRAIN: iteration 19928 : loss : 0.140647, loss_ce: 0.003594, loss_dice: 0.277699
[11:01:18.595] TRAIN: iteration 19929 : loss : 0.083854, loss_ce: 0.011514, loss_dice: 0.156195
[11:01:18.804] TRAIN: iteration 19930 : loss : 0.099479, loss_ce: 0.002879, loss_dice: 0.196078
[11:01:19.013] TRAIN: iteration 19931 : loss : 0.048076, loss_ce: 0.005208, loss_dice: 0.090944
[11:01:19.251] TRAIN: iteration 19932 : loss : 0.217242, loss_ce: 0.007950, loss_dice: 0.426533
[11:01:19.459] TRAIN: iteration 19933 : loss : 0.068816, loss_ce: 0.003488, loss_dice: 0.134144
[11:01:19.672] TRAIN: iteration 19934 : loss : 0.112139, loss_ce: 0.004039, loss_dice: 0.220239
[11:01:19.880] TRAIN: iteration 19935 : loss : 0.102581, loss_ce: 0.006764, loss_dice: 0.198398
[11:01:20.089] TRAIN: iteration 19936 : loss : 0.110221, loss_ce: 0.003655, loss_dice: 0.216787
[11:01:20.297] TRAIN: iteration 19937 : loss : 0.080278, loss_ce: 0.004680, loss_dice: 0.155877
[11:01:20.507] TRAIN: iteration 19938 : loss : 0.070310, loss_ce: 0.001966, loss_dice: 0.138653
[11:01:21.264] TRAIN: iteration 19939 : loss : 0.137743, loss_ce: 0.009202, loss_dice: 0.266285
[11:01:21.472] TRAIN: iteration 19940 : loss : 0.070704, loss_ce: 0.004039, loss_dice: 0.137369
[11:01:21.714] TRAIN: iteration 19941 : loss : 0.141425, loss_ce: 0.024961, loss_dice: 0.257890
[11:01:21.923] TRAIN: iteration 19942 : loss : 0.052125, loss_ce: 0.000750, loss_dice: 0.103499
[11:01:22.133] TRAIN: iteration 19943 : loss : 0.180562, loss_ce: 0.034171, loss_dice: 0.326952
[11:01:22.340] TRAIN: iteration 19944 : loss : 0.060781, loss_ce: 0.004405, loss_dice: 0.117157
[11:01:22.551] TRAIN: iteration 19945 : loss : 0.048344, loss_ce: 0.003540, loss_dice: 0.093149
[11:01:22.761] TRAIN: iteration 19946 : loss : 0.070154, loss_ce: 0.004381, loss_dice: 0.135927
[11:01:24.332] TRAIN: iteration 19947 : loss : 0.243807, loss_ce: 0.004958, loss_dice: 0.482656
[11:01:24.539] TRAIN: iteration 19948 : loss : 0.252055, loss_ce: 0.003866, loss_dice: 0.500245
[11:01:24.747] TRAIN: iteration 19949 : loss : 0.048623, loss_ce: 0.006726, loss_dice: 0.090521
[11:01:24.954] TRAIN: iteration 19950 : loss : 0.250599, loss_ce: 0.005126, loss_dice: 0.496071
[11:01:25.163] TRAIN: iteration 19951 : loss : 0.090555, loss_ce: 0.002481, loss_dice: 0.178629
[11:01:25.371] TRAIN: iteration 19952 : loss : 0.064975, loss_ce: 0.004693, loss_dice: 0.125256
[11:01:25.578] TRAIN: iteration 19953 : loss : 0.059605, loss_ce: 0.003025, loss_dice: 0.116186
[11:01:25.795] TRAIN: iteration 19954 : loss : 0.047818, loss_ce: 0.001923, loss_dice: 0.093713
[11:01:26.005] TRAIN: iteration 19955 : loss : 0.035962, loss_ce: 0.002013, loss_dice: 0.069911
[11:01:26.215] TRAIN: iteration 19956 : loss : 0.047576, loss_ce: 0.011514, loss_dice: 0.083638
[11:01:26.423] TRAIN: iteration 19957 : loss : 0.158418, loss_ce: 0.005614, loss_dice: 0.311222
[11:01:26.642] TRAIN: iteration 19958 : loss : 0.097358, loss_ce: 0.004062, loss_dice: 0.190654
[11:01:26.856] TRAIN: iteration 19959 : loss : 0.160746, loss_ce: 0.003319, loss_dice: 0.318172
[11:01:27.066] TRAIN: iteration 19960 : loss : 0.215869, loss_ce: 0.002861, loss_dice: 0.428878
[11:01:27.067] NaN or Inf found in input tensor.
[11:01:27.289] TRAIN: iteration 19961 : loss : 0.251724, loss_ce: 0.003250, loss_dice: 0.500199
[11:01:27.497] TRAIN: iteration 19962 : loss : 0.245905, loss_ce: 0.001230, loss_dice: 0.490581
[11:01:27.712] TRAIN: iteration 19963 : loss : 0.057631, loss_ce: 0.012903, loss_dice: 0.102358
[11:01:27.920] TRAIN: iteration 19964 : loss : 0.251654, loss_ce: 0.003134, loss_dice: 0.500173
[11:01:28.127] TRAIN: iteration 19965 : loss : 0.064267, loss_ce: 0.002684, loss_dice: 0.125850
[11:01:28.335] TRAIN: iteration 19966 : loss : 0.251242, loss_ce: 0.002372, loss_dice: 0.500111
[11:01:28.542] TRAIN: iteration 19967 : loss : 0.107843, loss_ce: 0.005848, loss_dice: 0.209837
[11:01:28.751] TRAIN: iteration 19968 : loss : 0.147991, loss_ce: 0.002722, loss_dice: 0.293260
[11:01:28.959] TRAIN: iteration 19969 : loss : 0.086257, loss_ce: 0.003799, loss_dice: 0.168715
[11:01:29.168] TRAIN: iteration 19970 : loss : 0.057915, loss_ce: 0.003702, loss_dice: 0.112128
[11:01:29.376] TRAIN: iteration 19971 : loss : 0.096244, loss_ce: 0.001685, loss_dice: 0.190804
[11:01:29.760] TRAIN: iteration 19972 : loss : 0.250789, loss_ce: 0.001538, loss_dice: 0.500041
[11:01:29.969] TRAIN: iteration 19973 : loss : 0.066203, loss_ce: 0.003244, loss_dice: 0.129163
[11:01:30.179] TRAIN: iteration 19974 : loss : 0.076268, loss_ce: 0.002732, loss_dice: 0.149803
[11:01:30.386] TRAIN: iteration 19975 : loss : 0.088587, loss_ce: 0.002625, loss_dice: 0.174548
[11:01:30.593] TRAIN: iteration 19976 : loss : 0.029776, loss_ce: 0.000689, loss_dice: 0.058862
[11:01:30.807] TRAIN: iteration 19977 : loss : 0.153579, loss_ce: 0.009219, loss_dice: 0.297939
[11:01:31.026] TRAIN: iteration 19978 : loss : 0.054528, loss_ce: 0.002234, loss_dice: 0.106821
[11:01:31.236] TRAIN: iteration 19979 : loss : 0.080263, loss_ce: 0.010901, loss_dice: 0.149625
[11:01:31.902] TRAIN: iteration 19980 : loss : 0.237603, loss_ce: 0.004841, loss_dice: 0.470366
[11:01:32.136] TRAIN: iteration 19981 : loss : 0.246442, loss_ce: 0.002643, loss_dice: 0.490241
[11:01:32.343] TRAIN: iteration 19982 : loss : 0.096521, loss_ce: 0.001716, loss_dice: 0.191327
[11:01:33.153] TRAIN: iteration 19983 : loss : 0.124496, loss_ce: 0.002175, loss_dice: 0.246818
[11:01:33.367] TRAIN: iteration 19984 : loss : 0.216957, loss_ce: 0.003109, loss_dice: 0.430804
[11:01:33.575] TRAIN: iteration 19985 : loss : 0.149030, loss_ce: 0.004132, loss_dice: 0.293928
[11:01:33.783] TRAIN: iteration 19986 : loss : 0.125380, loss_ce: 0.004339, loss_dice: 0.246421
[11:01:33.991] TRAIN: iteration 19987 : loss : 0.104378, loss_ce: 0.003116, loss_dice: 0.205639
[11:01:34.200] TRAIN: iteration 19988 : loss : 0.128220, loss_ce: 0.003931, loss_dice: 0.252509
[11:01:34.406] TRAIN: iteration 19989 : loss : 0.096447, loss_ce: 0.008077, loss_dice: 0.184816
[11:01:34.614] TRAIN: iteration 19990 : loss : 0.250678, loss_ce: 0.001885, loss_dice: 0.499472
[11:01:34.822] TRAIN: iteration 19991 : loss : 0.138499, loss_ce: 0.004574, loss_dice: 0.272424
[11:01:35.030] TRAIN: iteration 19992 : loss : 0.141820, loss_ce: 0.007323, loss_dice: 0.276316
[11:01:35.246] TRAIN: iteration 19993 : loss : 0.232426, loss_ce: 0.002100, loss_dice: 0.462751
[11:01:35.454] TRAIN: iteration 19994 : loss : 0.250824, loss_ce: 0.001579, loss_dice: 0.500070
[11:01:35.662] TRAIN: iteration 19995 : loss : 0.117249, loss_ce: 0.006718, loss_dice: 0.227780
[11:01:35.869] TRAIN: iteration 19996 : loss : 0.141152, loss_ce: 0.002637, loss_dice: 0.279667
[11:01:36.079] TRAIN: iteration 19997 : loss : 0.183635, loss_ce: 0.002594, loss_dice: 0.364676
[11:01:36.295] TRAIN: iteration 19998 : loss : 0.089813, loss_ce: 0.003003, loss_dice: 0.176624
[11:01:36.605] TRAIN: iteration 19999 : loss : 0.252074, loss_ce: 0.004003, loss_dice: 0.500146
[11:01:36.814] TRAIN: iteration 20000 : loss : 0.075553, loss_ce: 0.002615, loss_dice: 0.148491
[11:01:37.058] TRAIN: iteration 20001 : loss : 0.091744, loss_ce: 0.026744, loss_dice: 0.156744
[11:01:37.408] TRAIN: iteration 20002 : loss : 0.127195, loss_ce: 0.002921, loss_dice: 0.251469
[11:01:37.617] TRAIN: iteration 20003 : loss : 0.173956, loss_ce: 0.002167, loss_dice: 0.345745
[11:01:37.824] TRAIN: iteration 20004 : loss : 0.132921, loss_ce: 0.005908, loss_dice: 0.259935
[11:01:38.117] TRAIN: iteration 20005 : loss : 0.163222, loss_ce: 0.004210, loss_dice: 0.322235
[11:01:38.328] TRAIN: iteration 20006 : loss : 0.131053, loss_ce: 0.002892, loss_dice: 0.259214
[11:01:38.545] TRAIN: iteration 20007 : loss : 0.137751, loss_ce: 0.005001, loss_dice: 0.270501
[11:01:39.912] TRAIN: iteration 20008 : loss : 0.110868, loss_ce: 0.012211, loss_dice: 0.209526
[11:01:40.121] TRAIN: iteration 20009 : loss : 0.251855, loss_ce: 0.003528, loss_dice: 0.500182
[11:01:40.331] TRAIN: iteration 20010 : loss : 0.252195, loss_ce: 0.004091, loss_dice: 0.500298
[11:01:40.544] TRAIN: iteration 20011 : loss : 0.252244, loss_ce: 0.005256, loss_dice: 0.499232
[11:01:40.753] TRAIN: iteration 20012 : loss : 0.077690, loss_ce: 0.006945, loss_dice: 0.148435
[11:01:40.962] TRAIN: iteration 20013 : loss : 0.244662, loss_ce: 0.003044, loss_dice: 0.486279
[11:01:41.171] TRAIN: iteration 20014 : loss : 0.096611, loss_ce: 0.003699, loss_dice: 0.189523
[11:01:41.769] TRAIN: iteration 20015 : loss : 0.035194, loss_ce: 0.003252, loss_dice: 0.067135
[11:01:42.383] TRAIN: iteration 20016 : loss : 0.171551, loss_ce: 0.005736, loss_dice: 0.337365
[11:01:42.598] TRAIN: iteration 20017 : loss : 0.251523, loss_ce: 0.002887, loss_dice: 0.500159
[11:01:42.822] TRAIN: iteration 20018 : loss : 0.152009, loss_ce: 0.009721, loss_dice: 0.294297
[11:01:43.029] TRAIN: iteration 20019 : loss : 0.193817, loss_ce: 0.004398, loss_dice: 0.383237
[11:01:43.237] TRAIN: iteration 20020 : loss : 0.112822, loss_ce: 0.006754, loss_dice: 0.218890
[11:01:43.472] TRAIN: iteration 20021 : loss : 0.205260, loss_ce: 0.014505, loss_dice: 0.396015
[11:01:43.682] TRAIN: iteration 20022 : loss : 0.040500, loss_ce: 0.002468, loss_dice: 0.078532
[11:01:44.056] TRAIN: iteration 20023 : loss : 0.168339, loss_ce: 0.002828, loss_dice: 0.333850
[11:01:44.611] TRAIN: iteration 20024 : loss : 0.127103, loss_ce: 0.006813, loss_dice: 0.247393
[11:01:44.818] TRAIN: iteration 20025 : loss : 0.251578, loss_ce: 0.002973, loss_dice: 0.500184
[11:01:45.026] TRAIN: iteration 20026 : loss : 0.216646, loss_ce: 0.002623, loss_dice: 0.430670
[11:01:45.235] TRAIN: iteration 20027 : loss : 0.218382, loss_ce: 0.005021, loss_dice: 0.431744
[11:01:45.442] TRAIN: iteration 20028 : loss : 0.104616, loss_ce: 0.002282, loss_dice: 0.206950
[11:01:45.651] TRAIN: iteration 20029 : loss : 0.193965, loss_ce: 0.009444, loss_dice: 0.378487
[11:01:45.860] TRAIN: iteration 20030 : loss : 0.251972, loss_ce: 0.004095, loss_dice: 0.499849
[11:01:46.694] TRAIN: iteration 20031 : loss : 0.156686, loss_ce: 0.003028, loss_dice: 0.310344
[11:01:47.853] TRAIN: iteration 20032 : loss : 0.207199, loss_ce: 0.004480, loss_dice: 0.409918
[11:01:48.060] TRAIN: iteration 20033 : loss : 0.250906, loss_ce: 0.001744, loss_dice: 0.500068
[11:01:48.269] TRAIN: iteration 20034 : loss : 0.083566, loss_ce: 0.001992, loss_dice: 0.165140
[11:01:48.478] TRAIN: iteration 20035 : loss : 0.251087, loss_ce: 0.002052, loss_dice: 0.500121
[11:01:48.684] TRAIN: iteration 20036 : loss : 0.094190, loss_ce: 0.001515, loss_dice: 0.186865
[11:01:48.893] TRAIN: iteration 20037 : loss : 0.095203, loss_ce: 0.004452, loss_dice: 0.185955
[11:01:49.104] TRAIN: iteration 20038 : loss : 0.161707, loss_ce: 0.006937, loss_dice: 0.316478
[11:01:49.312] TRAIN: iteration 20039 : loss : 0.062411, loss_ce: 0.003485, loss_dice: 0.121337
[11:01:49.536] TRAIN: iteration 20040 : loss : 0.141769, loss_ce: 0.008088, loss_dice: 0.275451
[11:01:49.769] TRAIN: iteration 20041 : loss : 0.251102, loss_ce: 0.002100, loss_dice: 0.500105
[11:01:49.977] TRAIN: iteration 20042 : loss : 0.111059, loss_ce: 0.002073, loss_dice: 0.220044
[11:01:50.186] TRAIN: iteration 20043 : loss : 0.058542, loss_ce: 0.004899, loss_dice: 0.112186
[11:01:50.395] TRAIN: iteration 20044 : loss : 0.068004, loss_ce: 0.005129, loss_dice: 0.130879
[11:01:50.605] TRAIN: iteration 20045 : loss : 0.106330, loss_ce: 0.005952, loss_dice: 0.206708
[11:01:50.815] TRAIN: iteration 20046 : loss : 0.250905, loss_ce: 0.001852, loss_dice: 0.499959
[11:01:51.027] TRAIN: iteration 20047 : loss : 0.074025, loss_ce: 0.002576, loss_dice: 0.145474
[11:01:51.235] TRAIN: iteration 20048 : loss : 0.199843, loss_ce: 0.005888, loss_dice: 0.393798
[11:01:51.444] TRAIN: iteration 20049 : loss : 0.028106, loss_ce: 0.004073, loss_dice: 0.052139
[11:01:51.659] TRAIN: iteration 20050 : loss : 0.240065, loss_ce: 0.004887, loss_dice: 0.475242
[11:01:51.869] TRAIN: iteration 20051 : loss : 0.119142, loss_ce: 0.004261, loss_dice: 0.234023
[11:01:52.079] TRAIN: iteration 20052 : loss : 0.192282, loss_ce: 0.004651, loss_dice: 0.379912
[11:01:52.286] TRAIN: iteration 20053 : loss : 0.041046, loss_ce: 0.001487, loss_dice: 0.080606
[11:01:53.299] TRAIN: iteration 20054 : loss : 0.251562, loss_ce: 0.002953, loss_dice: 0.500170
[11:01:53.507] TRAIN: iteration 20055 : loss : 0.122930, loss_ce: 0.002008, loss_dice: 0.243852
[11:01:53.716] TRAIN: iteration 20056 : loss : 0.199138, loss_ce: 0.001587, loss_dice: 0.396690
[11:01:53.924] TRAIN: iteration 20057 : loss : 0.025381, loss_ce: 0.001850, loss_dice: 0.048912
[11:01:54.131] TRAIN: iteration 20058 : loss : 0.092934, loss_ce: 0.001560, loss_dice: 0.184308
[11:01:54.342] TRAIN: iteration 20059 : loss : 0.080934, loss_ce: 0.007378, loss_dice: 0.154490
[11:01:54.551] TRAIN: iteration 20060 : loss : 0.046484, loss_ce: 0.000975, loss_dice: 0.091993
[11:01:54.796] TRAIN: iteration 20061 : loss : 0.121388, loss_ce: 0.003230, loss_dice: 0.239547
[11:01:55.743] TRAIN: iteration 20062 : loss : 0.156495, loss_ce: 0.011092, loss_dice: 0.301899
[11:01:55.952] TRAIN: iteration 20063 : loss : 0.131996, loss_ce: 0.005849, loss_dice: 0.258143
[11:01:56.160] TRAIN: iteration 20064 : loss : 0.088132, loss_ce: 0.001871, loss_dice: 0.174392
[11:01:56.367] TRAIN: iteration 20065 : loss : 0.188746, loss_ce: 0.001275, loss_dice: 0.376217
[11:01:56.575] TRAIN: iteration 20066 : loss : 0.226373, loss_ce: 0.002525, loss_dice: 0.450221
[11:01:56.782] TRAIN: iteration 20067 : loss : 0.250817, loss_ce: 0.001535, loss_dice: 0.500099
[11:01:56.990] TRAIN: iteration 20068 : loss : 0.045504, loss_ce: 0.002823, loss_dice: 0.088186
[11:01:57.197] TRAIN: iteration 20069 : loss : 0.215010, loss_ce: 0.008213, loss_dice: 0.421806
[11:01:57.406] TRAIN: iteration 20070 : loss : 0.092954, loss_ce: 0.001679, loss_dice: 0.184230
[11:01:57.616] TRAIN: iteration 20071 : loss : 0.250474, loss_ce: 0.000925, loss_dice: 0.500023
[11:01:57.823] TRAIN: iteration 20072 : loss : 0.080432, loss_ce: 0.006508, loss_dice: 0.154356
[11:01:58.030] TRAIN: iteration 20073 : loss : 0.250746, loss_ce: 0.001441, loss_dice: 0.500051
[11:01:58.237] TRAIN: iteration 20074 : loss : 0.205858, loss_ce: 0.003461, loss_dice: 0.408254
[11:01:58.478] TRAIN: iteration 20075 : loss : 0.109909, loss_ce: 0.007513, loss_dice: 0.212304
[11:01:58.686] TRAIN: iteration 20076 : loss : 0.076395, loss_ce: 0.001829, loss_dice: 0.150961
[11:01:58.893] TRAIN: iteration 20077 : loss : 0.077481, loss_ce: 0.006374, loss_dice: 0.148589
[11:01:59.101] TRAIN: iteration 20078 : loss : 0.043053, loss_ce: 0.001214, loss_dice: 0.084893
[11:01:59.307] TRAIN: iteration 20079 : loss : 0.118036, loss_ce: 0.001502, loss_dice: 0.234570
[11:01:59.515] TRAIN: iteration 20080 : loss : 0.251052, loss_ce: 0.001989, loss_dice: 0.500114
[11:01:59.516] NaN or Inf found in input tensor.
[11:01:59.731] TRAIN: iteration 20081 : loss : 0.032728, loss_ce: 0.004830, loss_dice: 0.060626
[11:01:59.938] TRAIN: iteration 20082 : loss : 0.096582, loss_ce: 0.005114, loss_dice: 0.188050
[11:02:00.278] TRAIN: iteration 20083 : loss : 0.083455, loss_ce: 0.002383, loss_dice: 0.164527
[11:02:00.487] TRAIN: iteration 20084 : loss : 0.112791, loss_ce: 0.002637, loss_dice: 0.222945
[11:02:01.267] TRAIN: iteration 20085 : loss : 0.094895, loss_ce: 0.005360, loss_dice: 0.184431
[11:02:01.475] TRAIN: iteration 20086 : loss : 0.111539, loss_ce: 0.005195, loss_dice: 0.217883
[11:02:01.682] TRAIN: iteration 20087 : loss : 0.165236, loss_ce: 0.002564, loss_dice: 0.327909
[11:02:01.890] TRAIN: iteration 20088 : loss : 0.062480, loss_ce: 0.016668, loss_dice: 0.108293
[11:02:02.098] TRAIN: iteration 20089 : loss : 0.216050, loss_ce: 0.003863, loss_dice: 0.428236
[11:02:02.307] TRAIN: iteration 20090 : loss : 0.079042, loss_ce: 0.003057, loss_dice: 0.155026
[11:02:02.514] TRAIN: iteration 20091 : loss : 0.105594, loss_ce: 0.005756, loss_dice: 0.205431
[11:02:02.722] TRAIN: iteration 20092 : loss : 0.131483, loss_ce: 0.006395, loss_dice: 0.256570
[11:02:04.232] TRAIN: iteration 20093 : loss : 0.251132, loss_ce: 0.002148, loss_dice: 0.500117
[11:02:04.439] TRAIN: iteration 20094 : loss : 0.154274, loss_ce: 0.005053, loss_dice: 0.303495
[11:02:04.829] TRAIN: iteration 20095 : loss : 0.114356, loss_ce: 0.011835, loss_dice: 0.216876
[11:02:05.038] TRAIN: iteration 20096 : loss : 0.252577, loss_ce: 0.004798, loss_dice: 0.500357
[11:02:05.245] TRAIN: iteration 20097 : loss : 0.066128, loss_ce: 0.004909, loss_dice: 0.127347
[11:02:05.451] TRAIN: iteration 20098 : loss : 0.105425, loss_ce: 0.002048, loss_dice: 0.208803
[11:02:05.659] TRAIN: iteration 20099 : loss : 0.081695, loss_ce: 0.003133, loss_dice: 0.160256
[11:02:05.866] TRAIN: iteration 20100 : loss : 0.251141, loss_ce: 0.002154, loss_dice: 0.500129
[11:02:06.494] TRAIN: iteration 20101 : loss : 0.251812, loss_ce: 0.003389, loss_dice: 0.500235
[11:02:06.702] TRAIN: iteration 20102 : loss : 0.040083, loss_ce: 0.002613, loss_dice: 0.077554
[11:02:06.909] TRAIN: iteration 20103 : loss : 0.041770, loss_ce: 0.002598, loss_dice: 0.080942
[11:02:07.116] TRAIN: iteration 20104 : loss : 0.054132, loss_ce: 0.002864, loss_dice: 0.105399
[11:02:07.329] TRAIN: iteration 20105 : loss : 0.087069, loss_ce: 0.005831, loss_dice: 0.168307
[11:02:07.537] TRAIN: iteration 20106 : loss : 0.250903, loss_ce: 0.001713, loss_dice: 0.500094
[11:02:07.745] TRAIN: iteration 20107 : loss : 0.066551, loss_ce: 0.001141, loss_dice: 0.131960
[11:02:07.955] TRAIN: iteration 20108 : loss : 0.103219, loss_ce: 0.008809, loss_dice: 0.197628
[11:02:08.693] TRAIN: iteration 20109 : loss : 0.077759, loss_ce: 0.002370, loss_dice: 0.153149
[11:02:08.903] TRAIN: iteration 20110 : loss : 0.068839, loss_ce: 0.003497, loss_dice: 0.134182
[11:02:09.113] TRAIN: iteration 20111 : loss : 0.120567, loss_ce: 0.004023, loss_dice: 0.237110
[11:02:09.320] TRAIN: iteration 20112 : loss : 0.073215, loss_ce: 0.004427, loss_dice: 0.142003
[11:02:09.527] TRAIN: iteration 20113 : loss : 0.102170, loss_ce: 0.007626, loss_dice: 0.196713
[11:02:09.739] TRAIN: iteration 20114 : loss : 0.062412, loss_ce: 0.002596, loss_dice: 0.122228
[11:02:09.946] TRAIN: iteration 20115 : loss : 0.250525, loss_ce: 0.001020, loss_dice: 0.500029
[11:02:10.233] TRAIN: iteration 20116 : loss : 0.075145, loss_ce: 0.003830, loss_dice: 0.146461
[11:02:10.443] TRAIN: iteration 20117 : loss : 0.026502, loss_ce: 0.003084, loss_dice: 0.049921
[11:02:10.651] TRAIN: iteration 20118 : loss : 0.199232, loss_ce: 0.001312, loss_dice: 0.397152
[11:02:10.859] TRAIN: iteration 20119 : loss : 0.076635, loss_ce: 0.012821, loss_dice: 0.140448
[11:02:11.067] TRAIN: iteration 20120 : loss : 0.233963, loss_ce: 0.003519, loss_dice: 0.464407
[11:02:11.317] TRAIN: iteration 20121 : loss : 0.093224, loss_ce: 0.009259, loss_dice: 0.177189
[11:02:11.526] TRAIN: iteration 20122 : loss : 0.090038, loss_ce: 0.003664, loss_dice: 0.176412
[11:02:11.737] TRAIN: iteration 20123 : loss : 0.032391, loss_ce: 0.003883, loss_dice: 0.060899
[11:02:11.944] TRAIN: iteration 20124 : loss : 0.049026, loss_ce: 0.001805, loss_dice: 0.096247
[11:02:12.503] TRAIN: iteration 20125 : loss : 0.057820, loss_ce: 0.002122, loss_dice: 0.113519
[11:02:12.712] TRAIN: iteration 20126 : loss : 0.183668, loss_ce: 0.006603, loss_dice: 0.360734
[11:02:12.928] TRAIN: iteration 20127 : loss : 0.250903, loss_ce: 0.001700, loss_dice: 0.500105
[11:02:13.145] TRAIN: iteration 20128 : loss : 0.157500, loss_ce: 0.002227, loss_dice: 0.312773
[11:02:13.352] TRAIN: iteration 20129 : loss : 0.137375, loss_ce: 0.005596, loss_dice: 0.269154
[11:02:13.558] TRAIN: iteration 20130 : loss : 0.045915, loss_ce: 0.002093, loss_dice: 0.089737
[11:02:13.767] TRAIN: iteration 20131 : loss : 0.070677, loss_ce: 0.004040, loss_dice: 0.137315
[11:02:13.975] TRAIN: iteration 20132 : loss : 0.166125, loss_ce: 0.004693, loss_dice: 0.327556
[11:02:14.183] TRAIN: iteration 20133 : loss : 0.043337, loss_ce: 0.003772, loss_dice: 0.082901
[11:02:14.390] TRAIN: iteration 20134 : loss : 0.249204, loss_ce: 0.001879, loss_dice: 0.496530
[11:02:14.601] TRAIN: iteration 20135 : loss : 0.094044, loss_ce: 0.005192, loss_dice: 0.182897
[11:02:14.811] TRAIN: iteration 20136 : loss : 0.086441, loss_ce: 0.003936, loss_dice: 0.168947
[11:02:15.026] TRAIN: iteration 20137 : loss : 0.172144, loss_ce: 0.009708, loss_dice: 0.334580
[11:02:15.235] TRAIN: iteration 20138 : loss : 0.145660, loss_ce: 0.009436, loss_dice: 0.281884
[11:02:15.443] TRAIN: iteration 20139 : loss : 0.142276, loss_ce: 0.004046, loss_dice: 0.280506
[11:02:16.515] TRAIN: iteration 20140 : loss : 0.187550, loss_ce: 0.012504, loss_dice: 0.362597
[11:02:16.754] TRAIN: iteration 20141 : loss : 0.102809, loss_ce: 0.006422, loss_dice: 0.199196
[11:02:16.968] TRAIN: iteration 20142 : loss : 0.087025, loss_ce: 0.003212, loss_dice: 0.170837
[11:02:17.176] TRAIN: iteration 20143 : loss : 0.105470, loss_ce: 0.010871, loss_dice: 0.200069
[11:02:17.383] TRAIN: iteration 20144 : loss : 0.045813, loss_ce: 0.004661, loss_dice: 0.086964
[11:02:17.591] TRAIN: iteration 20145 : loss : 0.111619, loss_ce: 0.003193, loss_dice: 0.220044
[11:02:17.798] TRAIN: iteration 20146 : loss : 0.165154, loss_ce: 0.024285, loss_dice: 0.306023
[11:02:18.006] TRAIN: iteration 20147 : loss : 0.033107, loss_ce: 0.003023, loss_dice: 0.063191
[11:02:18.759] TRAIN: iteration 20148 : loss : 0.092254, loss_ce: 0.003808, loss_dice: 0.180700
[11:02:18.973] TRAIN: iteration 20149 : loss : 0.046410, loss_ce: 0.002985, loss_dice: 0.089834
[11:02:19.181] TRAIN: iteration 20150 : loss : 0.225322, loss_ce: 0.004137, loss_dice: 0.446506
[11:02:19.390] TRAIN: iteration 20151 : loss : 0.055487, loss_ce: 0.008912, loss_dice: 0.102062
[11:02:19.598] TRAIN: iteration 20152 : loss : 0.036916, loss_ce: 0.003595, loss_dice: 0.070237
[11:02:19.805] TRAIN: iteration 20153 : loss : 0.040042, loss_ce: 0.001964, loss_dice: 0.078119
[11:02:20.012] TRAIN: iteration 20154 : loss : 0.241388, loss_ce: 0.002995, loss_dice: 0.479782
[11:02:20.221] TRAIN: iteration 20155 : loss : 0.045586, loss_ce: 0.003698, loss_dice: 0.087474
[11:02:20.670] TRAIN: iteration 20156 : loss : 0.039491, loss_ce: 0.004393, loss_dice: 0.074589
[11:02:20.883] TRAIN: iteration 20157 : loss : 0.095443, loss_ce: 0.003890, loss_dice: 0.186997
[11:02:21.091] TRAIN: iteration 20158 : loss : 0.052196, loss_ce: 0.008339, loss_dice: 0.096054
[11:02:21.301] TRAIN: iteration 20159 : loss : 0.251202, loss_ce: 0.002259, loss_dice: 0.500145
[11:02:21.509] TRAIN: iteration 20160 : loss : 0.049426, loss_ce: 0.007085, loss_dice: 0.091767
[11:02:21.743] TRAIN: iteration 20161 : loss : 0.117371, loss_ce: 0.005169, loss_dice: 0.229574
[11:02:21.953] TRAIN: iteration 20162 : loss : 0.114736, loss_ce: 0.005914, loss_dice: 0.223559
[11:02:22.166] TRAIN: iteration 20163 : loss : 0.131991, loss_ce: 0.002837, loss_dice: 0.261145
[11:02:22.954] TRAIN: iteration 20164 : loss : 0.096583, loss_ce: 0.003471, loss_dice: 0.189694
[11:02:23.161] TRAIN: iteration 20165 : loss : 0.057145, loss_ce: 0.002356, loss_dice: 0.111934
[11:02:23.369] TRAIN: iteration 20166 : loss : 0.063104, loss_ce: 0.003088, loss_dice: 0.123121
[11:02:23.577] TRAIN: iteration 20167 : loss : 0.044851, loss_ce: 0.001058, loss_dice: 0.088644
[11:02:23.786] TRAIN: iteration 20168 : loss : 0.050604, loss_ce: 0.004846, loss_dice: 0.096362
[11:02:26.881] TRAIN: iteration 20169 : loss : 0.092983, loss_ce: 0.009567, loss_dice: 0.176398
[11:02:27.094] TRAIN: iteration 20170 : loss : 0.067959, loss_ce: 0.004179, loss_dice: 0.131739
[11:02:27.303] TRAIN: iteration 20171 : loss : 0.238379, loss_ce: 0.001726, loss_dice: 0.475033
[11:02:27.511] TRAIN: iteration 20172 : loss : 0.042863, loss_ce: 0.004100, loss_dice: 0.081626
[11:02:27.718] TRAIN: iteration 20173 : loss : 0.251264, loss_ce: 0.002369, loss_dice: 0.500159
[11:02:27.933] TRAIN: iteration 20174 : loss : 0.090679, loss_ce: 0.002385, loss_dice: 0.178974
[11:02:28.142] TRAIN: iteration 20175 : loss : 0.226973, loss_ce: 0.003694, loss_dice: 0.450252
[11:02:28.355] TRAIN: iteration 20176 : loss : 0.098468, loss_ce: 0.002389, loss_dice: 0.194546
[11:02:29.998] TRAIN: iteration 20177 : loss : 0.251385, loss_ce: 0.002607, loss_dice: 0.500162
[11:02:30.205] TRAIN: iteration 20178 : loss : 0.251117, loss_ce: 0.002107, loss_dice: 0.500128
[11:02:30.420] TRAIN: iteration 20179 : loss : 0.037246, loss_ce: 0.004525, loss_dice: 0.069966
[11:02:30.635] TRAIN: iteration 20180 : loss : 0.094199, loss_ce: 0.006094, loss_dice: 0.182304
[11:02:30.873] TRAIN: iteration 20181 : loss : 0.180049, loss_ce: 0.005674, loss_dice: 0.354424
[11:02:31.086] TRAIN: iteration 20182 : loss : 0.112547, loss_ce: 0.008833, loss_dice: 0.216262
[11:02:31.295] TRAIN: iteration 20183 : loss : 0.226867, loss_ce: 0.002583, loss_dice: 0.451151
[11:02:31.517] TRAIN: iteration 20184 : loss : 0.084052, loss_ce: 0.007427, loss_dice: 0.160677
[11:02:31.726] TRAIN: iteration 20185 : loss : 0.147305, loss_ce: 0.004876, loss_dice: 0.289734
[11:02:31.935] TRAIN: iteration 20186 : loss : 0.222069, loss_ce: 0.002276, loss_dice: 0.441862
[11:02:32.143] TRAIN: iteration 20187 : loss : 0.056665, loss_ce: 0.002647, loss_dice: 0.110683
[11:02:32.352] TRAIN: iteration 20188 : loss : 0.174897, loss_ce: 0.003474, loss_dice: 0.346320
[11:02:32.562] TRAIN: iteration 20189 : loss : 0.187307, loss_ce: 0.005177, loss_dice: 0.369437
[11:02:32.773] TRAIN: iteration 20190 : loss : 0.204457, loss_ce: 0.003000, loss_dice: 0.405914
[11:02:32.982] TRAIN: iteration 20191 : loss : 0.060190, loss_ce: 0.001421, loss_dice: 0.118960
[11:02:33.196] TRAIN: iteration 20192 : loss : 0.020610, loss_ce: 0.001266, loss_dice: 0.039954
[11:02:34.653] TRAIN: iteration 20193 : loss : 0.251401, loss_ce: 0.002664, loss_dice: 0.500138
[11:02:34.866] TRAIN: iteration 20194 : loss : 0.107440, loss_ce: 0.003849, loss_dice: 0.211032
[11:02:35.074] TRAIN: iteration 20195 : loss : 0.114301, loss_ce: 0.003907, loss_dice: 0.224695
[11:02:35.282] TRAIN: iteration 20196 : loss : 0.101764, loss_ce: 0.005266, loss_dice: 0.198261
[11:02:35.492] TRAIN: iteration 20197 : loss : 0.202510, loss_ce: 0.009495, loss_dice: 0.395525
[11:02:35.706] TRAIN: iteration 20198 : loss : 0.065799, loss_ce: 0.003526, loss_dice: 0.128071
[11:02:35.916] TRAIN: iteration 20199 : loss : 0.245919, loss_ce: 0.005323, loss_dice: 0.486514
[11:02:36.126] TRAIN: iteration 20200 : loss : 0.055032, loss_ce: 0.002200, loss_dice: 0.107863
[11:02:38.641] TRAIN: iteration 20201 : loss : 0.138946, loss_ce: 0.003522, loss_dice: 0.274369
[11:02:38.853] TRAIN: iteration 20202 : loss : 0.153141, loss_ce: 0.013276, loss_dice: 0.293006
[11:02:39.066] TRAIN: iteration 20203 : loss : 0.112852, loss_ce: 0.003311, loss_dice: 0.222393
[11:02:39.282] TRAIN: iteration 20204 : loss : 0.031611, loss_ce: 0.003904, loss_dice: 0.059319
[11:02:39.491] TRAIN: iteration 20205 : loss : 0.119782, loss_ce: 0.001502, loss_dice: 0.238062
[11:02:39.699] TRAIN: iteration 20206 : loss : 0.209033, loss_ce: 0.003218, loss_dice: 0.414847
[11:02:39.911] TRAIN: iteration 20207 : loss : 0.061879, loss_ce: 0.002107, loss_dice: 0.121652
[11:02:40.125] TRAIN: iteration 20208 : loss : 0.250851, loss_ce: 0.001654, loss_dice: 0.500048
[11:02:43.008] TRAIN: iteration 20209 : loss : 0.250574, loss_ce: 0.001108, loss_dice: 0.500040
[11:02:43.217] TRAIN: iteration 20210 : loss : 0.087878, loss_ce: 0.003930, loss_dice: 0.171827
[11:02:43.427] TRAIN: iteration 20211 : loss : 0.033253, loss_ce: 0.002193, loss_dice: 0.064313
[11:02:43.635] TRAIN: iteration 20212 : loss : 0.047705, loss_ce: 0.001955, loss_dice: 0.093455
[11:02:43.844] TRAIN: iteration 20213 : loss : 0.100304, loss_ce: 0.001360, loss_dice: 0.199247
[11:02:44.053] TRAIN: iteration 20214 : loss : 0.121899, loss_ce: 0.004826, loss_dice: 0.238971
[11:02:44.273] TRAIN: iteration 20215 : loss : 0.119343, loss_ce: 0.004043, loss_dice: 0.234643
[11:02:44.485] TRAIN: iteration 20216 : loss : 0.102419, loss_ce: 0.002063, loss_dice: 0.202774
[11:02:44.692] TRAIN: iteration 20217 : loss : 0.155839, loss_ce: 0.002982, loss_dice: 0.308696
[11:02:44.905] TRAIN: iteration 20218 : loss : 0.173887, loss_ce: 0.010156, loss_dice: 0.337617
[11:02:45.116] TRAIN: iteration 20219 : loss : 0.073364, loss_ce: 0.004240, loss_dice: 0.142489
[11:02:45.323] TRAIN: iteration 20220 : loss : 0.100316, loss_ce: 0.003166, loss_dice: 0.197467
[11:02:45.323] NaN or Inf found in input tensor.
[11:02:45.538] TRAIN: iteration 20221 : loss : 0.184867, loss_ce: 0.005362, loss_dice: 0.364373
[11:02:45.766] TRAIN: iteration 20222 : loss : 0.252855, loss_ce: 0.005318, loss_dice: 0.500391
[11:02:45.973] TRAIN: iteration 20223 : loss : 0.251258, loss_ce: 0.002372, loss_dice: 0.500143
[11:02:46.190] TRAIN: iteration 20224 : loss : 0.036317, loss_ce: 0.001908, loss_dice: 0.070725
[11:02:49.435] TRAIN: iteration 20225 : loss : 0.115689, loss_ce: 0.003336, loss_dice: 0.228041
[11:02:49.642] TRAIN: iteration 20226 : loss : 0.056384, loss_ce: 0.008805, loss_dice: 0.103963
[11:02:49.851] TRAIN: iteration 20227 : loss : 0.070090, loss_ce: 0.006742, loss_dice: 0.133437
[11:02:50.059] TRAIN: iteration 20228 : loss : 0.215997, loss_ce: 0.002748, loss_dice: 0.429247
[11:02:50.269] TRAIN: iteration 20229 : loss : 0.070765, loss_ce: 0.003254, loss_dice: 0.138276
[11:02:50.478] TRAIN: iteration 20230 : loss : 0.096584, loss_ce: 0.008225, loss_dice: 0.184943
[11:02:50.689] TRAIN: iteration 20231 : loss : 0.055155, loss_ce: 0.003529, loss_dice: 0.106780
[11:02:50.898] TRAIN: iteration 20232 : loss : 0.219126, loss_ce: 0.002716, loss_dice: 0.435536
[11:02:51.825] TRAIN: iteration 20233 : loss : 0.227671, loss_ce: 0.010463, loss_dice: 0.444879
[11:02:52.038] TRAIN: iteration 20234 : loss : 0.055909, loss_ce: 0.003760, loss_dice: 0.108058
[11:02:52.247] TRAIN: iteration 20235 : loss : 0.074777, loss_ce: 0.004982, loss_dice: 0.144571
[11:02:52.459] TRAIN: iteration 20236 : loss : 0.022894, loss_ce: 0.001228, loss_dice: 0.044561
[11:02:52.668] TRAIN: iteration 20237 : loss : 0.080511, loss_ce: 0.004260, loss_dice: 0.156762
[11:02:52.876] TRAIN: iteration 20238 : loss : 0.191987, loss_ce: 0.003542, loss_dice: 0.380431
[11:02:53.085] TRAIN: iteration 20239 : loss : 0.012282, loss_ce: 0.001222, loss_dice: 0.023342
[11:02:53.294] TRAIN: iteration 20240 : loss : 0.047895, loss_ce: 0.001006, loss_dice: 0.094783
[11:02:55.874] TRAIN: iteration 20241 : loss : 0.206538, loss_ce: 0.003615, loss_dice: 0.409461
[11:02:56.083] TRAIN: iteration 20242 : loss : 0.079488, loss_ce: 0.001354, loss_dice: 0.157622
[11:02:56.292] TRAIN: iteration 20243 : loss : 0.156474, loss_ce: 0.002212, loss_dice: 0.310735
[11:02:56.501] TRAIN: iteration 20244 : loss : 0.176981, loss_ce: 0.003316, loss_dice: 0.350647
[11:02:56.709] TRAIN: iteration 20245 : loss : 0.103731, loss_ce: 0.006374, loss_dice: 0.201088
[11:02:56.916] TRAIN: iteration 20246 : loss : 0.232455, loss_ce: 0.011667, loss_dice: 0.453242
[11:02:57.127] TRAIN: iteration 20247 : loss : 0.126725, loss_ce: 0.005838, loss_dice: 0.247613
[11:02:57.339] TRAIN: iteration 20248 : loss : 0.250515, loss_ce: 0.000995, loss_dice: 0.500036
[11:02:59.338] TRAIN: iteration 20249 : loss : 0.208133, loss_ce: 0.029535, loss_dice: 0.386731
[11:02:59.549] TRAIN: iteration 20250 : loss : 0.250870, loss_ce: 0.001643, loss_dice: 0.500097
[11:02:59.761] TRAIN: iteration 20251 : loss : 0.027455, loss_ce: 0.002641, loss_dice: 0.052269
[11:02:59.969] TRAIN: iteration 20252 : loss : 0.097064, loss_ce: 0.002501, loss_dice: 0.191626
[11:03:00.179] TRAIN: iteration 20253 : loss : 0.106692, loss_ce: 0.006871, loss_dice: 0.206512
[11:03:00.389] TRAIN: iteration 20254 : loss : 0.042562, loss_ce: 0.001411, loss_dice: 0.083713
[11:03:00.598] TRAIN: iteration 20255 : loss : 0.076561, loss_ce: 0.001341, loss_dice: 0.151782
[11:03:00.807] TRAIN: iteration 20256 : loss : 0.238378, loss_ce: 0.004091, loss_dice: 0.472664
[11:03:01.559] TRAIN: iteration 20257 : loss : 0.113086, loss_ce: 0.016134, loss_dice: 0.210038
[11:03:01.768] TRAIN: iteration 20258 : loss : 0.047996, loss_ce: 0.001973, loss_dice: 0.094019
[11:03:01.981] TRAIN: iteration 20259 : loss : 0.124488, loss_ce: 0.008039, loss_dice: 0.240937
[11:03:02.189] TRAIN: iteration 20260 : loss : 0.039277, loss_ce: 0.001710, loss_dice: 0.076844
[11:03:02.425] TRAIN: iteration 20261 : loss : 0.251458, loss_ce: 0.002751, loss_dice: 0.500165
[11:03:02.635] TRAIN: iteration 20262 : loss : 0.235392, loss_ce: 0.004004, loss_dice: 0.466779
[11:03:02.844] TRAIN: iteration 20263 : loss : 0.045926, loss_ce: 0.001341, loss_dice: 0.090511
[11:03:03.065] TRAIN: iteration 20264 : loss : 0.250789, loss_ce: 0.001530, loss_dice: 0.500048
[11:03:03.274] TRAIN: iteration 20265 : loss : 0.141946, loss_ce: 0.004612, loss_dice: 0.279281
[11:03:05.038] TRAIN: iteration 20266 : loss : 0.231940, loss_ce: 0.003746, loss_dice: 0.460135
[11:03:05.245] TRAIN: iteration 20267 : loss : 0.087712, loss_ce: 0.009134, loss_dice: 0.166290
[11:03:05.453] TRAIN: iteration 20268 : loss : 0.036346, loss_ce: 0.002863, loss_dice: 0.069829
[11:03:05.662] TRAIN: iteration 20269 : loss : 0.251337, loss_ce: 0.002566, loss_dice: 0.500109
[11:03:05.871] TRAIN: iteration 20270 : loss : 0.154106, loss_ce: 0.004575, loss_dice: 0.303637
[11:03:06.083] TRAIN: iteration 20271 : loss : 0.071281, loss_ce: 0.006605, loss_dice: 0.135957
[11:03:06.297] TRAIN: iteration 20272 : loss : 0.136598, loss_ce: 0.005733, loss_dice: 0.267464
[11:03:06.504] TRAIN: iteration 20273 : loss : 0.101422, loss_ce: 0.003073, loss_dice: 0.199770
[11:03:07.113] TRAIN: iteration 20274 : loss : 0.204727, loss_ce: 0.003646, loss_dice: 0.405807
[11:03:07.321] TRAIN: iteration 20275 : loss : 0.244975, loss_ce: 0.002980, loss_dice: 0.486971
[11:03:07.529] TRAIN: iteration 20276 : loss : 0.060695, loss_ce: 0.005468, loss_dice: 0.115921
[11:03:07.737] TRAIN: iteration 20277 : loss : 0.110941, loss_ce: 0.007182, loss_dice: 0.214699
[11:03:07.946] TRAIN: iteration 20278 : loss : 0.075431, loss_ce: 0.005376, loss_dice: 0.145487
[11:03:08.154] TRAIN: iteration 20279 : loss : 0.127014, loss_ce: 0.001871, loss_dice: 0.252157
[11:03:08.363] TRAIN: iteration 20280 : loss : 0.249608, loss_ce: 0.005949, loss_dice: 0.493267
[11:03:10.220] TRAIN: iteration 20281 : loss : 0.091912, loss_ce: 0.004565, loss_dice: 0.179258
[11:03:10.435] TRAIN: iteration 20282 : loss : 0.103897, loss_ce: 0.004371, loss_dice: 0.203423
[11:03:10.642] TRAIN: iteration 20283 : loss : 0.109146, loss_ce: 0.002603, loss_dice: 0.215690
[11:03:10.849] TRAIN: iteration 20284 : loss : 0.109936, loss_ce: 0.005545, loss_dice: 0.214326
[11:03:11.057] TRAIN: iteration 20285 : loss : 0.194969, loss_ce: 0.006170, loss_dice: 0.383768
[11:03:11.265] TRAIN: iteration 20286 : loss : 0.080071, loss_ce: 0.006541, loss_dice: 0.153601
[11:03:11.473] TRAIN: iteration 20287 : loss : 0.251119, loss_ce: 0.002148, loss_dice: 0.500091
[11:03:11.681] TRAIN: iteration 20288 : loss : 0.183186, loss_ce: 0.004972, loss_dice: 0.361401
[11:03:15.179] TRAIN: iteration 20289 : loss : 0.096929, loss_ce: 0.006718, loss_dice: 0.187141
[11:03:15.393] TRAIN: iteration 20290 : loss : 0.107138, loss_ce: 0.008228, loss_dice: 0.206049
[11:03:15.603] TRAIN: iteration 20291 : loss : 0.129500, loss_ce: 0.011145, loss_dice: 0.247855
[11:03:15.811] TRAIN: iteration 20292 : loss : 0.207178, loss_ce: 0.003995, loss_dice: 0.410362
[11:03:16.020] TRAIN: iteration 20293 : loss : 0.025318, loss_ce: 0.001743, loss_dice: 0.048892
[11:03:16.229] TRAIN: iteration 20294 : loss : 0.056164, loss_ce: 0.004393, loss_dice: 0.107934
[11:03:16.443] TRAIN: iteration 20295 : loss : 0.199806, loss_ce: 0.002768, loss_dice: 0.396844
[11:03:16.652] TRAIN: iteration 20296 : loss : 0.070675, loss_ce: 0.002232, loss_dice: 0.139119
[11:03:17.114] TRAIN: iteration 20297 : loss : 0.155443, loss_ce: 0.003695, loss_dice: 0.307190
[11:03:18.171] TRAIN: iteration 20298 : loss : 0.167543, loss_ce: 0.002279, loss_dice: 0.332807
[11:03:18.382] TRAIN: iteration 20299 : loss : 0.041716, loss_ce: 0.005502, loss_dice: 0.077930
[11:03:18.590] TRAIN: iteration 20300 : loss : 0.151876, loss_ce: 0.005401, loss_dice: 0.298351
[11:03:18.827] TRAIN: iteration 20301 : loss : 0.225928, loss_ce: 0.003042, loss_dice: 0.448814
[11:03:19.035] TRAIN: iteration 20302 : loss : 0.252335, loss_ce: 0.004369, loss_dice: 0.500301
[11:03:19.248] TRAIN: iteration 20303 : loss : 0.065025, loss_ce: 0.001309, loss_dice: 0.128741
[11:03:19.455] TRAIN: iteration 20304 : loss : 0.039966, loss_ce: 0.002411, loss_dice: 0.077522
[11:03:19.663] TRAIN: iteration 20305 : loss : 0.250999, loss_ce: 0.002039, loss_dice: 0.499960
[11:03:22.168] TRAIN: iteration 20306 : loss : 0.054772, loss_ce: 0.003405, loss_dice: 0.106138
[11:03:22.378] TRAIN: iteration 20307 : loss : 0.084761, loss_ce: 0.004114, loss_dice: 0.165408
[11:03:22.593] TRAIN: iteration 20308 : loss : 0.050169, loss_ce: 0.008744, loss_dice: 0.091595
[11:03:22.805] TRAIN: iteration 20309 : loss : 0.053479, loss_ce: 0.003859, loss_dice: 0.103098
[11:03:23.016] TRAIN: iteration 20310 : loss : 0.152834, loss_ce: 0.003734, loss_dice: 0.301935
[11:03:23.229] TRAIN: iteration 20311 : loss : 0.227507, loss_ce: 0.004646, loss_dice: 0.450367
[11:03:23.437] TRAIN: iteration 20312 : loss : 0.105964, loss_ce: 0.002334, loss_dice: 0.209594
[11:03:23.644] TRAIN: iteration 20313 : loss : 0.069549, loss_ce: 0.008513, loss_dice: 0.130586
[11:03:24.984] TRAIN: iteration 20314 : loss : 0.083826, loss_ce: 0.004755, loss_dice: 0.162896
[11:03:25.192] TRAIN: iteration 20315 : loss : 0.161126, loss_ce: 0.004929, loss_dice: 0.317323
[11:03:25.400] TRAIN: iteration 20316 : loss : 0.105252, loss_ce: 0.013555, loss_dice: 0.196948
[11:03:25.609] TRAIN: iteration 20317 : loss : 0.238206, loss_ce: 0.007670, loss_dice: 0.468742
[11:03:25.817] TRAIN: iteration 20318 : loss : 0.061190, loss_ce: 0.004416, loss_dice: 0.117964
[11:03:26.026] TRAIN: iteration 20319 : loss : 0.073198, loss_ce: 0.001570, loss_dice: 0.144826
[11:03:26.233] TRAIN: iteration 20320 : loss : 0.083576, loss_ce: 0.002877, loss_dice: 0.164276
[11:03:26.475] TRAIN: iteration 20321 : loss : 0.021432, loss_ce: 0.001206, loss_dice: 0.041657
[11:03:28.567] TRAIN: iteration 20322 : loss : 0.105108, loss_ce: 0.001596, loss_dice: 0.208620
[11:03:28.784] TRAIN: iteration 20323 : loss : 0.131536, loss_ce: 0.004559, loss_dice: 0.258514
[11:03:28.992] TRAIN: iteration 20324 : loss : 0.191035, loss_ce: 0.002866, loss_dice: 0.379204
[11:03:29.202] TRAIN: iteration 20325 : loss : 0.161429, loss_ce: 0.003076, loss_dice: 0.319781
[11:03:29.410] TRAIN: iteration 20326 : loss : 0.130941, loss_ce: 0.009817, loss_dice: 0.252065
[11:03:29.618] TRAIN: iteration 20327 : loss : 0.061335, loss_ce: 0.002385, loss_dice: 0.120286
[11:03:29.826] TRAIN: iteration 20328 : loss : 0.055296, loss_ce: 0.009134, loss_dice: 0.101459
[11:03:30.034] TRAIN: iteration 20329 : loss : 0.235117, loss_ce: 0.004139, loss_dice: 0.466096
[11:03:30.242] TRAIN: iteration 20330 : loss : 0.245683, loss_ce: 0.004320, loss_dice: 0.487047
[11:03:30.449] TRAIN: iteration 20331 : loss : 0.030059, loss_ce: 0.003406, loss_dice: 0.056712
[11:03:30.656] TRAIN: iteration 20332 : loss : 0.167940, loss_ce: 0.005231, loss_dice: 0.330648
[11:03:30.864] TRAIN: iteration 20333 : loss : 0.135244, loss_ce: 0.002188, loss_dice: 0.268301
[11:03:31.072] TRAIN: iteration 20334 : loss : 0.103670, loss_ce: 0.003815, loss_dice: 0.203526
[11:03:31.279] TRAIN: iteration 20335 : loss : 0.114663, loss_ce: 0.002781, loss_dice: 0.226544
[11:03:31.487] TRAIN: iteration 20336 : loss : 0.116659, loss_ce: 0.003274, loss_dice: 0.230044
[11:03:31.695] TRAIN: iteration 20337 : loss : 0.245432, loss_ce: 0.002859, loss_dice: 0.488005
[11:03:32.843] TRAIN: iteration 20338 : loss : 0.142874, loss_ce: 0.031418, loss_dice: 0.254330
[11:03:33.050] TRAIN: iteration 20339 : loss : 0.074038, loss_ce: 0.003658, loss_dice: 0.144417
[11:03:33.258] TRAIN: iteration 20340 : loss : 0.068394, loss_ce: 0.003296, loss_dice: 0.133491
[11:03:33.490] TRAIN: iteration 20341 : loss : 0.047022, loss_ce: 0.004254, loss_dice: 0.089791
[11:03:35.063] TRAIN: iteration 20342 : loss : 0.147111, loss_ce: 0.004725, loss_dice: 0.289496
[11:03:35.271] TRAIN: iteration 20343 : loss : 0.251583, loss_ce: 0.002996, loss_dice: 0.500171
[11:03:35.478] TRAIN: iteration 20344 : loss : 0.145644, loss_ce: 0.003771, loss_dice: 0.287517
[11:03:35.685] TRAIN: iteration 20345 : loss : 0.230914, loss_ce: 0.007060, loss_dice: 0.454768
[11:03:36.626] TRAIN: iteration 20346 : loss : 0.057307, loss_ce: 0.006769, loss_dice: 0.107845
[11:03:36.834] TRAIN: iteration 20347 : loss : 0.069224, loss_ce: 0.006542, loss_dice: 0.131905
[11:03:37.045] TRAIN: iteration 20348 : loss : 0.071802, loss_ce: 0.002312, loss_dice: 0.141293
[11:03:37.252] TRAIN: iteration 20349 : loss : 0.038281, loss_ce: 0.003680, loss_dice: 0.072882
[11:03:37.459] TRAIN: iteration 20350 : loss : 0.193600, loss_ce: 0.004245, loss_dice: 0.382954
[11:03:37.666] TRAIN: iteration 20351 : loss : 0.035496, loss_ce: 0.001480, loss_dice: 0.069512
[11:03:37.917] TRAIN: iteration 20352 : loss : 0.087936, loss_ce: 0.007682, loss_dice: 0.168190
[11:03:40.306] TRAIN: iteration 20353 : loss : 0.250939, loss_ce: 0.001785, loss_dice: 0.500094
[11:03:40.517] TRAIN: iteration 20354 : loss : 0.062043, loss_ce: 0.004857, loss_dice: 0.119228
[11:03:40.726] TRAIN: iteration 20355 : loss : 0.097928, loss_ce: 0.001301, loss_dice: 0.194555
[11:03:40.938] TRAIN: iteration 20356 : loss : 0.132362, loss_ce: 0.005141, loss_dice: 0.259584
[11:03:41.148] TRAIN: iteration 20357 : loss : 0.214674, loss_ce: 0.002615, loss_dice: 0.426734
[11:03:41.357] TRAIN: iteration 20358 : loss : 0.139400, loss_ce: 0.002417, loss_dice: 0.276383
[11:03:41.569] TRAIN: iteration 20359 : loss : 0.076194, loss_ce: 0.003419, loss_dice: 0.148968
[11:03:41.779] TRAIN: iteration 20360 : loss : 0.147748, loss_ce: 0.004039, loss_dice: 0.291458
[11:03:44.304] TRAIN: iteration 20361 : loss : 0.073985, loss_ce: 0.003028, loss_dice: 0.144941
[11:03:44.515] TRAIN: iteration 20362 : loss : 0.139841, loss_ce: 0.008386, loss_dice: 0.271295
[11:03:44.722] TRAIN: iteration 20363 : loss : 0.083650, loss_ce: 0.003998, loss_dice: 0.163303
[11:03:44.929] TRAIN: iteration 20364 : loss : 0.202218, loss_ce: 0.002120, loss_dice: 0.402317
[11:03:45.137] TRAIN: iteration 20365 : loss : 0.219911, loss_ce: 0.001839, loss_dice: 0.437984
[11:03:45.343] TRAIN: iteration 20366 : loss : 0.146655, loss_ce: 0.001586, loss_dice: 0.291724
[11:03:45.550] TRAIN: iteration 20367 : loss : 0.086407, loss_ce: 0.001605, loss_dice: 0.171210
[11:03:45.757] TRAIN: iteration 20368 : loss : 0.070452, loss_ce: 0.001390, loss_dice: 0.139514
[11:03:49.115] TRAIN: iteration 20369 : loss : 0.094592, loss_ce: 0.002253, loss_dice: 0.186931
[11:03:49.328] TRAIN: iteration 20370 : loss : 0.060873, loss_ce: 0.003437, loss_dice: 0.118309
[11:03:49.542] TRAIN: iteration 20371 : loss : 0.157197, loss_ce: 0.007915, loss_dice: 0.306479
[11:03:49.751] TRAIN: iteration 20372 : loss : 0.090709, loss_ce: 0.009422, loss_dice: 0.171996
[11:03:49.963] TRAIN: iteration 20373 : loss : 0.126956, loss_ce: 0.001679, loss_dice: 0.252232
[11:03:50.174] TRAIN: iteration 20374 : loss : 0.248969, loss_ce: 0.001973, loss_dice: 0.495966
[11:03:50.383] TRAIN: iteration 20375 : loss : 0.228081, loss_ce: 0.003484, loss_dice: 0.452678
[11:03:50.591] TRAIN: iteration 20376 : loss : 0.251869, loss_ce: 0.003487, loss_dice: 0.500251
[11:03:52.009] TRAIN: iteration 20377 : loss : 0.143952, loss_ce: 0.007255, loss_dice: 0.280649
[11:03:52.217] TRAIN: iteration 20378 : loss : 0.108875, loss_ce: 0.004257, loss_dice: 0.213494
[11:03:52.426] TRAIN: iteration 20379 : loss : 0.084463, loss_ce: 0.001448, loss_dice: 0.167479
[11:03:52.636] TRAIN: iteration 20380 : loss : 0.037028, loss_ce: 0.001464, loss_dice: 0.072591
[11:03:52.878] TRAIN: iteration 20381 : loss : 0.116478, loss_ce: 0.004659, loss_dice: 0.228296
[11:03:53.087] TRAIN: iteration 20382 : loss : 0.250974, loss_ce: 0.001881, loss_dice: 0.500067
[11:03:53.296] TRAIN: iteration 20383 : loss : 0.251606, loss_ce: 0.002999, loss_dice: 0.500213
[11:03:53.506] TRAIN: iteration 20384 : loss : 0.049711, loss_ce: 0.003921, loss_dice: 0.095501
[11:03:53.717] TRAIN: iteration 20385 : loss : 0.022124, loss_ce: 0.003637, loss_dice: 0.040611
[11:03:53.925] TRAIN: iteration 20386 : loss : 0.057228, loss_ce: 0.002996, loss_dice: 0.111460
[11:03:54.315] TRAIN: iteration 20387 : loss : 0.243895, loss_ce: 0.002267, loss_dice: 0.485524
[11:03:54.526] TRAIN: iteration 20388 : loss : 0.028846, loss_ce: 0.001764, loss_dice: 0.055928
[11:03:54.737] TRAIN: iteration 20389 : loss : 0.070369, loss_ce: 0.001325, loss_dice: 0.139414
[11:03:54.947] TRAIN: iteration 20390 : loss : 0.244895, loss_ce: 0.006001, loss_dice: 0.483789
[11:03:55.156] TRAIN: iteration 20391 : loss : 0.041604, loss_ce: 0.004247, loss_dice: 0.078961
[11:03:55.365] TRAIN: iteration 20392 : loss : 0.096826, loss_ce: 0.002700, loss_dice: 0.190953
[11:03:55.574] TRAIN: iteration 20393 : loss : 0.045400, loss_ce: 0.003366, loss_dice: 0.087434
[11:03:55.781] TRAIN: iteration 20394 : loss : 0.055247, loss_ce: 0.007465, loss_dice: 0.103029
[11:03:55.989] TRAIN: iteration 20395 : loss : 0.235888, loss_ce: 0.006586, loss_dice: 0.465189
[11:03:56.200] TRAIN: iteration 20396 : loss : 0.177341, loss_ce: 0.004160, loss_dice: 0.350523
[11:03:57.887] TRAIN: iteration 20397 : loss : 0.187196, loss_ce: 0.005913, loss_dice: 0.368480
[11:03:58.095] TRAIN: iteration 20398 : loss : 0.203810, loss_ce: 0.003028, loss_dice: 0.404593
[11:03:58.303] TRAIN: iteration 20399 : loss : 0.119281, loss_ce: 0.005132, loss_dice: 0.233431
[11:03:58.511] TRAIN: iteration 20400 : loss : 0.250082, loss_ce: 0.004532, loss_dice: 0.495631
[11:03:58.748] TRAIN: iteration 20401 : loss : 0.046267, loss_ce: 0.001762, loss_dice: 0.090772
[11:03:58.955] TRAIN: iteration 20402 : loss : 0.022451, loss_ce: 0.001287, loss_dice: 0.043614
[11:03:59.170] TRAIN: iteration 20403 : loss : 0.129014, loss_ce: 0.002990, loss_dice: 0.255037
[11:03:59.377] TRAIN: iteration 20404 : loss : 0.100691, loss_ce: 0.002601, loss_dice: 0.198781
[11:03:59.586] TRAIN: iteration 20405 : loss : 0.251285, loss_ce: 0.002444, loss_dice: 0.500125
[11:03:59.918] TRAIN: iteration 20406 : loss : 0.208661, loss_ce: 0.002528, loss_dice: 0.414795
[11:04:00.126] TRAIN: iteration 20407 : loss : 0.045535, loss_ce: 0.003874, loss_dice: 0.087195
[11:04:00.780] TRAIN: iteration 20408 : loss : 0.250650, loss_ce: 0.001273, loss_dice: 0.500028
[11:04:00.988] TRAIN: iteration 20409 : loss : 0.086925, loss_ce: 0.003511, loss_dice: 0.170340
[11:04:01.511] TRAIN: iteration 20410 : loss : 0.133642, loss_ce: 0.003103, loss_dice: 0.264181
[11:04:01.720] TRAIN: iteration 20411 : loss : 0.082893, loss_ce: 0.003302, loss_dice: 0.162484
[11:04:02.276] TRAIN: iteration 20412 : loss : 0.187402, loss_ce: 0.001908, loss_dice: 0.372896
[11:04:02.500] TRAIN: iteration 20413 : loss : 0.102872, loss_ce: 0.002133, loss_dice: 0.203611
[11:04:02.711] TRAIN: iteration 20414 : loss : 0.090088, loss_ce: 0.001502, loss_dice: 0.178673
[11:04:02.921] TRAIN: iteration 20415 : loss : 0.250534, loss_ce: 0.001041, loss_dice: 0.500026
[11:04:04.839] TRAIN: iteration 20416 : loss : 0.248026, loss_ce: 0.001209, loss_dice: 0.494843
[11:04:05.049] TRAIN: iteration 20417 : loss : 0.105030, loss_ce: 0.002691, loss_dice: 0.207369
[11:04:05.633] TRAIN: iteration 20418 : loss : 0.067928, loss_ce: 0.009362, loss_dice: 0.126493
[11:04:05.842] TRAIN: iteration 20419 : loss : 0.209592, loss_ce: 0.003542, loss_dice: 0.415642
[11:04:06.051] TRAIN: iteration 20420 : loss : 0.215929, loss_ce: 0.005962, loss_dice: 0.425895
[11:04:07.998] TRAIN: iteration 20421 : loss : 0.044553, loss_ce: 0.001026, loss_dice: 0.088079
[11:04:08.215] TRAIN: iteration 20422 : loss : 0.227300, loss_ce: 0.006811, loss_dice: 0.447789
[11:04:08.424] TRAIN: iteration 20423 : loss : 0.147909, loss_ce: 0.004212, loss_dice: 0.291606
[11:04:08.632] TRAIN: iteration 20424 : loss : 0.122071, loss_ce: 0.002879, loss_dice: 0.241262
[11:04:08.843] TRAIN: iteration 20425 : loss : 0.202208, loss_ce: 0.011483, loss_dice: 0.392933
[11:04:09.052] TRAIN: iteration 20426 : loss : 0.221044, loss_ce: 0.039709, loss_dice: 0.402379
[11:04:09.261] TRAIN: iteration 20427 : loss : 0.158352, loss_ce: 0.004190, loss_dice: 0.312513
[11:04:09.592] TRAIN: iteration 20428 : loss : 0.123313, loss_ce: 0.003576, loss_dice: 0.243050
[11:04:14.126] TRAIN: iteration 20429 : loss : 0.068691, loss_ce: 0.003652, loss_dice: 0.133731
[11:04:14.334] TRAIN: iteration 20430 : loss : 0.188063, loss_ce: 0.002819, loss_dice: 0.373306
[11:04:14.546] TRAIN: iteration 20431 : loss : 0.044265, loss_ce: 0.005690, loss_dice: 0.082840
[11:04:14.754] TRAIN: iteration 20432 : loss : 0.189583, loss_ce: 0.003251, loss_dice: 0.375914
[11:04:14.965] TRAIN: iteration 20433 : loss : 0.191897, loss_ce: 0.011308, loss_dice: 0.372485
[11:04:15.174] TRAIN: iteration 20434 : loss : 0.167191, loss_ce: 0.002997, loss_dice: 0.331385
[11:04:15.480] TRAIN: iteration 20435 : loss : 0.211235, loss_ce: 0.041752, loss_dice: 0.380718
[11:04:15.687] TRAIN: iteration 20436 : loss : 0.103269, loss_ce: 0.001556, loss_dice: 0.204982
[11:04:16.234] TRAIN: iteration 20437 : loss : 0.128671, loss_ce: 0.004932, loss_dice: 0.252409
[11:04:16.444] TRAIN: iteration 20438 : loss : 0.169110, loss_ce: 0.002844, loss_dice: 0.335377
[11:04:16.652] TRAIN: iteration 20439 : loss : 0.208447, loss_ce: 0.003748, loss_dice: 0.413147
[11:04:16.860] TRAIN: iteration 20440 : loss : 0.103556, loss_ce: 0.002758, loss_dice: 0.204353
[11:04:17.100] TRAIN: iteration 20441 : loss : 0.122702, loss_ce: 0.011234, loss_dice: 0.234171
[11:04:17.308] TRAIN: iteration 20442 : loss : 0.219961, loss_ce: 0.003096, loss_dice: 0.436826
[11:04:17.516] TRAIN: iteration 20443 : loss : 0.067542, loss_ce: 0.004123, loss_dice: 0.130961
[11:04:17.731] TRAIN: iteration 20444 : loss : 0.080714, loss_ce: 0.004338, loss_dice: 0.157091
[11:04:20.001] TRAIN: iteration 20445 : loss : 0.068463, loss_ce: 0.003024, loss_dice: 0.133903
[11:04:20.211] TRAIN: iteration 20446 : loss : 0.122265, loss_ce: 0.005762, loss_dice: 0.238768
[11:04:20.418] TRAIN: iteration 20447 : loss : 0.251057, loss_ce: 0.001987, loss_dice: 0.500128
[11:04:20.625] TRAIN: iteration 20448 : loss : 0.251590, loss_ce: 0.002979, loss_dice: 0.500201
[11:04:21.486] TRAIN: iteration 20449 : loss : 0.183489, loss_ce: 0.003319, loss_dice: 0.363658
[11:04:21.693] TRAIN: iteration 20450 : loss : 0.250955, loss_ce: 0.001797, loss_dice: 0.500112
[11:04:21.900] TRAIN: iteration 20451 : loss : 0.097757, loss_ce: 0.005514, loss_dice: 0.190001
[11:04:22.109] TRAIN: iteration 20452 : loss : 0.070370, loss_ce: 0.008000, loss_dice: 0.132739
[11:04:23.890] TRAIN: iteration 20453 : loss : 0.249868, loss_ce: 0.002219, loss_dice: 0.497516
[11:04:24.097] TRAIN: iteration 20454 : loss : 0.083286, loss_ce: 0.004842, loss_dice: 0.161730
[11:04:24.303] TRAIN: iteration 20455 : loss : 0.019773, loss_ce: 0.001508, loss_dice: 0.038038
[11:04:24.589] TRAIN: iteration 20456 : loss : 0.078959, loss_ce: 0.002268, loss_dice: 0.155649
[11:04:25.313] TRAIN: iteration 20457 : loss : 0.250883, loss_ce: 0.001675, loss_dice: 0.500091
[11:04:25.520] TRAIN: iteration 20458 : loss : 0.250624, loss_ce: 0.001197, loss_dice: 0.500052
[11:04:25.727] TRAIN: iteration 20459 : loss : 0.050245, loss_ce: 0.002916, loss_dice: 0.097573
[11:04:25.937] TRAIN: iteration 20460 : loss : 0.086729, loss_ce: 0.004112, loss_dice: 0.169347
[11:04:26.309] TRAIN: iteration 20461 : loss : 0.022081, loss_ce: 0.001541, loss_dice: 0.042622
[11:04:26.516] TRAIN: iteration 20462 : loss : 0.054901, loss_ce: 0.003142, loss_dice: 0.106661
[11:04:27.328] TRAIN: iteration 20463 : loss : 0.049917, loss_ce: 0.004194, loss_dice: 0.095640
[11:04:27.536] TRAIN: iteration 20464 : loss : 0.202549, loss_ce: 0.004609, loss_dice: 0.400490
[11:04:27.744] TRAIN: iteration 20465 : loss : 0.065936, loss_ce: 0.003870, loss_dice: 0.128002
[11:04:27.951] TRAIN: iteration 20466 : loss : 0.053639, loss_ce: 0.004956, loss_dice: 0.102321
[11:04:28.159] TRAIN: iteration 20467 : loss : 0.096873, loss_ce: 0.011153, loss_dice: 0.182594
[11:04:28.367] TRAIN: iteration 20468 : loss : 0.059523, loss_ce: 0.003075, loss_dice: 0.115970
[11:04:32.061] TRAIN: iteration 20469 : loss : 0.251798, loss_ce: 0.003370, loss_dice: 0.500225
[11:04:32.268] TRAIN: iteration 20470 : loss : 0.235101, loss_ce: 0.003860, loss_dice: 0.466343
[11:04:32.475] TRAIN: iteration 20471 : loss : 0.250697, loss_ce: 0.001352, loss_dice: 0.500041
[11:04:32.684] TRAIN: iteration 20472 : loss : 0.201961, loss_ce: 0.003070, loss_dice: 0.400852
[11:04:32.891] TRAIN: iteration 20473 : loss : 0.166437, loss_ce: 0.005702, loss_dice: 0.327172
[11:04:33.099] TRAIN: iteration 20474 : loss : 0.109862, loss_ce: 0.002963, loss_dice: 0.216762
[11:04:34.661] TRAIN: iteration 20475 : loss : 0.077529, loss_ce: 0.005353, loss_dice: 0.149704
[11:04:34.868] TRAIN: iteration 20476 : loss : 0.207073, loss_ce: 0.004536, loss_dice: 0.409611
[11:04:36.585] TRAIN: iteration 20477 : loss : 0.074547, loss_ce: 0.002196, loss_dice: 0.146898
[11:04:36.792] TRAIN: iteration 20478 : loss : 0.073707, loss_ce: 0.004310, loss_dice: 0.143105
[11:04:36.999] TRAIN: iteration 20479 : loss : 0.176247, loss_ce: 0.007168, loss_dice: 0.345326
[11:04:37.210] TRAIN: iteration 20480 : loss : 0.132768, loss_ce: 0.014880, loss_dice: 0.250655
[11:04:37.448] TRAIN: iteration 20481 : loss : 0.122977, loss_ce: 0.003595, loss_dice: 0.242360
[11:04:37.657] TRAIN: iteration 20482 : loss : 0.118453, loss_ce: 0.002935, loss_dice: 0.233971
[11:04:38.465] TRAIN: iteration 20483 : loss : 0.089914, loss_ce: 0.009892, loss_dice: 0.169936
[11:04:38.672] TRAIN: iteration 20484 : loss : 0.114347, loss_ce: 0.002717, loss_dice: 0.225977
[11:04:41.701] TRAIN: iteration 20485 : loss : 0.251088, loss_ce: 0.002053, loss_dice: 0.500122
[11:04:41.911] TRAIN: iteration 20486 : loss : 0.211312, loss_ce: 0.019595, loss_dice: 0.403029
[11:04:42.122] TRAIN: iteration 20487 : loss : 0.250673, loss_ce: 0.001294, loss_dice: 0.500052
[11:04:42.330] TRAIN: iteration 20488 : loss : 0.104838, loss_ce: 0.011361, loss_dice: 0.198314
[11:04:42.539] TRAIN: iteration 20489 : loss : 0.053054, loss_ce: 0.003563, loss_dice: 0.102544
[11:04:42.748] TRAIN: iteration 20490 : loss : 0.041813, loss_ce: 0.008142, loss_dice: 0.075484
[11:04:42.957] TRAIN: iteration 20491 : loss : 0.251754, loss_ce: 0.003313, loss_dice: 0.500195
[11:04:43.165] TRAIN: iteration 20492 : loss : 0.101211, loss_ce: 0.005259, loss_dice: 0.197162
[11:04:44.782] TRAIN: iteration 20493 : loss : 0.125320, loss_ce: 0.002966, loss_dice: 0.247675
[11:04:44.994] TRAIN: iteration 20494 : loss : 0.250462, loss_ce: 0.003189, loss_dice: 0.497736
[11:04:45.272] TRAIN: iteration 20495 : loss : 0.143748, loss_ce: 0.005119, loss_dice: 0.282376
[11:04:45.480] TRAIN: iteration 20496 : loss : 0.251117, loss_ce: 0.023954, loss_dice: 0.478281
[11:04:45.687] TRAIN: iteration 20497 : loss : 0.129940, loss_ce: 0.002801, loss_dice: 0.257078
[11:04:45.894] TRAIN: iteration 20498 : loss : 0.167319, loss_ce: 0.007003, loss_dice: 0.327635
[11:04:46.102] TRAIN: iteration 20499 : loss : 0.084524, loss_ce: 0.003293, loss_dice: 0.165754
[11:04:46.313] TRAIN: iteration 20500 : loss : 0.245613, loss_ce: 0.003167, loss_dice: 0.488059
[11:04:51.906] TRAIN: iteration 20501 : loss : 0.050658, loss_ce: 0.006782, loss_dice: 0.094534
[11:04:52.119] TRAIN: iteration 20502 : loss : 0.090188, loss_ce: 0.003604, loss_dice: 0.176772
[11:04:52.327] TRAIN: iteration 20503 : loss : 0.054035, loss_ce: 0.005000, loss_dice: 0.103070
[11:04:52.537] TRAIN: iteration 20504 : loss : 0.151315, loss_ce: 0.002241, loss_dice: 0.300389
[11:04:52.747] TRAIN: iteration 20505 : loss : 0.063949, loss_ce: 0.002680, loss_dice: 0.125218
[11:04:52.961] TRAIN: iteration 20506 : loss : 0.042415, loss_ce: 0.001605, loss_dice: 0.083225
[11:04:53.169] TRAIN: iteration 20507 : loss : 0.123539, loss_ce: 0.006551, loss_dice: 0.240527
[11:04:53.381] TRAIN: iteration 20508 : loss : 0.250852, loss_ce: 0.001630, loss_dice: 0.500075
[11:04:56.418] TRAIN: iteration 20509 : loss : 0.135426, loss_ce: 0.003358, loss_dice: 0.267493
[11:04:56.626] TRAIN: iteration 20510 : loss : 0.251104, loss_ce: 0.002067, loss_dice: 0.500141
[11:04:56.834] TRAIN: iteration 20511 : loss : 0.117010, loss_ce: 0.014252, loss_dice: 0.219769
[11:04:57.042] TRAIN: iteration 20512 : loss : 0.124185, loss_ce: 0.001882, loss_dice: 0.246488
[11:04:57.250] TRAIN: iteration 20513 : loss : 0.072132, loss_ce: 0.004484, loss_dice: 0.139781
[11:04:57.457] TRAIN: iteration 20514 : loss : 0.106367, loss_ce: 0.005054, loss_dice: 0.207679
[11:04:57.665] TRAIN: iteration 20515 : loss : 0.124973, loss_ce: 0.003602, loss_dice: 0.246343
[11:04:57.875] TRAIN: iteration 20516 : loss : 0.118430, loss_ce: 0.002950, loss_dice: 0.233911
[11:05:01.576] TRAIN: iteration 20517 : loss : 0.250926, loss_ce: 0.001883, loss_dice: 0.499970
[11:05:01.787] TRAIN: iteration 20518 : loss : 0.112967, loss_ce: 0.008639, loss_dice: 0.217295
[11:05:01.995] TRAIN: iteration 20519 : loss : 0.095645, loss_ce: 0.002278, loss_dice: 0.189011
[11:05:02.205] TRAIN: iteration 20520 : loss : 0.139091, loss_ce: 0.008613, loss_dice: 0.269568
[11:05:02.444] TRAIN: iteration 20521 : loss : 0.111918, loss_ce: 0.002116, loss_dice: 0.221720
[11:05:02.651] TRAIN: iteration 20522 : loss : 0.101755, loss_ce: 0.005269, loss_dice: 0.198240
[11:05:02.858] TRAIN: iteration 20523 : loss : 0.106174, loss_ce: 0.002082, loss_dice: 0.210265
[11:05:03.065] TRAIN: iteration 20524 : loss : 0.065044, loss_ce: 0.001933, loss_dice: 0.128155
[11:05:06.929] TRAIN: iteration 20525 : loss : 0.047836, loss_ce: 0.004411, loss_dice: 0.091262
[11:05:07.137] TRAIN: iteration 20526 : loss : 0.045258, loss_ce: 0.001571, loss_dice: 0.088946
[11:05:07.344] TRAIN: iteration 20527 : loss : 0.091935, loss_ce: 0.002351, loss_dice: 0.181520
[11:05:07.551] TRAIN: iteration 20528 : loss : 0.113899, loss_ce: 0.005956, loss_dice: 0.221842
[11:05:07.760] TRAIN: iteration 20529 : loss : 0.055577, loss_ce: 0.003638, loss_dice: 0.107517
[11:05:07.969] TRAIN: iteration 20530 : loss : 0.055845, loss_ce: 0.002488, loss_dice: 0.109201
[11:05:08.176] TRAIN: iteration 20531 : loss : 0.070383, loss_ce: 0.002716, loss_dice: 0.138051
[11:05:08.383] TRAIN: iteration 20532 : loss : 0.216118, loss_ce: 0.005553, loss_dice: 0.426682
[11:05:12.251] TRAIN: iteration 20533 : loss : 0.093069, loss_ce: 0.005798, loss_dice: 0.180339
[11:05:12.461] TRAIN: iteration 20534 : loss : 0.029611, loss_ce: 0.000954, loss_dice: 0.058268
[11:05:12.668] TRAIN: iteration 20535 : loss : 0.071083, loss_ce: 0.002067, loss_dice: 0.140099
[11:05:12.878] TRAIN: iteration 20536 : loss : 0.038709, loss_ce: 0.001159, loss_dice: 0.076259
[11:05:13.085] TRAIN: iteration 20537 : loss : 0.198545, loss_ce: 0.001008, loss_dice: 0.396081
[11:05:13.292] TRAIN: iteration 20538 : loss : 0.212294, loss_ce: 0.002481, loss_dice: 0.422108
[11:05:13.498] TRAIN: iteration 20539 : loss : 0.020696, loss_ce: 0.001442, loss_dice: 0.039950
[11:05:13.705] TRAIN: iteration 20540 : loss : 0.101068, loss_ce: 0.008557, loss_dice: 0.193580
[11:05:18.924] TRAIN: iteration 20541 : loss : 0.124387, loss_ce: 0.002955, loss_dice: 0.245820
[11:05:19.132] TRAIN: iteration 20542 : loss : 0.089993, loss_ce: 0.002170, loss_dice: 0.177817
[11:05:19.340] TRAIN: iteration 20543 : loss : 0.054999, loss_ce: 0.004105, loss_dice: 0.105893
[11:05:19.547] TRAIN: iteration 20544 : loss : 0.150144, loss_ce: 0.003643, loss_dice: 0.296645
[11:05:19.754] TRAIN: iteration 20545 : loss : 0.040556, loss_ce: 0.000904, loss_dice: 0.080208
[11:05:19.977] TRAIN: iteration 20546 : loss : 0.007097, loss_ce: 0.000845, loss_dice: 0.013350
[11:05:20.185] TRAIN: iteration 20547 : loss : 0.251149, loss_ce: 0.002152, loss_dice: 0.500146
[11:05:20.395] TRAIN: iteration 20548 : loss : 0.251199, loss_ce: 0.007158, loss_dice: 0.495240
[11:05:24.289] TRAIN: iteration 20549 : loss : 0.047653, loss_ce: 0.004906, loss_dice: 0.090400
[11:05:24.502] TRAIN: iteration 20550 : loss : 0.084887, loss_ce: 0.004547, loss_dice: 0.165226
[11:05:24.709] TRAIN: iteration 20551 : loss : 0.061503, loss_ce: 0.004454, loss_dice: 0.118551
[11:05:24.916] TRAIN: iteration 20552 : loss : 0.070035, loss_ce: 0.010210, loss_dice: 0.129860
[11:05:25.123] TRAIN: iteration 20553 : loss : 0.205053, loss_ce: 0.004497, loss_dice: 0.405610
[11:05:25.330] TRAIN: iteration 20554 : loss : 0.097226, loss_ce: 0.003854, loss_dice: 0.190598
[11:05:26.192] TRAIN: iteration 20555 : loss : 0.170211, loss_ce: 0.003370, loss_dice: 0.337052
[11:05:26.400] TRAIN: iteration 20556 : loss : 0.174365, loss_ce: 0.002754, loss_dice: 0.345976
[11:05:29.615] TRAIN: iteration 20557 : loss : 0.139850, loss_ce: 0.005807, loss_dice: 0.273894
[11:05:29.822] TRAIN: iteration 20558 : loss : 0.136390, loss_ce: 0.013679, loss_dice: 0.259100
[11:05:30.078] TRAIN: iteration 20559 : loss : 0.043729, loss_ce: 0.001712, loss_dice: 0.085746
[11:05:30.286] TRAIN: iteration 20560 : loss : 0.214791, loss_ce: 0.015150, loss_dice: 0.414433
[11:05:30.526] TRAIN: iteration 20561 : loss : 0.034809, loss_ce: 0.001737, loss_dice: 0.067881
[11:05:30.734] TRAIN: iteration 20562 : loss : 0.232936, loss_ce: 0.004678, loss_dice: 0.461194
[11:05:31.637] TRAIN: iteration 20563 : loss : 0.072438, loss_ce: 0.003625, loss_dice: 0.141252
[11:05:31.848] TRAIN: iteration 20564 : loss : 0.179230, loss_ce: 0.003923, loss_dice: 0.354538
[11:05:34.880] TRAIN: iteration 20565 : loss : 0.106488, loss_ce: 0.004552, loss_dice: 0.208425
[11:05:35.092] TRAIN: iteration 20566 : loss : 0.247714, loss_ce: 0.008701, loss_dice: 0.486727
[11:05:35.302] TRAIN: iteration 20567 : loss : 0.087258, loss_ce: 0.003588, loss_dice: 0.170928
[11:05:35.514] TRAIN: iteration 20568 : loss : 0.072387, loss_ce: 0.002921, loss_dice: 0.141852
[11:05:35.729] TRAIN: iteration 20569 : loss : 0.102957, loss_ce: 0.007285, loss_dice: 0.198630
[11:05:35.936] TRAIN: iteration 20570 : loss : 0.209064, loss_ce: 0.003512, loss_dice: 0.414616
[11:05:36.143] TRAIN: iteration 20571 : loss : 0.117116, loss_ce: 0.005733, loss_dice: 0.228500
[11:05:36.350] TRAIN: iteration 20572 : loss : 0.105273, loss_ce: 0.005013, loss_dice: 0.205534
[11:05:39.469] TRAIN: iteration 20573 : loss : 0.122541, loss_ce: 0.004787, loss_dice: 0.240295
[11:05:39.678] TRAIN: iteration 20574 : loss : 0.223598, loss_ce: 0.009387, loss_dice: 0.437809
[11:05:39.885] TRAIN: iteration 20575 : loss : 0.089695, loss_ce: 0.007100, loss_dice: 0.172290
[11:05:40.094] TRAIN: iteration 20576 : loss : 0.032089, loss_ce: 0.001670, loss_dice: 0.062508
[11:05:40.300] TRAIN: iteration 20577 : loss : 0.173844, loss_ce: 0.003092, loss_dice: 0.344597
[11:05:40.508] TRAIN: iteration 20578 : loss : 0.045440, loss_ce: 0.003024, loss_dice: 0.087857
[11:05:40.717] TRAIN: iteration 20579 : loss : 0.251305, loss_ce: 0.002451, loss_dice: 0.500160
[11:05:40.925] TRAIN: iteration 20580 : loss : 0.251585, loss_ce: 0.002973, loss_dice: 0.500196
[11:05:41.577] TRAIN: iteration 20581 : loss : 0.248074, loss_ce: 0.002952, loss_dice: 0.493196
[11:05:41.787] TRAIN: iteration 20582 : loss : 0.087862, loss_ce: 0.002158, loss_dice: 0.173566
[11:05:41.996] TRAIN: iteration 20583 : loss : 0.251122, loss_ce: 0.002137, loss_dice: 0.500107
[11:05:42.203] TRAIN: iteration 20584 : loss : 0.193717, loss_ce: 0.003216, loss_dice: 0.384218
[11:05:42.411] TRAIN: iteration 20585 : loss : 0.125171, loss_ce: 0.007299, loss_dice: 0.243043
[11:05:44.621] TRAIN: iteration 20586 : loss : 0.097304, loss_ce: 0.001608, loss_dice: 0.193000
[11:05:45.774] TRAIN: iteration 20587 : loss : 0.132218, loss_ce: 0.013719, loss_dice: 0.250717
[11:05:45.983] TRAIN: iteration 20588 : loss : 0.191638, loss_ce: 0.007588, loss_dice: 0.375687
[11:05:46.190] TRAIN: iteration 20589 : loss : 0.172730, loss_ce: 0.003951, loss_dice: 0.341508
[11:05:49.715] TRAIN: iteration 20590 : loss : 0.182390, loss_ce: 0.008736, loss_dice: 0.356044
[11:05:49.924] TRAIN: iteration 20591 : loss : 0.251622, loss_ce: 0.003137, loss_dice: 0.500107
[11:05:50.131] TRAIN: iteration 20592 : loss : 0.085168, loss_ce: 0.001407, loss_dice: 0.168929
[11:05:50.338] TRAIN: iteration 20593 : loss : 0.250694, loss_ce: 0.001347, loss_dice: 0.500041
[11:05:50.550] TRAIN: iteration 20594 : loss : 0.038919, loss_ce: 0.003067, loss_dice: 0.074771
[11:05:53.901] TRAIN: iteration 20595 : loss : 0.077991, loss_ce: 0.005252, loss_dice: 0.150729
[11:05:54.116] TRAIN: iteration 20596 : loss : 0.114384, loss_ce: 0.003369, loss_dice: 0.225398
[11:05:54.324] TRAIN: iteration 20597 : loss : 0.164929, loss_ce: 0.002488, loss_dice: 0.327371
[11:05:55.844] TRAIN: iteration 20598 : loss : 0.250713, loss_ce: 0.001381, loss_dice: 0.500045
[11:05:56.051] TRAIN: iteration 20599 : loss : 0.250721, loss_ce: 0.001393, loss_dice: 0.500050
[11:05:56.261] TRAIN: iteration 20600 : loss : 0.244898, loss_ce: 0.009287, loss_dice: 0.480509
[11:05:56.500] TRAIN: iteration 20601 : loss : 0.086369, loss_ce: 0.001918, loss_dice: 0.170821
[11:05:56.708] TRAIN: iteration 20602 : loss : 0.046499, loss_ce: 0.002579, loss_dice: 0.090419
[11:06:00.156] TRAIN: iteration 20603 : loss : 0.048547, loss_ce: 0.001286, loss_dice: 0.095809
[11:06:00.363] TRAIN: iteration 20604 : loss : 0.248933, loss_ce: 0.003131, loss_dice: 0.494734
[11:06:00.796] TRAIN: iteration 20605 : loss : 0.178609, loss_ce: 0.004475, loss_dice: 0.352742
[11:06:04.357] TRAIN: iteration 20606 : loss : 0.250598, loss_ce: 0.001162, loss_dice: 0.500035
[11:06:04.568] TRAIN: iteration 20607 : loss : 0.086138, loss_ce: 0.002199, loss_dice: 0.170078
[11:06:04.775] TRAIN: iteration 20608 : loss : 0.094268, loss_ce: 0.010182, loss_dice: 0.178354
[11:06:04.984] TRAIN: iteration 20609 : loss : 0.106145, loss_ce: 0.003647, loss_dice: 0.208644
[11:06:05.192] TRAIN: iteration 20610 : loss : 0.090966, loss_ce: 0.005460, loss_dice: 0.176472
[11:06:05.400] TRAIN: iteration 20611 : loss : 0.091076, loss_ce: 0.008586, loss_dice: 0.173565
[11:06:05.611] TRAIN: iteration 20612 : loss : 0.087231, loss_ce: 0.001783, loss_dice: 0.172679
[11:06:05.932] TRAIN: iteration 20613 : loss : 0.086938, loss_ce: 0.004694, loss_dice: 0.169183
[11:06:11.995] TRAIN: iteration 20614 : loss : 0.153609, loss_ce: 0.002485, loss_dice: 0.304734
[11:06:12.208] TRAIN: iteration 20615 : loss : 0.111042, loss_ce: 0.006795, loss_dice: 0.215289
[11:06:12.416] TRAIN: iteration 20616 : loss : 0.122062, loss_ce: 0.004665, loss_dice: 0.239458
[11:06:12.623] TRAIN: iteration 20617 : loss : 0.098150, loss_ce: 0.011920, loss_dice: 0.184381
[11:06:12.831] TRAIN: iteration 20618 : loss : 0.251249, loss_ce: 0.002363, loss_dice: 0.500135
[11:06:13.038] TRAIN: iteration 20619 : loss : 0.251406, loss_ce: 0.002628, loss_dice: 0.500185
[11:06:13.244] TRAIN: iteration 20620 : loss : 0.083075, loss_ce: 0.004876, loss_dice: 0.161274
[11:06:13.734] TRAIN: iteration 20621 : loss : 0.245856, loss_ce: 0.004853, loss_dice: 0.486858
[11:06:16.421] TRAIN: iteration 20622 : loss : 0.160451, loss_ce: 0.006240, loss_dice: 0.314662
[11:06:16.633] TRAIN: iteration 20623 : loss : 0.052981, loss_ce: 0.009156, loss_dice: 0.096807
[11:06:16.842] TRAIN: iteration 20624 : loss : 0.251711, loss_ce: 0.003233, loss_dice: 0.500189
[11:06:17.049] TRAIN: iteration 20625 : loss : 0.128784, loss_ce: 0.005052, loss_dice: 0.252516
[11:06:17.263] TRAIN: iteration 20626 : loss : 0.041060, loss_ce: 0.006768, loss_dice: 0.075352
[11:06:17.471] TRAIN: iteration 20627 : loss : 0.042689, loss_ce: 0.003137, loss_dice: 0.082241
[11:06:17.684] TRAIN: iteration 20628 : loss : 0.039025, loss_ce: 0.002969, loss_dice: 0.075082
[11:06:18.363] TRAIN: iteration 20629 : loss : 0.251599, loss_ce: 0.004161, loss_dice: 0.499038
[11:06:22.767] TRAIN: iteration 20630 : loss : 0.190576, loss_ce: 0.006116, loss_dice: 0.375036
[11:06:22.973] TRAIN: iteration 20631 : loss : 0.074371, loss_ce: 0.004329, loss_dice: 0.144414
[11:06:23.180] TRAIN: iteration 20632 : loss : 0.143284, loss_ce: 0.003425, loss_dice: 0.283143
[11:06:23.387] TRAIN: iteration 20633 : loss : 0.177627, loss_ce: 0.006743, loss_dice: 0.348510
[11:06:23.594] TRAIN: iteration 20634 : loss : 0.062066, loss_ce: 0.005599, loss_dice: 0.118533
[11:06:23.801] TRAIN: iteration 20635 : loss : 0.040455, loss_ce: 0.006239, loss_dice: 0.074671
[11:06:24.008] TRAIN: iteration 20636 : loss : 0.253086, loss_ce: 0.005796, loss_dice: 0.500375
[11:06:24.216] TRAIN: iteration 20637 : loss : 0.136467, loss_ce: 0.006170, loss_dice: 0.266765
[11:06:31.084] TRAIN: iteration 20638 : loss : 0.148386, loss_ce: 0.008081, loss_dice: 0.288691
[11:06:31.298] TRAIN: iteration 20639 : loss : 0.101520, loss_ce: 0.005206, loss_dice: 0.197833
[11:06:31.510] TRAIN: iteration 20640 : loss : 0.072176, loss_ce: 0.005892, loss_dice: 0.138460
[11:06:31.755] TRAIN: iteration 20641 : loss : 0.101562, loss_ce: 0.004563, loss_dice: 0.198562
[11:06:31.963] TRAIN: iteration 20642 : loss : 0.070651, loss_ce: 0.004223, loss_dice: 0.137079
[11:06:32.170] TRAIN: iteration 20643 : loss : 0.066773, loss_ce: 0.012714, loss_dice: 0.120831
[11:06:32.377] TRAIN: iteration 20644 : loss : 0.118779, loss_ce: 0.001477, loss_dice: 0.236082
[11:06:32.584] TRAIN: iteration 20645 : loss : 0.064956, loss_ce: 0.003310, loss_dice: 0.126602
[11:06:40.294] TRAIN: iteration 20646 : loss : 0.052532, loss_ce: 0.001633, loss_dice: 0.103431
[11:06:40.502] TRAIN: iteration 20647 : loss : 0.107162, loss_ce: 0.001662, loss_dice: 0.212663
[11:06:40.710] TRAIN: iteration 20648 : loss : 0.025148, loss_ce: 0.003503, loss_dice: 0.046792
[11:06:40.918] TRAIN: iteration 20649 : loss : 0.162388, loss_ce: 0.003260, loss_dice: 0.321516
[11:06:41.126] TRAIN: iteration 20650 : loss : 0.075642, loss_ce: 0.002325, loss_dice: 0.148959
[11:06:41.334] TRAIN: iteration 20651 : loss : 0.141185, loss_ce: 0.001957, loss_dice: 0.280412
[11:06:41.677] TRAIN: iteration 20652 : loss : 0.110678, loss_ce: 0.015923, loss_dice: 0.205433
[11:06:41.884] TRAIN: iteration 20653 : loss : 0.115132, loss_ce: 0.001857, loss_dice: 0.228407
[11:06:47.458] TRAIN: iteration 20654 : loss : 0.255611, loss_ce: 0.017365, loss_dice: 0.493856
[11:06:47.665] TRAIN: iteration 20655 : loss : 0.036209, loss_ce: 0.001489, loss_dice: 0.070929
[11:06:47.872] TRAIN: iteration 20656 : loss : 0.132007, loss_ce: 0.005482, loss_dice: 0.258532
[11:06:48.079] TRAIN: iteration 20657 : loss : 0.250628, loss_ce: 0.001202, loss_dice: 0.500054
[11:06:48.286] TRAIN: iteration 20658 : loss : 0.165808, loss_ce: 0.001703, loss_dice: 0.329913
[11:06:48.493] TRAIN: iteration 20659 : loss : 0.041074, loss_ce: 0.007823, loss_dice: 0.074325
[11:06:48.702] TRAIN: iteration 20660 : loss : 0.074628, loss_ce: 0.005728, loss_dice: 0.143528
[11:06:48.939] TRAIN: iteration 20661 : loss : 0.053976, loss_ce: 0.007149, loss_dice: 0.100803
[11:06:54.566] TRAIN: iteration 20662 : loss : 0.038324, loss_ce: 0.002333, loss_dice: 0.074314
[11:06:54.776] TRAIN: iteration 20663 : loss : 0.251253, loss_ce: 0.003528, loss_dice: 0.498978
[11:06:54.984] TRAIN: iteration 20664 : loss : 0.063002, loss_ce: 0.007098, loss_dice: 0.118906
[11:06:55.191] TRAIN: iteration 20665 : loss : 0.078645, loss_ce: 0.001063, loss_dice: 0.156226
[11:06:55.399] TRAIN: iteration 20666 : loss : 0.119039, loss_ce: 0.001954, loss_dice: 0.236124
[11:06:55.608] TRAIN: iteration 20667 : loss : 0.101342, loss_ce: 0.007870, loss_dice: 0.194813
[11:06:55.816] TRAIN: iteration 20668 : loss : 0.061404, loss_ce: 0.002498, loss_dice: 0.120310
[11:06:56.025] TRAIN: iteration 20669 : loss : 0.250380, loss_ce: 0.003656, loss_dice: 0.497104
[11:07:01.960] TRAIN: iteration 20670 : loss : 0.112118, loss_ce: 0.003465, loss_dice: 0.220770
[11:07:02.168] TRAIN: iteration 20671 : loss : 0.087006, loss_ce: 0.003603, loss_dice: 0.170409
[11:07:02.377] TRAIN: iteration 20672 : loss : 0.120423, loss_ce: 0.000686, loss_dice: 0.240160
[11:07:02.585] TRAIN: iteration 20673 : loss : 0.241021, loss_ce: 0.003174, loss_dice: 0.478869
[11:07:02.794] TRAIN: iteration 20674 : loss : 0.101635, loss_ce: 0.002895, loss_dice: 0.200374
[11:07:03.002] TRAIN: iteration 20675 : loss : 0.180551, loss_ce: 0.002263, loss_dice: 0.358839
[11:07:03.210] TRAIN: iteration 20676 : loss : 0.151253, loss_ce: 0.011850, loss_dice: 0.290657
[11:07:03.417] TRAIN: iteration 20677 : loss : 0.131051, loss_ce: 0.002387, loss_dice: 0.259716
[11:07:13.090] TRAIN: iteration 20678 : loss : 0.036540, loss_ce: 0.001729, loss_dice: 0.071351
[11:07:13.302] TRAIN: iteration 20679 : loss : 0.048637, loss_ce: 0.002095, loss_dice: 0.095179
[11:07:13.509] TRAIN: iteration 20680 : loss : 0.078858, loss_ce: 0.006662, loss_dice: 0.151053
[11:07:13.752] TRAIN: iteration 20681 : loss : 0.245039, loss_ce: 0.001846, loss_dice: 0.488233
[11:07:13.959] TRAIN: iteration 20682 : loss : 0.097775, loss_ce: 0.003797, loss_dice: 0.191753
[11:07:14.166] TRAIN: iteration 20683 : loss : 0.149054, loss_ce: 0.002191, loss_dice: 0.295916
[11:07:14.374] TRAIN: iteration 20684 : loss : 0.045411, loss_ce: 0.002574, loss_dice: 0.088248
[11:07:14.581] TRAIN: iteration 20685 : loss : 0.024644, loss_ce: 0.002717, loss_dice: 0.046570
[11:07:22.590] TRAIN: iteration 20686 : loss : 0.171181, loss_ce: 0.003509, loss_dice: 0.338854
[11:07:22.800] TRAIN: iteration 20687 : loss : 0.049747, loss_ce: 0.003217, loss_dice: 0.096278
[11:07:23.007] TRAIN: iteration 20688 : loss : 0.095965, loss_ce: 0.003597, loss_dice: 0.188333
[11:07:23.216] TRAIN: iteration 20689 : loss : 0.125722, loss_ce: 0.001274, loss_dice: 0.250170
[11:07:23.423] TRAIN: iteration 20690 : loss : 0.251040, loss_ce: 0.001941, loss_dice: 0.500138
[11:07:23.630] TRAIN: iteration 20691 : loss : 0.064141, loss_ce: 0.004012, loss_dice: 0.124271
[11:07:23.837] TRAIN: iteration 20692 : loss : 0.053739, loss_ce: 0.005242, loss_dice: 0.102237
[11:07:24.043] TRAIN: iteration 20693 : loss : 0.083077, loss_ce: 0.002253, loss_dice: 0.163901
[11:07:29.300] TRAIN: iteration 20694 : loss : 0.040192, loss_ce: 0.003186, loss_dice: 0.077199
[11:07:29.509] TRAIN: iteration 20695 : loss : 0.151809, loss_ce: 0.019777, loss_dice: 0.283842
[11:07:29.721] TRAIN: iteration 20696 : loss : 0.129106, loss_ce: 0.001018, loss_dice: 0.257193
[11:07:29.928] TRAIN: iteration 20697 : loss : 0.250777, loss_ce: 0.001453, loss_dice: 0.500102
[11:07:30.136] TRAIN: iteration 20698 : loss : 0.070582, loss_ce: 0.001780, loss_dice: 0.139384
[11:07:30.344] TRAIN: iteration 20699 : loss : 0.250464, loss_ce: 0.000889, loss_dice: 0.500038
[11:07:30.552] TRAIN: iteration 20700 : loss : 0.251245, loss_ce: 0.002310, loss_dice: 0.500180
[11:07:30.794] TRAIN: iteration 20701 : loss : 0.108079, loss_ce: 0.007964, loss_dice: 0.208194
[11:07:38.738] TRAIN: iteration 20702 : loss : 0.037395, loss_ce: 0.000442, loss_dice: 0.074349
[11:07:38.946] TRAIN: iteration 20703 : loss : 0.093763, loss_ce: 0.006151, loss_dice: 0.181375
[11:07:39.158] TRAIN: iteration 20704 : loss : 0.250171, loss_ce: 0.000342, loss_dice: 0.500001
[11:07:39.365] TRAIN: iteration 20705 : loss : 0.113774, loss_ce: 0.007686, loss_dice: 0.219863
[11:07:39.573] TRAIN: iteration 20706 : loss : 0.058914, loss_ce: 0.001651, loss_dice: 0.116178
[11:07:39.783] TRAIN: iteration 20707 : loss : 0.204836, loss_ce: 0.006727, loss_dice: 0.402945
[11:07:39.991] TRAIN: iteration 20708 : loss : 0.107347, loss_ce: 0.001347, loss_dice: 0.213347
[11:07:40.199] TRAIN: iteration 20709 : loss : 0.202128, loss_ce: 0.001814, loss_dice: 0.402442
[11:07:48.180] TRAIN: iteration 20710 : loss : 0.181629, loss_ce: 0.011247, loss_dice: 0.352011
[11:07:48.388] TRAIN: iteration 20711 : loss : 0.148841, loss_ce: 0.002529, loss_dice: 0.295153
[11:07:48.597] TRAIN: iteration 20712 : loss : 0.075560, loss_ce: 0.005127, loss_dice: 0.145993
[11:07:48.805] TRAIN: iteration 20713 : loss : 0.063192, loss_ce: 0.002740, loss_dice: 0.123644
[11:07:49.013] TRAIN: iteration 20714 : loss : 0.071440, loss_ce: 0.005686, loss_dice: 0.137194
[11:07:49.220] TRAIN: iteration 20715 : loss : 0.055431, loss_ce: 0.003393, loss_dice: 0.107469
[11:07:49.428] TRAIN: iteration 20716 : loss : 0.236411, loss_ce: 0.003602, loss_dice: 0.469219
[11:07:49.636] TRAIN: iteration 20717 : loss : 0.108231, loss_ce: 0.004360, loss_dice: 0.212102
[11:07:56.012] TRAIN: iteration 20718 : loss : 0.056328, loss_ce: 0.002052, loss_dice: 0.110604
[11:07:56.220] TRAIN: iteration 20719 : loss : 0.221679, loss_ce: 0.003455, loss_dice: 0.439903
[11:07:56.428] TRAIN: iteration 20720 : loss : 0.217821, loss_ce: 0.009758, loss_dice: 0.425885
[11:07:56.668] TRAIN: iteration 20721 : loss : 0.251041, loss_ce: 0.001957, loss_dice: 0.500124
[11:07:56.879] TRAIN: iteration 20722 : loss : 0.113377, loss_ce: 0.000618, loss_dice: 0.226135
[11:07:57.090] TRAIN: iteration 20723 : loss : 0.202914, loss_ce: 0.011358, loss_dice: 0.394471
[11:07:57.298] TRAIN: iteration 20724 : loss : 0.201149, loss_ce: 0.011179, loss_dice: 0.391118
[11:07:57.505] TRAIN: iteration 20725 : loss : 0.130642, loss_ce: 0.003772, loss_dice: 0.257512
[11:08:03.867] TRAIN: iteration 20726 : loss : 0.094230, loss_ce: 0.007231, loss_dice: 0.181230
[11:08:04.075] TRAIN: iteration 20727 : loss : 0.073058, loss_ce: 0.001160, loss_dice: 0.144956
[11:08:04.282] TRAIN: iteration 20728 : loss : 0.104822, loss_ce: 0.007603, loss_dice: 0.202041
[11:08:04.490] TRAIN: iteration 20729 : loss : 0.072038, loss_ce: 0.003437, loss_dice: 0.140639
[11:08:04.697] TRAIN: iteration 20730 : loss : 0.250154, loss_ce: 0.000307, loss_dice: 0.500001
[11:08:04.906] TRAIN: iteration 20731 : loss : 0.250504, loss_ce: 0.001954, loss_dice: 0.499053
[11:08:05.118] TRAIN: iteration 20732 : loss : 0.213467, loss_ce: 0.001273, loss_dice: 0.425660
[11:08:05.328] TRAIN: iteration 20733 : loss : 0.059183, loss_ce: 0.004986, loss_dice: 0.113380
[11:08:13.934] TRAIN: iteration 20734 : loss : 0.250172, loss_ce: 0.000338, loss_dice: 0.500005
[11:08:14.142] TRAIN: iteration 20735 : loss : 0.041371, loss_ce: 0.002672, loss_dice: 0.080071
[11:08:14.348] TRAIN: iteration 20736 : loss : 0.246549, loss_ce: 0.010416, loss_dice: 0.482682
[11:08:14.556] TRAIN: iteration 20737 : loss : 0.029251, loss_ce: 0.001317, loss_dice: 0.057186
[11:08:14.764] TRAIN: iteration 20738 : loss : 0.149563, loss_ce: 0.001101, loss_dice: 0.298025
[11:08:14.972] TRAIN: iteration 20739 : loss : 0.232211, loss_ce: 0.006025, loss_dice: 0.458398
[11:08:15.180] TRAIN: iteration 20740 : loss : 0.252369, loss_ce: 0.004402, loss_dice: 0.500336
[11:08:15.413] TRAIN: iteration 20741 : loss : 0.262433, loss_ce: 0.028850, loss_dice: 0.496017
[11:08:23.615] TRAIN: iteration 20742 : loss : 0.066942, loss_ce: 0.002912, loss_dice: 0.130972
[11:08:23.824] TRAIN: iteration 20743 : loss : 0.032562, loss_ce: 0.000636, loss_dice: 0.064488
[11:08:24.030] TRAIN: iteration 20744 : loss : 0.250254, loss_ce: 0.001249, loss_dice: 0.499258
[11:08:24.238] TRAIN: iteration 20745 : loss : 0.110249, loss_ce: 0.003614, loss_dice: 0.216885
[11:08:24.445] TRAIN: iteration 20746 : loss : 0.009453, loss_ce: 0.000756, loss_dice: 0.018151
[11:08:24.653] TRAIN: iteration 20747 : loss : 0.060296, loss_ce: 0.001708, loss_dice: 0.118884
[11:08:24.867] TRAIN: iteration 20748 : loss : 0.250636, loss_ce: 0.001196, loss_dice: 0.500076
[11:08:25.076] TRAIN: iteration 20749 : loss : 0.179778, loss_ce: 0.004320, loss_dice: 0.355235
[11:08:33.823] TRAIN: iteration 20750 : loss : 0.092819, loss_ce: 0.003119, loss_dice: 0.182519
[11:08:34.029] TRAIN: iteration 20751 : loss : 0.192481, loss_ce: 0.007190, loss_dice: 0.377772
[11:08:34.236] TRAIN: iteration 20752 : loss : 0.066129, loss_ce: 0.001671, loss_dice: 0.130588
[11:08:34.449] TRAIN: iteration 20753 : loss : 0.144744, loss_ce: 0.003191, loss_dice: 0.286298
[11:08:34.657] TRAIN: iteration 20754 : loss : 0.074871, loss_ce: 0.000709, loss_dice: 0.149033
[11:08:34.864] TRAIN: iteration 20755 : loss : 0.108986, loss_ce: 0.002929, loss_dice: 0.215043
[11:08:35.072] TRAIN: iteration 20756 : loss : 0.241942, loss_ce: 0.007164, loss_dice: 0.476721
[11:08:35.279] TRAIN: iteration 20757 : loss : 0.083399, loss_ce: 0.006231, loss_dice: 0.160567
[11:08:43.838] TRAIN: iteration 20758 : loss : 0.049554, loss_ce: 0.002478, loss_dice: 0.096629
[11:08:44.046] TRAIN: iteration 20759 : loss : 0.251623, loss_ce: 0.003037, loss_dice: 0.500208
[11:08:44.253] TRAIN: iteration 20760 : loss : 0.200941, loss_ce: 0.004058, loss_dice: 0.397824
[11:08:44.491] TRAIN: iteration 20761 : loss : 0.250506, loss_ce: 0.000985, loss_dice: 0.500026
[11:08:44.697] TRAIN: iteration 20762 : loss : 0.251054, loss_ce: 0.001994, loss_dice: 0.500115
[11:08:44.907] TRAIN: iteration 20763 : loss : 0.031287, loss_ce: 0.004844, loss_dice: 0.057731
[11:08:45.115] TRAIN: iteration 20764 : loss : 0.245942, loss_ce: 0.002539, loss_dice: 0.489345
[11:08:45.325] TRAIN: iteration 20765 : loss : 0.251026, loss_ce: 0.001931, loss_dice: 0.500120
[11:08:52.268] TRAIN: iteration 20766 : loss : 0.071288, loss_ce: 0.003732, loss_dice: 0.138844
[11:08:52.480] TRAIN: iteration 20767 : loss : 0.120586, loss_ce: 0.005617, loss_dice: 0.235555
[11:08:52.688] TRAIN: iteration 20768 : loss : 0.251199, loss_ce: 0.002691, loss_dice: 0.499707
[11:08:52.896] TRAIN: iteration 20769 : loss : 0.250346, loss_ce: 0.000683, loss_dice: 0.500010
[11:08:53.104] TRAIN: iteration 20770 : loss : 0.131460, loss_ce: 0.009984, loss_dice: 0.252936
[11:08:53.312] TRAIN: iteration 20771 : loss : 0.122817, loss_ce: 0.007192, loss_dice: 0.238442
[11:08:53.521] TRAIN: iteration 20772 : loss : 0.053922, loss_ce: 0.003955, loss_dice: 0.103890
[11:08:53.727] TRAIN: iteration 20773 : loss : 0.106588, loss_ce: 0.006694, loss_dice: 0.206482
[11:09:02.667] TRAIN: iteration 20774 : loss : 0.206317, loss_ce: 0.010243, loss_dice: 0.402391
[11:09:02.873] TRAIN: iteration 20775 : loss : 0.124232, loss_ce: 0.006917, loss_dice: 0.241547
[11:09:03.081] TRAIN: iteration 20776 : loss : 0.160012, loss_ce: 0.007926, loss_dice: 0.312099
[11:09:03.288] TRAIN: iteration 20777 : loss : 0.245950, loss_ce: 0.001500, loss_dice: 0.490401
[11:09:03.543] TRAIN: iteration 20778 : loss : 0.082229, loss_ce: 0.002186, loss_dice: 0.162271
[11:09:03.750] TRAIN: iteration 20779 : loss : 0.202496, loss_ce: 0.001516, loss_dice: 0.403477
[11:09:03.958] TRAIN: iteration 20780 : loss : 0.250347, loss_ce: 0.000687, loss_dice: 0.500007
[11:09:04.196] TRAIN: iteration 20781 : loss : 0.055987, loss_ce: 0.004865, loss_dice: 0.107110
[11:09:09.990] TRAIN: iteration 20782 : loss : 0.207712, loss_ce: 0.002137, loss_dice: 0.413287
[11:09:10.198] TRAIN: iteration 20783 : loss : 0.084888, loss_ce: 0.006279, loss_dice: 0.163496
[11:09:10.406] TRAIN: iteration 20784 : loss : 0.114349, loss_ce: 0.005671, loss_dice: 0.223027
[11:09:10.613] TRAIN: iteration 20785 : loss : 0.058657, loss_ce: 0.002946, loss_dice: 0.114368
[11:09:10.821] TRAIN: iteration 20786 : loss : 0.133002, loss_ce: 0.002449, loss_dice: 0.263554
[11:09:11.028] TRAIN: iteration 20787 : loss : 0.078470, loss_ce: 0.003313, loss_dice: 0.153626
[11:09:11.235] TRAIN: iteration 20788 : loss : 0.070504, loss_ce: 0.006088, loss_dice: 0.134920
[11:09:11.442] TRAIN: iteration 20789 : loss : 0.103964, loss_ce: 0.002598, loss_dice: 0.205330
[11:09:17.958] TRAIN: iteration 20790 : loss : 0.034112, loss_ce: 0.004203, loss_dice: 0.064020
[11:09:18.168] TRAIN: iteration 20791 : loss : 0.029760, loss_ce: 0.004057, loss_dice: 0.055462
[11:09:18.375] TRAIN: iteration 20792 : loss : 0.250414, loss_ce: 0.000820, loss_dice: 0.500009
[11:09:18.583] TRAIN: iteration 20793 : loss : 0.133605, loss_ce: 0.003240, loss_dice: 0.263971
[11:09:18.916] TRAIN: iteration 20794 : loss : 0.251241, loss_ce: 0.002334, loss_dice: 0.500148
[11:09:19.124] TRAIN: iteration 20795 : loss : 0.249440, loss_ce: 0.000540, loss_dice: 0.498341
[11:09:19.331] TRAIN: iteration 20796 : loss : 0.250756, loss_ce: 0.001451, loss_dice: 0.500061
[11:09:19.538] TRAIN: iteration 20797 : loss : 0.061458, loss_ce: 0.000945, loss_dice: 0.121972
[11:09:25.378] TRAIN: iteration 20798 : loss : 0.061986, loss_ce: 0.003005, loss_dice: 0.120967
[11:09:25.590] TRAIN: iteration 20799 : loss : 0.044267, loss_ce: 0.003261, loss_dice: 0.085273
[11:09:25.798] TRAIN: iteration 20800 : loss : 0.063037, loss_ce: 0.003559, loss_dice: 0.122516
[11:09:26.037] TRAIN: iteration 20801 : loss : 0.250468, loss_ce: 0.000894, loss_dice: 0.500041
[11:09:26.244] TRAIN: iteration 20802 : loss : 0.056179, loss_ce: 0.003357, loss_dice: 0.109000
[11:09:26.452] TRAIN: iteration 20803 : loss : 0.228086, loss_ce: 0.002447, loss_dice: 0.453724
[11:09:26.660] TRAIN: iteration 20804 : loss : 0.053211, loss_ce: 0.005176, loss_dice: 0.101246
[11:09:26.867] TRAIN: iteration 20805 : loss : 0.065735, loss_ce: 0.001815, loss_dice: 0.129654
[11:09:34.161] TRAIN: iteration 20806 : loss : 0.250917, loss_ce: 0.001745, loss_dice: 0.500088
[11:09:34.368] TRAIN: iteration 20807 : loss : 0.136348, loss_ce: 0.005321, loss_dice: 0.267375
[11:09:34.578] TRAIN: iteration 20808 : loss : 0.197812, loss_ce: 0.008263, loss_dice: 0.387361
[11:09:34.787] TRAIN: iteration 20809 : loss : 0.097171, loss_ce: 0.005776, loss_dice: 0.188567
[11:09:34.996] TRAIN: iteration 20810 : loss : 0.114440, loss_ce: 0.005218, loss_dice: 0.223663
[11:09:35.204] TRAIN: iteration 20811 : loss : 0.250752, loss_ce: 0.001439, loss_dice: 0.500064
[11:09:35.414] TRAIN: iteration 20812 : loss : 0.045416, loss_ce: 0.005738, loss_dice: 0.085095
[11:09:35.620] TRAIN: iteration 20813 : loss : 0.212275, loss_ce: 0.018017, loss_dice: 0.406532
[11:09:42.590] TRAIN: iteration 20814 : loss : 0.063667, loss_ce: 0.002695, loss_dice: 0.124640
[11:09:42.799] TRAIN: iteration 20815 : loss : 0.115773, loss_ce: 0.006650, loss_dice: 0.224897
[11:09:43.007] TRAIN: iteration 20816 : loss : 0.184639, loss_ce: 0.012444, loss_dice: 0.356833
[11:09:43.216] TRAIN: iteration 20817 : loss : 0.247669, loss_ce: 0.001897, loss_dice: 0.493440
[11:09:43.424] TRAIN: iteration 20818 : loss : 0.244753, loss_ce: 0.002267, loss_dice: 0.487240
[11:09:43.633] TRAIN: iteration 20819 : loss : 0.040061, loss_ce: 0.004008, loss_dice: 0.076114
[11:09:43.728] TRAIN: iteration 20820 : loss : 0.243718, loss_ce: 0.002307, loss_dice: 0.485129
[11:15:13.816] VALIDATION: iteration 11 : loss : 0.121251, loss_ce: 0.004579, loss_dice: 0.237922
[11:15:15.122] TRAIN: iteration 20821 : loss : 0.251166, loss_ce: 0.002210, loss_dice: 0.500123
[11:15:15.789] TRAIN: iteration 20822 : loss : 0.049936, loss_ce: 0.004533, loss_dice: 0.095339
[11:15:15.998] TRAIN: iteration 20823 : loss : 0.057806, loss_ce: 0.009460, loss_dice: 0.106152
[11:15:16.206] TRAIN: iteration 20824 : loss : 0.072874, loss_ce: 0.011913, loss_dice: 0.133835
[11:15:16.414] TRAIN: iteration 20825 : loss : 0.109937, loss_ce: 0.007555, loss_dice: 0.212319
[11:15:16.625] TRAIN: iteration 20826 : loss : 0.084844, loss_ce: 0.004239, loss_dice: 0.165449
[11:15:16.832] TRAIN: iteration 20827 : loss : 0.091527, loss_ce: 0.002772, loss_dice: 0.180282
[11:15:17.042] TRAIN: iteration 20828 : loss : 0.090439, loss_ce: 0.005574, loss_dice: 0.175305
[11:15:17.252] TRAIN: iteration 20829 : loss : 0.144310, loss_ce: 0.002477, loss_dice: 0.286142
[11:15:17.538] TRAIN: iteration 20830 : loss : 0.073815, loss_ce: 0.010610, loss_dice: 0.137020
[11:15:17.748] TRAIN: iteration 20831 : loss : 0.083489, loss_ce: 0.005109, loss_dice: 0.161869
[11:15:17.960] TRAIN: iteration 20832 : loss : 0.220790, loss_ce: 0.002650, loss_dice: 0.438930
[11:15:18.173] TRAIN: iteration 20833 : loss : 0.066909, loss_ce: 0.002064, loss_dice: 0.131753
[11:15:18.381] TRAIN: iteration 20834 : loss : 0.098174, loss_ce: 0.003030, loss_dice: 0.193317
[11:15:18.590] TRAIN: iteration 20835 : loss : 0.069059, loss_ce: 0.002297, loss_dice: 0.135820
[11:15:18.806] TRAIN: iteration 20836 : loss : 0.250501, loss_ce: 0.000992, loss_dice: 0.500010
[11:15:19.014] TRAIN: iteration 20837 : loss : 0.163965, loss_ce: 0.003180, loss_dice: 0.324750
[11:15:19.229] TRAIN: iteration 20838 : loss : 0.098783, loss_ce: 0.004606, loss_dice: 0.192959
[11:15:19.440] TRAIN: iteration 20839 : loss : 0.063201, loss_ce: 0.003454, loss_dice: 0.122948
[11:15:19.651] TRAIN: iteration 20840 : loss : 0.213694, loss_ce: 0.002309, loss_dice: 0.425080
[11:15:19.894] TRAIN: iteration 20841 : loss : 0.246696, loss_ce: 0.003346, loss_dice: 0.490046
[11:15:20.102] TRAIN: iteration 20842 : loss : 0.118100, loss_ce: 0.004211, loss_dice: 0.231989
[11:15:20.310] TRAIN: iteration 20843 : loss : 0.044811, loss_ce: 0.004591, loss_dice: 0.085030
[11:15:20.517] TRAIN: iteration 20844 : loss : 0.071737, loss_ce: 0.003822, loss_dice: 0.139653
[11:15:20.726] TRAIN: iteration 20845 : loss : 0.037609, loss_ce: 0.002747, loss_dice: 0.072472
[11:15:20.936] TRAIN: iteration 20846 : loss : 0.092711, loss_ce: 0.004608, loss_dice: 0.180813
[11:15:21.144] TRAIN: iteration 20847 : loss : 0.040639, loss_ce: 0.004430, loss_dice: 0.076848
[11:15:21.351] TRAIN: iteration 20848 : loss : 0.116896, loss_ce: 0.003690, loss_dice: 0.230102
[11:15:21.562] TRAIN: iteration 20849 : loss : 0.073556, loss_ce: 0.004085, loss_dice: 0.143027
[11:15:21.771] TRAIN: iteration 20850 : loss : 0.062983, loss_ce: 0.001680, loss_dice: 0.124285
[11:15:21.985] TRAIN: iteration 20851 : loss : 0.040396, loss_ce: 0.003049, loss_dice: 0.077744
[11:15:22.193] TRAIN: iteration 20852 : loss : 0.044966, loss_ce: 0.003608, loss_dice: 0.086323
[11:15:22.402] TRAIN: iteration 20853 : loss : 0.125947, loss_ce: 0.001433, loss_dice: 0.250461
[11:15:22.609] TRAIN: iteration 20854 : loss : 0.250603, loss_ce: 0.001175, loss_dice: 0.500031
[11:15:22.817] TRAIN: iteration 20855 : loss : 0.058808, loss_ce: 0.002254, loss_dice: 0.115362
[11:15:23.026] TRAIN: iteration 20856 : loss : 0.048669, loss_ce: 0.001560, loss_dice: 0.095777
[11:15:23.235] TRAIN: iteration 20857 : loss : 0.249344, loss_ce: 0.006663, loss_dice: 0.492026
[11:15:23.443] TRAIN: iteration 20858 : loss : 0.026555, loss_ce: 0.001622, loss_dice: 0.051488
[11:15:23.652] TRAIN: iteration 20859 : loss : 0.062387, loss_ce: 0.005970, loss_dice: 0.118803
[11:15:23.862] TRAIN: iteration 20860 : loss : 0.069169, loss_ce: 0.003527, loss_dice: 0.134810
[11:15:24.108] TRAIN: iteration 20861 : loss : 0.048919, loss_ce: 0.004192, loss_dice: 0.093646
[11:15:24.318] TRAIN: iteration 20862 : loss : 0.092747, loss_ce: 0.002661, loss_dice: 0.182833
[11:15:24.529] TRAIN: iteration 20863 : loss : 0.084820, loss_ce: 0.001157, loss_dice: 0.168484
[11:15:24.743] TRAIN: iteration 20864 : loss : 0.098309, loss_ce: 0.024352, loss_dice: 0.172266
[11:15:24.951] TRAIN: iteration 20865 : loss : 0.069604, loss_ce: 0.001076, loss_dice: 0.138133
[11:15:25.160] TRAIN: iteration 20866 : loss : 0.132449, loss_ce: 0.013223, loss_dice: 0.251675
[11:15:25.380] TRAIN: iteration 20867 : loss : 0.139480, loss_ce: 0.003487, loss_dice: 0.275473
[11:15:25.589] TRAIN: iteration 20868 : loss : 0.250296, loss_ce: 0.000586, loss_dice: 0.500006
[11:15:25.800] TRAIN: iteration 20869 : loss : 0.079961, loss_ce: 0.003802, loss_dice: 0.156120
[11:15:26.010] TRAIN: iteration 20870 : loss : 0.037384, loss_ce: 0.007106, loss_dice: 0.067662
[11:15:26.227] TRAIN: iteration 20871 : loss : 0.130815, loss_ce: 0.003030, loss_dice: 0.258601
[11:15:26.444] TRAIN: iteration 20872 : loss : 0.029118, loss_ce: 0.001341, loss_dice: 0.056895
[11:15:26.654] TRAIN: iteration 20873 : loss : 0.101422, loss_ce: 0.003741, loss_dice: 0.199102
[11:15:26.862] TRAIN: iteration 20874 : loss : 0.248102, loss_ce: 0.001085, loss_dice: 0.495120
[11:15:27.075] TRAIN: iteration 20875 : loss : 0.061764, loss_ce: 0.004462, loss_dice: 0.119067
[11:15:27.283] TRAIN: iteration 20876 : loss : 0.118523, loss_ce: 0.004680, loss_dice: 0.232365
[11:15:27.491] TRAIN: iteration 20877 : loss : 0.097870, loss_ce: 0.004651, loss_dice: 0.191090
[11:15:27.706] TRAIN: iteration 20878 : loss : 0.248619, loss_ce: 0.001562, loss_dice: 0.495677
[11:15:27.914] TRAIN: iteration 20879 : loss : 0.083897, loss_ce: 0.001760, loss_dice: 0.166034
[11:15:28.121] TRAIN: iteration 20880 : loss : 0.056678, loss_ce: 0.004027, loss_dice: 0.109329
[11:15:28.362] TRAIN: iteration 20881 : loss : 0.046133, loss_ce: 0.006367, loss_dice: 0.085899
[11:15:28.569] TRAIN: iteration 20882 : loss : 0.083709, loss_ce: 0.003876, loss_dice: 0.163542
[11:15:28.777] TRAIN: iteration 20883 : loss : 0.244026, loss_ce: 0.004787, loss_dice: 0.483265
[11:15:28.985] TRAIN: iteration 20884 : loss : 0.031521, loss_ce: 0.004722, loss_dice: 0.058319
[11:15:29.193] TRAIN: iteration 20885 : loss : 0.117843, loss_ce: 0.005418, loss_dice: 0.230268
[11:15:29.425] TRAIN: iteration 20886 : loss : 0.251156, loss_ce: 0.002180, loss_dice: 0.500131
[11:15:29.631] TRAIN: iteration 20887 : loss : 0.103057, loss_ce: 0.003235, loss_dice: 0.202879
[11:15:29.839] TRAIN: iteration 20888 : loss : 0.108475, loss_ce: 0.008387, loss_dice: 0.208563
[11:15:30.277] TRAIN: iteration 20889 : loss : 0.070931, loss_ce: 0.001153, loss_dice: 0.140708
[11:15:30.483] TRAIN: iteration 20890 : loss : 0.055014, loss_ce: 0.011496, loss_dice: 0.098533
[11:15:30.703] TRAIN: iteration 20891 : loss : 0.116574, loss_ce: 0.012846, loss_dice: 0.220302
[11:15:30.910] TRAIN: iteration 20892 : loss : 0.201089, loss_ce: 0.002172, loss_dice: 0.400006
[11:15:31.120] TRAIN: iteration 20893 : loss : 0.102018, loss_ce: 0.002100, loss_dice: 0.201937
[11:15:31.327] TRAIN: iteration 20894 : loss : 0.080787, loss_ce: 0.000651, loss_dice: 0.160923
[11:15:31.537] TRAIN: iteration 20895 : loss : 0.188691, loss_ce: 0.002486, loss_dice: 0.374896
[11:15:31.774] TRAIN: iteration 20896 : loss : 0.135810, loss_ce: 0.001393, loss_dice: 0.270226
[11:15:31.982] TRAIN: iteration 20897 : loss : 0.064731, loss_ce: 0.001978, loss_dice: 0.127484
[11:15:32.190] TRAIN: iteration 20898 : loss : 0.081641, loss_ce: 0.002913, loss_dice: 0.160368
[11:15:32.399] TRAIN: iteration 20899 : loss : 0.096489, loss_ce: 0.005201, loss_dice: 0.187776
[11:15:32.613] TRAIN: iteration 20900 : loss : 0.191864, loss_ce: 0.001605, loss_dice: 0.382123
[11:15:32.846] TRAIN: iteration 20901 : loss : 0.035349, loss_ce: 0.000543, loss_dice: 0.070154
[11:15:33.055] TRAIN: iteration 20902 : loss : 0.062424, loss_ce: 0.003518, loss_dice: 0.121330
[11:15:33.261] TRAIN: iteration 20903 : loss : 0.051763, loss_ce: 0.003462, loss_dice: 0.100064
[11:15:33.468] TRAIN: iteration 20904 : loss : 0.251195, loss_ce: 0.002234, loss_dice: 0.500157
[11:15:33.676] TRAIN: iteration 20905 : loss : 0.247059, loss_ce: 0.000603, loss_dice: 0.493514
[11:15:33.885] TRAIN: iteration 20906 : loss : 0.250218, loss_ce: 0.000426, loss_dice: 0.500010
[11:15:34.099] TRAIN: iteration 20907 : loss : 0.250240, loss_ce: 0.000477, loss_dice: 0.500002
[11:15:34.308] TRAIN: iteration 20908 : loss : 0.060098, loss_ce: 0.000926, loss_dice: 0.119269
[11:15:34.517] TRAIN: iteration 20909 : loss : 0.106676, loss_ce: 0.006034, loss_dice: 0.207318
[11:15:34.728] TRAIN: iteration 20910 : loss : 0.068066, loss_ce: 0.002898, loss_dice: 0.133235
[11:15:34.938] TRAIN: iteration 20911 : loss : 0.245839, loss_ce: 0.005986, loss_dice: 0.485692
[11:15:35.147] TRAIN: iteration 20912 : loss : 0.048477, loss_ce: 0.002572, loss_dice: 0.094383
[11:15:35.358] TRAIN: iteration 20913 : loss : 0.186420, loss_ce: 0.008004, loss_dice: 0.364837
[11:15:35.569] TRAIN: iteration 20914 : loss : 0.163998, loss_ce: 0.003206, loss_dice: 0.324789
[11:15:35.776] TRAIN: iteration 20915 : loss : 0.083646, loss_ce: 0.006453, loss_dice: 0.160840
[11:15:35.983] TRAIN: iteration 20916 : loss : 0.227715, loss_ce: 0.043679, loss_dice: 0.411751
[11:15:36.192] TRAIN: iteration 20917 : loss : 0.050477, loss_ce: 0.002446, loss_dice: 0.098508
[11:15:36.420] TRAIN: iteration 20918 : loss : 0.243434, loss_ce: 0.001121, loss_dice: 0.485747
[11:15:36.636] TRAIN: iteration 20919 : loss : 0.056406, loss_ce: 0.003745, loss_dice: 0.109066
[11:15:36.850] TRAIN: iteration 20920 : loss : 0.073640, loss_ce: 0.006461, loss_dice: 0.140819
[11:15:37.067] TRAIN: iteration 20921 : loss : 0.043127, loss_ce: 0.003999, loss_dice: 0.082256
[11:15:37.275] TRAIN: iteration 20922 : loss : 0.115416, loss_ce: 0.001725, loss_dice: 0.229107
[11:15:37.483] TRAIN: iteration 20923 : loss : 0.064202, loss_ce: 0.005171, loss_dice: 0.123234
[11:15:37.692] TRAIN: iteration 20924 : loss : 0.058592, loss_ce: 0.006242, loss_dice: 0.110942
[11:15:37.900] TRAIN: iteration 20925 : loss : 0.237401, loss_ce: 0.011070, loss_dice: 0.463733
[11:15:38.115] TRAIN: iteration 20926 : loss : 0.081940, loss_ce: 0.004331, loss_dice: 0.159549
[11:15:38.326] TRAIN: iteration 20927 : loss : 0.078521, loss_ce: 0.003442, loss_dice: 0.153601
[11:15:38.536] TRAIN: iteration 20928 : loss : 0.217532, loss_ce: 0.005417, loss_dice: 0.429648
[11:15:38.748] TRAIN: iteration 20929 : loss : 0.052640, loss_ce: 0.009359, loss_dice: 0.095922
[11:15:38.957] TRAIN: iteration 20930 : loss : 0.249448, loss_ce: 0.005717, loss_dice: 0.493179
[11:15:39.172] TRAIN: iteration 20931 : loss : 0.242185, loss_ce: 0.004547, loss_dice: 0.479823
[11:15:39.379] TRAIN: iteration 20932 : loss : 0.055090, loss_ce: 0.006976, loss_dice: 0.103204
[11:15:39.594] TRAIN: iteration 20933 : loss : 0.070713, loss_ce: 0.009036, loss_dice: 0.132390
[11:15:39.809] TRAIN: iteration 20934 : loss : 0.102154, loss_ce: 0.007054, loss_dice: 0.197253
[11:15:40.017] TRAIN: iteration 20935 : loss : 0.251209, loss_ce: 0.002319, loss_dice: 0.500100
[11:15:40.224] TRAIN: iteration 20936 : loss : 0.185132, loss_ce: 0.012863, loss_dice: 0.357401
[11:15:40.435] TRAIN: iteration 20937 : loss : 0.120456, loss_ce: 0.004609, loss_dice: 0.236303
[11:15:40.653] TRAIN: iteration 20938 : loss : 0.145590, loss_ce: 0.005164, loss_dice: 0.286016
[11:15:40.860] TRAIN: iteration 20939 : loss : 0.131394, loss_ce: 0.017945, loss_dice: 0.244844
[11:15:41.067] TRAIN: iteration 20940 : loss : 0.162734, loss_ce: 0.005220, loss_dice: 0.320247
[11:15:41.302] TRAIN: iteration 20941 : loss : 0.226732, loss_ce: 0.004195, loss_dice: 0.449268
[11:15:41.521] TRAIN: iteration 20942 : loss : 0.082973, loss_ce: 0.006184, loss_dice: 0.159763
[11:15:41.728] TRAIN: iteration 20943 : loss : 0.050401, loss_ce: 0.005947, loss_dice: 0.094855
[11:15:41.935] TRAIN: iteration 20944 : loss : 0.113414, loss_ce: 0.004846, loss_dice: 0.221982
[11:15:42.147] TRAIN: iteration 20945 : loss : 0.135638, loss_ce: 0.006642, loss_dice: 0.264635
[11:15:42.359] TRAIN: iteration 20946 : loss : 0.169619, loss_ce: 0.005366, loss_dice: 0.333872
[11:15:42.580] TRAIN: iteration 20947 : loss : 0.036573, loss_ce: 0.005805, loss_dice: 0.067342
[11:15:42.786] TRAIN: iteration 20948 : loss : 0.099215, loss_ce: 0.002505, loss_dice: 0.195925
[11:15:42.992] TRAIN: iteration 20949 : loss : 0.096386, loss_ce: 0.002829, loss_dice: 0.189942
[11:15:43.200] TRAIN: iteration 20950 : loss : 0.070404, loss_ce: 0.002539, loss_dice: 0.138269
[11:15:43.408] TRAIN: iteration 20951 : loss : 0.250705, loss_ce: 0.004424, loss_dice: 0.496986
[11:15:43.616] TRAIN: iteration 20952 : loss : 0.098647, loss_ce: 0.008795, loss_dice: 0.188500
[11:15:43.825] TRAIN: iteration 20953 : loss : 0.236700, loss_ce: 0.007388, loss_dice: 0.466012
[11:15:44.036] TRAIN: iteration 20954 : loss : 0.071497, loss_ce: 0.003668, loss_dice: 0.139326
[11:15:44.246] TRAIN: iteration 20955 : loss : 0.126230, loss_ce: 0.003806, loss_dice: 0.248655
[11:15:44.456] TRAIN: iteration 20956 : loss : 0.086509, loss_ce: 0.014730, loss_dice: 0.158289
[11:15:44.664] TRAIN: iteration 20957 : loss : 0.182346, loss_ce: 0.002442, loss_dice: 0.362249
[11:15:44.871] TRAIN: iteration 20958 : loss : 0.079138, loss_ce: 0.005672, loss_dice: 0.152603
[11:15:45.079] TRAIN: iteration 20959 : loss : 0.243087, loss_ce: 0.001303, loss_dice: 0.484872
[11:15:45.286] TRAIN: iteration 20960 : loss : 0.034835, loss_ce: 0.001224, loss_dice: 0.068446
[11:15:45.526] TRAIN: iteration 20961 : loss : 0.059942, loss_ce: 0.009402, loss_dice: 0.110482
[11:15:45.735] TRAIN: iteration 20962 : loss : 0.037709, loss_ce: 0.002262, loss_dice: 0.073156
[11:15:45.943] TRAIN: iteration 20963 : loss : 0.160345, loss_ce: 0.002909, loss_dice: 0.317782
[11:15:46.153] TRAIN: iteration 20964 : loss : 0.138759, loss_ce: 0.003695, loss_dice: 0.273822
[11:15:46.362] TRAIN: iteration 20965 : loss : 0.131420, loss_ce: 0.013448, loss_dice: 0.249391
[11:15:46.574] TRAIN: iteration 20966 : loss : 0.063845, loss_ce: 0.004554, loss_dice: 0.123137
[11:15:46.784] TRAIN: iteration 20967 : loss : 0.213030, loss_ce: 0.006453, loss_dice: 0.419607
[11:15:46.990] TRAIN: iteration 20968 : loss : 0.063830, loss_ce: 0.005532, loss_dice: 0.122128
[11:15:47.200] TRAIN: iteration 20969 : loss : 0.250897, loss_ce: 0.001747, loss_dice: 0.500047
[11:15:47.412] TRAIN: iteration 20970 : loss : 0.023856, loss_ce: 0.004943, loss_dice: 0.042768
[11:15:47.626] TRAIN: iteration 20971 : loss : 0.243555, loss_ce: 0.002984, loss_dice: 0.484126
[11:15:47.834] TRAIN: iteration 20972 : loss : 0.251089, loss_ce: 0.003120, loss_dice: 0.499058
[11:15:48.043] TRAIN: iteration 20973 : loss : 0.191361, loss_ce: 0.003411, loss_dice: 0.379310
[11:15:48.253] TRAIN: iteration 20974 : loss : 0.233795, loss_ce: 0.003262, loss_dice: 0.464329
[11:15:48.462] TRAIN: iteration 20975 : loss : 0.251644, loss_ce: 0.005845, loss_dice: 0.497443
[11:15:48.670] TRAIN: iteration 20976 : loss : 0.057801, loss_ce: 0.002350, loss_dice: 0.113253
[11:15:48.879] TRAIN: iteration 20977 : loss : 0.075281, loss_ce: 0.004118, loss_dice: 0.146445
[11:15:49.090] TRAIN: iteration 20978 : loss : 0.068835, loss_ce: 0.004547, loss_dice: 0.133123
[11:15:49.300] TRAIN: iteration 20979 : loss : 0.051941, loss_ce: 0.006198, loss_dice: 0.097684
[11:15:49.508] TRAIN: iteration 20980 : loss : 0.128243, loss_ce: 0.002611, loss_dice: 0.253874
[11:15:49.757] TRAIN: iteration 20981 : loss : 0.081305, loss_ce: 0.004093, loss_dice: 0.158518
[11:15:49.971] TRAIN: iteration 20982 : loss : 0.051522, loss_ce: 0.003640, loss_dice: 0.099405
[11:15:50.178] TRAIN: iteration 20983 : loss : 0.185508, loss_ce: 0.007154, loss_dice: 0.363862
[11:15:50.388] TRAIN: iteration 20984 : loss : 0.050380, loss_ce: 0.001862, loss_dice: 0.098898
[11:15:50.599] TRAIN: iteration 20985 : loss : 0.107996, loss_ce: 0.006504, loss_dice: 0.209487
[11:15:50.815] TRAIN: iteration 20986 : loss : 0.171649, loss_ce: 0.001609, loss_dice: 0.341689
[11:15:51.032] TRAIN: iteration 20987 : loss : 0.239558, loss_ce: 0.004470, loss_dice: 0.474646
[11:15:51.240] TRAIN: iteration 20988 : loss : 0.034486, loss_ce: 0.002155, loss_dice: 0.066816
[11:15:51.448] TRAIN: iteration 20989 : loss : 0.057869, loss_ce: 0.005372, loss_dice: 0.110366
[11:15:51.656] TRAIN: iteration 20990 : loss : 0.085281, loss_ce: 0.005982, loss_dice: 0.164580
[11:15:51.868] TRAIN: iteration 20991 : loss : 0.055913, loss_ce: 0.001257, loss_dice: 0.110568
[11:15:52.081] TRAIN: iteration 20992 : loss : 0.100224, loss_ce: 0.004495, loss_dice: 0.195952
[11:15:52.288] TRAIN: iteration 20993 : loss : 0.106644, loss_ce: 0.004730, loss_dice: 0.208558
[11:15:52.496] TRAIN: iteration 20994 : loss : 0.129927, loss_ce: 0.002861, loss_dice: 0.256993
[11:15:52.703] TRAIN: iteration 20995 : loss : 0.250627, loss_ce: 0.001195, loss_dice: 0.500059
[11:15:52.916] TRAIN: iteration 20996 : loss : 0.043648, loss_ce: 0.003907, loss_dice: 0.083388
[11:15:53.132] TRAIN: iteration 20997 : loss : 0.079349, loss_ce: 0.007835, loss_dice: 0.150862
[11:15:53.340] TRAIN: iteration 20998 : loss : 0.250794, loss_ce: 0.001516, loss_dice: 0.500072
[11:15:53.551] TRAIN: iteration 20999 : loss : 0.076541, loss_ce: 0.003299, loss_dice: 0.149783
[11:15:53.758] TRAIN: iteration 21000 : loss : 0.251642, loss_ce: 0.003072, loss_dice: 0.500213
[11:15:53.994] TRAIN: iteration 21001 : loss : 0.236821, loss_ce: 0.001184, loss_dice: 0.472458
[11:15:54.209] TRAIN: iteration 21002 : loss : 0.221300, loss_ce: 0.006107, loss_dice: 0.436493
[11:15:54.418] TRAIN: iteration 21003 : loss : 0.124567, loss_ce: 0.010896, loss_dice: 0.238237
[11:15:54.631] TRAIN: iteration 21004 : loss : 0.163132, loss_ce: 0.001840, loss_dice: 0.324424
[11:15:54.842] TRAIN: iteration 21005 : loss : 0.250866, loss_ce: 0.001649, loss_dice: 0.500084
[11:15:55.050] TRAIN: iteration 21006 : loss : 0.244246, loss_ce: 0.003289, loss_dice: 0.485203
[11:15:55.260] TRAIN: iteration 21007 : loss : 0.036721, loss_ce: 0.002762, loss_dice: 0.070681
[11:15:55.477] TRAIN: iteration 21008 : loss : 0.043179, loss_ce: 0.003811, loss_dice: 0.082546
[11:15:55.684] TRAIN: iteration 21009 : loss : 0.157929, loss_ce: 0.002423, loss_dice: 0.313436
[11:15:55.892] TRAIN: iteration 21010 : loss : 0.251250, loss_ce: 0.002687, loss_dice: 0.499814
[11:15:56.105] TRAIN: iteration 21011 : loss : 0.084751, loss_ce: 0.001844, loss_dice: 0.167658
[11:15:56.313] TRAIN: iteration 21012 : loss : 0.237910, loss_ce: 0.001928, loss_dice: 0.473891
[11:15:56.520] TRAIN: iteration 21013 : loss : 0.169841, loss_ce: 0.004047, loss_dice: 0.335635
[11:15:56.728] TRAIN: iteration 21014 : loss : 0.028504, loss_ce: 0.002522, loss_dice: 0.054486
[11:15:56.943] TRAIN: iteration 21015 : loss : 0.251036, loss_ce: 0.001964, loss_dice: 0.500108
[11:15:57.153] TRAIN: iteration 21016 : loss : 0.057915, loss_ce: 0.001116, loss_dice: 0.114715
[11:15:57.361] TRAIN: iteration 21017 : loss : 0.103362, loss_ce: 0.001085, loss_dice: 0.205639
[11:15:57.570] TRAIN: iteration 21018 : loss : 0.064973, loss_ce: 0.004132, loss_dice: 0.125814
[11:15:57.779] TRAIN: iteration 21019 : loss : 0.094339, loss_ce: 0.008820, loss_dice: 0.179859
[11:15:57.988] TRAIN: iteration 21020 : loss : 0.246022, loss_ce: 0.001840, loss_dice: 0.490204
[11:15:58.226] TRAIN: iteration 21021 : loss : 0.250392, loss_ce: 0.000757, loss_dice: 0.500026
[11:15:58.434] TRAIN: iteration 21022 : loss : 0.152774, loss_ce: 0.003222, loss_dice: 0.302327
[11:15:58.648] TRAIN: iteration 21023 : loss : 0.193821, loss_ce: 0.003049, loss_dice: 0.384592
[11:15:58.863] TRAIN: iteration 21024 : loss : 0.070416, loss_ce: 0.002550, loss_dice: 0.138282
[11:15:59.071] TRAIN: iteration 21025 : loss : 0.212806, loss_ce: 0.008041, loss_dice: 0.417571
[11:15:59.280] TRAIN: iteration 21026 : loss : 0.159979, loss_ce: 0.007633, loss_dice: 0.312325
[11:15:59.515] TRAIN: iteration 21027 : loss : 0.039038, loss_ce: 0.002769, loss_dice: 0.075308
[11:15:59.722] TRAIN: iteration 21028 : loss : 0.244138, loss_ce: 0.026393, loss_dice: 0.461883
[11:15:59.932] TRAIN: iteration 21029 : loss : 0.070138, loss_ce: 0.002261, loss_dice: 0.138016
[11:16:00.142] TRAIN: iteration 21030 : loss : 0.050970, loss_ce: 0.007354, loss_dice: 0.094585
[11:16:00.374] TRAIN: iteration 21031 : loss : 0.220802, loss_ce: 0.004242, loss_dice: 0.437361
[11:16:00.582] TRAIN: iteration 21032 : loss : 0.034455, loss_ce: 0.001872, loss_dice: 0.067037
[11:16:00.789] TRAIN: iteration 21033 : loss : 0.250904, loss_ce: 0.001732, loss_dice: 0.500075
[11:16:00.998] TRAIN: iteration 21034 : loss : 0.107800, loss_ce: 0.005654, loss_dice: 0.209947
[11:16:01.209] TRAIN: iteration 21035 : loss : 0.032235, loss_ce: 0.002083, loss_dice: 0.062387
[11:16:01.417] TRAIN: iteration 21036 : loss : 0.223492, loss_ce: 0.003115, loss_dice: 0.443870
[11:16:01.626] TRAIN: iteration 21037 : loss : 0.147556, loss_ce: 0.008257, loss_dice: 0.286854
[11:16:01.839] TRAIN: iteration 21038 : loss : 0.200952, loss_ce: 0.005889, loss_dice: 0.396015
[11:16:02.052] TRAIN: iteration 21039 : loss : 0.232867, loss_ce: 0.003690, loss_dice: 0.462044
[11:16:02.263] TRAIN: iteration 21040 : loss : 0.249328, loss_ce: 0.005069, loss_dice: 0.493587
[11:16:02.530] TRAIN: iteration 21041 : loss : 0.118878, loss_ce: 0.002009, loss_dice: 0.235746
[11:16:02.740] TRAIN: iteration 21042 : loss : 0.233039, loss_ce: 0.005491, loss_dice: 0.460586
[11:16:02.948] TRAIN: iteration 21043 : loss : 0.203803, loss_ce: 0.011478, loss_dice: 0.396128
[11:16:03.157] TRAIN: iteration 21044 : loss : 0.122780, loss_ce: 0.005934, loss_dice: 0.239626
[11:16:03.364] TRAIN: iteration 21045 : loss : 0.179656, loss_ce: 0.006578, loss_dice: 0.352733
[11:16:03.592] TRAIN: iteration 21046 : loss : 0.048264, loss_ce: 0.005164, loss_dice: 0.091364
[11:16:03.849] TRAIN: iteration 21047 : loss : 0.252346, loss_ce: 0.004434, loss_dice: 0.500258
[11:16:04.061] TRAIN: iteration 21048 : loss : 0.144754, loss_ce: 0.004409, loss_dice: 0.285099
[11:16:04.271] TRAIN: iteration 21049 : loss : 0.072938, loss_ce: 0.005622, loss_dice: 0.140253
[11:16:04.486] TRAIN: iteration 21050 : loss : 0.068331, loss_ce: 0.003393, loss_dice: 0.133269
[11:16:04.701] TRAIN: iteration 21051 : loss : 0.061566, loss_ce: 0.004638, loss_dice: 0.118494
[11:16:04.911] TRAIN: iteration 21052 : loss : 0.226865, loss_ce: 0.002278, loss_dice: 0.451452
[11:16:05.121] TRAIN: iteration 21053 : loss : 0.092035, loss_ce: 0.004962, loss_dice: 0.179108
[11:16:05.333] TRAIN: iteration 21054 : loss : 0.098419, loss_ce: 0.004727, loss_dice: 0.192111
[11:16:05.542] TRAIN: iteration 21055 : loss : 0.112731, loss_ce: 0.007290, loss_dice: 0.218172
[11:16:05.762] TRAIN: iteration 21056 : loss : 0.051678, loss_ce: 0.001938, loss_dice: 0.101418
[11:16:05.970] TRAIN: iteration 21057 : loss : 0.066714, loss_ce: 0.003867, loss_dice: 0.129561
[11:16:06.179] TRAIN: iteration 21058 : loss : 0.250970, loss_ce: 0.002941, loss_dice: 0.498998
[11:16:06.389] TRAIN: iteration 21059 : loss : 0.250954, loss_ce: 0.001820, loss_dice: 0.500088
[11:16:06.595] TRAIN: iteration 21060 : loss : 0.101122, loss_ce: 0.005556, loss_dice: 0.196688
[11:16:06.834] TRAIN: iteration 21061 : loss : 0.167732, loss_ce: 0.003024, loss_dice: 0.332440
[11:16:07.043] TRAIN: iteration 21062 : loss : 0.225317, loss_ce: 0.030808, loss_dice: 0.419826
[11:16:07.275] TRAIN: iteration 21063 : loss : 0.124409, loss_ce: 0.005367, loss_dice: 0.243451
[11:16:07.482] TRAIN: iteration 21064 : loss : 0.071228, loss_ce: 0.001659, loss_dice: 0.140796
[11:16:07.689] TRAIN: iteration 21065 : loss : 0.070700, loss_ce: 0.007452, loss_dice: 0.133948
[11:16:07.899] TRAIN: iteration 21066 : loss : 0.042572, loss_ce: 0.002032, loss_dice: 0.083113
[11:16:08.109] TRAIN: iteration 21067 : loss : 0.212378, loss_ce: 0.002926, loss_dice: 0.421830
[11:16:08.317] TRAIN: iteration 21068 : loss : 0.116101, loss_ce: 0.022375, loss_dice: 0.209826
[11:16:08.529] TRAIN: iteration 21069 : loss : 0.250627, loss_ce: 0.001202, loss_dice: 0.500052
[11:16:08.743] TRAIN: iteration 21070 : loss : 0.066841, loss_ce: 0.003824, loss_dice: 0.129858
[11:16:08.963] TRAIN: iteration 21071 : loss : 0.117315, loss_ce: 0.003916, loss_dice: 0.230713
[11:16:09.175] TRAIN: iteration 21072 : loss : 0.221194, loss_ce: 0.006400, loss_dice: 0.435987
[11:16:09.382] TRAIN: iteration 21073 : loss : 0.198086, loss_ce: 0.006679, loss_dice: 0.389493
[11:16:09.589] TRAIN: iteration 21074 : loss : 0.119174, loss_ce: 0.003522, loss_dice: 0.234827
[11:16:09.797] TRAIN: iteration 21075 : loss : 0.230323, loss_ce: 0.003334, loss_dice: 0.457311
[11:16:10.005] TRAIN: iteration 21076 : loss : 0.082197, loss_ce: 0.001866, loss_dice: 0.162528
[11:16:10.212] TRAIN: iteration 21077 : loss : 0.250808, loss_ce: 0.001563, loss_dice: 0.500053
[11:16:10.419] TRAIN: iteration 21078 : loss : 0.050804, loss_ce: 0.004421, loss_dice: 0.097186
[11:16:10.626] TRAIN: iteration 21079 : loss : 0.111165, loss_ce: 0.003599, loss_dice: 0.218732
[11:16:10.832] TRAIN: iteration 21080 : loss : 0.060221, loss_ce: 0.008739, loss_dice: 0.111703
[11:16:11.071] TRAIN: iteration 21081 : loss : 0.220148, loss_ce: 0.010098, loss_dice: 0.430198
[11:16:11.278] TRAIN: iteration 21082 : loss : 0.186924, loss_ce: 0.003271, loss_dice: 0.370578
[11:16:11.500] TRAIN: iteration 21083 : loss : 0.099999, loss_ce: 0.004674, loss_dice: 0.195325
[11:16:11.708] TRAIN: iteration 21084 : loss : 0.192754, loss_ce: 0.010377, loss_dice: 0.375130
[11:16:11.922] TRAIN: iteration 21085 : loss : 0.095643, loss_ce: 0.008761, loss_dice: 0.182526
[11:16:12.131] TRAIN: iteration 21086 : loss : 0.052542, loss_ce: 0.004556, loss_dice: 0.100529
[11:16:12.338] TRAIN: iteration 21087 : loss : 0.233481, loss_ce: 0.008873, loss_dice: 0.458088
[11:16:12.553] TRAIN: iteration 21088 : loss : 0.141277, loss_ce: 0.004613, loss_dice: 0.277942
[11:16:12.767] TRAIN: iteration 21089 : loss : 0.173573, loss_ce: 0.008128, loss_dice: 0.339018
[11:16:12.974] TRAIN: iteration 21090 : loss : 0.105387, loss_ce: 0.002416, loss_dice: 0.208357
[11:16:13.182] TRAIN: iteration 21091 : loss : 0.111963, loss_ce: 0.004368, loss_dice: 0.219558
[11:16:13.400] TRAIN: iteration 21092 : loss : 0.215671, loss_ce: 0.002270, loss_dice: 0.429071
[11:16:13.609] TRAIN: iteration 21093 : loss : 0.210777, loss_ce: 0.013758, loss_dice: 0.407796
[11:16:13.818] TRAIN: iteration 21094 : loss : 0.240963, loss_ce: 0.002565, loss_dice: 0.479361
[11:16:14.025] TRAIN: iteration 21095 : loss : 0.065958, loss_ce: 0.003565, loss_dice: 0.128352
[11:16:14.235] TRAIN: iteration 21096 : loss : 0.075866, loss_ce: 0.002701, loss_dice: 0.149030
[11:16:14.451] TRAIN: iteration 21097 : loss : 0.135502, loss_ce: 0.002629, loss_dice: 0.268375
[11:16:14.660] TRAIN: iteration 21098 : loss : 0.262820, loss_ce: 0.025207, loss_dice: 0.500432
[11:16:14.869] TRAIN: iteration 21099 : loss : 0.142225, loss_ce: 0.006921, loss_dice: 0.277528
[11:16:15.179] TRAIN: iteration 21100 : loss : 0.103259, loss_ce: 0.004230, loss_dice: 0.202288
[11:16:15.416] TRAIN: iteration 21101 : loss : 0.183263, loss_ce: 0.005032, loss_dice: 0.361493
[11:16:15.625] TRAIN: iteration 21102 : loss : 0.211380, loss_ce: 0.004203, loss_dice: 0.418557
[11:16:15.834] TRAIN: iteration 21103 : loss : 0.240741, loss_ce: 0.006483, loss_dice: 0.475000
[11:16:16.042] TRAIN: iteration 21104 : loss : 0.250724, loss_ce: 0.001407, loss_dice: 0.500042
[11:16:16.254] TRAIN: iteration 21105 : loss : 0.159895, loss_ce: 0.001369, loss_dice: 0.318422
[11:16:16.462] TRAIN: iteration 21106 : loss : 0.049306, loss_ce: 0.002397, loss_dice: 0.096215
[11:16:16.672] TRAIN: iteration 21107 : loss : 0.136173, loss_ce: 0.009204, loss_dice: 0.263142
[11:16:16.884] TRAIN: iteration 21108 : loss : 0.099998, loss_ce: 0.002960, loss_dice: 0.197036
[11:16:17.094] TRAIN: iteration 21109 : loss : 0.239951, loss_ce: 0.006760, loss_dice: 0.473142
[11:16:17.301] TRAIN: iteration 21110 : loss : 0.111855, loss_ce: 0.004980, loss_dice: 0.218729
[11:16:17.566] TRAIN: iteration 21111 : loss : 0.180517, loss_ce: 0.001678, loss_dice: 0.359356
[11:16:17.774] TRAIN: iteration 21112 : loss : 0.168659, loss_ce: 0.020773, loss_dice: 0.316546
[11:16:17.982] TRAIN: iteration 21113 : loss : 0.090744, loss_ce: 0.004105, loss_dice: 0.177382
[11:16:18.190] TRAIN: iteration 21114 : loss : 0.104849, loss_ce: 0.002577, loss_dice: 0.207120
[11:16:18.399] TRAIN: iteration 21115 : loss : 0.055137, loss_ce: 0.002074, loss_dice: 0.108201
[11:16:18.677] TRAIN: iteration 21116 : loss : 0.118656, loss_ce: 0.002627, loss_dice: 0.234686
[11:16:18.885] TRAIN: iteration 21117 : loss : 0.238931, loss_ce: 0.001318, loss_dice: 0.476545
[11:16:19.092] TRAIN: iteration 21118 : loss : 0.229256, loss_ce: 0.002395, loss_dice: 0.456117
[11:16:19.298] TRAIN: iteration 21119 : loss : 0.250508, loss_ce: 0.000995, loss_dice: 0.500021
[11:16:19.511] TRAIN: iteration 21120 : loss : 0.026022, loss_ce: 0.001673, loss_dice: 0.050371
[11:16:19.765] TRAIN: iteration 21121 : loss : 0.042088, loss_ce: 0.001022, loss_dice: 0.083153
[11:16:19.976] TRAIN: iteration 21122 : loss : 0.056332, loss_ce: 0.005880, loss_dice: 0.106784
[11:16:20.184] TRAIN: iteration 21123 : loss : 0.099071, loss_ce: 0.005960, loss_dice: 0.192183
[11:16:20.391] TRAIN: iteration 21124 : loss : 0.237554, loss_ce: 0.002424, loss_dice: 0.472683
[11:16:20.600] TRAIN: iteration 21125 : loss : 0.030850, loss_ce: 0.001641, loss_dice: 0.060058
[11:16:20.809] TRAIN: iteration 21126 : loss : 0.115893, loss_ce: 0.003131, loss_dice: 0.228656
[11:16:21.016] TRAIN: iteration 21127 : loss : 0.068723, loss_ce: 0.001964, loss_dice: 0.135482
[11:16:21.224] TRAIN: iteration 21128 : loss : 0.061308, loss_ce: 0.004199, loss_dice: 0.118416
[11:16:21.433] TRAIN: iteration 21129 : loss : 0.176778, loss_ce: 0.002807, loss_dice: 0.350749
[11:16:21.639] TRAIN: iteration 21130 : loss : 0.071632, loss_ce: 0.006530, loss_dice: 0.136733
[11:16:21.854] TRAIN: iteration 21131 : loss : 0.232696, loss_ce: 0.005012, loss_dice: 0.460379
[11:16:22.061] TRAIN: iteration 21132 : loss : 0.056071, loss_ce: 0.002675, loss_dice: 0.109466
[11:16:22.270] TRAIN: iteration 21133 : loss : 0.177962, loss_ce: 0.014423, loss_dice: 0.341502
[11:16:22.477] TRAIN: iteration 21134 : loss : 0.222326, loss_ce: 0.004190, loss_dice: 0.440461
[11:16:22.688] TRAIN: iteration 21135 : loss : 0.234986, loss_ce: 0.003079, loss_dice: 0.466892
[11:16:22.896] TRAIN: iteration 21136 : loss : 0.075902, loss_ce: 0.015731, loss_dice: 0.136072
[11:16:23.112] TRAIN: iteration 21137 : loss : 0.250910, loss_ce: 0.001740, loss_dice: 0.500080
[11:16:23.323] TRAIN: iteration 21138 : loss : 0.128375, loss_ce: 0.001670, loss_dice: 0.255079
[11:16:23.538] TRAIN: iteration 21139 : loss : 0.051153, loss_ce: 0.001227, loss_dice: 0.101080
[11:16:23.753] TRAIN: iteration 21140 : loss : 0.142879, loss_ce: 0.007306, loss_dice: 0.278452
[11:16:23.992] TRAIN: iteration 21141 : loss : 0.244185, loss_ce: 0.002659, loss_dice: 0.485711
[11:16:24.203] TRAIN: iteration 21142 : loss : 0.119001, loss_ce: 0.004712, loss_dice: 0.233290
[11:16:24.411] TRAIN: iteration 21143 : loss : 0.175269, loss_ce: 0.004764, loss_dice: 0.345775
[11:16:24.618] TRAIN: iteration 21144 : loss : 0.043072, loss_ce: 0.002163, loss_dice: 0.083982
[11:16:24.827] TRAIN: iteration 21145 : loss : 0.153989, loss_ce: 0.005866, loss_dice: 0.302113
[11:16:25.043] TRAIN: iteration 21146 : loss : 0.228244, loss_ce: 0.003616, loss_dice: 0.452873
[11:16:25.251] TRAIN: iteration 21147 : loss : 0.078830, loss_ce: 0.004827, loss_dice: 0.152833
[11:16:25.460] TRAIN: iteration 21148 : loss : 0.074450, loss_ce: 0.002896, loss_dice: 0.146003
[11:16:25.672] TRAIN: iteration 21149 : loss : 0.251156, loss_ce: 0.002185, loss_dice: 0.500127
[11:16:25.880] TRAIN: iteration 21150 : loss : 0.219156, loss_ce: 0.002080, loss_dice: 0.436231
[11:16:26.095] TRAIN: iteration 21151 : loss : 0.251314, loss_ce: 0.002477, loss_dice: 0.500152
[11:16:26.306] TRAIN: iteration 21152 : loss : 0.077457, loss_ce: 0.002325, loss_dice: 0.152589
[11:16:26.518] TRAIN: iteration 21153 : loss : 0.251389, loss_ce: 0.002602, loss_dice: 0.500176
[11:16:26.728] TRAIN: iteration 21154 : loss : 0.145522, loss_ce: 0.006821, loss_dice: 0.284223
[11:16:26.937] TRAIN: iteration 21155 : loss : 0.167344, loss_ce: 0.004538, loss_dice: 0.330151
[11:16:27.146] TRAIN: iteration 21156 : loss : 0.251375, loss_ce: 0.002592, loss_dice: 0.500158
[11:16:27.366] TRAIN: iteration 21157 : loss : 0.082647, loss_ce: 0.002786, loss_dice: 0.162509
[11:16:27.578] TRAIN: iteration 21158 : loss : 0.129927, loss_ce: 0.004059, loss_dice: 0.255795
[11:16:27.793] TRAIN: iteration 21159 : loss : 0.102014, loss_ce: 0.004389, loss_dice: 0.199640
[11:16:28.539] TRAIN: iteration 21160 : loss : 0.166992, loss_ce: 0.003977, loss_dice: 0.330006
[11:16:28.775] TRAIN: iteration 21161 : loss : 0.164782, loss_ce: 0.035345, loss_dice: 0.294219
[11:16:28.985] TRAIN: iteration 21162 : loss : 0.135838, loss_ce: 0.001700, loss_dice: 0.269977
[11:16:29.202] TRAIN: iteration 21163 : loss : 0.158380, loss_ce: 0.002536, loss_dice: 0.314225
[11:16:29.411] TRAIN: iteration 21164 : loss : 0.023436, loss_ce: 0.001663, loss_dice: 0.045208
[11:16:29.621] TRAIN: iteration 21165 : loss : 0.250846, loss_ce: 0.001639, loss_dice: 0.500053
[11:16:29.828] TRAIN: iteration 21166 : loss : 0.081120, loss_ce: 0.001603, loss_dice: 0.160637
[11:16:30.036] TRAIN: iteration 21167 : loss : 0.158105, loss_ce: 0.003714, loss_dice: 0.312496
[11:16:30.247] TRAIN: iteration 21168 : loss : 0.122213, loss_ce: 0.001517, loss_dice: 0.242908
[11:16:30.456] TRAIN: iteration 21169 : loss : 0.077265, loss_ce: 0.002961, loss_dice: 0.151569
[11:16:30.665] TRAIN: iteration 21170 : loss : 0.016710, loss_ce: 0.001040, loss_dice: 0.032380
[11:16:30.874] TRAIN: iteration 21171 : loss : 0.175893, loss_ce: 0.001223, loss_dice: 0.350564
[11:16:31.084] TRAIN: iteration 21172 : loss : 0.098629, loss_ce: 0.001173, loss_dice: 0.196086
[11:16:31.294] TRAIN: iteration 21173 : loss : 0.103679, loss_ce: 0.005225, loss_dice: 0.202133
[11:16:31.501] TRAIN: iteration 21174 : loss : 0.081005, loss_ce: 0.002152, loss_dice: 0.159859
[11:16:31.708] TRAIN: iteration 21175 : loss : 0.234662, loss_ce: 0.007082, loss_dice: 0.462241
[11:16:31.917] TRAIN: iteration 21176 : loss : 0.044171, loss_ce: 0.002660, loss_dice: 0.085683
[11:16:32.132] TRAIN: iteration 21177 : loss : 0.225644, loss_ce: 0.007616, loss_dice: 0.443671
[11:16:32.346] TRAIN: iteration 21178 : loss : 0.078489, loss_ce: 0.001439, loss_dice: 0.155539
[11:16:32.553] TRAIN: iteration 21179 : loss : 0.250487, loss_ce: 0.000951, loss_dice: 0.500024
[11:16:32.760] TRAIN: iteration 21180 : loss : 0.139881, loss_ce: 0.008413, loss_dice: 0.271350
[11:16:32.999] TRAIN: iteration 21181 : loss : 0.074333, loss_ce: 0.002667, loss_dice: 0.145999
[11:16:33.209] TRAIN: iteration 21182 : loss : 0.052088, loss_ce: 0.003051, loss_dice: 0.101124
[11:16:33.423] TRAIN: iteration 21183 : loss : 0.031424, loss_ce: 0.002875, loss_dice: 0.059974
[11:16:33.630] TRAIN: iteration 21184 : loss : 0.115969, loss_ce: 0.012224, loss_dice: 0.219714
[11:16:33.842] TRAIN: iteration 21185 : loss : 0.084947, loss_ce: 0.002731, loss_dice: 0.167162
[11:16:34.049] TRAIN: iteration 21186 : loss : 0.124856, loss_ce: 0.004037, loss_dice: 0.245675
[11:16:34.261] TRAIN: iteration 21187 : loss : 0.079173, loss_ce: 0.003435, loss_dice: 0.154911
[11:16:34.469] TRAIN: iteration 21188 : loss : 0.092497, loss_ce: 0.002917, loss_dice: 0.182077
[11:16:34.677] TRAIN: iteration 21189 : loss : 0.130912, loss_ce: 0.006086, loss_dice: 0.255737
[11:16:34.887] TRAIN: iteration 21190 : loss : 0.251014, loss_ce: 0.001925, loss_dice: 0.500103
[11:16:35.095] TRAIN: iteration 21191 : loss : 0.078791, loss_ce: 0.005151, loss_dice: 0.152430
[11:16:35.305] TRAIN: iteration 21192 : loss : 0.115123, loss_ce: 0.003714, loss_dice: 0.226533
[11:16:35.513] TRAIN: iteration 21193 : loss : 0.020989, loss_ce: 0.001480, loss_dice: 0.040498
[11:16:35.723] TRAIN: iteration 21194 : loss : 0.111608, loss_ce: 0.002119, loss_dice: 0.221097
[11:16:35.935] TRAIN: iteration 21195 : loss : 0.181822, loss_ce: 0.002691, loss_dice: 0.360954
[11:16:36.144] TRAIN: iteration 21196 : loss : 0.130790, loss_ce: 0.006244, loss_dice: 0.255335
[11:16:36.357] TRAIN: iteration 21197 : loss : 0.250915, loss_ce: 0.001756, loss_dice: 0.500073
[11:16:36.565] TRAIN: iteration 21198 : loss : 0.080712, loss_ce: 0.002420, loss_dice: 0.159004
[11:16:36.774] TRAIN: iteration 21199 : loss : 0.170406, loss_ce: 0.004244, loss_dice: 0.336568
[11:16:36.983] TRAIN: iteration 21200 : loss : 0.076864, loss_ce: 0.001426, loss_dice: 0.152301
[11:16:37.220] TRAIN: iteration 21201 : loss : 0.174874, loss_ce: 0.008159, loss_dice: 0.341589
[11:16:37.427] TRAIN: iteration 21202 : loss : 0.140956, loss_ce: 0.003310, loss_dice: 0.278601
[11:16:37.639] TRAIN: iteration 21203 : loss : 0.207193, loss_ce: 0.001862, loss_dice: 0.412523
[11:16:37.873] TRAIN: iteration 21204 : loss : 0.161416, loss_ce: 0.023373, loss_dice: 0.299460
[11:16:38.084] TRAIN: iteration 21205 : loss : 0.044159, loss_ce: 0.001891, loss_dice: 0.086426
[11:16:38.296] TRAIN: iteration 21206 : loss : 0.079518, loss_ce: 0.002752, loss_dice: 0.156284
[11:16:38.506] TRAIN: iteration 21207 : loss : 0.207929, loss_ce: 0.002582, loss_dice: 0.413275
[11:16:38.715] TRAIN: iteration 21208 : loss : 0.115992, loss_ce: 0.007079, loss_dice: 0.224904
[11:16:38.924] TRAIN: iteration 21209 : loss : 0.093855, loss_ce: 0.001956, loss_dice: 0.185754
[11:16:39.134] TRAIN: iteration 21210 : loss : 0.072073, loss_ce: 0.001664, loss_dice: 0.142482
[11:16:39.343] TRAIN: iteration 21211 : loss : 0.071685, loss_ce: 0.009848, loss_dice: 0.133522
[11:16:39.551] TRAIN: iteration 21212 : loss : 0.072052, loss_ce: 0.006710, loss_dice: 0.137394
[11:16:39.763] TRAIN: iteration 21213 : loss : 0.250618, loss_ce: 0.002002, loss_dice: 0.499234
[11:16:39.979] TRAIN: iteration 21214 : loss : 0.251843, loss_ce: 0.003465, loss_dice: 0.500221
[11:16:40.187] TRAIN: iteration 21215 : loss : 0.056352, loss_ce: 0.002735, loss_dice: 0.109969
[11:16:40.396] TRAIN: iteration 21216 : loss : 0.054689, loss_ce: 0.007626, loss_dice: 0.101753
[11:16:40.603] TRAIN: iteration 21217 : loss : 0.064388, loss_ce: 0.003290, loss_dice: 0.125485
[11:16:40.811] TRAIN: iteration 21218 : loss : 0.117499, loss_ce: 0.027157, loss_dice: 0.207840
[11:16:41.022] TRAIN: iteration 21219 : loss : 0.056683, loss_ce: 0.004160, loss_dice: 0.109205
[11:16:41.237] TRAIN: iteration 21220 : loss : 0.082441, loss_ce: 0.004664, loss_dice: 0.160217
[11:16:41.477] TRAIN: iteration 21221 : loss : 0.092085, loss_ce: 0.005156, loss_dice: 0.179014
[11:16:41.685] TRAIN: iteration 21222 : loss : 0.062999, loss_ce: 0.004415, loss_dice: 0.121583
[11:16:41.894] TRAIN: iteration 21223 : loss : 0.059372, loss_ce: 0.006076, loss_dice: 0.112669
[11:16:42.106] TRAIN: iteration 21224 : loss : 0.087608, loss_ce: 0.003299, loss_dice: 0.171917
[11:16:42.313] TRAIN: iteration 21225 : loss : 0.249651, loss_ce: 0.001790, loss_dice: 0.497513
[11:16:42.521] TRAIN: iteration 21226 : loss : 0.085122, loss_ce: 0.005955, loss_dice: 0.164288
[11:16:42.730] TRAIN: iteration 21227 : loss : 0.087919, loss_ce: 0.004196, loss_dice: 0.171642
[11:16:42.944] TRAIN: iteration 21228 : loss : 0.110194, loss_ce: 0.003839, loss_dice: 0.216549
[11:16:43.154] TRAIN: iteration 21229 : loss : 0.058704, loss_ce: 0.002837, loss_dice: 0.114571
[11:16:43.363] TRAIN: iteration 21230 : loss : 0.082921, loss_ce: 0.002776, loss_dice: 0.163066
[11:16:43.571] TRAIN: iteration 21231 : loss : 0.069151, loss_ce: 0.002867, loss_dice: 0.135435
[11:16:43.779] TRAIN: iteration 21232 : loss : 0.103907, loss_ce: 0.002162, loss_dice: 0.205652
[11:16:43.986] TRAIN: iteration 21233 : loss : 0.088988, loss_ce: 0.003523, loss_dice: 0.174452
[11:16:44.194] TRAIN: iteration 21234 : loss : 0.126669, loss_ce: 0.003520, loss_dice: 0.249819
[11:16:44.401] TRAIN: iteration 21235 : loss : 0.251358, loss_ce: 0.002550, loss_dice: 0.500165
[11:16:44.608] TRAIN: iteration 21236 : loss : 0.160757, loss_ce: 0.001878, loss_dice: 0.319635
[11:16:44.818] TRAIN: iteration 21237 : loss : 0.049044, loss_ce: 0.002128, loss_dice: 0.095961
[11:16:45.027] TRAIN: iteration 21238 : loss : 0.250161, loss_ce: 0.000923, loss_dice: 0.499399
[11:16:45.235] TRAIN: iteration 21239 : loss : 0.074638, loss_ce: 0.007829, loss_dice: 0.141447
[11:16:45.444] TRAIN: iteration 21240 : loss : 0.069161, loss_ce: 0.002336, loss_dice: 0.135986
[11:16:45.684] TRAIN: iteration 21241 : loss : 0.099613, loss_ce: 0.003483, loss_dice: 0.195742
[11:16:45.892] TRAIN: iteration 21242 : loss : 0.105915, loss_ce: 0.004289, loss_dice: 0.207540
[11:16:46.107] TRAIN: iteration 21243 : loss : 0.104182, loss_ce: 0.003068, loss_dice: 0.205297
[11:16:46.339] TRAIN: iteration 21244 : loss : 0.116383, loss_ce: 0.004204, loss_dice: 0.228561
[11:16:46.546] TRAIN: iteration 21245 : loss : 0.032773, loss_ce: 0.004604, loss_dice: 0.060943
[11:16:46.754] TRAIN: iteration 21246 : loss : 0.083077, loss_ce: 0.002191, loss_dice: 0.163963
[11:16:46.963] TRAIN: iteration 21247 : loss : 0.123038, loss_ce: 0.001286, loss_dice: 0.244789
[11:16:47.176] TRAIN: iteration 21248 : loss : 0.045822, loss_ce: 0.004001, loss_dice: 0.087643
[11:16:47.384] TRAIN: iteration 21249 : loss : 0.142682, loss_ce: 0.000661, loss_dice: 0.284703
[11:16:47.595] TRAIN: iteration 21250 : loss : 0.119939, loss_ce: 0.002165, loss_dice: 0.237713
[11:16:47.803] TRAIN: iteration 21251 : loss : 0.042141, loss_ce: 0.001582, loss_dice: 0.082701
[11:16:48.013] TRAIN: iteration 21252 : loss : 0.242813, loss_ce: 0.002650, loss_dice: 0.482975
[11:16:48.222] TRAIN: iteration 21253 : loss : 0.120938, loss_ce: 0.003853, loss_dice: 0.238023
[11:16:48.430] TRAIN: iteration 21254 : loss : 0.015768, loss_ce: 0.000651, loss_dice: 0.030884
[11:16:48.644] TRAIN: iteration 21255 : loss : 0.111995, loss_ce: 0.023217, loss_dice: 0.200773
[11:16:48.850] TRAIN: iteration 21256 : loss : 0.171591, loss_ce: 0.003027, loss_dice: 0.340154
[11:16:49.058] TRAIN: iteration 21257 : loss : 0.077724, loss_ce: 0.006915, loss_dice: 0.148534
[11:16:49.272] TRAIN: iteration 21258 : loss : 0.208412, loss_ce: 0.002537, loss_dice: 0.414288
[11:16:49.483] TRAIN: iteration 21259 : loss : 0.094235, loss_ce: 0.002103, loss_dice: 0.186367
[11:16:49.693] TRAIN: iteration 21260 : loss : 0.035177, loss_ce: 0.004462, loss_dice: 0.065892
[11:16:49.946] TRAIN: iteration 21261 : loss : 0.112501, loss_ce: 0.001615, loss_dice: 0.223386
[11:16:50.159] TRAIN: iteration 21262 : loss : 0.035624, loss_ce: 0.001873, loss_dice: 0.069375
[11:16:50.369] TRAIN: iteration 21263 : loss : 0.113994, loss_ce: 0.005654, loss_dice: 0.222335
[11:16:50.584] TRAIN: iteration 21264 : loss : 0.183074, loss_ce: 0.004484, loss_dice: 0.361664
[11:16:50.799] TRAIN: iteration 21265 : loss : 0.128162, loss_ce: 0.003779, loss_dice: 0.252545
[11:16:51.011] TRAIN: iteration 21266 : loss : 0.050474, loss_ce: 0.002098, loss_dice: 0.098850
[11:16:51.225] TRAIN: iteration 21267 : loss : 0.055120, loss_ce: 0.004473, loss_dice: 0.105767
[11:16:51.437] TRAIN: iteration 21268 : loss : 0.197714, loss_ce: 0.003001, loss_dice: 0.392427
[11:16:51.661] TRAIN: iteration 21269 : loss : 0.101450, loss_ce: 0.003825, loss_dice: 0.199074
[11:16:51.872] TRAIN: iteration 21270 : loss : 0.171866, loss_ce: 0.004431, loss_dice: 0.339300
[11:16:52.082] TRAIN: iteration 21271 : loss : 0.207028, loss_ce: 0.002029, loss_dice: 0.412027
[11:16:52.293] TRAIN: iteration 21272 : loss : 0.250497, loss_ce: 0.000960, loss_dice: 0.500034
[11:16:52.501] TRAIN: iteration 21273 : loss : 0.250745, loss_ce: 0.001423, loss_dice: 0.500066
[11:16:52.708] TRAIN: iteration 21274 : loss : 0.250973, loss_ce: 0.001852, loss_dice: 0.500093
[11:16:52.937] TRAIN: iteration 21275 : loss : 0.237906, loss_ce: 0.001977, loss_dice: 0.473836
[11:16:53.145] TRAIN: iteration 21276 : loss : 0.250917, loss_ce: 0.001731, loss_dice: 0.500102
[11:16:53.358] TRAIN: iteration 21277 : loss : 0.251169, loss_ce: 0.002198, loss_dice: 0.500141
[11:16:53.567] TRAIN: iteration 21278 : loss : 0.039587, loss_ce: 0.003700, loss_dice: 0.075474
[11:16:53.776] TRAIN: iteration 21279 : loss : 0.227861, loss_ce: 0.001146, loss_dice: 0.454576
[11:16:53.986] TRAIN: iteration 21280 : loss : 0.103452, loss_ce: 0.005556, loss_dice: 0.201348
[11:16:54.224] TRAIN: iteration 21281 : loss : 0.229412, loss_ce: 0.053601, loss_dice: 0.405222
[11:16:54.437] TRAIN: iteration 21282 : loss : 0.230647, loss_ce: 0.003961, loss_dice: 0.457332
[11:16:54.648] TRAIN: iteration 21283 : loss : 0.042136, loss_ce: 0.001220, loss_dice: 0.083052
[11:16:54.861] TRAIN: iteration 21284 : loss : 0.050397, loss_ce: 0.001713, loss_dice: 0.099080
[11:16:55.069] TRAIN: iteration 21285 : loss : 0.196256, loss_ce: 0.002272, loss_dice: 0.390240
[11:16:55.280] TRAIN: iteration 21286 : loss : 0.089244, loss_ce: 0.002456, loss_dice: 0.176031
[11:16:55.486] TRAIN: iteration 21287 : loss : 0.081517, loss_ce: 0.006717, loss_dice: 0.156316
[11:16:55.694] TRAIN: iteration 21288 : loss : 0.038942, loss_ce: 0.001926, loss_dice: 0.075959
[11:16:55.906] TRAIN: iteration 21289 : loss : 0.035651, loss_ce: 0.004300, loss_dice: 0.067001
[11:16:56.114] TRAIN: iteration 21290 : loss : 0.227898, loss_ce: 0.001948, loss_dice: 0.453848
[11:16:56.320] TRAIN: iteration 21291 : loss : 0.098402, loss_ce: 0.003719, loss_dice: 0.193084
[11:16:56.529] TRAIN: iteration 21292 : loss : 0.210594, loss_ce: 0.002287, loss_dice: 0.418902
[11:16:56.736] TRAIN: iteration 21293 : loss : 0.058286, loss_ce: 0.006785, loss_dice: 0.109787
[11:16:56.943] TRAIN: iteration 21294 : loss : 0.250922, loss_ce: 0.001752, loss_dice: 0.500093
[11:16:57.151] TRAIN: iteration 21295 : loss : 0.035665, loss_ce: 0.005019, loss_dice: 0.066312
[11:16:57.358] TRAIN: iteration 21296 : loss : 0.096717, loss_ce: 0.008182, loss_dice: 0.185252
[11:16:57.565] TRAIN: iteration 21297 : loss : 0.085661, loss_ce: 0.002453, loss_dice: 0.168869
[11:16:57.773] TRAIN: iteration 21298 : loss : 0.178180, loss_ce: 0.005660, loss_dice: 0.350699
[11:16:57.980] TRAIN: iteration 21299 : loss : 0.248758, loss_ce: 0.003446, loss_dice: 0.494070
[11:16:58.187] TRAIN: iteration 21300 : loss : 0.251457, loss_ce: 0.004296, loss_dice: 0.498619
[11:16:58.834] TRAIN: iteration 21301 : loss : 0.044084, loss_ce: 0.003737, loss_dice: 0.084430
[11:16:59.042] TRAIN: iteration 21302 : loss : 0.251093, loss_ce: 0.002076, loss_dice: 0.500111
[11:16:59.249] TRAIN: iteration 21303 : loss : 0.099630, loss_ce: 0.001431, loss_dice: 0.197828
[11:16:59.456] TRAIN: iteration 21304 : loss : 0.041717, loss_ce: 0.001897, loss_dice: 0.081537
[11:16:59.664] TRAIN: iteration 21305 : loss : 0.076929, loss_ce: 0.005472, loss_dice: 0.148386
[11:16:59.872] TRAIN: iteration 21306 : loss : 0.110538, loss_ce: 0.003153, loss_dice: 0.217923
[11:17:00.080] TRAIN: iteration 21307 : loss : 0.024514, loss_ce: 0.002244, loss_dice: 0.046784
[11:17:00.287] TRAIN: iteration 21308 : loss : 0.152544, loss_ce: 0.008194, loss_dice: 0.296893
[11:17:00.517] TRAIN: iteration 21309 : loss : 0.250816, loss_ce: 0.001879, loss_dice: 0.499753
[11:17:00.726] TRAIN: iteration 21310 : loss : 0.212001, loss_ce: 0.005803, loss_dice: 0.418199
[11:17:00.934] TRAIN: iteration 21311 : loss : 0.078887, loss_ce: 0.003506, loss_dice: 0.154268
[11:17:01.141] TRAIN: iteration 21312 : loss : 0.121671, loss_ce: 0.004171, loss_dice: 0.239171
[11:17:01.349] TRAIN: iteration 21313 : loss : 0.163653, loss_ce: 0.001808, loss_dice: 0.325497
[11:17:01.556] TRAIN: iteration 21314 : loss : 0.246084, loss_ce: 0.002699, loss_dice: 0.489470
[11:17:01.771] TRAIN: iteration 21315 : loss : 0.065641, loss_ce: 0.006630, loss_dice: 0.124651
[11:17:01.978] TRAIN: iteration 21316 : loss : 0.086503, loss_ce: 0.001694, loss_dice: 0.171313
[11:17:02.186] TRAIN: iteration 21317 : loss : 0.083259, loss_ce: 0.008485, loss_dice: 0.158032
[11:17:02.394] TRAIN: iteration 21318 : loss : 0.102875, loss_ce: 0.002585, loss_dice: 0.203164
[11:17:02.601] TRAIN: iteration 21319 : loss : 0.197300, loss_ce: 0.005550, loss_dice: 0.389051
[11:17:02.818] TRAIN: iteration 21320 : loss : 0.047563, loss_ce: 0.010174, loss_dice: 0.084951
[11:17:03.073] TRAIN: iteration 21321 : loss : 0.078236, loss_ce: 0.005441, loss_dice: 0.151032
[11:17:03.280] TRAIN: iteration 21322 : loss : 0.096899, loss_ce: 0.007134, loss_dice: 0.186664
[11:17:03.487] TRAIN: iteration 21323 : loss : 0.128469, loss_ce: 0.010442, loss_dice: 0.246496
[11:17:03.695] TRAIN: iteration 21324 : loss : 0.251368, loss_ce: 0.002578, loss_dice: 0.500157
[11:17:03.909] TRAIN: iteration 21325 : loss : 0.127908, loss_ce: 0.004455, loss_dice: 0.251361
[11:17:04.123] TRAIN: iteration 21326 : loss : 0.074562, loss_ce: 0.004807, loss_dice: 0.144317
[11:17:04.334] TRAIN: iteration 21327 : loss : 0.072304, loss_ce: 0.006278, loss_dice: 0.138331
[11:17:04.549] TRAIN: iteration 21328 : loss : 0.199959, loss_ce: 0.004664, loss_dice: 0.395254
[11:17:04.758] TRAIN: iteration 21329 : loss : 0.074173, loss_ce: 0.004686, loss_dice: 0.143659
[11:17:05.046] TRAIN: iteration 21330 : loss : 0.251432, loss_ce: 0.003186, loss_dice: 0.499678
[11:17:05.262] TRAIN: iteration 21331 : loss : 0.032278, loss_ce: 0.001177, loss_dice: 0.063380
[11:17:05.468] TRAIN: iteration 21332 : loss : 0.067634, loss_ce: 0.011110, loss_dice: 0.124157
[11:17:05.680] TRAIN: iteration 21333 : loss : 0.108539, loss_ce: 0.004815, loss_dice: 0.212263
[11:17:05.894] TRAIN: iteration 21334 : loss : 0.091119, loss_ce: 0.007450, loss_dice: 0.174787
[11:17:06.103] TRAIN: iteration 21335 : loss : 0.242709, loss_ce: 0.004447, loss_dice: 0.480972
[11:17:06.312] TRAIN: iteration 21336 : loss : 0.251291, loss_ce: 0.002441, loss_dice: 0.500142
[11:17:06.520] TRAIN: iteration 21337 : loss : 0.068273, loss_ce: 0.005035, loss_dice: 0.131510
[11:17:06.728] TRAIN: iteration 21338 : loss : 0.076829, loss_ce: 0.001557, loss_dice: 0.152101
[11:17:06.943] TRAIN: iteration 21339 : loss : 0.071402, loss_ce: 0.006152, loss_dice: 0.136652
[11:17:07.150] TRAIN: iteration 21340 : loss : 0.130311, loss_ce: 0.002349, loss_dice: 0.258273
[11:17:07.387] TRAIN: iteration 21341 : loss : 0.079867, loss_ce: 0.004148, loss_dice: 0.155586
[11:17:07.596] TRAIN: iteration 21342 : loss : 0.039132, loss_ce: 0.001165, loss_dice: 0.077098
[11:17:07.803] TRAIN: iteration 21343 : loss : 0.034734, loss_ce: 0.002615, loss_dice: 0.066853
[11:17:08.012] TRAIN: iteration 21344 : loss : 0.103452, loss_ce: 0.003764, loss_dice: 0.203140
[11:17:08.226] TRAIN: iteration 21345 : loss : 0.108124, loss_ce: 0.002480, loss_dice: 0.213768
[11:17:08.434] TRAIN: iteration 21346 : loss : 0.101731, loss_ce: 0.012113, loss_dice: 0.191350
[11:17:08.642] TRAIN: iteration 21347 : loss : 0.063514, loss_ce: 0.002488, loss_dice: 0.124540
[11:17:08.851] TRAIN: iteration 21348 : loss : 0.031917, loss_ce: 0.001619, loss_dice: 0.062215
[11:17:09.065] TRAIN: iteration 21349 : loss : 0.069467, loss_ce: 0.004078, loss_dice: 0.134856
[11:17:09.272] TRAIN: iteration 21350 : loss : 0.064013, loss_ce: 0.005593, loss_dice: 0.122433
[11:17:09.486] TRAIN: iteration 21351 : loss : 0.035274, loss_ce: 0.003226, loss_dice: 0.067321
[11:17:10.390] TRAIN: iteration 21352 : loss : 0.250649, loss_ce: 0.001235, loss_dice: 0.500062
[11:17:10.597] TRAIN: iteration 21353 : loss : 0.112098, loss_ce: 0.002585, loss_dice: 0.221612
[11:17:10.804] TRAIN: iteration 21354 : loss : 0.086210, loss_ce: 0.001689, loss_dice: 0.170730
[11:17:11.019] TRAIN: iteration 21355 : loss : 0.072381, loss_ce: 0.003850, loss_dice: 0.140911
[11:17:11.230] TRAIN: iteration 21356 : loss : 0.060143, loss_ce: 0.004010, loss_dice: 0.116276
[11:17:11.443] TRAIN: iteration 21357 : loss : 0.240296, loss_ce: 0.002728, loss_dice: 0.477865
[11:17:11.652] TRAIN: iteration 21358 : loss : 0.248225, loss_ce: 0.003737, loss_dice: 0.492714
[11:17:11.862] TRAIN: iteration 21359 : loss : 0.055723, loss_ce: 0.001603, loss_dice: 0.109842
[11:17:12.768] TRAIN: iteration 21360 : loss : 0.200068, loss_ce: 0.003123, loss_dice: 0.397014
[11:17:13.021] TRAIN: iteration 21361 : loss : 0.127670, loss_ce: 0.007749, loss_dice: 0.247590
[11:17:13.230] TRAIN: iteration 21362 : loss : 0.030532, loss_ce: 0.001805, loss_dice: 0.059259
[11:17:13.437] TRAIN: iteration 21363 : loss : 0.231484, loss_ce: 0.015360, loss_dice: 0.447609
[11:17:13.645] TRAIN: iteration 21364 : loss : 0.187498, loss_ce: 0.008552, loss_dice: 0.366444
[11:17:13.854] TRAIN: iteration 21365 : loss : 0.251550, loss_ce: 0.003024, loss_dice: 0.500077
[11:17:14.065] TRAIN: iteration 21366 : loss : 0.150623, loss_ce: 0.005110, loss_dice: 0.296136
[11:17:14.276] TRAIN: iteration 21367 : loss : 0.037166, loss_ce: 0.001964, loss_dice: 0.072368
[11:17:14.849] TRAIN: iteration 21368 : loss : 0.089647, loss_ce: 0.002566, loss_dice: 0.176728
[11:17:15.062] TRAIN: iteration 21369 : loss : 0.177635, loss_ce: 0.004407, loss_dice: 0.350863
[11:17:15.270] TRAIN: iteration 21370 : loss : 0.070186, loss_ce: 0.003127, loss_dice: 0.137244
[11:17:15.484] TRAIN: iteration 21371 : loss : 0.072165, loss_ce: 0.002491, loss_dice: 0.141840
[11:17:15.694] TRAIN: iteration 21372 : loss : 0.206681, loss_ce: 0.002678, loss_dice: 0.410683
[11:17:15.904] TRAIN: iteration 21373 : loss : 0.236499, loss_ce: 0.009279, loss_dice: 0.463719
[11:17:16.114] TRAIN: iteration 21374 : loss : 0.251004, loss_ce: 0.001916, loss_dice: 0.500093
[11:17:16.326] TRAIN: iteration 21375 : loss : 0.185419, loss_ce: 0.001802, loss_dice: 0.369036
[11:17:16.535] TRAIN: iteration 21376 : loss : 0.092827, loss_ce: 0.011055, loss_dice: 0.174598
[11:17:16.746] TRAIN: iteration 21377 : loss : 0.011185, loss_ce: 0.000991, loss_dice: 0.021379
[11:17:16.956] TRAIN: iteration 21378 : loss : 0.026459, loss_ce: 0.001168, loss_dice: 0.051749
[11:17:17.165] TRAIN: iteration 21379 : loss : 0.128850, loss_ce: 0.001985, loss_dice: 0.255714
[11:17:17.373] TRAIN: iteration 21380 : loss : 0.251351, loss_ce: 0.002526, loss_dice: 0.500175
[11:17:17.618] TRAIN: iteration 21381 : loss : 0.179464, loss_ce: 0.001493, loss_dice: 0.357435
[11:17:17.829] TRAIN: iteration 21382 : loss : 0.192673, loss_ce: 0.004893, loss_dice: 0.380452
[11:17:18.038] TRAIN: iteration 21383 : loss : 0.108795, loss_ce: 0.016743, loss_dice: 0.200848
[11:17:18.251] TRAIN: iteration 21384 : loss : 0.250522, loss_ce: 0.001015, loss_dice: 0.500029
[11:17:18.458] TRAIN: iteration 21385 : loss : 0.090711, loss_ce: 0.010512, loss_dice: 0.170911
[11:17:18.666] TRAIN: iteration 21386 : loss : 0.089588, loss_ce: 0.002965, loss_dice: 0.176211
[11:17:18.873] TRAIN: iteration 21387 : loss : 0.160312, loss_ce: 0.002849, loss_dice: 0.317775
[11:17:19.081] TRAIN: iteration 21388 : loss : 0.181620, loss_ce: 0.007447, loss_dice: 0.355792
[11:17:19.289] TRAIN: iteration 21389 : loss : 0.229320, loss_ce: 0.001540, loss_dice: 0.457099
[11:17:19.500] TRAIN: iteration 21390 : loss : 0.250872, loss_ce: 0.002670, loss_dice: 0.499074
[11:17:19.707] TRAIN: iteration 21391 : loss : 0.102393, loss_ce: 0.003358, loss_dice: 0.201427
[11:17:19.921] TRAIN: iteration 21392 : loss : 0.184029, loss_ce: 0.026853, loss_dice: 0.341205
[11:17:20.128] TRAIN: iteration 21393 : loss : 0.039330, loss_ce: 0.003182, loss_dice: 0.075477
[11:17:20.338] TRAIN: iteration 21394 : loss : 0.184714, loss_ce: 0.002662, loss_dice: 0.366766
[11:17:20.548] TRAIN: iteration 21395 : loss : 0.251724, loss_ce: 0.003257, loss_dice: 0.500191
[11:17:21.205] TRAIN: iteration 21396 : loss : 0.251666, loss_ce: 0.003133, loss_dice: 0.500198
[11:17:21.412] TRAIN: iteration 21397 : loss : 0.180918, loss_ce: 0.004447, loss_dice: 0.357389
[11:17:21.619] TRAIN: iteration 21398 : loss : 0.248068, loss_ce: 0.004132, loss_dice: 0.492004
[11:17:21.828] TRAIN: iteration 21399 : loss : 0.084976, loss_ce: 0.015020, loss_dice: 0.154933
[11:17:22.038] TRAIN: iteration 21400 : loss : 0.156528, loss_ce: 0.002671, loss_dice: 0.310384
[11:17:22.280] TRAIN: iteration 21401 : loss : 0.121188, loss_ce: 0.003008, loss_dice: 0.239367
[11:17:22.487] TRAIN: iteration 21402 : loss : 0.251627, loss_ce: 0.003068, loss_dice: 0.500186
[11:17:22.695] TRAIN: iteration 21403 : loss : 0.038229, loss_ce: 0.004375, loss_dice: 0.072083
[11:17:22.902] TRAIN: iteration 21404 : loss : 0.144345, loss_ce: 0.004162, loss_dice: 0.284527
[11:17:23.110] TRAIN: iteration 21405 : loss : 0.251255, loss_ce: 0.002397, loss_dice: 0.500113
[11:17:23.330] TRAIN: iteration 21406 : loss : 0.051277, loss_ce: 0.002497, loss_dice: 0.100058
[11:17:23.539] TRAIN: iteration 21407 : loss : 0.126609, loss_ce: 0.005815, loss_dice: 0.247403
[11:17:23.749] TRAIN: iteration 21408 : loss : 0.039509, loss_ce: 0.002038, loss_dice: 0.076980
[11:17:23.958] TRAIN: iteration 21409 : loss : 0.073484, loss_ce: 0.007971, loss_dice: 0.138996
[11:17:24.166] TRAIN: iteration 21410 : loss : 0.100827, loss_ce: 0.003279, loss_dice: 0.198376
[11:17:24.374] TRAIN: iteration 21411 : loss : 0.251060, loss_ce: 0.003116, loss_dice: 0.499003
[11:17:24.584] TRAIN: iteration 21412 : loss : 0.138793, loss_ce: 0.004995, loss_dice: 0.272590
[11:17:24.796] TRAIN: iteration 21413 : loss : 0.251373, loss_ce: 0.002626, loss_dice: 0.500120
[11:17:25.005] TRAIN: iteration 21414 : loss : 0.070004, loss_ce: 0.005259, loss_dice: 0.134750
[11:17:25.217] TRAIN: iteration 21415 : loss : 0.251950, loss_ce: 0.003670, loss_dice: 0.500229
[11:17:25.425] TRAIN: iteration 21416 : loss : 0.251034, loss_ce: 0.001987, loss_dice: 0.500080
[11:17:25.632] TRAIN: iteration 21417 : loss : 0.097756, loss_ce: 0.003894, loss_dice: 0.191618
[11:17:25.841] TRAIN: iteration 21418 : loss : 0.078393, loss_ce: 0.006219, loss_dice: 0.150567
[11:17:26.049] TRAIN: iteration 21419 : loss : 0.060910, loss_ce: 0.001414, loss_dice: 0.120406
[11:17:26.257] TRAIN: iteration 21420 : loss : 0.244159, loss_ce: 0.002247, loss_dice: 0.486070
[11:17:26.507] TRAIN: iteration 21421 : loss : 0.071612, loss_ce: 0.001907, loss_dice: 0.141316
[11:17:26.715] TRAIN: iteration 21422 : loss : 0.088983, loss_ce: 0.001639, loss_dice: 0.176326
[11:17:26.923] TRAIN: iteration 21423 : loss : 0.114857, loss_ce: 0.003230, loss_dice: 0.226483
[11:17:27.132] TRAIN: iteration 21424 : loss : 0.087086, loss_ce: 0.014542, loss_dice: 0.159630
[11:17:27.367] TRAIN: iteration 21425 : loss : 0.097351, loss_ce: 0.003638, loss_dice: 0.191064
[11:17:27.579] TRAIN: iteration 21426 : loss : 0.076021, loss_ce: 0.002671, loss_dice: 0.149371
[11:17:27.786] TRAIN: iteration 21427 : loss : 0.085237, loss_ce: 0.002290, loss_dice: 0.168185
[11:17:27.993] TRAIN: iteration 21428 : loss : 0.127893, loss_ce: 0.004681, loss_dice: 0.251106
[11:17:28.201] TRAIN: iteration 21429 : loss : 0.236403, loss_ce: 0.002829, loss_dice: 0.469976
[11:17:28.410] TRAIN: iteration 21430 : loss : 0.106033, loss_ce: 0.004400, loss_dice: 0.207666
[11:17:28.620] TRAIN: iteration 21431 : loss : 0.111528, loss_ce: 0.002498, loss_dice: 0.220558
[11:17:28.831] TRAIN: iteration 21432 : loss : 0.089900, loss_ce: 0.004980, loss_dice: 0.174820
[11:17:29.044] TRAIN: iteration 21433 : loss : 0.043596, loss_ce: 0.002092, loss_dice: 0.085101
[11:17:29.258] TRAIN: iteration 21434 : loss : 0.054370, loss_ce: 0.003690, loss_dice: 0.105049
[11:17:29.465] TRAIN: iteration 21435 : loss : 0.065785, loss_ce: 0.002606, loss_dice: 0.128964
[11:17:29.672] TRAIN: iteration 21436 : loss : 0.251849, loss_ce: 0.003470, loss_dice: 0.500229
[11:17:29.878] TRAIN: iteration 21437 : loss : 0.060011, loss_ce: 0.004133, loss_dice: 0.115888
[11:17:30.094] TRAIN: iteration 21438 : loss : 0.250635, loss_ce: 0.001248, loss_dice: 0.500023
[11:17:30.314] TRAIN: iteration 21439 : loss : 0.250793, loss_ce: 0.001502, loss_dice: 0.500085
[11:17:30.523] TRAIN: iteration 21440 : loss : 0.242033, loss_ce: 0.004158, loss_dice: 0.479908
[11:17:30.761] TRAIN: iteration 21441 : loss : 0.096256, loss_ce: 0.005222, loss_dice: 0.187290
[11:17:30.969] TRAIN: iteration 21442 : loss : 0.116072, loss_ce: 0.008506, loss_dice: 0.223637
[11:17:31.176] TRAIN: iteration 21443 : loss : 0.192897, loss_ce: 0.003916, loss_dice: 0.381877
[11:17:32.038] TRAIN: iteration 21444 : loss : 0.084956, loss_ce: 0.008040, loss_dice: 0.161872
[11:17:32.245] TRAIN: iteration 21445 : loss : 0.070927, loss_ce: 0.004663, loss_dice: 0.137191
[11:17:32.452] TRAIN: iteration 21446 : loss : 0.023524, loss_ce: 0.002225, loss_dice: 0.044824
[11:17:32.659] TRAIN: iteration 21447 : loss : 0.247088, loss_ce: 0.003371, loss_dice: 0.490806
[11:17:32.866] TRAIN: iteration 21448 : loss : 0.072932, loss_ce: 0.006144, loss_dice: 0.139720
[11:17:33.076] TRAIN: iteration 21449 : loss : 0.095289, loss_ce: 0.002532, loss_dice: 0.188047
[11:17:33.284] TRAIN: iteration 21450 : loss : 0.203234, loss_ce: 0.016566, loss_dice: 0.389902
[11:17:33.491] TRAIN: iteration 21451 : loss : 0.251431, loss_ce: 0.002720, loss_dice: 0.500141
[11:17:34.727] TRAIN: iteration 21452 : loss : 0.055928, loss_ce: 0.005309, loss_dice: 0.106546
[11:17:34.934] TRAIN: iteration 21453 : loss : 0.087769, loss_ce: 0.007794, loss_dice: 0.167743
[11:17:35.141] TRAIN: iteration 21454 : loss : 0.095338, loss_ce: 0.007900, loss_dice: 0.182777
[11:17:35.350] TRAIN: iteration 21455 : loss : 0.251204, loss_ce: 0.002283, loss_dice: 0.500125
[11:17:35.557] TRAIN: iteration 21456 : loss : 0.065159, loss_ce: 0.008076, loss_dice: 0.122242
[11:17:35.765] TRAIN: iteration 21457 : loss : 0.072209, loss_ce: 0.007224, loss_dice: 0.137195
[11:17:35.974] TRAIN: iteration 21458 : loss : 0.195573, loss_ce: 0.003718, loss_dice: 0.387429
[11:17:36.186] TRAIN: iteration 21459 : loss : 0.049413, loss_ce: 0.002839, loss_dice: 0.095987
[11:17:36.392] TRAIN: iteration 21460 : loss : 0.029040, loss_ce: 0.003237, loss_dice: 0.054843
[11:17:36.608] TRAIN: iteration 21461 : loss : 0.053904, loss_ce: 0.003063, loss_dice: 0.104746
[11:17:36.814] TRAIN: iteration 21462 : loss : 0.239323, loss_ce: 0.002060, loss_dice: 0.476586
[11:17:37.022] TRAIN: iteration 21463 : loss : 0.071651, loss_ce: 0.006555, loss_dice: 0.136747
[11:17:37.230] TRAIN: iteration 21464 : loss : 0.058723, loss_ce: 0.002760, loss_dice: 0.114687
[11:17:37.440] TRAIN: iteration 21465 : loss : 0.042979, loss_ce: 0.001407, loss_dice: 0.084552
[11:17:37.648] TRAIN: iteration 21466 : loss : 0.103699, loss_ce: 0.003389, loss_dice: 0.204010
[11:17:38.318] TRAIN: iteration 21467 : loss : 0.088338, loss_ce: 0.002855, loss_dice: 0.173820
[11:17:38.526] TRAIN: iteration 21468 : loss : 0.250467, loss_ce: 0.003039, loss_dice: 0.497895
[11:17:38.734] TRAIN: iteration 21469 : loss : 0.060313, loss_ce: 0.002583, loss_dice: 0.118042
[11:17:38.942] TRAIN: iteration 21470 : loss : 0.201828, loss_ce: 0.004116, loss_dice: 0.399540
[11:17:39.155] TRAIN: iteration 21471 : loss : 0.251687, loss_ce: 0.003160, loss_dice: 0.500213
[11:17:39.377] TRAIN: iteration 21472 : loss : 0.039316, loss_ce: 0.001776, loss_dice: 0.076857
[11:17:39.586] TRAIN: iteration 21473 : loss : 0.054147, loss_ce: 0.005982, loss_dice: 0.102313
[11:17:39.795] TRAIN: iteration 21474 : loss : 0.051003, loss_ce: 0.003202, loss_dice: 0.098804
[11:17:40.004] TRAIN: iteration 21475 : loss : 0.232602, loss_ce: 0.003083, loss_dice: 0.462121
[11:17:40.214] TRAIN: iteration 21476 : loss : 0.232574, loss_ce: 0.001795, loss_dice: 0.463353
[11:17:40.421] TRAIN: iteration 21477 : loss : 0.250523, loss_ce: 0.003255, loss_dice: 0.497792
[11:17:40.633] TRAIN: iteration 21478 : loss : 0.073750, loss_ce: 0.007368, loss_dice: 0.140132
[11:17:40.847] TRAIN: iteration 21479 : loss : 0.251419, loss_ce: 0.002696, loss_dice: 0.500142
[11:17:41.060] TRAIN: iteration 21480 : loss : 0.210115, loss_ce: 0.009880, loss_dice: 0.410351
[11:17:41.307] TRAIN: iteration 21481 : loss : 0.053388, loss_ce: 0.003368, loss_dice: 0.103408
[11:17:41.515] TRAIN: iteration 21482 : loss : 0.074456, loss_ce: 0.003687, loss_dice: 0.145224
[11:17:41.722] TRAIN: iteration 21483 : loss : 0.125273, loss_ce: 0.002882, loss_dice: 0.247664
[11:17:42.552] TRAIN: iteration 21484 : loss : 0.136932, loss_ce: 0.003235, loss_dice: 0.270628
[11:17:42.759] TRAIN: iteration 21485 : loss : 0.120949, loss_ce: 0.002158, loss_dice: 0.239739
[11:17:42.966] TRAIN: iteration 21486 : loss : 0.043003, loss_ce: 0.002819, loss_dice: 0.083187
[11:17:43.173] TRAIN: iteration 21487 : loss : 0.194377, loss_ce: 0.002460, loss_dice: 0.386294
[11:17:43.382] TRAIN: iteration 21488 : loss : 0.251103, loss_ce: 0.002090, loss_dice: 0.500116
[11:17:43.589] TRAIN: iteration 21489 : loss : 0.057656, loss_ce: 0.003079, loss_dice: 0.112233
[11:17:43.797] TRAIN: iteration 21490 : loss : 0.250863, loss_ce: 0.002165, loss_dice: 0.499562
[11:17:44.005] TRAIN: iteration 21491 : loss : 0.113464, loss_ce: 0.005603, loss_dice: 0.221326
[11:17:44.215] TRAIN: iteration 21492 : loss : 0.130530, loss_ce: 0.001925, loss_dice: 0.259134
[11:17:44.476] TRAIN: iteration 21493 : loss : 0.066553, loss_ce: 0.002283, loss_dice: 0.130823
[11:17:44.683] TRAIN: iteration 21494 : loss : 0.124810, loss_ce: 0.006670, loss_dice: 0.242950
[11:17:44.890] TRAIN: iteration 21495 : loss : 0.097077, loss_ce: 0.002117, loss_dice: 0.192038
[11:17:45.099] TRAIN: iteration 21496 : loss : 0.155767, loss_ce: 0.005707, loss_dice: 0.305826
[11:17:45.319] TRAIN: iteration 21497 : loss : 0.082337, loss_ce: 0.008712, loss_dice: 0.155962
[11:17:45.528] TRAIN: iteration 21498 : loss : 0.087989, loss_ce: 0.003826, loss_dice: 0.172152
[11:17:45.735] TRAIN: iteration 21499 : loss : 0.173379, loss_ce: 0.025598, loss_dice: 0.321159
[11:17:45.943] TRAIN: iteration 21500 : loss : 0.251508, loss_ce: 0.002837, loss_dice: 0.500179
[11:17:46.178] TRAIN: iteration 21501 : loss : 0.023904, loss_ce: 0.001412, loss_dice: 0.046397
[11:17:47.354] TRAIN: iteration 21502 : loss : 0.180800, loss_ce: 0.007237, loss_dice: 0.354362
[11:17:47.567] TRAIN: iteration 21503 : loss : 0.089371, loss_ce: 0.007414, loss_dice: 0.171329
[11:17:47.775] TRAIN: iteration 21504 : loss : 0.157705, loss_ce: 0.010928, loss_dice: 0.304482
[11:17:47.987] TRAIN: iteration 21505 : loss : 0.069429, loss_ce: 0.002159, loss_dice: 0.136700
[11:17:48.195] TRAIN: iteration 21506 : loss : 0.252525, loss_ce: 0.005249, loss_dice: 0.499800
[11:17:48.403] TRAIN: iteration 21507 : loss : 0.211317, loss_ce: 0.006815, loss_dice: 0.415819
[11:17:48.609] TRAIN: iteration 21508 : loss : 0.055031, loss_ce: 0.004141, loss_dice: 0.105922
[11:17:48.816] TRAIN: iteration 21509 : loss : 0.087952, loss_ce: 0.006723, loss_dice: 0.169181
[11:17:49.025] TRAIN: iteration 21510 : loss : 0.075710, loss_ce: 0.003143, loss_dice: 0.148278
[11:17:49.232] TRAIN: iteration 21511 : loss : 0.115930, loss_ce: 0.005099, loss_dice: 0.226761
[11:17:49.441] TRAIN: iteration 21512 : loss : 0.251220, loss_ce: 0.002318, loss_dice: 0.500122
[11:17:49.649] TRAIN: iteration 21513 : loss : 0.156627, loss_ce: 0.007001, loss_dice: 0.306253
[11:17:49.885] TRAIN: iteration 21514 : loss : 0.066245, loss_ce: 0.002483, loss_dice: 0.130007
[11:17:50.097] TRAIN: iteration 21515 : loss : 0.049156, loss_ce: 0.002565, loss_dice: 0.095747
[11:17:50.307] TRAIN: iteration 21516 : loss : 0.087805, loss_ce: 0.005664, loss_dice: 0.169946
[11:17:50.517] TRAIN: iteration 21517 : loss : 0.021514, loss_ce: 0.002764, loss_dice: 0.040264
[11:17:50.728] TRAIN: iteration 21518 : loss : 0.246525, loss_ce: 0.003417, loss_dice: 0.489632
[11:17:50.937] TRAIN: iteration 21519 : loss : 0.165806, loss_ce: 0.002751, loss_dice: 0.328862
[11:17:51.145] TRAIN: iteration 21520 : loss : 0.099302, loss_ce: 0.004662, loss_dice: 0.193941
[11:17:51.383] TRAIN: iteration 21521 : loss : 0.081511, loss_ce: 0.001363, loss_dice: 0.161658
[11:17:51.593] TRAIN: iteration 21522 : loss : 0.108508, loss_ce: 0.003468, loss_dice: 0.213549
[11:17:51.804] TRAIN: iteration 21523 : loss : 0.251556, loss_ce: 0.002910, loss_dice: 0.500203
[11:17:52.020] TRAIN: iteration 21524 : loss : 0.046050, loss_ce: 0.001317, loss_dice: 0.090784
[11:17:52.231] TRAIN: iteration 21525 : loss : 0.017553, loss_ce: 0.001174, loss_dice: 0.033931
[11:17:52.438] TRAIN: iteration 21526 : loss : 0.051856, loss_ce: 0.007164, loss_dice: 0.096548
[11:17:52.796] TRAIN: iteration 21527 : loss : 0.146641, loss_ce: 0.002912, loss_dice: 0.290370
[11:17:53.003] TRAIN: iteration 21528 : loss : 0.079839, loss_ce: 0.002051, loss_dice: 0.157626
[11:17:53.213] TRAIN: iteration 21529 : loss : 0.081329, loss_ce: 0.005891, loss_dice: 0.156768
[11:17:53.424] TRAIN: iteration 21530 : loss : 0.125883, loss_ce: 0.002330, loss_dice: 0.249435
[11:17:53.632] TRAIN: iteration 21531 : loss : 0.075437, loss_ce: 0.002764, loss_dice: 0.148110
[11:17:53.839] TRAIN: iteration 21532 : loss : 0.243643, loss_ce: 0.001334, loss_dice: 0.485953
[11:17:54.248] TRAIN: iteration 21533 : loss : 0.072293, loss_ce: 0.003530, loss_dice: 0.141057
[11:17:54.457] TRAIN: iteration 21534 : loss : 0.083951, loss_ce: 0.001971, loss_dice: 0.165931
[11:17:54.666] TRAIN: iteration 21535 : loss : 0.250621, loss_ce: 0.001204, loss_dice: 0.500038
[11:17:54.875] TRAIN: iteration 21536 : loss : 0.215971, loss_ce: 0.003139, loss_dice: 0.428803
[11:17:55.086] TRAIN: iteration 21537 : loss : 0.070907, loss_ce: 0.003930, loss_dice: 0.137885
[11:17:55.295] TRAIN: iteration 21538 : loss : 0.065612, loss_ce: 0.003706, loss_dice: 0.127517
[11:17:55.510] TRAIN: iteration 21539 : loss : 0.235044, loss_ce: 0.003671, loss_dice: 0.466417
[11:17:55.716] TRAIN: iteration 21540 : loss : 0.034689, loss_ce: 0.002697, loss_dice: 0.066680
[11:17:56.149] TRAIN: iteration 21541 : loss : 0.046585, loss_ce: 0.001287, loss_dice: 0.091882
[11:17:56.359] TRAIN: iteration 21542 : loss : 0.246694, loss_ce: 0.002492, loss_dice: 0.490895
[11:17:56.737] TRAIN: iteration 21543 : loss : 0.078486, loss_ce: 0.005623, loss_dice: 0.151348
[11:17:56.946] TRAIN: iteration 21544 : loss : 0.146128, loss_ce: 0.005182, loss_dice: 0.287074
[11:17:57.155] TRAIN: iteration 21545 : loss : 0.106341, loss_ce: 0.038047, loss_dice: 0.174636
[11:17:57.362] TRAIN: iteration 21546 : loss : 0.070641, loss_ce: 0.002098, loss_dice: 0.139185
[11:17:57.569] TRAIN: iteration 21547 : loss : 0.105505, loss_ce: 0.002961, loss_dice: 0.208049
[11:17:57.776] TRAIN: iteration 21548 : loss : 0.030124, loss_ce: 0.002284, loss_dice: 0.057965
[11:17:58.588] TRAIN: iteration 21549 : loss : 0.119945, loss_ce: 0.001645, loss_dice: 0.238244
[11:17:58.796] TRAIN: iteration 21550 : loss : 0.251034, loss_ce: 0.001979, loss_dice: 0.500089
[11:17:59.004] TRAIN: iteration 21551 : loss : 0.066256, loss_ce: 0.002318, loss_dice: 0.130195
[11:17:59.211] TRAIN: iteration 21552 : loss : 0.251414, loss_ce: 0.002680, loss_dice: 0.500148
[11:17:59.419] TRAIN: iteration 21553 : loss : 0.249737, loss_ce: 0.002270, loss_dice: 0.497205
[11:17:59.627] TRAIN: iteration 21554 : loss : 0.073940, loss_ce: 0.001906, loss_dice: 0.145974
[11:17:59.837] TRAIN: iteration 21555 : loss : 0.044179, loss_ce: 0.003341, loss_dice: 0.085018
[11:18:00.053] TRAIN: iteration 21556 : loss : 0.151421, loss_ce: 0.002476, loss_dice: 0.300366
[11:18:00.262] TRAIN: iteration 21557 : loss : 0.032790, loss_ce: 0.002154, loss_dice: 0.063426
[11:18:00.471] TRAIN: iteration 21558 : loss : 0.229062, loss_ce: 0.001959, loss_dice: 0.456164
[11:18:00.681] TRAIN: iteration 21559 : loss : 0.208558, loss_ce: 0.005696, loss_dice: 0.411420
[11:18:00.893] TRAIN: iteration 21560 : loss : 0.058747, loss_ce: 0.003101, loss_dice: 0.114392
[11:18:01.131] TRAIN: iteration 21561 : loss : 0.073481, loss_ce: 0.009178, loss_dice: 0.137784
[11:18:01.339] TRAIN: iteration 21562 : loss : 0.251947, loss_ce: 0.004088, loss_dice: 0.499805
[11:18:01.546] TRAIN: iteration 21563 : loss : 0.250698, loss_ce: 0.001351, loss_dice: 0.500045
[11:18:01.753] TRAIN: iteration 21564 : loss : 0.086753, loss_ce: 0.004129, loss_dice: 0.169377
[11:18:01.960] TRAIN: iteration 21565 : loss : 0.247661, loss_ce: 0.009614, loss_dice: 0.485708
[11:18:02.167] TRAIN: iteration 21566 : loss : 0.234621, loss_ce: 0.023082, loss_dice: 0.446160
[11:18:02.375] TRAIN: iteration 21567 : loss : 0.065945, loss_ce: 0.009513, loss_dice: 0.122378
[11:18:02.587] TRAIN: iteration 21568 : loss : 0.019199, loss_ce: 0.001646, loss_dice: 0.036753
[11:18:02.818] TRAIN: iteration 21569 : loss : 0.043442, loss_ce: 0.008610, loss_dice: 0.078275
[11:18:03.032] TRAIN: iteration 21570 : loss : 0.042988, loss_ce: 0.002846, loss_dice: 0.083131
[11:18:03.240] TRAIN: iteration 21571 : loss : 0.251241, loss_ce: 0.002358, loss_dice: 0.500124
[11:18:03.639] TRAIN: iteration 21572 : loss : 0.173223, loss_ce: 0.020228, loss_dice: 0.326218
[11:18:03.850] TRAIN: iteration 21573 : loss : 0.083127, loss_ce: 0.004021, loss_dice: 0.162233
[11:18:04.058] TRAIN: iteration 21574 : loss : 0.088110, loss_ce: 0.002759, loss_dice: 0.173461
[11:18:04.271] TRAIN: iteration 21575 : loss : 0.240582, loss_ce: 0.005878, loss_dice: 0.475286
[11:18:04.480] TRAIN: iteration 21576 : loss : 0.105368, loss_ce: 0.001911, loss_dice: 0.208825
[11:18:04.689] TRAIN: iteration 21577 : loss : 0.046046, loss_ce: 0.003871, loss_dice: 0.088222
[11:18:04.897] TRAIN: iteration 21578 : loss : 0.041479, loss_ce: 0.004410, loss_dice: 0.078548
[11:18:05.108] TRAIN: iteration 21579 : loss : 0.156472, loss_ce: 0.005125, loss_dice: 0.307819
[11:18:05.318] TRAIN: iteration 21580 : loss : 0.251015, loss_ce: 0.001943, loss_dice: 0.500087
[11:18:05.562] TRAIN: iteration 21581 : loss : 0.250650, loss_ce: 0.001261, loss_dice: 0.500039
[11:18:05.769] TRAIN: iteration 21582 : loss : 0.014057, loss_ce: 0.001343, loss_dice: 0.026771
[11:18:05.983] TRAIN: iteration 21583 : loss : 0.064768, loss_ce: 0.004007, loss_dice: 0.125529
[11:18:06.192] TRAIN: iteration 21584 : loss : 0.022835, loss_ce: 0.002900, loss_dice: 0.042770
[11:18:06.399] TRAIN: iteration 21585 : loss : 0.052903, loss_ce: 0.005305, loss_dice: 0.100502
[11:18:06.607] TRAIN: iteration 21586 : loss : 0.008216, loss_ce: 0.000731, loss_dice: 0.015700
[11:18:06.814] TRAIN: iteration 21587 : loss : 0.086674, loss_ce: 0.005977, loss_dice: 0.167372
[11:18:07.022] TRAIN: iteration 21588 : loss : 0.230419, loss_ce: 0.001992, loss_dice: 0.458846
[11:18:07.234] TRAIN: iteration 21589 : loss : 0.250888, loss_ce: 0.001693, loss_dice: 0.500084
[11:18:07.443] TRAIN: iteration 21590 : loss : 0.051496, loss_ce: 0.001346, loss_dice: 0.101646
[11:18:07.650] TRAIN: iteration 21591 : loss : 0.250902, loss_ce: 0.001723, loss_dice: 0.500081
[11:18:07.859] TRAIN: iteration 21592 : loss : 0.066660, loss_ce: 0.003953, loss_dice: 0.129367
[11:18:08.721] TRAIN: iteration 21593 : loss : 0.080205, loss_ce: 0.003808, loss_dice: 0.156601
[11:18:08.929] TRAIN: iteration 21594 : loss : 0.058286, loss_ce: 0.004794, loss_dice: 0.111778
[11:18:09.138] TRAIN: iteration 21595 : loss : 0.047278, loss_ce: 0.001628, loss_dice: 0.092928
[11:18:09.345] TRAIN: iteration 21596 : loss : 0.184136, loss_ce: 0.002485, loss_dice: 0.365788
[11:18:09.554] TRAIN: iteration 21597 : loss : 0.221204, loss_ce: 0.001841, loss_dice: 0.440566
[11:18:09.763] TRAIN: iteration 21598 : loss : 0.218877, loss_ce: 0.003218, loss_dice: 0.434536
[11:18:09.974] TRAIN: iteration 21599 : loss : 0.103743, loss_ce: 0.012157, loss_dice: 0.195330
[11:18:10.184] TRAIN: iteration 21600 : loss : 0.237766, loss_ce: 0.002043, loss_dice: 0.473489
[11:18:10.185] NaN or Inf found in input tensor.
[11:18:10.404] TRAIN: iteration 21601 : loss : 0.040070, loss_ce: 0.004878, loss_dice: 0.075263
[11:18:10.613] TRAIN: iteration 21602 : loss : 0.187577, loss_ce: 0.002986, loss_dice: 0.372168
[11:18:10.822] TRAIN: iteration 21603 : loss : 0.051681, loss_ce: 0.003492, loss_dice: 0.099870
[11:18:11.030] TRAIN: iteration 21604 : loss : 0.232068, loss_ce: 0.008838, loss_dice: 0.455298
[11:18:11.239] TRAIN: iteration 21605 : loss : 0.133890, loss_ce: 0.003798, loss_dice: 0.263983
[11:18:11.446] TRAIN: iteration 21606 : loss : 0.031028, loss_ce: 0.005963, loss_dice: 0.056093
[11:18:11.653] TRAIN: iteration 21607 : loss : 0.250730, loss_ce: 0.001426, loss_dice: 0.500035
[11:18:11.861] TRAIN: iteration 21608 : loss : 0.106356, loss_ce: 0.003145, loss_dice: 0.209567
[11:18:12.071] TRAIN: iteration 21609 : loss : 0.047814, loss_ce: 0.001805, loss_dice: 0.093823
[11:18:12.278] TRAIN: iteration 21610 : loss : 0.087875, loss_ce: 0.003419, loss_dice: 0.172331
[11:18:12.486] TRAIN: iteration 21611 : loss : 0.216549, loss_ce: 0.009729, loss_dice: 0.423368
[11:18:12.696] TRAIN: iteration 21612 : loss : 0.093367, loss_ce: 0.003639, loss_dice: 0.183096
[11:18:12.903] TRAIN: iteration 21613 : loss : 0.046057, loss_ce: 0.003286, loss_dice: 0.088828
[11:18:13.112] TRAIN: iteration 21614 : loss : 0.035797, loss_ce: 0.005204, loss_dice: 0.066390
[11:18:13.321] TRAIN: iteration 21615 : loss : 0.050680, loss_ce: 0.005554, loss_dice: 0.095805
[11:18:13.534] TRAIN: iteration 21616 : loss : 0.164320, loss_ce: 0.001570, loss_dice: 0.327071
[11:18:13.743] TRAIN: iteration 21617 : loss : 0.057162, loss_ce: 0.004810, loss_dice: 0.109514
[11:18:13.952] TRAIN: iteration 21618 : loss : 0.251084, loss_ce: 0.002082, loss_dice: 0.500085
[11:18:14.160] TRAIN: iteration 21619 : loss : 0.110922, loss_ce: 0.002447, loss_dice: 0.219397
[11:18:14.367] TRAIN: iteration 21620 : loss : 0.102541, loss_ce: 0.001651, loss_dice: 0.203431
[11:18:14.608] TRAIN: iteration 21621 : loss : 0.122287, loss_ce: 0.002543, loss_dice: 0.242031
[11:18:14.818] TRAIN: iteration 21622 : loss : 0.119458, loss_ce: 0.002332, loss_dice: 0.236584
[11:18:15.025] TRAIN: iteration 21623 : loss : 0.038166, loss_ce: 0.006679, loss_dice: 0.069652
[11:18:15.232] TRAIN: iteration 21624 : loss : 0.069029, loss_ce: 0.003779, loss_dice: 0.134279
[11:18:16.470] TRAIN: iteration 21625 : loss : 0.040764, loss_ce: 0.001317, loss_dice: 0.080212
[11:18:16.679] TRAIN: iteration 21626 : loss : 0.089099, loss_ce: 0.002788, loss_dice: 0.175409
[11:18:16.895] TRAIN: iteration 21627 : loss : 0.089724, loss_ce: 0.002655, loss_dice: 0.176792
[11:18:17.103] TRAIN: iteration 21628 : loss : 0.171108, loss_ce: 0.004394, loss_dice: 0.337821
[11:18:17.311] TRAIN: iteration 21629 : loss : 0.228680, loss_ce: 0.001418, loss_dice: 0.455943
[11:18:17.527] TRAIN: iteration 21630 : loss : 0.104127, loss_ce: 0.001631, loss_dice: 0.206622
[11:18:17.742] TRAIN: iteration 21631 : loss : 0.250359, loss_ce: 0.000701, loss_dice: 0.500018
[11:18:17.954] TRAIN: iteration 21632 : loss : 0.242234, loss_ce: 0.003255, loss_dice: 0.481212
[11:18:20.390] TRAIN: iteration 21633 : loss : 0.209506, loss_ce: 0.001435, loss_dice: 0.417576
[11:18:20.598] TRAIN: iteration 21634 : loss : 0.077157, loss_ce: 0.003648, loss_dice: 0.150667
[11:18:20.805] TRAIN: iteration 21635 : loss : 0.089403, loss_ce: 0.005520, loss_dice: 0.173286
[11:18:21.012] TRAIN: iteration 21636 : loss : 0.076776, loss_ce: 0.008731, loss_dice: 0.144822
[11:18:21.229] TRAIN: iteration 21637 : loss : 0.128954, loss_ce: 0.006035, loss_dice: 0.251872
[11:18:21.437] TRAIN: iteration 21638 : loss : 0.197670, loss_ce: 0.001837, loss_dice: 0.393503
[11:18:21.646] TRAIN: iteration 21639 : loss : 0.051371, loss_ce: 0.002712, loss_dice: 0.100029
[11:18:21.853] TRAIN: iteration 21640 : loss : 0.113867, loss_ce: 0.004568, loss_dice: 0.223167
[11:18:22.093] TRAIN: iteration 21641 : loss : 0.053538, loss_ce: 0.011714, loss_dice: 0.095363
[11:18:22.301] TRAIN: iteration 21642 : loss : 0.200061, loss_ce: 0.004241, loss_dice: 0.395881
[11:18:22.510] TRAIN: iteration 21643 : loss : 0.054856, loss_ce: 0.003184, loss_dice: 0.106528
[11:18:22.719] TRAIN: iteration 21644 : loss : 0.220828, loss_ce: 0.002684, loss_dice: 0.438973
[11:18:22.928] TRAIN: iteration 21645 : loss : 0.198376, loss_ce: 0.017979, loss_dice: 0.378773
[11:18:23.136] TRAIN: iteration 21646 : loss : 0.024695, loss_ce: 0.000917, loss_dice: 0.048474
[11:18:23.345] TRAIN: iteration 21647 : loss : 0.165951, loss_ce: 0.009467, loss_dice: 0.322434
[11:18:23.554] TRAIN: iteration 21648 : loss : 0.117445, loss_ce: 0.002206, loss_dice: 0.232684
[11:18:23.762] TRAIN: iteration 21649 : loss : 0.124461, loss_ce: 0.004394, loss_dice: 0.244528
[11:18:23.970] TRAIN: iteration 21650 : loss : 0.029066, loss_ce: 0.002812, loss_dice: 0.055321
[11:18:24.177] TRAIN: iteration 21651 : loss : 0.221469, loss_ce: 0.004687, loss_dice: 0.438251
[11:18:24.392] TRAIN: iteration 21652 : loss : 0.037551, loss_ce: 0.001475, loss_dice: 0.073627
[11:18:24.600] TRAIN: iteration 21653 : loss : 0.075177, loss_ce: 0.004457, loss_dice: 0.145898
[11:18:24.823] TRAIN: iteration 21654 : loss : 0.110898, loss_ce: 0.004233, loss_dice: 0.217564
[11:18:25.033] TRAIN: iteration 21655 : loss : 0.052850, loss_ce: 0.002767, loss_dice: 0.102934
[11:18:25.240] TRAIN: iteration 21656 : loss : 0.236337, loss_ce: 0.001967, loss_dice: 0.470708
[11:18:25.449] TRAIN: iteration 21657 : loss : 0.062605, loss_ce: 0.003136, loss_dice: 0.122074
[11:18:25.698] TRAIN: iteration 21658 : loss : 0.190448, loss_ce: 0.001960, loss_dice: 0.378937
[11:18:25.905] TRAIN: iteration 21659 : loss : 0.146108, loss_ce: 0.005397, loss_dice: 0.286819
[11:18:26.122] TRAIN: iteration 21660 : loss : 0.029757, loss_ce: 0.001640, loss_dice: 0.057873
[11:18:26.355] TRAIN: iteration 21661 : loss : 0.194999, loss_ce: 0.002620, loss_dice: 0.387378
[11:18:26.562] TRAIN: iteration 21662 : loss : 0.251025, loss_ce: 0.002737, loss_dice: 0.499313
[11:18:26.777] TRAIN: iteration 21663 : loss : 0.039316, loss_ce: 0.002716, loss_dice: 0.075916
[11:18:28.076] TRAIN: iteration 21664 : loss : 0.086431, loss_ce: 0.002865, loss_dice: 0.169996
[11:18:28.283] TRAIN: iteration 21665 : loss : 0.251476, loss_ce: 0.002797, loss_dice: 0.500156
[11:18:28.491] TRAIN: iteration 21666 : loss : 0.051792, loss_ce: 0.002591, loss_dice: 0.100993
[11:18:28.699] TRAIN: iteration 21667 : loss : 0.121598, loss_ce: 0.009559, loss_dice: 0.233637
[11:18:28.905] TRAIN: iteration 21668 : loss : 0.021147, loss_ce: 0.002249, loss_dice: 0.040045
[11:18:29.118] TRAIN: iteration 21669 : loss : 0.151013, loss_ce: 0.003396, loss_dice: 0.298629
[11:18:29.334] TRAIN: iteration 21670 : loss : 0.105677, loss_ce: 0.003336, loss_dice: 0.208018
[11:18:29.544] TRAIN: iteration 21671 : loss : 0.175385, loss_ce: 0.005024, loss_dice: 0.345746
[11:18:29.751] TRAIN: iteration 21672 : loss : 0.143930, loss_ce: 0.003521, loss_dice: 0.284339
[11:18:29.960] TRAIN: iteration 21673 : loss : 0.030497, loss_ce: 0.003273, loss_dice: 0.057722
[11:18:30.170] TRAIN: iteration 21674 : loss : 0.087059, loss_ce: 0.002230, loss_dice: 0.171888
[11:18:30.378] TRAIN: iteration 21675 : loss : 0.065465, loss_ce: 0.002913, loss_dice: 0.128018
[11:18:30.585] TRAIN: iteration 21676 : loss : 0.049815, loss_ce: 0.002429, loss_dice: 0.097200
[11:18:30.794] TRAIN: iteration 21677 : loss : 0.023307, loss_ce: 0.001280, loss_dice: 0.045334
[11:18:31.400] TRAIN: iteration 21678 : loss : 0.098991, loss_ce: 0.007369, loss_dice: 0.190613
[11:18:31.607] TRAIN: iteration 21679 : loss : 0.061411, loss_ce: 0.002070, loss_dice: 0.120751
[11:18:31.820] TRAIN: iteration 21680 : loss : 0.120664, loss_ce: 0.003134, loss_dice: 0.238195
[11:18:32.057] TRAIN: iteration 21681 : loss : 0.077091, loss_ce: 0.008096, loss_dice: 0.146085
[11:18:32.266] TRAIN: iteration 21682 : loss : 0.033181, loss_ce: 0.008454, loss_dice: 0.057907
[11:18:32.475] TRAIN: iteration 21683 : loss : 0.113905, loss_ce: 0.004911, loss_dice: 0.222899
[11:18:32.685] TRAIN: iteration 21684 : loss : 0.251040, loss_ce: 0.001969, loss_dice: 0.500111
[11:18:32.896] TRAIN: iteration 21685 : loss : 0.251202, loss_ce: 0.002247, loss_dice: 0.500157
[11:18:33.104] TRAIN: iteration 21686 : loss : 0.102171, loss_ce: 0.004757, loss_dice: 0.199584
[11:18:33.316] TRAIN: iteration 21687 : loss : 0.243696, loss_ce: 0.002204, loss_dice: 0.485188
[11:18:34.063] TRAIN: iteration 21688 : loss : 0.250607, loss_ce: 0.001155, loss_dice: 0.500059
[11:18:34.271] TRAIN: iteration 21689 : loss : 0.251007, loss_ce: 0.001888, loss_dice: 0.500127
[11:18:34.479] TRAIN: iteration 21690 : loss : 0.219937, loss_ce: 0.009979, loss_dice: 0.429894
[11:18:34.687] TRAIN: iteration 21691 : loss : 0.063947, loss_ce: 0.003501, loss_dice: 0.124394
[11:18:34.896] TRAIN: iteration 21692 : loss : 0.110815, loss_ce: 0.008994, loss_dice: 0.212636
[11:18:35.105] TRAIN: iteration 21693 : loss : 0.252012, loss_ce: 0.003758, loss_dice: 0.500266
[11:18:35.316] TRAIN: iteration 21694 : loss : 0.078066, loss_ce: 0.004341, loss_dice: 0.151790
[11:18:35.526] TRAIN: iteration 21695 : loss : 0.114069, loss_ce: 0.008886, loss_dice: 0.219251
[11:18:35.849] TRAIN: iteration 21696 : loss : 0.149337, loss_ce: 0.002564, loss_dice: 0.296109
[11:18:37.693] TRAIN: iteration 21697 : loss : 0.054483, loss_ce: 0.006283, loss_dice: 0.102683
[11:18:37.901] TRAIN: iteration 21698 : loss : 0.251648, loss_ce: 0.003090, loss_dice: 0.500206
[11:18:38.108] TRAIN: iteration 21699 : loss : 0.115744, loss_ce: 0.003991, loss_dice: 0.227497
[11:18:38.315] TRAIN: iteration 21700 : loss : 0.046245, loss_ce: 0.005696, loss_dice: 0.086795
[11:18:38.551] TRAIN: iteration 21701 : loss : 0.072001, loss_ce: 0.003247, loss_dice: 0.140754
[11:18:38.760] TRAIN: iteration 21702 : loss : 0.061467, loss_ce: 0.005993, loss_dice: 0.116942
[11:18:38.971] TRAIN: iteration 21703 : loss : 0.174370, loss_ce: 0.008334, loss_dice: 0.340406
[11:18:39.182] TRAIN: iteration 21704 : loss : 0.092718, loss_ce: 0.004871, loss_dice: 0.180565
[11:18:39.390] TRAIN: iteration 21705 : loss : 0.172778, loss_ce: 0.003128, loss_dice: 0.342427
[11:18:39.598] TRAIN: iteration 21706 : loss : 0.251807, loss_ce: 0.005386, loss_dice: 0.498228
[11:18:39.806] TRAIN: iteration 21707 : loss : 0.144652, loss_ce: 0.002853, loss_dice: 0.286450
[11:18:40.015] TRAIN: iteration 21708 : loss : 0.184537, loss_ce: 0.005501, loss_dice: 0.363573
[11:18:40.230] TRAIN: iteration 21709 : loss : 0.250784, loss_ce: 0.002409, loss_dice: 0.499159
[11:18:40.438] TRAIN: iteration 21710 : loss : 0.038305, loss_ce: 0.003531, loss_dice: 0.073079
[11:18:40.648] TRAIN: iteration 21711 : loss : 0.072689, loss_ce: 0.005801, loss_dice: 0.139578
[11:18:40.862] TRAIN: iteration 21712 : loss : 0.251349, loss_ce: 0.002700, loss_dice: 0.499997
[11:18:41.070] TRAIN: iteration 21713 : loss : 0.170582, loss_ce: 0.005294, loss_dice: 0.335870
[11:18:41.279] TRAIN: iteration 21714 : loss : 0.251284, loss_ce: 0.002400, loss_dice: 0.500168
[11:18:41.489] TRAIN: iteration 21715 : loss : 0.086048, loss_ce: 0.003855, loss_dice: 0.168242
[11:18:41.699] TRAIN: iteration 21716 : loss : 0.076883, loss_ce: 0.001162, loss_dice: 0.152605
[11:18:41.914] TRAIN: iteration 21717 : loss : 0.052148, loss_ce: 0.002384, loss_dice: 0.101912
[11:18:42.122] TRAIN: iteration 21718 : loss : 0.110795, loss_ce: 0.007571, loss_dice: 0.214019
[11:18:42.332] TRAIN: iteration 21719 : loss : 0.060601, loss_ce: 0.002029, loss_dice: 0.119172
[11:18:42.542] TRAIN: iteration 21720 : loss : 0.052639, loss_ce: 0.002373, loss_dice: 0.102904
[11:18:42.789] TRAIN: iteration 21721 : loss : 0.057002, loss_ce: 0.001475, loss_dice: 0.112530
[11:18:42.997] TRAIN: iteration 21722 : loss : 0.122351, loss_ce: 0.003568, loss_dice: 0.241135
[11:18:43.213] TRAIN: iteration 21723 : loss : 0.036079, loss_ce: 0.001532, loss_dice: 0.070626
[11:18:43.422] TRAIN: iteration 21724 : loss : 0.098902, loss_ce: 0.003729, loss_dice: 0.194076
[11:18:43.630] TRAIN: iteration 21725 : loss : 0.053873, loss_ce: 0.002304, loss_dice: 0.105442
[11:18:43.838] TRAIN: iteration 21726 : loss : 0.250867, loss_ce: 0.001659, loss_dice: 0.500076
[11:18:44.047] TRAIN: iteration 21727 : loss : 0.073330, loss_ce: 0.004336, loss_dice: 0.142324
[11:18:44.257] TRAIN: iteration 21728 : loss : 0.107133, loss_ce: 0.003089, loss_dice: 0.211177
[11:18:44.465] TRAIN: iteration 21729 : loss : 0.180770, loss_ce: 0.003258, loss_dice: 0.358283
[11:18:44.898] TRAIN: iteration 21730 : loss : 0.054799, loss_ce: 0.006541, loss_dice: 0.103057
[11:18:45.456] TRAIN: iteration 21731 : loss : 0.098631, loss_ce: 0.001351, loss_dice: 0.195911
[11:18:45.666] TRAIN: iteration 21732 : loss : 0.251245, loss_ce: 0.002353, loss_dice: 0.500137
[11:18:45.876] TRAIN: iteration 21733 : loss : 0.066873, loss_ce: 0.003244, loss_dice: 0.130502
[11:18:46.085] TRAIN: iteration 21734 : loss : 0.078132, loss_ce: 0.003257, loss_dice: 0.153006
[11:18:46.293] TRAIN: iteration 21735 : loss : 0.251185, loss_ce: 0.002248, loss_dice: 0.500123
[11:18:46.501] TRAIN: iteration 21736 : loss : 0.071942, loss_ce: 0.002799, loss_dice: 0.141086
[11:18:46.710] TRAIN: iteration 21737 : loss : 0.171409, loss_ce: 0.005722, loss_dice: 0.337096
[11:18:50.113] TRAIN: iteration 21738 : loss : 0.250520, loss_ce: 0.002197, loss_dice: 0.498843
[11:18:50.322] TRAIN: iteration 21739 : loss : 0.121066, loss_ce: 0.003724, loss_dice: 0.238408
[11:18:50.531] TRAIN: iteration 21740 : loss : 0.171814, loss_ce: 0.001969, loss_dice: 0.341660
[11:18:50.770] TRAIN: iteration 21741 : loss : 0.106831, loss_ce: 0.003593, loss_dice: 0.210068
[11:18:50.978] TRAIN: iteration 21742 : loss : 0.055266, loss_ce: 0.001364, loss_dice: 0.109169
[11:18:51.185] TRAIN: iteration 21743 : loss : 0.089070, loss_ce: 0.012250, loss_dice: 0.165890
[11:18:51.394] TRAIN: iteration 21744 : loss : 0.130089, loss_ce: 0.006804, loss_dice: 0.253374
[11:18:51.605] TRAIN: iteration 21745 : loss : 0.092219, loss_ce: 0.002880, loss_dice: 0.181557
[11:18:51.812] TRAIN: iteration 21746 : loss : 0.081174, loss_ce: 0.003983, loss_dice: 0.158366
[11:18:52.021] TRAIN: iteration 21747 : loss : 0.119497, loss_ce: 0.006295, loss_dice: 0.232698
[11:18:52.242] TRAIN: iteration 21748 : loss : 0.089077, loss_ce: 0.003565, loss_dice: 0.174589
[11:18:52.455] TRAIN: iteration 21749 : loss : 0.191447, loss_ce: 0.002779, loss_dice: 0.380115
[11:18:52.663] TRAIN: iteration 21750 : loss : 0.073945, loss_ce: 0.003458, loss_dice: 0.144433
[11:18:52.871] TRAIN: iteration 21751 : loss : 0.215651, loss_ce: 0.003301, loss_dice: 0.428001
[11:18:53.081] TRAIN: iteration 21752 : loss : 0.047454, loss_ce: 0.003115, loss_dice: 0.091794
[11:18:53.289] TRAIN: iteration 21753 : loss : 0.070270, loss_ce: 0.002502, loss_dice: 0.138038
[11:18:53.505] TRAIN: iteration 21754 : loss : 0.077016, loss_ce: 0.001207, loss_dice: 0.152825
[11:18:53.731] TRAIN: iteration 21755 : loss : 0.250076, loss_ce: 0.002172, loss_dice: 0.497980
[11:18:53.951] TRAIN: iteration 21756 : loss : 0.234736, loss_ce: 0.011242, loss_dice: 0.458230
[11:18:54.159] TRAIN: iteration 21757 : loss : 0.079897, loss_ce: 0.001947, loss_dice: 0.157848
[11:18:54.375] TRAIN: iteration 21758 : loss : 0.089993, loss_ce: 0.015471, loss_dice: 0.164515
[11:18:54.602] TRAIN: iteration 21759 : loss : 0.048678, loss_ce: 0.007584, loss_dice: 0.089771
[11:18:55.007] TRAIN: iteration 21760 : loss : 0.096735, loss_ce: 0.001024, loss_dice: 0.192446
[11:18:55.241] TRAIN: iteration 21761 : loss : 0.045414, loss_ce: 0.001835, loss_dice: 0.088992
[11:18:55.449] TRAIN: iteration 21762 : loss : 0.154005, loss_ce: 0.002165, loss_dice: 0.305845
[11:18:55.660] TRAIN: iteration 21763 : loss : 0.173436, loss_ce: 0.002987, loss_dice: 0.343885
[11:18:55.876] TRAIN: iteration 21764 : loss : 0.064275, loss_ce: 0.005609, loss_dice: 0.122941
[11:18:56.087] TRAIN: iteration 21765 : loss : 0.077554, loss_ce: 0.001163, loss_dice: 0.153944
[11:18:56.295] TRAIN: iteration 21766 : loss : 0.250427, loss_ce: 0.000881, loss_dice: 0.499973
[11:18:56.509] TRAIN: iteration 21767 : loss : 0.044276, loss_ce: 0.002421, loss_dice: 0.086132
[11:18:58.018] TRAIN: iteration 21768 : loss : 0.027806, loss_ce: 0.004018, loss_dice: 0.051595
[11:18:58.227] TRAIN: iteration 21769 : loss : 0.051840, loss_ce: 0.001124, loss_dice: 0.102556
[11:18:58.450] TRAIN: iteration 21770 : loss : 0.050983, loss_ce: 0.001827, loss_dice: 0.100140
[11:18:58.662] TRAIN: iteration 21771 : loss : 0.104025, loss_ce: 0.009413, loss_dice: 0.198637
[11:18:58.871] TRAIN: iteration 21772 : loss : 0.167710, loss_ce: 0.006711, loss_dice: 0.328709
[11:18:59.082] TRAIN: iteration 21773 : loss : 0.056391, loss_ce: 0.004168, loss_dice: 0.108614
[11:18:59.293] TRAIN: iteration 21774 : loss : 0.066896, loss_ce: 0.002807, loss_dice: 0.130985
[11:18:59.504] TRAIN: iteration 21775 : loss : 0.201233, loss_ce: 0.015753, loss_dice: 0.386712
[11:19:01.799] TRAIN: iteration 21776 : loss : 0.147215, loss_ce: 0.001757, loss_dice: 0.292672
[11:19:02.007] TRAIN: iteration 21777 : loss : 0.181586, loss_ce: 0.007741, loss_dice: 0.355431
[11:19:02.217] TRAIN: iteration 21778 : loss : 0.058691, loss_ce: 0.002655, loss_dice: 0.114728
[11:19:02.428] TRAIN: iteration 21779 : loss : 0.080890, loss_ce: 0.003888, loss_dice: 0.157892
[11:19:02.636] TRAIN: iteration 21780 : loss : 0.100236, loss_ce: 0.002661, loss_dice: 0.197811
[11:19:02.882] TRAIN: iteration 21781 : loss : 0.066013, loss_ce: 0.010130, loss_dice: 0.121896
[11:19:03.091] TRAIN: iteration 21782 : loss : 0.132721, loss_ce: 0.003627, loss_dice: 0.261815
[11:19:03.301] TRAIN: iteration 21783 : loss : 0.166160, loss_ce: 0.025198, loss_dice: 0.307122
[11:19:05.554] TRAIN: iteration 21784 : loss : 0.249146, loss_ce: 0.007134, loss_dice: 0.491157
[11:19:05.765] TRAIN: iteration 21785 : loss : 0.142362, loss_ce: 0.004822, loss_dice: 0.279901
[11:19:05.974] TRAIN: iteration 21786 : loss : 0.251239, loss_ce: 0.002337, loss_dice: 0.500140
[11:19:06.184] TRAIN: iteration 21787 : loss : 0.031306, loss_ce: 0.002934, loss_dice: 0.059678
[11:19:06.393] TRAIN: iteration 21788 : loss : 0.033273, loss_ce: 0.004766, loss_dice: 0.061781
[11:19:06.602] TRAIN: iteration 21789 : loss : 0.201218, loss_ce: 0.007822, loss_dice: 0.394613
[11:19:06.871] TRAIN: iteration 21790 : loss : 0.251258, loss_ce: 0.002354, loss_dice: 0.500162
[11:19:07.079] TRAIN: iteration 21791 : loss : 0.250589, loss_ce: 0.001133, loss_dice: 0.500045
[11:19:07.605] TRAIN: iteration 21792 : loss : 0.062096, loss_ce: 0.003141, loss_dice: 0.121050
[11:19:07.814] TRAIN: iteration 21793 : loss : 0.138358, loss_ce: 0.004227, loss_dice: 0.272488
[11:19:08.022] TRAIN: iteration 21794 : loss : 0.055212, loss_ce: 0.002520, loss_dice: 0.107903
[11:19:08.234] TRAIN: iteration 21795 : loss : 0.090490, loss_ce: 0.002464, loss_dice: 0.178515
[11:19:08.447] TRAIN: iteration 21796 : loss : 0.139101, loss_ce: 0.005838, loss_dice: 0.272363
[11:19:08.660] TRAIN: iteration 21797 : loss : 0.242906, loss_ce: 0.002389, loss_dice: 0.483424
[11:19:08.869] TRAIN: iteration 21798 : loss : 0.040380, loss_ce: 0.006363, loss_dice: 0.074398
[11:19:09.082] TRAIN: iteration 21799 : loss : 0.178817, loss_ce: 0.004527, loss_dice: 0.353107
[11:19:09.811] TRAIN: iteration 21800 : loss : 0.075911, loss_ce: 0.004234, loss_dice: 0.147588
[11:19:09.812] NaN or Inf found in input tensor.
[11:19:10.028] TRAIN: iteration 21801 : loss : 0.186798, loss_ce: 0.006565, loss_dice: 0.367031
[11:19:10.236] TRAIN: iteration 21802 : loss : 0.170586, loss_ce: 0.001782, loss_dice: 0.339390
[11:19:10.444] TRAIN: iteration 21803 : loss : 0.250489, loss_ce: 0.000965, loss_dice: 0.500013
[11:19:10.652] TRAIN: iteration 21804 : loss : 0.251688, loss_ce: 0.003186, loss_dice: 0.500190
[11:19:10.859] TRAIN: iteration 21805 : loss : 0.074114, loss_ce: 0.002426, loss_dice: 0.145803
[11:19:11.067] TRAIN: iteration 21806 : loss : 0.124571, loss_ce: 0.005452, loss_dice: 0.243690
[11:19:11.278] TRAIN: iteration 21807 : loss : 0.122804, loss_ce: 0.002495, loss_dice: 0.243113
[11:19:11.487] TRAIN: iteration 21808 : loss : 0.216812, loss_ce: 0.010290, loss_dice: 0.423334
[11:19:11.700] TRAIN: iteration 21809 : loss : 0.064312, loss_ce: 0.006684, loss_dice: 0.121941
[11:19:11.908] TRAIN: iteration 21810 : loss : 0.034399, loss_ce: 0.003600, loss_dice: 0.065198
[11:19:12.116] TRAIN: iteration 21811 : loss : 0.032773, loss_ce: 0.006573, loss_dice: 0.058972
[11:19:12.325] TRAIN: iteration 21812 : loss : 0.128781, loss_ce: 0.005247, loss_dice: 0.252315
[11:19:12.534] TRAIN: iteration 21813 : loss : 0.100307, loss_ce: 0.004469, loss_dice: 0.196145
[11:19:12.746] TRAIN: iteration 21814 : loss : 0.022889, loss_ce: 0.003465, loss_dice: 0.042312
[11:19:12.958] TRAIN: iteration 21815 : loss : 0.120481, loss_ce: 0.010909, loss_dice: 0.230053
[11:19:14.956] TRAIN: iteration 21816 : loss : 0.050525, loss_ce: 0.007303, loss_dice: 0.093747
[11:19:15.164] TRAIN: iteration 21817 : loss : 0.029772, loss_ce: 0.002204, loss_dice: 0.057340
[11:19:15.372] TRAIN: iteration 21818 : loss : 0.251167, loss_ce: 0.002210, loss_dice: 0.500124
[11:19:15.579] TRAIN: iteration 21819 : loss : 0.088652, loss_ce: 0.003734, loss_dice: 0.173569
[11:19:15.788] TRAIN: iteration 21820 : loss : 0.074443, loss_ce: 0.001763, loss_dice: 0.147124
[11:19:16.026] TRAIN: iteration 21821 : loss : 0.072640, loss_ce: 0.002248, loss_dice: 0.143032
[11:19:16.238] TRAIN: iteration 21822 : loss : 0.068439, loss_ce: 0.002689, loss_dice: 0.134189
[11:19:16.448] TRAIN: iteration 21823 : loss : 0.059136, loss_ce: 0.002001, loss_dice: 0.116272
[11:19:16.658] TRAIN: iteration 21824 : loss : 0.114312, loss_ce: 0.004987, loss_dice: 0.223637
[11:19:16.873] TRAIN: iteration 21825 : loss : 0.102355, loss_ce: 0.003534, loss_dice: 0.201176
[11:19:17.084] TRAIN: iteration 21826 : loss : 0.068209, loss_ce: 0.006832, loss_dice: 0.129587
[11:19:17.298] TRAIN: iteration 21827 : loss : 0.036798, loss_ce: 0.004144, loss_dice: 0.069452
[11:19:17.507] TRAIN: iteration 21828 : loss : 0.129721, loss_ce: 0.004555, loss_dice: 0.254888
[11:19:17.719] TRAIN: iteration 21829 : loss : 0.207224, loss_ce: 0.003625, loss_dice: 0.410824
[11:19:17.939] TRAIN: iteration 21830 : loss : 0.035272, loss_ce: 0.001806, loss_dice: 0.068738
[11:19:18.147] TRAIN: iteration 21831 : loss : 0.125995, loss_ce: 0.005656, loss_dice: 0.246335
[11:19:18.360] TRAIN: iteration 21832 : loss : 0.134245, loss_ce: 0.005418, loss_dice: 0.263072
[11:19:18.570] TRAIN: iteration 21833 : loss : 0.124318, loss_ce: 0.002279, loss_dice: 0.246357
[11:19:18.787] TRAIN: iteration 21834 : loss : 0.048157, loss_ce: 0.002423, loss_dice: 0.093892
[11:19:18.995] TRAIN: iteration 21835 : loss : 0.213174, loss_ce: 0.003349, loss_dice: 0.423000
[11:19:19.203] TRAIN: iteration 21836 : loss : 0.202935, loss_ce: 0.005355, loss_dice: 0.400515
[11:19:19.411] TRAIN: iteration 21837 : loss : 0.134347, loss_ce: 0.003145, loss_dice: 0.265549
[11:19:19.619] TRAIN: iteration 21838 : loss : 0.056384, loss_ce: 0.003124, loss_dice: 0.109644
[11:19:19.826] TRAIN: iteration 21839 : loss : 0.090913, loss_ce: 0.002424, loss_dice: 0.179402
[11:19:20.034] TRAIN: iteration 21840 : loss : 0.038317, loss_ce: 0.001802, loss_dice: 0.074833
[11:19:20.264] TRAIN: iteration 21841 : loss : 0.100211, loss_ce: 0.006222, loss_dice: 0.194200
[11:19:20.472] TRAIN: iteration 21842 : loss : 0.065705, loss_ce: 0.002453, loss_dice: 0.128956
[11:19:20.679] TRAIN: iteration 21843 : loss : 0.163904, loss_ce: 0.011072, loss_dice: 0.316735
[11:19:20.935] TRAIN: iteration 21844 : loss : 0.229825, loss_ce: 0.003358, loss_dice: 0.456293
[11:19:21.142] TRAIN: iteration 21845 : loss : 0.137847, loss_ce: 0.004432, loss_dice: 0.271262
[11:19:21.355] TRAIN: iteration 21846 : loss : 0.056354, loss_ce: 0.003014, loss_dice: 0.109694
[11:19:21.568] TRAIN: iteration 21847 : loss : 0.083454, loss_ce: 0.001820, loss_dice: 0.165088
[11:19:21.780] TRAIN: iteration 21848 : loss : 0.097224, loss_ce: 0.002332, loss_dice: 0.192117
[11:19:21.991] TRAIN: iteration 21849 : loss : 0.061755, loss_ce: 0.009659, loss_dice: 0.113850
[11:19:22.200] TRAIN: iteration 21850 : loss : 0.041057, loss_ce: 0.002398, loss_dice: 0.079715
[11:19:22.409] TRAIN: iteration 21851 : loss : 0.081086, loss_ce: 0.002075, loss_dice: 0.160098
[11:19:22.616] TRAIN: iteration 21852 : loss : 0.034576, loss_ce: 0.000852, loss_dice: 0.068300
[11:19:24.273] TRAIN: iteration 21853 : loss : 0.056219, loss_ce: 0.001523, loss_dice: 0.110916
[11:19:24.481] TRAIN: iteration 21854 : loss : 0.079837, loss_ce: 0.007228, loss_dice: 0.152447
[11:19:24.690] TRAIN: iteration 21855 : loss : 0.057480, loss_ce: 0.002146, loss_dice: 0.112813
[11:19:24.899] TRAIN: iteration 21856 : loss : 0.116147, loss_ce: 0.005666, loss_dice: 0.226628
[11:19:25.109] TRAIN: iteration 21857 : loss : 0.154144, loss_ce: 0.022952, loss_dice: 0.285337
[11:19:25.318] TRAIN: iteration 21858 : loss : 0.139019, loss_ce: 0.005967, loss_dice: 0.272071
[11:19:25.530] TRAIN: iteration 21859 : loss : 0.112819, loss_ce: 0.001824, loss_dice: 0.223815
[11:19:25.737] TRAIN: iteration 21860 : loss : 0.188832, loss_ce: 0.005376, loss_dice: 0.372289
[11:19:26.905] TRAIN: iteration 21861 : loss : 0.212080, loss_ce: 0.007530, loss_dice: 0.416630
[11:19:27.113] TRAIN: iteration 21862 : loss : 0.084241, loss_ce: 0.002002, loss_dice: 0.166480
[11:19:27.321] TRAIN: iteration 21863 : loss : 0.196289, loss_ce: 0.003717, loss_dice: 0.388860
[11:19:27.531] TRAIN: iteration 21864 : loss : 0.141131, loss_ce: 0.003625, loss_dice: 0.278638
[11:19:27.739] TRAIN: iteration 21865 : loss : 0.149270, loss_ce: 0.004418, loss_dice: 0.294122
[11:19:27.946] TRAIN: iteration 21866 : loss : 0.080740, loss_ce: 0.002622, loss_dice: 0.158859
[11:19:28.156] TRAIN: iteration 21867 : loss : 0.092656, loss_ce: 0.003202, loss_dice: 0.182111
[11:19:28.363] TRAIN: iteration 21868 : loss : 0.214500, loss_ce: 0.005309, loss_dice: 0.423691
[11:19:28.571] TRAIN: iteration 21869 : loss : 0.151931, loss_ce: 0.002199, loss_dice: 0.301664
[11:19:28.780] TRAIN: iteration 21870 : loss : 0.116127, loss_ce: 0.002672, loss_dice: 0.229583
[11:19:28.993] TRAIN: iteration 21871 : loss : 0.115031, loss_ce: 0.002016, loss_dice: 0.228046
[11:19:29.202] TRAIN: iteration 21872 : loss : 0.084050, loss_ce: 0.002709, loss_dice: 0.165391
[11:19:29.409] TRAIN: iteration 21873 : loss : 0.034215, loss_ce: 0.001438, loss_dice: 0.066992
[11:19:29.618] TRAIN: iteration 21874 : loss : 0.110392, loss_ce: 0.005000, loss_dice: 0.215784
[11:19:29.828] TRAIN: iteration 21875 : loss : 0.251912, loss_ce: 0.003574, loss_dice: 0.500249
[11:19:30.365] TRAIN: iteration 21876 : loss : 0.182349, loss_ce: 0.002774, loss_dice: 0.361924
[11:19:30.573] TRAIN: iteration 21877 : loss : 0.119781, loss_ce: 0.002372, loss_dice: 0.237189
[11:19:30.781] TRAIN: iteration 21878 : loss : 0.121571, loss_ce: 0.002359, loss_dice: 0.240782
[11:19:30.989] TRAIN: iteration 21879 : loss : 0.160877, loss_ce: 0.022354, loss_dice: 0.299401
[11:19:31.197] TRAIN: iteration 21880 : loss : 0.063239, loss_ce: 0.004295, loss_dice: 0.122182
[11:19:31.435] TRAIN: iteration 21881 : loss : 0.125764, loss_ce: 0.003479, loss_dice: 0.248048
[11:19:31.643] TRAIN: iteration 21882 : loss : 0.115360, loss_ce: 0.002529, loss_dice: 0.228190
[11:19:31.850] TRAIN: iteration 21883 : loss : 0.036338, loss_ce: 0.000869, loss_dice: 0.071807
[11:19:33.857] TRAIN: iteration 21884 : loss : 0.123371, loss_ce: 0.005906, loss_dice: 0.240836
[11:19:34.067] TRAIN: iteration 21885 : loss : 0.250526, loss_ce: 0.001012, loss_dice: 0.500040
[11:19:34.274] TRAIN: iteration 21886 : loss : 0.035486, loss_ce: 0.000859, loss_dice: 0.070112
[11:19:34.484] TRAIN: iteration 21887 : loss : 0.105731, loss_ce: 0.012111, loss_dice: 0.199351
[11:19:35.468] TRAIN: iteration 21888 : loss : 0.057591, loss_ce: 0.001393, loss_dice: 0.113790
[11:19:35.678] TRAIN: iteration 21889 : loss : 0.125596, loss_ce: 0.002209, loss_dice: 0.248982
[11:19:35.885] TRAIN: iteration 21890 : loss : 0.103653, loss_ce: 0.002451, loss_dice: 0.204855
[11:19:36.095] TRAIN: iteration 21891 : loss : 0.048447, loss_ce: 0.004840, loss_dice: 0.092053
[11:19:36.304] TRAIN: iteration 21892 : loss : 0.064653, loss_ce: 0.006171, loss_dice: 0.123134
[11:19:36.512] TRAIN: iteration 21893 : loss : 0.249558, loss_ce: 0.001404, loss_dice: 0.497712
[11:19:36.719] TRAIN: iteration 21894 : loss : 0.251628, loss_ce: 0.004060, loss_dice: 0.499195
[11:19:36.932] TRAIN: iteration 21895 : loss : 0.175097, loss_ce: 0.003257, loss_dice: 0.346938
[11:19:38.744] TRAIN: iteration 21896 : loss : 0.250908, loss_ce: 0.001713, loss_dice: 0.500103
[11:19:38.950] TRAIN: iteration 21897 : loss : 0.179288, loss_ce: 0.004014, loss_dice: 0.354561
[11:19:39.157] TRAIN: iteration 21898 : loss : 0.151276, loss_ce: 0.006181, loss_dice: 0.296372
[11:19:39.366] TRAIN: iteration 21899 : loss : 0.173385, loss_ce: 0.007322, loss_dice: 0.339449
[11:19:39.573] TRAIN: iteration 21900 : loss : 0.251458, loss_ce: 0.002718, loss_dice: 0.500197
[11:19:39.812] TRAIN: iteration 21901 : loss : 0.205666, loss_ce: 0.002118, loss_dice: 0.409214
[11:19:40.019] TRAIN: iteration 21902 : loss : 0.020350, loss_ce: 0.000599, loss_dice: 0.040101
[11:19:40.227] TRAIN: iteration 21903 : loss : 0.195533, loss_ce: 0.006090, loss_dice: 0.384976
[11:19:40.642] TRAIN: iteration 21904 : loss : 0.153686, loss_ce: 0.004642, loss_dice: 0.302730
[11:19:40.850] TRAIN: iteration 21905 : loss : 0.061952, loss_ce: 0.002997, loss_dice: 0.120908
[11:19:41.058] TRAIN: iteration 21906 : loss : 0.057551, loss_ce: 0.001351, loss_dice: 0.113750
[11:19:41.267] TRAIN: iteration 21907 : loss : 0.081395, loss_ce: 0.005735, loss_dice: 0.157055
[11:19:41.478] TRAIN: iteration 21908 : loss : 0.067952, loss_ce: 0.001848, loss_dice: 0.134056
[11:19:41.692] TRAIN: iteration 21909 : loss : 0.053362, loss_ce: 0.003693, loss_dice: 0.103031
[11:19:41.902] TRAIN: iteration 21910 : loss : 0.187207, loss_ce: 0.001077, loss_dice: 0.373337
[11:19:42.109] TRAIN: iteration 21911 : loss : 0.090736, loss_ce: 0.005827, loss_dice: 0.175646
[11:19:42.426] TRAIN: iteration 21912 : loss : 0.142386, loss_ce: 0.005810, loss_dice: 0.278962
[11:19:42.640] TRAIN: iteration 21913 : loss : 0.090913, loss_ce: 0.005844, loss_dice: 0.175983
[11:19:42.856] TRAIN: iteration 21914 : loss : 0.036487, loss_ce: 0.001598, loss_dice: 0.071377
[11:19:43.064] TRAIN: iteration 21915 : loss : 0.208283, loss_ce: 0.006050, loss_dice: 0.410516
[11:19:43.272] TRAIN: iteration 21916 : loss : 0.250703, loss_ce: 0.001320, loss_dice: 0.500087
[11:19:43.481] TRAIN: iteration 21917 : loss : 0.046258, loss_ce: 0.001120, loss_dice: 0.091397
[11:19:43.690] TRAIN: iteration 21918 : loss : 0.053682, loss_ce: 0.001429, loss_dice: 0.105936
[11:19:43.904] TRAIN: iteration 21919 : loss : 0.244218, loss_ce: 0.006061, loss_dice: 0.482375
[11:19:44.114] TRAIN: iteration 21920 : loss : 0.030743, loss_ce: 0.001439, loss_dice: 0.060047
[11:19:44.350] TRAIN: iteration 21921 : loss : 0.119820, loss_ce: 0.002377, loss_dice: 0.237263
[11:19:44.557] TRAIN: iteration 21922 : loss : 0.119140, loss_ce: 0.002935, loss_dice: 0.235345
[11:19:44.900] TRAIN: iteration 21923 : loss : 0.159740, loss_ce: 0.003498, loss_dice: 0.315981
[11:19:45.115] TRAIN: iteration 21924 : loss : 0.046525, loss_ce: 0.001362, loss_dice: 0.091687
[11:19:45.327] TRAIN: iteration 21925 : loss : 0.100483, loss_ce: 0.001828, loss_dice: 0.199137
[11:19:45.555] TRAIN: iteration 21926 : loss : 0.050473, loss_ce: 0.004950, loss_dice: 0.095997
[11:19:45.764] TRAIN: iteration 21927 : loss : 0.132069, loss_ce: 0.002097, loss_dice: 0.262041
[11:19:48.724] TRAIN: iteration 21928 : loss : 0.032909, loss_ce: 0.000688, loss_dice: 0.065131
[11:19:48.931] TRAIN: iteration 21929 : loss : 0.016172, loss_ce: 0.001342, loss_dice: 0.031002
[11:19:49.143] TRAIN: iteration 21930 : loss : 0.077455, loss_ce: 0.002448, loss_dice: 0.152462
[11:19:49.351] TRAIN: iteration 21931 : loss : 0.034914, loss_ce: 0.002687, loss_dice: 0.067141
[11:19:49.562] TRAIN: iteration 21932 : loss : 0.081583, loss_ce: 0.007382, loss_dice: 0.155785
[11:19:49.771] TRAIN: iteration 21933 : loss : 0.072502, loss_ce: 0.003519, loss_dice: 0.141484
[11:19:49.980] TRAIN: iteration 21934 : loss : 0.065655, loss_ce: 0.002915, loss_dice: 0.128396
[11:19:50.193] TRAIN: iteration 21935 : loss : 0.040004, loss_ce: 0.000735, loss_dice: 0.079273
[11:19:50.401] TRAIN: iteration 21936 : loss : 0.069302, loss_ce: 0.002232, loss_dice: 0.136371
[11:19:50.611] TRAIN: iteration 21937 : loss : 0.246884, loss_ce: 0.002054, loss_dice: 0.491714
[11:19:50.833] TRAIN: iteration 21938 : loss : 0.146286, loss_ce: 0.001842, loss_dice: 0.290730
[11:19:51.044] TRAIN: iteration 21939 : loss : 0.052481, loss_ce: 0.002485, loss_dice: 0.102478
[11:19:51.251] TRAIN: iteration 21940 : loss : 0.060084, loss_ce: 0.007450, loss_dice: 0.112718
[11:19:51.492] TRAIN: iteration 21941 : loss : 0.094240, loss_ce: 0.005342, loss_dice: 0.183138
[11:19:51.701] TRAIN: iteration 21942 : loss : 0.066686, loss_ce: 0.003261, loss_dice: 0.130111
[11:19:51.909] TRAIN: iteration 21943 : loss : 0.039210, loss_ce: 0.004692, loss_dice: 0.073729
[11:19:52.518] TRAIN: iteration 21944 : loss : 0.128983, loss_ce: 0.002348, loss_dice: 0.255619
[11:19:52.725] TRAIN: iteration 21945 : loss : 0.100581, loss_ce: 0.001700, loss_dice: 0.199463
[11:19:52.934] TRAIN: iteration 21946 : loss : 0.081932, loss_ce: 0.003702, loss_dice: 0.160162
[11:19:53.142] TRAIN: iteration 21947 : loss : 0.252996, loss_ce: 0.007081, loss_dice: 0.498911
[11:19:53.353] TRAIN: iteration 21948 : loss : 0.088836, loss_ce: 0.001553, loss_dice: 0.176118
[11:19:53.561] TRAIN: iteration 21949 : loss : 0.157566, loss_ce: 0.001547, loss_dice: 0.313585
[11:19:53.769] TRAIN: iteration 21950 : loss : 0.074281, loss_ce: 0.014159, loss_dice: 0.134402
[11:19:54.203] TRAIN: iteration 21951 : loss : 0.215207, loss_ce: 0.001320, loss_dice: 0.429093
[11:19:57.212] TRAIN: iteration 21952 : loss : 0.250772, loss_ce: 0.001449, loss_dice: 0.500094
[11:19:57.420] TRAIN: iteration 21953 : loss : 0.079454, loss_ce: 0.004835, loss_dice: 0.154072
[11:19:57.628] TRAIN: iteration 21954 : loss : 0.143303, loss_ce: 0.005065, loss_dice: 0.281541
[11:19:57.836] TRAIN: iteration 21955 : loss : 0.036349, loss_ce: 0.002694, loss_dice: 0.070004
[11:19:58.043] TRAIN: iteration 21956 : loss : 0.213113, loss_ce: 0.002128, loss_dice: 0.424098
[11:19:58.251] TRAIN: iteration 21957 : loss : 0.037925, loss_ce: 0.001048, loss_dice: 0.074803
[11:19:58.459] TRAIN: iteration 21958 : loss : 0.089118, loss_ce: 0.008942, loss_dice: 0.169293
[11:19:58.667] TRAIN: iteration 21959 : loss : 0.251198, loss_ce: 0.002237, loss_dice: 0.500159
[11:19:59.444] TRAIN: iteration 21960 : loss : 0.250242, loss_ce: 0.000476, loss_dice: 0.500007
[11:19:59.682] TRAIN: iteration 21961 : loss : 0.110487, loss_ce: 0.002468, loss_dice: 0.218506
[11:19:59.891] TRAIN: iteration 21962 : loss : 0.250618, loss_ce: 0.001840, loss_dice: 0.499395
[11:20:00.102] TRAIN: iteration 21963 : loss : 0.154556, loss_ce: 0.002885, loss_dice: 0.306228
[11:20:00.310] TRAIN: iteration 21964 : loss : 0.028485, loss_ce: 0.001015, loss_dice: 0.055955
[11:20:00.517] TRAIN: iteration 21965 : loss : 0.167274, loss_ce: 0.004763, loss_dice: 0.329786
[11:20:00.724] TRAIN: iteration 21966 : loss : 0.126391, loss_ce: 0.003081, loss_dice: 0.249702
[11:20:00.954] TRAIN: iteration 21967 : loss : 0.123930, loss_ce: 0.008358, loss_dice: 0.239503
[11:20:01.948] TRAIN: iteration 21968 : loss : 0.065585, loss_ce: 0.005364, loss_dice: 0.125806
[11:20:02.156] TRAIN: iteration 21969 : loss : 0.060073, loss_ce: 0.004423, loss_dice: 0.115722
[11:20:02.365] TRAIN: iteration 21970 : loss : 0.090981, loss_ce: 0.002956, loss_dice: 0.179007
[11:20:02.574] TRAIN: iteration 21971 : loss : 0.234268, loss_ce: 0.000764, loss_dice: 0.467772
[11:20:02.787] TRAIN: iteration 21972 : loss : 0.117058, loss_ce: 0.004201, loss_dice: 0.229915
[11:20:02.995] TRAIN: iteration 21973 : loss : 0.121543, loss_ce: 0.003790, loss_dice: 0.239296
[11:20:03.202] TRAIN: iteration 21974 : loss : 0.039711, loss_ce: 0.006518, loss_dice: 0.072904
[11:20:03.414] TRAIN: iteration 21975 : loss : 0.046248, loss_ce: 0.002766, loss_dice: 0.089731
[11:20:03.622] TRAIN: iteration 21976 : loss : 0.067758, loss_ce: 0.005248, loss_dice: 0.130268
[11:20:03.832] TRAIN: iteration 21977 : loss : 0.070093, loss_ce: 0.002124, loss_dice: 0.138063
[11:20:04.043] TRAIN: iteration 21978 : loss : 0.015678, loss_ce: 0.001601, loss_dice: 0.029755
[11:20:04.253] TRAIN: iteration 21979 : loss : 0.027479, loss_ce: 0.002115, loss_dice: 0.052843
[11:20:04.468] TRAIN: iteration 21980 : loss : 0.125889, loss_ce: 0.004210, loss_dice: 0.247568
[11:20:04.709] TRAIN: iteration 21981 : loss : 0.165380, loss_ce: 0.002502, loss_dice: 0.328258
[11:20:04.917] TRAIN: iteration 21982 : loss : 0.161530, loss_ce: 0.007087, loss_dice: 0.315972
[11:20:05.361] TRAIN: iteration 21983 : loss : 0.250930, loss_ce: 0.001766, loss_dice: 0.500094
[11:20:06.095] TRAIN: iteration 21984 : loss : 0.038599, loss_ce: 0.001968, loss_dice: 0.075229
[11:20:06.506] TRAIN: iteration 21985 : loss : 0.086048, loss_ce: 0.002241, loss_dice: 0.169856
[11:20:06.717] TRAIN: iteration 21986 : loss : 0.101882, loss_ce: 0.002185, loss_dice: 0.201579
[11:20:06.925] TRAIN: iteration 21987 : loss : 0.097633, loss_ce: 0.001504, loss_dice: 0.193763
[11:20:07.136] TRAIN: iteration 21988 : loss : 0.162501, loss_ce: 0.002216, loss_dice: 0.322786
[11:20:07.346] TRAIN: iteration 21989 : loss : 0.238254, loss_ce: 0.001941, loss_dice: 0.474566
[11:20:07.555] TRAIN: iteration 21990 : loss : 0.090359, loss_ce: 0.004796, loss_dice: 0.175922
[11:20:09.508] TRAIN: iteration 21991 : loss : 0.188793, loss_ce: 0.002887, loss_dice: 0.374699
[11:20:09.721] TRAIN: iteration 21992 : loss : 0.039943, loss_ce: 0.002275, loss_dice: 0.077610
[11:20:11.447] TRAIN: iteration 21993 : loss : 0.059843, loss_ce: 0.003035, loss_dice: 0.116652
[11:20:11.655] TRAIN: iteration 21994 : loss : 0.088477, loss_ce: 0.001513, loss_dice: 0.175440
[11:20:11.862] TRAIN: iteration 21995 : loss : 0.079127, loss_ce: 0.005701, loss_dice: 0.152553
[11:20:12.070] TRAIN: iteration 21996 : loss : 0.081619, loss_ce: 0.006494, loss_dice: 0.156744
[11:20:12.279] TRAIN: iteration 21997 : loss : 0.145265, loss_ce: 0.007285, loss_dice: 0.283245
[11:20:12.493] TRAIN: iteration 21998 : loss : 0.064049, loss_ce: 0.001309, loss_dice: 0.126790
[11:20:12.704] TRAIN: iteration 21999 : loss : 0.069716, loss_ce: 0.004520, loss_dice: 0.134911
[11:20:12.912] TRAIN: iteration 22000 : loss : 0.094749, loss_ce: 0.002141, loss_dice: 0.187358
[11:20:14.306] TRAIN: iteration 22001 : loss : 0.171293, loss_ce: 0.038112, loss_dice: 0.304475
[11:20:14.514] TRAIN: iteration 22002 : loss : 0.073151, loss_ce: 0.004032, loss_dice: 0.142270
[11:20:14.722] TRAIN: iteration 22003 : loss : 0.101754, loss_ce: 0.003123, loss_dice: 0.200386
[11:20:14.933] TRAIN: iteration 22004 : loss : 0.028842, loss_ce: 0.002417, loss_dice: 0.055266
[11:20:15.141] TRAIN: iteration 22005 : loss : 0.232837, loss_ce: 0.001609, loss_dice: 0.464065
[11:20:15.354] TRAIN: iteration 22006 : loss : 0.106192, loss_ce: 0.006787, loss_dice: 0.205597
[11:20:15.561] TRAIN: iteration 22007 : loss : 0.081565, loss_ce: 0.004046, loss_dice: 0.159084
[11:20:15.768] TRAIN: iteration 22008 : loss : 0.066886, loss_ce: 0.004776, loss_dice: 0.128996
[11:20:15.976] TRAIN: iteration 22009 : loss : 0.059828, loss_ce: 0.002228, loss_dice: 0.117427
[11:20:16.184] TRAIN: iteration 22010 : loss : 0.091807, loss_ce: 0.010640, loss_dice: 0.172974
[11:20:16.391] TRAIN: iteration 22011 : loss : 0.072843, loss_ce: 0.001508, loss_dice: 0.144179
[11:20:16.599] TRAIN: iteration 22012 : loss : 0.136404, loss_ce: 0.002652, loss_dice: 0.270155
[11:20:16.810] TRAIN: iteration 22013 : loss : 0.136960, loss_ce: 0.011548, loss_dice: 0.262372
[11:20:17.018] TRAIN: iteration 22014 : loss : 0.061631, loss_ce: 0.002169, loss_dice: 0.121094
[11:20:17.226] TRAIN: iteration 22015 : loss : 0.144534, loss_ce: 0.004400, loss_dice: 0.284669
[11:20:17.435] TRAIN: iteration 22016 : loss : 0.211278, loss_ce: 0.001581, loss_dice: 0.420976
[11:20:17.649] TRAIN: iteration 22017 : loss : 0.153184, loss_ce: 0.007146, loss_dice: 0.299222
[11:20:19.215] TRAIN: iteration 22018 : loss : 0.250551, loss_ce: 0.001080, loss_dice: 0.500021
[11:20:19.423] TRAIN: iteration 22019 : loss : 0.059029, loss_ce: 0.002502, loss_dice: 0.115557
[11:20:19.631] TRAIN: iteration 22020 : loss : 0.152053, loss_ce: 0.005334, loss_dice: 0.298773
[11:20:19.869] TRAIN: iteration 22021 : loss : 0.078221, loss_ce: 0.004268, loss_dice: 0.152173
[11:20:20.077] TRAIN: iteration 22022 : loss : 0.101223, loss_ce: 0.007019, loss_dice: 0.195427
[11:20:20.287] TRAIN: iteration 22023 : loss : 0.052082, loss_ce: 0.005092, loss_dice: 0.099073
[11:20:20.494] TRAIN: iteration 22024 : loss : 0.041336, loss_ce: 0.003527, loss_dice: 0.079146
[11:20:20.703] TRAIN: iteration 22025 : loss : 0.253808, loss_ce: 0.007722, loss_dice: 0.499894
[11:20:21.767] TRAIN: iteration 22026 : loss : 0.060188, loss_ce: 0.003038, loss_dice: 0.117338
[11:20:23.154] TRAIN: iteration 22027 : loss : 0.120609, loss_ce: 0.002819, loss_dice: 0.238398
[11:20:23.362] TRAIN: iteration 22028 : loss : 0.251761, loss_ce: 0.003289, loss_dice: 0.500234
[11:20:23.571] TRAIN: iteration 22029 : loss : 0.114176, loss_ce: 0.002577, loss_dice: 0.225776
[11:20:23.778] TRAIN: iteration 22030 : loss : 0.250760, loss_ce: 0.001953, loss_dice: 0.499566
[11:20:23.986] TRAIN: iteration 22031 : loss : 0.109900, loss_ce: 0.009956, loss_dice: 0.209844
[11:20:24.194] TRAIN: iteration 22032 : loss : 0.102179, loss_ce: 0.010792, loss_dice: 0.193567
[11:20:24.403] TRAIN: iteration 22033 : loss : 0.062914, loss_ce: 0.001288, loss_dice: 0.124539
[11:20:24.611] TRAIN: iteration 22034 : loss : 0.250158, loss_ce: 0.003908, loss_dice: 0.496407
[11:20:25.186] TRAIN: iteration 22035 : loss : 0.088452, loss_ce: 0.003596, loss_dice: 0.173308
[11:20:25.396] TRAIN: iteration 22036 : loss : 0.193321, loss_ce: 0.001275, loss_dice: 0.385367
[11:20:25.603] TRAIN: iteration 22037 : loss : 0.118529, loss_ce: 0.001974, loss_dice: 0.235085
[11:20:25.813] TRAIN: iteration 22038 : loss : 0.151345, loss_ce: 0.003918, loss_dice: 0.298772
[11:20:26.022] TRAIN: iteration 22039 : loss : 0.136061, loss_ce: 0.004700, loss_dice: 0.267421
[11:20:26.230] TRAIN: iteration 22040 : loss : 0.057922, loss_ce: 0.002519, loss_dice: 0.113325
[11:20:26.472] TRAIN: iteration 22041 : loss : 0.251924, loss_ce: 0.004176, loss_dice: 0.499671
[11:20:26.681] TRAIN: iteration 22042 : loss : 0.063879, loss_ce: 0.003869, loss_dice: 0.123889
[11:20:28.361] TRAIN: iteration 22043 : loss : 0.232930, loss_ce: 0.001589, loss_dice: 0.464271
[11:20:28.570] TRAIN: iteration 22044 : loss : 0.138016, loss_ce: 0.001855, loss_dice: 0.274177
[11:20:28.777] TRAIN: iteration 22045 : loss : 0.250647, loss_ce: 0.001243, loss_dice: 0.500050
[11:20:28.985] TRAIN: iteration 22046 : loss : 0.109683, loss_ce: 0.007428, loss_dice: 0.211939
[11:20:29.193] TRAIN: iteration 22047 : loss : 0.252139, loss_ce: 0.007117, loss_dice: 0.497161
[11:20:29.411] TRAIN: iteration 22048 : loss : 0.150826, loss_ce: 0.004607, loss_dice: 0.297045
[11:20:29.618] TRAIN: iteration 22049 : loss : 0.250845, loss_ce: 0.001592, loss_dice: 0.500098
[11:20:29.825] TRAIN: iteration 22050 : loss : 0.153737, loss_ce: 0.001214, loss_dice: 0.306261
[11:20:30.564] TRAIN: iteration 22051 : loss : 0.178418, loss_ce: 0.005305, loss_dice: 0.351532
[11:20:30.961] TRAIN: iteration 22052 : loss : 0.250411, loss_ce: 0.000801, loss_dice: 0.500021
[11:20:31.168] TRAIN: iteration 22053 : loss : 0.204045, loss_ce: 0.001690, loss_dice: 0.406400
[11:20:31.381] TRAIN: iteration 22054 : loss : 0.043336, loss_ce: 0.002229, loss_dice: 0.084444
[11:20:31.596] TRAIN: iteration 22055 : loss : 0.061984, loss_ce: 0.003472, loss_dice: 0.120495
[11:20:31.805] TRAIN: iteration 22056 : loss : 0.208513, loss_ce: 0.030915, loss_dice: 0.386110
[11:20:32.015] TRAIN: iteration 22057 : loss : 0.068998, loss_ce: 0.002501, loss_dice: 0.135494
[11:20:32.222] TRAIN: iteration 22058 : loss : 0.044156, loss_ce: 0.001187, loss_dice: 0.087125
[11:20:33.665] TRAIN: iteration 22059 : loss : 0.083014, loss_ce: 0.006716, loss_dice: 0.159313
[11:20:33.873] TRAIN: iteration 22060 : loss : 0.144718, loss_ce: 0.002418, loss_dice: 0.287018
[11:20:34.597] TRAIN: iteration 22061 : loss : 0.084935, loss_ce: 0.002059, loss_dice: 0.167811
[11:20:35.525] TRAIN: iteration 22062 : loss : 0.066065, loss_ce: 0.004271, loss_dice: 0.127858
[11:20:35.738] TRAIN: iteration 22063 : loss : 0.122040, loss_ce: 0.003963, loss_dice: 0.240117
[11:20:35.946] TRAIN: iteration 22064 : loss : 0.250759, loss_ce: 0.001454, loss_dice: 0.500063
[11:20:36.162] TRAIN: iteration 22065 : loss : 0.068566, loss_ce: 0.002975, loss_dice: 0.134157
[11:20:36.371] TRAIN: iteration 22066 : loss : 0.251397, loss_ce: 0.002650, loss_dice: 0.500144
[11:20:36.582] TRAIN: iteration 22067 : loss : 0.086334, loss_ce: 0.007070, loss_dice: 0.165598
[11:20:36.790] TRAIN: iteration 22068 : loss : 0.117504, loss_ce: 0.005913, loss_dice: 0.229095
[11:20:36.999] TRAIN: iteration 22069 : loss : 0.041052, loss_ce: 0.007504, loss_dice: 0.074600
[11:20:38.908] TRAIN: iteration 22070 : loss : 0.066066, loss_ce: 0.005482, loss_dice: 0.126650
[11:20:39.122] TRAIN: iteration 22071 : loss : 0.130788, loss_ce: 0.010135, loss_dice: 0.251441
[11:20:39.337] TRAIN: iteration 22072 : loss : 0.075653, loss_ce: 0.007427, loss_dice: 0.143878
[11:20:39.545] TRAIN: iteration 22073 : loss : 0.033072, loss_ce: 0.001996, loss_dice: 0.064148
[11:20:39.755] TRAIN: iteration 22074 : loss : 0.154245, loss_ce: 0.004526, loss_dice: 0.303965
[11:20:41.592] TRAIN: iteration 22075 : loss : 0.233203, loss_ce: 0.003065, loss_dice: 0.463340
[11:20:41.800] TRAIN: iteration 22076 : loss : 0.151481, loss_ce: 0.009493, loss_dice: 0.293469
[11:20:42.009] TRAIN: iteration 22077 : loss : 0.088220, loss_ce: 0.002701, loss_dice: 0.173739
[11:20:42.222] TRAIN: iteration 22078 : loss : 0.122473, loss_ce: 0.007064, loss_dice: 0.237882
[11:20:42.435] TRAIN: iteration 22079 : loss : 0.183962, loss_ce: 0.006481, loss_dice: 0.361442
[11:20:42.643] TRAIN: iteration 22080 : loss : 0.134793, loss_ce: 0.002914, loss_dice: 0.266672
[11:20:42.885] TRAIN: iteration 22081 : loss : 0.166812, loss_ce: 0.003928, loss_dice: 0.329695
[11:20:43.094] TRAIN: iteration 22082 : loss : 0.026161, loss_ce: 0.001304, loss_dice: 0.051018
[11:20:43.551] TRAIN: iteration 22083 : loss : 0.063026, loss_ce: 0.003669, loss_dice: 0.122382
[11:20:43.760] TRAIN: iteration 22084 : loss : 0.250990, loss_ce: 0.003045, loss_dice: 0.498935
[11:20:43.969] TRAIN: iteration 22085 : loss : 0.211830, loss_ce: 0.004277, loss_dice: 0.419382
[11:20:44.872] TRAIN: iteration 22086 : loss : 0.144482, loss_ce: 0.005337, loss_dice: 0.283627
[11:20:45.079] TRAIN: iteration 22087 : loss : 0.245221, loss_ce: 0.002262, loss_dice: 0.488181
[11:20:45.291] TRAIN: iteration 22088 : loss : 0.157361, loss_ce: 0.007743, loss_dice: 0.306978
[11:20:45.500] TRAIN: iteration 22089 : loss : 0.069963, loss_ce: 0.002997, loss_dice: 0.136929
[11:20:45.710] TRAIN: iteration 22090 : loss : 0.074973, loss_ce: 0.005363, loss_dice: 0.144583
[11:20:47.975] TRAIN: iteration 22091 : loss : 0.143924, loss_ce: 0.015813, loss_dice: 0.272035
[11:20:48.190] TRAIN: iteration 22092 : loss : 0.250359, loss_ce: 0.000705, loss_dice: 0.500013
[11:20:48.403] TRAIN: iteration 22093 : loss : 0.141979, loss_ce: 0.004677, loss_dice: 0.279281
[11:20:48.611] TRAIN: iteration 22094 : loss : 0.068691, loss_ce: 0.001658, loss_dice: 0.135724
[11:20:48.823] TRAIN: iteration 22095 : loss : 0.059776, loss_ce: 0.005609, loss_dice: 0.113943
[11:20:49.031] TRAIN: iteration 22096 : loss : 0.221990, loss_ce: 0.001560, loss_dice: 0.442420
[11:20:49.242] TRAIN: iteration 22097 : loss : 0.120541, loss_ce: 0.003573, loss_dice: 0.237509
[11:20:49.449] TRAIN: iteration 22098 : loss : 0.246710, loss_ce: 0.002719, loss_dice: 0.490701
[11:20:50.267] TRAIN: iteration 22099 : loss : 0.086793, loss_ce: 0.003965, loss_dice: 0.169621
[11:20:50.475] TRAIN: iteration 22100 : loss : 0.115560, loss_ce: 0.009001, loss_dice: 0.222120
[11:20:50.715] TRAIN: iteration 22101 : loss : 0.248840, loss_ce: 0.003923, loss_dice: 0.493757
[11:20:50.925] TRAIN: iteration 22102 : loss : 0.251296, loss_ce: 0.002429, loss_dice: 0.500163
[11:20:51.134] TRAIN: iteration 22103 : loss : 0.086093, loss_ce: 0.002916, loss_dice: 0.169270
[11:20:51.341] TRAIN: iteration 22104 : loss : 0.126167, loss_ce: 0.002743, loss_dice: 0.249592
[11:20:51.549] TRAIN: iteration 22105 : loss : 0.034374, loss_ce: 0.006188, loss_dice: 0.062561
[11:20:51.824] TRAIN: iteration 22106 : loss : 0.127024, loss_ce: 0.006936, loss_dice: 0.247113
[11:20:52.314] TRAIN: iteration 22107 : loss : 0.066723, loss_ce: 0.010346, loss_dice: 0.123100
[11:20:54.691] TRAIN: iteration 22108 : loss : 0.052313, loss_ce: 0.003287, loss_dice: 0.101339
[11:20:55.084] TRAIN: iteration 22109 : loss : 0.154645, loss_ce: 0.002763, loss_dice: 0.306528
[11:20:56.093] TRAIN: iteration 22110 : loss : 0.138956, loss_ce: 0.002715, loss_dice: 0.275197
[11:20:56.306] TRAIN: iteration 22111 : loss : 0.251273, loss_ce: 0.002412, loss_dice: 0.500133
[11:20:56.514] TRAIN: iteration 22112 : loss : 0.249721, loss_ce: 0.001626, loss_dice: 0.497816
[11:20:56.722] TRAIN: iteration 22113 : loss : 0.189981, loss_ce: 0.002327, loss_dice: 0.377635
[11:20:56.930] TRAIN: iteration 22114 : loss : 0.071216, loss_ce: 0.003684, loss_dice: 0.138748
[11:20:57.138] TRAIN: iteration 22115 : loss : 0.160450, loss_ce: 0.002446, loss_dice: 0.318453
[11:20:57.346] TRAIN: iteration 22116 : loss : 0.130766, loss_ce: 0.002180, loss_dice: 0.259351
[11:20:58.456] TRAIN: iteration 22117 : loss : 0.236588, loss_ce: 0.002944, loss_dice: 0.470232
[11:21:00.919] TRAIN: iteration 22118 : loss : 0.232598, loss_ce: 0.003399, loss_dice: 0.461796
[11:21:01.128] TRAIN: iteration 22119 : loss : 0.209080, loss_ce: 0.004669, loss_dice: 0.413490
[11:21:01.336] TRAIN: iteration 22120 : loss : 0.051695, loss_ce: 0.005814, loss_dice: 0.097577
[11:21:01.577] TRAIN: iteration 22121 : loss : 0.056174, loss_ce: 0.002570, loss_dice: 0.109779
[11:21:01.784] TRAIN: iteration 22122 : loss : 0.029819, loss_ce: 0.001020, loss_dice: 0.058617
[11:21:01.994] TRAIN: iteration 22123 : loss : 0.043506, loss_ce: 0.005412, loss_dice: 0.081599
[11:21:02.209] TRAIN: iteration 22124 : loss : 0.091939, loss_ce: 0.002151, loss_dice: 0.181726
[11:21:02.417] TRAIN: iteration 22125 : loss : 0.025249, loss_ce: 0.001003, loss_dice: 0.049496
[11:21:04.694] TRAIN: iteration 22126 : loss : 0.062346, loss_ce: 0.004382, loss_dice: 0.120311
[11:21:04.905] TRAIN: iteration 22127 : loss : 0.109256, loss_ce: 0.018519, loss_dice: 0.199993
[11:21:05.113] TRAIN: iteration 22128 : loss : 0.029418, loss_ce: 0.002481, loss_dice: 0.056356
[11:21:05.321] TRAIN: iteration 22129 : loss : 0.073116, loss_ce: 0.003952, loss_dice: 0.142280
[11:21:05.532] TRAIN: iteration 22130 : loss : 0.062149, loss_ce: 0.007256, loss_dice: 0.117042
[11:21:05.744] TRAIN: iteration 22131 : loss : 0.180525, loss_ce: 0.002121, loss_dice: 0.358929
[11:21:05.959] TRAIN: iteration 22132 : loss : 0.089326, loss_ce: 0.006143, loss_dice: 0.172508
[11:21:06.167] TRAIN: iteration 22133 : loss : 0.139114, loss_ce: 0.002415, loss_dice: 0.275813
[11:21:08.941] TRAIN: iteration 22134 : loss : 0.050750, loss_ce: 0.003576, loss_dice: 0.097923
[11:21:09.149] TRAIN: iteration 22135 : loss : 0.104199, loss_ce: 0.001963, loss_dice: 0.206434
[11:21:09.358] TRAIN: iteration 22136 : loss : 0.250798, loss_ce: 0.001529, loss_dice: 0.500066
[11:21:09.567] TRAIN: iteration 22137 : loss : 0.105365, loss_ce: 0.003548, loss_dice: 0.207182
[11:21:09.774] TRAIN: iteration 22138 : loss : 0.189448, loss_ce: 0.004921, loss_dice: 0.373976
[11:21:09.982] TRAIN: iteration 22139 : loss : 0.250856, loss_ce: 0.001995, loss_dice: 0.499717
[11:21:10.196] TRAIN: iteration 22140 : loss : 0.056287, loss_ce: 0.001243, loss_dice: 0.111331
[11:21:10.197] NaN or Inf found in input tensor.
[11:21:10.412] TRAIN: iteration 22141 : loss : 0.099358, loss_ce: 0.004438, loss_dice: 0.194278
[11:21:16.198] TRAIN: iteration 22142 : loss : 0.138419, loss_ce: 0.003603, loss_dice: 0.273235
[11:21:16.406] TRAIN: iteration 22143 : loss : 0.088589, loss_ce: 0.005949, loss_dice: 0.171230
[11:21:16.615] TRAIN: iteration 22144 : loss : 0.032899, loss_ce: 0.001657, loss_dice: 0.064142
[11:21:16.824] TRAIN: iteration 22145 : loss : 0.062119, loss_ce: 0.002900, loss_dice: 0.121337
[11:21:17.034] TRAIN: iteration 22146 : loss : 0.032742, loss_ce: 0.002056, loss_dice: 0.063427
[11:21:17.242] TRAIN: iteration 22147 : loss : 0.092148, loss_ce: 0.002636, loss_dice: 0.181659
[11:21:17.451] TRAIN: iteration 22148 : loss : 0.051913, loss_ce: 0.006062, loss_dice: 0.097764
[11:21:17.660] TRAIN: iteration 22149 : loss : 0.250978, loss_ce: 0.001831, loss_dice: 0.500125
[11:21:23.423] TRAIN: iteration 22150 : loss : 0.107827, loss_ce: 0.004227, loss_dice: 0.211427
[11:21:23.634] TRAIN: iteration 22151 : loss : 0.238718, loss_ce: 0.000951, loss_dice: 0.476485
[11:21:23.843] TRAIN: iteration 22152 : loss : 0.070041, loss_ce: 0.002946, loss_dice: 0.137135
[11:21:24.053] TRAIN: iteration 22153 : loss : 0.125486, loss_ce: 0.001329, loss_dice: 0.249644
[11:21:24.261] TRAIN: iteration 22154 : loss : 0.249142, loss_ce: 0.002337, loss_dice: 0.495948
[11:21:24.593] TRAIN: iteration 22155 : loss : 0.039766, loss_ce: 0.003010, loss_dice: 0.076522
[11:21:24.802] TRAIN: iteration 22156 : loss : 0.251188, loss_ce: 0.002225, loss_dice: 0.500151
[11:21:25.010] TRAIN: iteration 22157 : loss : 0.045203, loss_ce: 0.003622, loss_dice: 0.086785
[11:21:26.695] TRAIN: iteration 22158 : loss : 0.115548, loss_ce: 0.002446, loss_dice: 0.228651
[11:21:26.902] TRAIN: iteration 22159 : loss : 0.128724, loss_ce: 0.011835, loss_dice: 0.245613
[11:21:27.112] TRAIN: iteration 22160 : loss : 0.074095, loss_ce: 0.003839, loss_dice: 0.144351
[11:21:27.347] TRAIN: iteration 22161 : loss : 0.194524, loss_ce: 0.022066, loss_dice: 0.366981
[11:21:27.554] TRAIN: iteration 22162 : loss : 0.245006, loss_ce: 0.001096, loss_dice: 0.488916
[11:21:27.761] TRAIN: iteration 22163 : loss : 0.079166, loss_ce: 0.006248, loss_dice: 0.152084
[11:21:27.973] TRAIN: iteration 22164 : loss : 0.066079, loss_ce: 0.005824, loss_dice: 0.126334
[11:21:28.185] TRAIN: iteration 22165 : loss : 0.236364, loss_ce: 0.001694, loss_dice: 0.471035
[11:21:29.513] TRAIN: iteration 22166 : loss : 0.119955, loss_ce: 0.002451, loss_dice: 0.237459
[11:21:29.720] TRAIN: iteration 22167 : loss : 0.068962, loss_ce: 0.007590, loss_dice: 0.130334
[11:21:29.928] TRAIN: iteration 22168 : loss : 0.054100, loss_ce: 0.005989, loss_dice: 0.102212
[11:21:30.137] TRAIN: iteration 22169 : loss : 0.043901, loss_ce: 0.002936, loss_dice: 0.084867
[11:21:30.351] TRAIN: iteration 22170 : loss : 0.119863, loss_ce: 0.009184, loss_dice: 0.230542
[11:21:30.673] TRAIN: iteration 22171 : loss : 0.200639, loss_ce: 0.003662, loss_dice: 0.397616
[11:21:30.885] TRAIN: iteration 22172 : loss : 0.139882, loss_ce: 0.003163, loss_dice: 0.276601
[11:21:31.094] TRAIN: iteration 22173 : loss : 0.058480, loss_ce: 0.005371, loss_dice: 0.111588
[11:21:33.529] TRAIN: iteration 22174 : loss : 0.142366, loss_ce: 0.002406, loss_dice: 0.282326
[11:21:33.737] TRAIN: iteration 22175 : loss : 0.151416, loss_ce: 0.004149, loss_dice: 0.298682
[11:21:33.947] TRAIN: iteration 22176 : loss : 0.119424, loss_ce: 0.002472, loss_dice: 0.236375
[11:21:34.156] TRAIN: iteration 22177 : loss : 0.041402, loss_ce: 0.004197, loss_dice: 0.078606
[11:21:34.366] TRAIN: iteration 22178 : loss : 0.068179, loss_ce: 0.008074, loss_dice: 0.128283
[11:21:34.574] TRAIN: iteration 22179 : loss : 0.192209, loss_ce: 0.014152, loss_dice: 0.370265
[11:21:34.783] TRAIN: iteration 22180 : loss : 0.054527, loss_ce: 0.001691, loss_dice: 0.107364
[11:21:34.824] NaN or Inf found in input tensor.
[11:21:35.044] TRAIN: iteration 22181 : loss : 0.154494, loss_ce: 0.001535, loss_dice: 0.307453
[11:21:37.569] TRAIN: iteration 22182 : loss : 0.143112, loss_ce: 0.005956, loss_dice: 0.280268
[11:21:37.779] TRAIN: iteration 22183 : loss : 0.064771, loss_ce: 0.001704, loss_dice: 0.127839
[11:21:37.996] TRAIN: iteration 22184 : loss : 0.098194, loss_ce: 0.001132, loss_dice: 0.195256
[11:21:38.206] TRAIN: iteration 22185 : loss : 0.251507, loss_ce: 0.002819, loss_dice: 0.500194
[11:21:38.414] TRAIN: iteration 22186 : loss : 0.103643, loss_ce: 0.003466, loss_dice: 0.203820
[11:21:38.623] TRAIN: iteration 22187 : loss : 0.246364, loss_ce: 0.002325, loss_dice: 0.490402
[11:21:38.833] TRAIN: iteration 22188 : loss : 0.254952, loss_ce: 0.015088, loss_dice: 0.494815
[11:21:39.041] TRAIN: iteration 22189 : loss : 0.039645, loss_ce: 0.007649, loss_dice: 0.071642
[11:21:41.566] TRAIN: iteration 22190 : loss : 0.067668, loss_ce: 0.004342, loss_dice: 0.130995
[11:21:41.774] TRAIN: iteration 22191 : loss : 0.033766, loss_ce: 0.001749, loss_dice: 0.065783
[11:21:41.984] TRAIN: iteration 22192 : loss : 0.169304, loss_ce: 0.003958, loss_dice: 0.334650
[11:21:42.195] TRAIN: iteration 22193 : loss : 0.105541, loss_ce: 0.004999, loss_dice: 0.206083
[11:21:42.404] TRAIN: iteration 22194 : loss : 0.040420, loss_ce: 0.004401, loss_dice: 0.076440
[11:21:42.612] TRAIN: iteration 22195 : loss : 0.073116, loss_ce: 0.002343, loss_dice: 0.143888
[11:21:42.821] TRAIN: iteration 22196 : loss : 0.143491, loss_ce: 0.001674, loss_dice: 0.285308
[11:21:43.031] TRAIN: iteration 22197 : loss : 0.061174, loss_ce: 0.003315, loss_dice: 0.119033
[11:21:43.735] TRAIN: iteration 22198 : loss : 0.073629, loss_ce: 0.002064, loss_dice: 0.145194
[11:21:43.943] TRAIN: iteration 22199 : loss : 0.032425, loss_ce: 0.003841, loss_dice: 0.061009
[11:21:44.151] TRAIN: iteration 22200 : loss : 0.047467, loss_ce: 0.005397, loss_dice: 0.089537
[11:21:44.392] TRAIN: iteration 22201 : loss : 0.081655, loss_ce: 0.002115, loss_dice: 0.161194
[11:21:44.600] TRAIN: iteration 22202 : loss : 0.250709, loss_ce: 0.001336, loss_dice: 0.500082
[11:21:44.812] TRAIN: iteration 22203 : loss : 0.067579, loss_ce: 0.006739, loss_dice: 0.128419
[11:21:45.027] TRAIN: iteration 22204 : loss : 0.249011, loss_ce: 0.004247, loss_dice: 0.493776
[11:21:46.808] TRAIN: iteration 22205 : loss : 0.250964, loss_ce: 0.001815, loss_dice: 0.500112
[11:21:47.015] TRAIN: iteration 22206 : loss : 0.109655, loss_ce: 0.008460, loss_dice: 0.210849
[11:21:47.225] TRAIN: iteration 22207 : loss : 0.095524, loss_ce: 0.002151, loss_dice: 0.188897
[11:21:47.435] TRAIN: iteration 22208 : loss : 0.162707, loss_ce: 0.010067, loss_dice: 0.315346
[11:21:47.646] TRAIN: iteration 22209 : loss : 0.030086, loss_ce: 0.003282, loss_dice: 0.056890
[11:21:47.853] TRAIN: iteration 22210 : loss : 0.251124, loss_ce: 0.002119, loss_dice: 0.500129
[11:21:49.179] TRAIN: iteration 22211 : loss : 0.251033, loss_ce: 0.001936, loss_dice: 0.500129
[11:21:49.387] TRAIN: iteration 22212 : loss : 0.238133, loss_ce: 0.005638, loss_dice: 0.470627
[11:21:53.342] TRAIN: iteration 22213 : loss : 0.152283, loss_ce: 0.007225, loss_dice: 0.297341
[11:21:53.552] TRAIN: iteration 22214 : loss : 0.083070, loss_ce: 0.005640, loss_dice: 0.160501
[11:21:53.761] TRAIN: iteration 22215 : loss : 0.210008, loss_ce: 0.002243, loss_dice: 0.417773
[11:21:53.973] TRAIN: iteration 22216 : loss : 0.062352, loss_ce: 0.005524, loss_dice: 0.119180
[11:21:54.187] TRAIN: iteration 22217 : loss : 0.249919, loss_ce: 0.002553, loss_dice: 0.497285
[11:21:54.395] TRAIN: iteration 22218 : loss : 0.250873, loss_ce: 0.002028, loss_dice: 0.499718
[11:21:56.338] TRAIN: iteration 22219 : loss : 0.251934, loss_ce: 0.003599, loss_dice: 0.500268
[11:21:56.545] TRAIN: iteration 22220 : loss : 0.077737, loss_ce: 0.004091, loss_dice: 0.151383
[11:21:56.546] NaN or Inf found in input tensor.
[11:21:56.762] TRAIN: iteration 22221 : loss : 0.250804, loss_ce: 0.001518, loss_dice: 0.500089
[11:21:56.973] TRAIN: iteration 22222 : loss : 0.251820, loss_ce: 0.003552, loss_dice: 0.500089
[11:21:57.181] TRAIN: iteration 22223 : loss : 0.088851, loss_ce: 0.012649, loss_dice: 0.165054
[11:21:57.389] TRAIN: iteration 22224 : loss : 0.252728, loss_ce: 0.008991, loss_dice: 0.496464
[11:21:57.597] TRAIN: iteration 22225 : loss : 0.157891, loss_ce: 0.006172, loss_dice: 0.309610
[11:21:58.106] TRAIN: iteration 22226 : loss : 0.074432, loss_ce: 0.007205, loss_dice: 0.141659
[11:22:00.804] TRAIN: iteration 22227 : loss : 0.059581, loss_ce: 0.003078, loss_dice: 0.116084
[11:22:01.017] TRAIN: iteration 22228 : loss : 0.052628, loss_ce: 0.004230, loss_dice: 0.101026
[11:22:01.225] TRAIN: iteration 22229 : loss : 0.058256, loss_ce: 0.004331, loss_dice: 0.112181
[11:22:01.433] TRAIN: iteration 22230 : loss : 0.246574, loss_ce: 0.003716, loss_dice: 0.489431
[11:22:01.641] TRAIN: iteration 22231 : loss : 0.225721, loss_ce: 0.001169, loss_dice: 0.450272
[11:22:01.849] TRAIN: iteration 22232 : loss : 0.123042, loss_ce: 0.002262, loss_dice: 0.243823
[11:22:02.059] TRAIN: iteration 22233 : loss : 0.074451, loss_ce: 0.003849, loss_dice: 0.145052
[11:22:02.267] TRAIN: iteration 22234 : loss : 0.250413, loss_ce: 0.000798, loss_dice: 0.500028
[11:22:05.204] TRAIN: iteration 22235 : loss : 0.250608, loss_ce: 0.001171, loss_dice: 0.500046
[11:22:05.411] TRAIN: iteration 22236 : loss : 0.119387, loss_ce: 0.000920, loss_dice: 0.237854
[11:22:05.619] TRAIN: iteration 22237 : loss : 0.145217, loss_ce: 0.002680, loss_dice: 0.287754
[11:22:05.825] TRAIN: iteration 22238 : loss : 0.072911, loss_ce: 0.003398, loss_dice: 0.142424
[11:22:06.033] TRAIN: iteration 22239 : loss : 0.250325, loss_ce: 0.000638, loss_dice: 0.500012
[11:22:06.240] TRAIN: iteration 22240 : loss : 0.250839, loss_ce: 0.001588, loss_dice: 0.500091
[11:22:06.474] TRAIN: iteration 22241 : loss : 0.142972, loss_ce: 0.001920, loss_dice: 0.284025
[11:22:06.682] TRAIN: iteration 22242 : loss : 0.178234, loss_ce: 0.032158, loss_dice: 0.324310
[11:22:10.931] TRAIN: iteration 22243 : loss : 0.141618, loss_ce: 0.004271, loss_dice: 0.278965
[11:22:11.139] TRAIN: iteration 22244 : loss : 0.084735, loss_ce: 0.001873, loss_dice: 0.167597
[11:22:11.345] TRAIN: iteration 22245 : loss : 0.081663, loss_ce: 0.005398, loss_dice: 0.157929
[11:22:11.552] TRAIN: iteration 22246 : loss : 0.235237, loss_ce: 0.001701, loss_dice: 0.468773
[11:22:11.760] TRAIN: iteration 22247 : loss : 0.250679, loss_ce: 0.001322, loss_dice: 0.500037
[11:22:11.967] TRAIN: iteration 22248 : loss : 0.093310, loss_ce: 0.003496, loss_dice: 0.183123
[11:22:12.175] TRAIN: iteration 22249 : loss : 0.222115, loss_ce: 0.002038, loss_dice: 0.442191
[11:22:12.382] TRAIN: iteration 22250 : loss : 0.090249, loss_ce: 0.002535, loss_dice: 0.177963
[11:22:14.660] TRAIN: iteration 22251 : loss : 0.251789, loss_ce: 0.003345, loss_dice: 0.500233
[11:22:14.867] TRAIN: iteration 22252 : loss : 0.071742, loss_ce: 0.009204, loss_dice: 0.134281
[11:22:15.074] TRAIN: iteration 22253 : loss : 0.233900, loss_ce: 0.002413, loss_dice: 0.465387
[11:22:15.284] TRAIN: iteration 22254 : loss : 0.081721, loss_ce: 0.003277, loss_dice: 0.160164
[11:22:15.492] TRAIN: iteration 22255 : loss : 0.253807, loss_ce: 0.011392, loss_dice: 0.496221
[11:22:15.704] TRAIN: iteration 22256 : loss : 0.056949, loss_ce: 0.003502, loss_dice: 0.110396
[11:22:15.967] TRAIN: iteration 22257 : loss : 0.076520, loss_ce: 0.003058, loss_dice: 0.149983
[11:22:16.178] TRAIN: iteration 22258 : loss : 0.251186, loss_ce: 0.002271, loss_dice: 0.500101
[11:22:19.477] TRAIN: iteration 22259 : loss : 0.102559, loss_ce: 0.006177, loss_dice: 0.198941
[11:22:19.687] TRAIN: iteration 22260 : loss : 0.069584, loss_ce: 0.005960, loss_dice: 0.133208
[11:22:19.919] TRAIN: iteration 22261 : loss : 0.251751, loss_ce: 0.003274, loss_dice: 0.500228
[11:22:20.129] TRAIN: iteration 22262 : loss : 0.098992, loss_ce: 0.007715, loss_dice: 0.190269
[11:22:20.338] TRAIN: iteration 22263 : loss : 0.130531, loss_ce: 0.008584, loss_dice: 0.252479
[11:22:20.547] TRAIN: iteration 22264 : loss : 0.250904, loss_ce: 0.001735, loss_dice: 0.500072
[11:22:20.755] TRAIN: iteration 22265 : loss : 0.067687, loss_ce: 0.001678, loss_dice: 0.133696
[11:22:20.965] TRAIN: iteration 22266 : loss : 0.126247, loss_ce: 0.002752, loss_dice: 0.249742
[11:22:22.542] TRAIN: iteration 22267 : loss : 0.252121, loss_ce: 0.004166, loss_dice: 0.500077
[11:22:22.751] TRAIN: iteration 22268 : loss : 0.070908, loss_ce: 0.001829, loss_dice: 0.139987
[11:22:23.136] TRAIN: iteration 22269 : loss : 0.030474, loss_ce: 0.003827, loss_dice: 0.057120
[11:22:23.343] TRAIN: iteration 22270 : loss : 0.138168, loss_ce: 0.005511, loss_dice: 0.270825
[11:22:23.551] TRAIN: iteration 22271 : loss : 0.245231, loss_ce: 0.004020, loss_dice: 0.486442
[11:22:23.762] TRAIN: iteration 22272 : loss : 0.077302, loss_ce: 0.002456, loss_dice: 0.152147
[11:22:23.971] TRAIN: iteration 22273 : loss : 0.045736, loss_ce: 0.002814, loss_dice: 0.088658
[11:22:25.032] TRAIN: iteration 22274 : loss : 0.196034, loss_ce: 0.002287, loss_dice: 0.389782
[11:22:28.745] TRAIN: iteration 22275 : loss : 0.113266, loss_ce: 0.003788, loss_dice: 0.222745
[11:22:28.953] TRAIN: iteration 22276 : loss : 0.251591, loss_ce: 0.002989, loss_dice: 0.500194
[11:22:29.163] TRAIN: iteration 22277 : loss : 0.075396, loss_ce: 0.001748, loss_dice: 0.149043
[11:22:29.372] TRAIN: iteration 22278 : loss : 0.059960, loss_ce: 0.002776, loss_dice: 0.117143
[11:22:29.581] TRAIN: iteration 22279 : loss : 0.223944, loss_ce: 0.002709, loss_dice: 0.445179
[11:22:29.789] TRAIN: iteration 22280 : loss : 0.054730, loss_ce: 0.002491, loss_dice: 0.106969
[11:22:30.031] TRAIN: iteration 22281 : loss : 0.115290, loss_ce: 0.003201, loss_dice: 0.227379
[11:22:30.243] TRAIN: iteration 22282 : loss : 0.077061, loss_ce: 0.004115, loss_dice: 0.150007
[11:22:32.371] TRAIN: iteration 22283 : loss : 0.202599, loss_ce: 0.002692, loss_dice: 0.402506
[11:22:32.580] TRAIN: iteration 22284 : loss : 0.109347, loss_ce: 0.003578, loss_dice: 0.215115
[11:22:32.792] TRAIN: iteration 22285 : loss : 0.080464, loss_ce: 0.006089, loss_dice: 0.154840
[11:22:33.003] TRAIN: iteration 22286 : loss : 0.249926, loss_ce: 0.004197, loss_dice: 0.495655
[11:22:33.215] TRAIN: iteration 22287 : loss : 0.112309, loss_ce: 0.005230, loss_dice: 0.219388
[11:22:33.937] TRAIN: iteration 22288 : loss : 0.251870, loss_ce: 0.003918, loss_dice: 0.499821
[11:22:34.146] TRAIN: iteration 22289 : loss : 0.148379, loss_ce: 0.022176, loss_dice: 0.274581
[11:22:34.353] TRAIN: iteration 22290 : loss : 0.095759, loss_ce: 0.003264, loss_dice: 0.188254
[11:22:39.980] TRAIN: iteration 22291 : loss : 0.145417, loss_ce: 0.002953, loss_dice: 0.287881
[11:22:40.189] TRAIN: iteration 22292 : loss : 0.083166, loss_ce: 0.006676, loss_dice: 0.159656
[11:22:40.396] TRAIN: iteration 22293 : loss : 0.243866, loss_ce: 0.006024, loss_dice: 0.481709
[11:22:40.630] TRAIN: iteration 22294 : loss : 0.052224, loss_ce: 0.001064, loss_dice: 0.103384
[11:22:40.838] TRAIN: iteration 22295 : loss : 0.243393, loss_ce: 0.002260, loss_dice: 0.484526
[11:22:41.045] TRAIN: iteration 22296 : loss : 0.251222, loss_ce: 0.002296, loss_dice: 0.500149
[11:22:41.253] TRAIN: iteration 22297 : loss : 0.217935, loss_ce: 0.007014, loss_dice: 0.428855
[11:22:41.460] TRAIN: iteration 22298 : loss : 0.096500, loss_ce: 0.015660, loss_dice: 0.177339
[11:22:45.006] TRAIN: iteration 22299 : loss : 0.248916, loss_ce: 0.003014, loss_dice: 0.494817
[11:22:45.214] TRAIN: iteration 22300 : loss : 0.060017, loss_ce: 0.001382, loss_dice: 0.118652
[11:22:45.457] TRAIN: iteration 22301 : loss : 0.089145, loss_ce: 0.002651, loss_dice: 0.175638
[11:22:45.671] TRAIN: iteration 22302 : loss : 0.180676, loss_ce: 0.008042, loss_dice: 0.353310
[11:22:45.880] TRAIN: iteration 22303 : loss : 0.126030, loss_ce: 0.004915, loss_dice: 0.247145
[11:22:46.088] TRAIN: iteration 22304 : loss : 0.252271, loss_ce: 0.004245, loss_dice: 0.500297
[11:22:46.296] TRAIN: iteration 22305 : loss : 0.211415, loss_ce: 0.003509, loss_dice: 0.419322
[11:22:46.503] TRAIN: iteration 22306 : loss : 0.158738, loss_ce: 0.003853, loss_dice: 0.313623
[11:22:51.531] TRAIN: iteration 22307 : loss : 0.039914, loss_ce: 0.001444, loss_dice: 0.078384
[11:22:51.745] TRAIN: iteration 22308 : loss : 0.032789, loss_ce: 0.002665, loss_dice: 0.062912
[11:22:51.953] TRAIN: iteration 22309 : loss : 0.154664, loss_ce: 0.002709, loss_dice: 0.306619
[11:22:52.161] TRAIN: iteration 22310 : loss : 0.099134, loss_ce: 0.003103, loss_dice: 0.195165
[11:22:52.368] TRAIN: iteration 22311 : loss : 0.176841, loss_ce: 0.024261, loss_dice: 0.329421
[11:22:52.577] TRAIN: iteration 22312 : loss : 0.130114, loss_ce: 0.007549, loss_dice: 0.252680
[11:22:52.789] TRAIN: iteration 22313 : loss : 0.186552, loss_ce: 0.002877, loss_dice: 0.370226
[11:22:52.999] TRAIN: iteration 22314 : loss : 0.028154, loss_ce: 0.004582, loss_dice: 0.051726
[11:22:56.354] TRAIN: iteration 22315 : loss : 0.251251, loss_ce: 0.002362, loss_dice: 0.500141
[11:22:56.568] TRAIN: iteration 22316 : loss : 0.069966, loss_ce: 0.005161, loss_dice: 0.134771
[11:22:56.777] TRAIN: iteration 22317 : loss : 0.042819, loss_ce: 0.003910, loss_dice: 0.081727
[11:22:56.987] TRAIN: iteration 22318 : loss : 0.150746, loss_ce: 0.003031, loss_dice: 0.298461
[11:22:57.196] TRAIN: iteration 22319 : loss : 0.141329, loss_ce: 0.012903, loss_dice: 0.269754
[11:22:57.406] TRAIN: iteration 22320 : loss : 0.057762, loss_ce: 0.006232, loss_dice: 0.109293
[11:22:59.694] TRAIN: iteration 22321 : loss : 0.068796, loss_ce: 0.004253, loss_dice: 0.133339
[11:22:59.906] TRAIN: iteration 22322 : loss : 0.211934, loss_ce: 0.001902, loss_dice: 0.421965
[11:23:01.245] TRAIN: iteration 22323 : loss : 0.086619, loss_ce: 0.003754, loss_dice: 0.169484
[11:23:01.455] TRAIN: iteration 22324 : loss : 0.114699, loss_ce: 0.002515, loss_dice: 0.226883
[11:23:01.663] TRAIN: iteration 22325 : loss : 0.032925, loss_ce: 0.003607, loss_dice: 0.062242
[11:23:01.872] TRAIN: iteration 22326 : loss : 0.057212, loss_ce: 0.005963, loss_dice: 0.108460
[11:23:02.080] TRAIN: iteration 22327 : loss : 0.042805, loss_ce: 0.001249, loss_dice: 0.084362
[11:23:02.287] TRAIN: iteration 22328 : loss : 0.241634, loss_ce: 0.020556, loss_dice: 0.462712
[11:23:04.441] TRAIN: iteration 22329 : loss : 0.250855, loss_ce: 0.001640, loss_dice: 0.500070
[11:23:04.652] TRAIN: iteration 22330 : loss : 0.121924, loss_ce: 0.003301, loss_dice: 0.240547
[11:23:04.859] TRAIN: iteration 22331 : loss : 0.182767, loss_ce: 0.005662, loss_dice: 0.359872
[11:23:05.071] TRAIN: iteration 22332 : loss : 0.048853, loss_ce: 0.004786, loss_dice: 0.092921
[11:23:05.278] TRAIN: iteration 22333 : loss : 0.017430, loss_ce: 0.001993, loss_dice: 0.032868
[11:23:05.485] TRAIN: iteration 22334 : loss : 0.048496, loss_ce: 0.006271, loss_dice: 0.090720
[11:23:05.693] TRAIN: iteration 22335 : loss : 0.089983, loss_ce: 0.009334, loss_dice: 0.170631
[11:23:05.901] TRAIN: iteration 22336 : loss : 0.139389, loss_ce: 0.002112, loss_dice: 0.276667
[11:23:09.367] TRAIN: iteration 22337 : loss : 0.080179, loss_ce: 0.002703, loss_dice: 0.157655
[11:23:09.575] TRAIN: iteration 22338 : loss : 0.071295, loss_ce: 0.007377, loss_dice: 0.135212
[11:23:09.785] TRAIN: iteration 22339 : loss : 0.167996, loss_ce: 0.004056, loss_dice: 0.331935
[11:23:11.772] TRAIN: iteration 22340 : loss : 0.184380, loss_ce: 0.007096, loss_dice: 0.361664
[11:23:12.011] TRAIN: iteration 22341 : loss : 0.206769, loss_ce: 0.002067, loss_dice: 0.411471
[11:23:12.219] TRAIN: iteration 22342 : loss : 0.057273, loss_ce: 0.005135, loss_dice: 0.109411
[11:23:12.428] TRAIN: iteration 22343 : loss : 0.095638, loss_ce: 0.002908, loss_dice: 0.188369
[11:23:12.640] TRAIN: iteration 22344 : loss : 0.073643, loss_ce: 0.003443, loss_dice: 0.143843
[11:23:13.849] TRAIN: iteration 22345 : loss : 0.119675, loss_ce: 0.001519, loss_dice: 0.237832
[11:23:14.057] TRAIN: iteration 22346 : loss : 0.064307, loss_ce: 0.003196, loss_dice: 0.125418
[11:23:14.398] TRAIN: iteration 22347 : loss : 0.067870, loss_ce: 0.004167, loss_dice: 0.131572
[11:23:17.358] TRAIN: iteration 22348 : loss : 0.089515, loss_ce: 0.008435, loss_dice: 0.170596
[11:23:17.566] TRAIN: iteration 22349 : loss : 0.064375, loss_ce: 0.004522, loss_dice: 0.124227
[11:23:17.774] TRAIN: iteration 22350 : loss : 0.100339, loss_ce: 0.003017, loss_dice: 0.197661
[11:23:18.431] TRAIN: iteration 22351 : loss : 0.030398, loss_ce: 0.004394, loss_dice: 0.056401
[11:23:18.638] TRAIN: iteration 22352 : loss : 0.059326, loss_ce: 0.008044, loss_dice: 0.110608
[11:23:19.843] TRAIN: iteration 22353 : loss : 0.138556, loss_ce: 0.002552, loss_dice: 0.274560
[11:23:20.052] TRAIN: iteration 22354 : loss : 0.084572, loss_ce: 0.003166, loss_dice: 0.165977
[11:23:22.742] TRAIN: iteration 22355 : loss : 0.041784, loss_ce: 0.001194, loss_dice: 0.082374
[11:23:24.268] TRAIN: iteration 22356 : loss : 0.097972, loss_ce: 0.004996, loss_dice: 0.190948
[11:23:24.475] TRAIN: iteration 22357 : loss : 0.125752, loss_ce: 0.001287, loss_dice: 0.250218
[11:23:24.686] TRAIN: iteration 22358 : loss : 0.058612, loss_ce: 0.003721, loss_dice: 0.113504
[11:23:24.894] TRAIN: iteration 22359 : loss : 0.250487, loss_ce: 0.000940, loss_dice: 0.500035
[11:23:25.102] TRAIN: iteration 22360 : loss : 0.068117, loss_ce: 0.003492, loss_dice: 0.132742
[11:23:26.413] TRAIN: iteration 22361 : loss : 0.098920, loss_ce: 0.002659, loss_dice: 0.195181
[11:23:26.621] TRAIN: iteration 22362 : loss : 0.087314, loss_ce: 0.001822, loss_dice: 0.172806
[11:23:26.828] TRAIN: iteration 22363 : loss : 0.081500, loss_ce: 0.004172, loss_dice: 0.158827
[11:23:31.280] TRAIN: iteration 22364 : loss : 0.225731, loss_ce: 0.001896, loss_dice: 0.449566
[11:23:31.614] TRAIN: iteration 22365 : loss : 0.058291, loss_ce: 0.011263, loss_dice: 0.105318
[11:23:31.823] TRAIN: iteration 22366 : loss : 0.050270, loss_ce: 0.005643, loss_dice: 0.094897
[11:23:32.066] TRAIN: iteration 22367 : loss : 0.079747, loss_ce: 0.006170, loss_dice: 0.153325
[11:23:32.274] TRAIN: iteration 22368 : loss : 0.086919, loss_ce: 0.007354, loss_dice: 0.166484
[11:23:34.502] TRAIN: iteration 22369 : loss : 0.169761, loss_ce: 0.006641, loss_dice: 0.332882
[11:23:34.870] TRAIN: iteration 22370 : loss : 0.101588, loss_ce: 0.007434, loss_dice: 0.195742
[11:23:35.078] TRAIN: iteration 22371 : loss : 0.080910, loss_ce: 0.006286, loss_dice: 0.155534
[11:23:37.991] TRAIN: iteration 22372 : loss : 0.022857, loss_ce: 0.001712, loss_dice: 0.044003
[11:23:40.452] TRAIN: iteration 22373 : loss : 0.180760, loss_ce: 0.006766, loss_dice: 0.354754
[11:23:40.663] TRAIN: iteration 22374 : loss : 0.252229, loss_ce: 0.004262, loss_dice: 0.500195
[11:23:40.877] TRAIN: iteration 22375 : loss : 0.094565, loss_ce: 0.007551, loss_dice: 0.181579
[11:23:41.085] TRAIN: iteration 22376 : loss : 0.149640, loss_ce: 0.001635, loss_dice: 0.297646
[11:23:43.009] TRAIN: iteration 22377 : loss : 0.078987, loss_ce: 0.007879, loss_dice: 0.150095
[11:23:43.216] TRAIN: iteration 22378 : loss : 0.069288, loss_ce: 0.007264, loss_dice: 0.131312
[11:23:43.423] TRAIN: iteration 22379 : loss : 0.047091, loss_ce: 0.001420, loss_dice: 0.092763
[11:23:48.254] TRAIN: iteration 22380 : loss : 0.121949, loss_ce: 0.004024, loss_dice: 0.239874
[11:23:48.492] TRAIN: iteration 22381 : loss : 0.039650, loss_ce: 0.007947, loss_dice: 0.071354
[11:23:48.699] TRAIN: iteration 22382 : loss : 0.249995, loss_ce: 0.008074, loss_dice: 0.491916
[11:23:48.907] TRAIN: iteration 22383 : loss : 0.253109, loss_ce: 0.005807, loss_dice: 0.500411
[11:23:49.120] TRAIN: iteration 22384 : loss : 0.079660, loss_ce: 0.007106, loss_dice: 0.152214
[11:23:49.328] TRAIN: iteration 22385 : loss : 0.233901, loss_ce: 0.004041, loss_dice: 0.463760
[11:23:49.536] TRAIN: iteration 22386 : loss : 0.237866, loss_ce: 0.001686, loss_dice: 0.474046
[11:23:49.743] TRAIN: iteration 22387 : loss : 0.071552, loss_ce: 0.001463, loss_dice: 0.141642
[11:23:56.051] TRAIN: iteration 22388 : loss : 0.090990, loss_ce: 0.003954, loss_dice: 0.178027
[11:23:56.259] TRAIN: iteration 22389 : loss : 0.079998, loss_ce: 0.006194, loss_dice: 0.153803
[11:23:56.467] TRAIN: iteration 22390 : loss : 0.184973, loss_ce: 0.010105, loss_dice: 0.359841
[11:23:56.674] TRAIN: iteration 22391 : loss : 0.096136, loss_ce: 0.010671, loss_dice: 0.181601
[11:23:56.882] TRAIN: iteration 22392 : loss : 0.191463, loss_ce: 0.001779, loss_dice: 0.381146
[11:23:57.091] TRAIN: iteration 22393 : loss : 0.142142, loss_ce: 0.002461, loss_dice: 0.281824
[11:23:57.304] TRAIN: iteration 22394 : loss : 0.035175, loss_ce: 0.003984, loss_dice: 0.066365
[11:23:57.512] TRAIN: iteration 22395 : loss : 0.040654, loss_ce: 0.004815, loss_dice: 0.076493
[11:24:03.064] TRAIN: iteration 22396 : loss : 0.251016, loss_ce: 0.001972, loss_dice: 0.500060
[11:24:03.272] TRAIN: iteration 22397 : loss : 0.023981, loss_ce: 0.001487, loss_dice: 0.046475
[11:24:03.480] TRAIN: iteration 22398 : loss : 0.145683, loss_ce: 0.009104, loss_dice: 0.282261
[11:24:03.687] TRAIN: iteration 22399 : loss : 0.056766, loss_ce: 0.002872, loss_dice: 0.110660
[11:24:03.894] TRAIN: iteration 22400 : loss : 0.238542, loss_ce: 0.006007, loss_dice: 0.471077
[11:24:04.133] TRAIN: iteration 22401 : loss : 0.137392, loss_ce: 0.001847, loss_dice: 0.272938
[11:24:04.341] TRAIN: iteration 22402 : loss : 0.030443, loss_ce: 0.001734, loss_dice: 0.059152
[11:24:04.548] TRAIN: iteration 22403 : loss : 0.102692, loss_ce: 0.002004, loss_dice: 0.203380
[11:24:11.891] TRAIN: iteration 22404 : loss : 0.249334, loss_ce: 0.003284, loss_dice: 0.495384
[11:24:12.099] TRAIN: iteration 22405 : loss : 0.036403, loss_ce: 0.004246, loss_dice: 0.068559
[11:24:12.307] TRAIN: iteration 22406 : loss : 0.062831, loss_ce: 0.005939, loss_dice: 0.119722
[11:24:12.516] TRAIN: iteration 22407 : loss : 0.175085, loss_ce: 0.003490, loss_dice: 0.346679
[11:24:12.724] TRAIN: iteration 22408 : loss : 0.250732, loss_ce: 0.001416, loss_dice: 0.500049
[11:24:12.931] TRAIN: iteration 22409 : loss : 0.212381, loss_ce: 0.004068, loss_dice: 0.420694
[11:24:13.142] TRAIN: iteration 22410 : loss : 0.196893, loss_ce: 0.002927, loss_dice: 0.390859
[11:24:13.349] TRAIN: iteration 22411 : loss : 0.133242, loss_ce: 0.003883, loss_dice: 0.262602
[11:24:20.008] TRAIN: iteration 22412 : loss : 0.084886, loss_ce: 0.001636, loss_dice: 0.168136
[11:24:20.216] TRAIN: iteration 22413 : loss : 0.119161, loss_ce: 0.004801, loss_dice: 0.233521
[11:24:20.424] TRAIN: iteration 22414 : loss : 0.250523, loss_ce: 0.001026, loss_dice: 0.500021
[11:24:20.632] TRAIN: iteration 22415 : loss : 0.241461, loss_ce: 0.002009, loss_dice: 0.480913
[11:24:20.839] TRAIN: iteration 22416 : loss : 0.060624, loss_ce: 0.013597, loss_dice: 0.107650
[11:24:21.047] TRAIN: iteration 22417 : loss : 0.091669, loss_ce: 0.010711, loss_dice: 0.172628
[11:24:21.256] TRAIN: iteration 22418 : loss : 0.109388, loss_ce: 0.002825, loss_dice: 0.215952
[11:24:21.463] TRAIN: iteration 22419 : loss : 0.095649, loss_ce: 0.001879, loss_dice: 0.189419
[11:24:25.442] TRAIN: iteration 22420 : loss : 0.251077, loss_ce: 0.002046, loss_dice: 0.500109
[11:24:25.685] TRAIN: iteration 22421 : loss : 0.246902, loss_ce: 0.003285, loss_dice: 0.490518
[11:24:25.893] TRAIN: iteration 22422 : loss : 0.114551, loss_ce: 0.002543, loss_dice: 0.226558
[11:24:26.101] TRAIN: iteration 22423 : loss : 0.014169, loss_ce: 0.001631, loss_dice: 0.026707
[11:24:26.309] TRAIN: iteration 22424 : loss : 0.069297, loss_ce: 0.001929, loss_dice: 0.136666
[11:24:26.517] TRAIN: iteration 22425 : loss : 0.119452, loss_ce: 0.001995, loss_dice: 0.236909
[11:24:26.724] TRAIN: iteration 22426 : loss : 0.193752, loss_ce: 0.002825, loss_dice: 0.384679
[11:24:26.931] TRAIN: iteration 22427 : loss : 0.219101, loss_ce: 0.002165, loss_dice: 0.436037
[11:24:35.716] TRAIN: iteration 22428 : loss : 0.029516, loss_ce: 0.001646, loss_dice: 0.057387
[11:24:35.929] TRAIN: iteration 22429 : loss : 0.078338, loss_ce: 0.001124, loss_dice: 0.155553
[11:24:36.140] TRAIN: iteration 22430 : loss : 0.224234, loss_ce: 0.004640, loss_dice: 0.443827
[11:24:36.350] TRAIN: iteration 22431 : loss : 0.058190, loss_ce: 0.002583, loss_dice: 0.113797
[11:24:36.557] TRAIN: iteration 22432 : loss : 0.044040, loss_ce: 0.001183, loss_dice: 0.086897
[11:24:36.764] TRAIN: iteration 22433 : loss : 0.219599, loss_ce: 0.001698, loss_dice: 0.437499
[11:24:36.975] TRAIN: iteration 22434 : loss : 0.128209, loss_ce: 0.001670, loss_dice: 0.254748
[11:24:37.183] TRAIN: iteration 22435 : loss : 0.129209, loss_ce: 0.001811, loss_dice: 0.256607
[11:24:44.453] TRAIN: iteration 22436 : loss : 0.067945, loss_ce: 0.001551, loss_dice: 0.134338
[11:24:44.660] TRAIN: iteration 22437 : loss : 0.111460, loss_ce: 0.005033, loss_dice: 0.217887
[11:24:44.869] TRAIN: iteration 22438 : loss : 0.023758, loss_ce: 0.000644, loss_dice: 0.046872
[11:24:45.079] TRAIN: iteration 22439 : loss : 0.147582, loss_ce: 0.007721, loss_dice: 0.287444
[11:24:45.287] TRAIN: iteration 22440 : loss : 0.147345, loss_ce: 0.013750, loss_dice: 0.280940
[11:24:45.527] TRAIN: iteration 22441 : loss : 0.211702, loss_ce: 0.002855, loss_dice: 0.420549
[11:24:45.777] TRAIN: iteration 22442 : loss : 0.062903, loss_ce: 0.004802, loss_dice: 0.121004
[11:24:45.987] TRAIN: iteration 22443 : loss : 0.049494, loss_ce: 0.001188, loss_dice: 0.097799
[11:24:50.720] TRAIN: iteration 22444 : loss : 0.250392, loss_ce: 0.000759, loss_dice: 0.500025
[11:24:50.928] TRAIN: iteration 22445 : loss : 0.129501, loss_ce: 0.003142, loss_dice: 0.255860
[11:24:51.136] TRAIN: iteration 22446 : loss : 0.250447, loss_ce: 0.000864, loss_dice: 0.500030
[11:24:51.344] TRAIN: iteration 22447 : loss : 0.045434, loss_ce: 0.001238, loss_dice: 0.089630
[11:24:51.553] TRAIN: iteration 22448 : loss : 0.224740, loss_ce: 0.006393, loss_dice: 0.443087
[11:24:51.762] TRAIN: iteration 22449 : loss : 0.249274, loss_ce: 0.001293, loss_dice: 0.497255
[11:24:51.976] TRAIN: iteration 22450 : loss : 0.050571, loss_ce: 0.001420, loss_dice: 0.099723
[11:24:52.183] TRAIN: iteration 22451 : loss : 0.161668, loss_ce: 0.003483, loss_dice: 0.319852
[11:24:58.777] TRAIN: iteration 22452 : loss : 0.246947, loss_ce: 0.004263, loss_dice: 0.489631
[11:24:58.985] TRAIN: iteration 22453 : loss : 0.072016, loss_ce: 0.001282, loss_dice: 0.142750
[11:24:59.193] TRAIN: iteration 22454 : loss : 0.034822, loss_ce: 0.000683, loss_dice: 0.068961
[11:24:59.401] TRAIN: iteration 22455 : loss : 0.250845, loss_ce: 0.001610, loss_dice: 0.500081
[11:24:59.738] TRAIN: iteration 22456 : loss : 0.081957, loss_ce: 0.001142, loss_dice: 0.162772
[11:24:59.948] TRAIN: iteration 22457 : loss : 0.061467, loss_ce: 0.006118, loss_dice: 0.116815
[11:25:00.155] TRAIN: iteration 22458 : loss : 0.246193, loss_ce: 0.001305, loss_dice: 0.491081
[11:25:00.364] TRAIN: iteration 22459 : loss : 0.027021, loss_ce: 0.002803, loss_dice: 0.051239
[11:25:10.373] TRAIN: iteration 22460 : loss : 0.213672, loss_ce: 0.005204, loss_dice: 0.422139
[11:25:10.612] TRAIN: iteration 22461 : loss : 0.127106, loss_ce: 0.002004, loss_dice: 0.252207
[11:25:10.820] TRAIN: iteration 22462 : loss : 0.022096, loss_ce: 0.000715, loss_dice: 0.043477
[11:25:11.030] TRAIN: iteration 22463 : loss : 0.045967, loss_ce: 0.003064, loss_dice: 0.088870
[11:25:11.239] TRAIN: iteration 22464 : loss : 0.039201, loss_ce: 0.001956, loss_dice: 0.076446
[11:25:11.449] TRAIN: iteration 22465 : loss : 0.070282, loss_ce: 0.006186, loss_dice: 0.134378
[11:25:11.657] TRAIN: iteration 22466 : loss : 0.114443, loss_ce: 0.003059, loss_dice: 0.225828
[11:25:11.865] TRAIN: iteration 22467 : loss : 0.100624, loss_ce: 0.003851, loss_dice: 0.197396
[11:25:18.973] TRAIN: iteration 22468 : loss : 0.159876, loss_ce: 0.004747, loss_dice: 0.315005
[11:25:19.188] TRAIN: iteration 22469 : loss : 0.036584, loss_ce: 0.001964, loss_dice: 0.071204
[11:25:19.397] TRAIN: iteration 22470 : loss : 0.037389, loss_ce: 0.002112, loss_dice: 0.072666
[11:25:19.607] TRAIN: iteration 22471 : loss : 0.047853, loss_ce: 0.007421, loss_dice: 0.088286
[11:25:19.814] TRAIN: iteration 22472 : loss : 0.148287, loss_ce: 0.003206, loss_dice: 0.293367
[11:25:20.023] TRAIN: iteration 22473 : loss : 0.064925, loss_ce: 0.001978, loss_dice: 0.127871
[11:25:20.237] TRAIN: iteration 22474 : loss : 0.250785, loss_ce: 0.001505, loss_dice: 0.500066
[11:25:20.445] TRAIN: iteration 22475 : loss : 0.044684, loss_ce: 0.002797, loss_dice: 0.086572
[11:25:26.930] TRAIN: iteration 22476 : loss : 0.179433, loss_ce: 0.004395, loss_dice: 0.354471
[11:25:27.139] TRAIN: iteration 22477 : loss : 0.063356, loss_ce: 0.002406, loss_dice: 0.124306
[11:25:27.347] TRAIN: iteration 22478 : loss : 0.253041, loss_ce: 0.005683, loss_dice: 0.500398
[11:25:27.556] TRAIN: iteration 22479 : loss : 0.189247, loss_ce: 0.001069, loss_dice: 0.377425
[11:25:27.765] TRAIN: iteration 22480 : loss : 0.250771, loss_ce: 0.001482, loss_dice: 0.500059
[11:25:28.004] TRAIN: iteration 22481 : loss : 0.103730, loss_ce: 0.001832, loss_dice: 0.205627
[11:25:28.212] TRAIN: iteration 22482 : loss : 0.058113, loss_ce: 0.003331, loss_dice: 0.112895
[11:25:28.419] TRAIN: iteration 22483 : loss : 0.144445, loss_ce: 0.003995, loss_dice: 0.284895
[11:25:36.087] TRAIN: iteration 22484 : loss : 0.251937, loss_ce: 0.003602, loss_dice: 0.500272
[11:25:36.297] TRAIN: iteration 22485 : loss : 0.113629, loss_ce: 0.002002, loss_dice: 0.225256
[11:25:36.505] TRAIN: iteration 22486 : loss : 0.251245, loss_ce: 0.002328, loss_dice: 0.500161
[11:25:36.712] TRAIN: iteration 22487 : loss : 0.250827, loss_ce: 0.001572, loss_dice: 0.500081
[11:25:36.919] TRAIN: iteration 22488 : loss : 0.109586, loss_ce: 0.003448, loss_dice: 0.215724
[11:25:37.126] TRAIN: iteration 22489 : loss : 0.233618, loss_ce: 0.001676, loss_dice: 0.465560
[11:25:37.334] TRAIN: iteration 22490 : loss : 0.067741, loss_ce: 0.006002, loss_dice: 0.129481
[11:25:37.541] TRAIN: iteration 22491 : loss : 0.125648, loss_ce: 0.003150, loss_dice: 0.248146
[11:25:43.986] TRAIN: iteration 22492 : loss : 0.081149, loss_ce: 0.005039, loss_dice: 0.157258
[11:25:44.194] TRAIN: iteration 22493 : loss : 0.068246, loss_ce: 0.004171, loss_dice: 0.132321
[11:25:44.404] TRAIN: iteration 22494 : loss : 0.057554, loss_ce: 0.001220, loss_dice: 0.113889
[11:25:44.612] TRAIN: iteration 22495 : loss : 0.078040, loss_ce: 0.006337, loss_dice: 0.149742
[11:25:44.821] TRAIN: iteration 22496 : loss : 0.161082, loss_ce: 0.002744, loss_dice: 0.319419
[11:25:45.028] TRAIN: iteration 22497 : loss : 0.090691, loss_ce: 0.005495, loss_dice: 0.175888
[11:25:45.236] TRAIN: iteration 22498 : loss : 0.056763, loss_ce: 0.002733, loss_dice: 0.110794
[11:25:45.444] TRAIN: iteration 22499 : loss : 0.084448, loss_ce: 0.003540, loss_dice: 0.165356
[11:25:53.831] TRAIN: iteration 22500 : loss : 0.147035, loss_ce: 0.005216, loss_dice: 0.288853
[11:25:54.075] TRAIN: iteration 22501 : loss : 0.075084, loss_ce: 0.006140, loss_dice: 0.144029
[11:25:54.283] TRAIN: iteration 22502 : loss : 0.136685, loss_ce: 0.003362, loss_dice: 0.270008
[11:25:54.490] TRAIN: iteration 22503 : loss : 0.200026, loss_ce: 0.003602, loss_dice: 0.396450
[11:25:54.697] TRAIN: iteration 22504 : loss : 0.087101, loss_ce: 0.005422, loss_dice: 0.168781
[11:25:54.908] TRAIN: iteration 22505 : loss : 0.210129, loss_ce: 0.001667, loss_dice: 0.418591
[11:25:55.117] TRAIN: iteration 22506 : loss : 0.125181, loss_ce: 0.006756, loss_dice: 0.243606
[11:25:55.324] TRAIN: iteration 22507 : loss : 0.079551, loss_ce: 0.002217, loss_dice: 0.156885
[11:26:01.918] TRAIN: iteration 22508 : loss : 0.089737, loss_ce: 0.003586, loss_dice: 0.175889
[11:26:02.126] TRAIN: iteration 22509 : loss : 0.085513, loss_ce: 0.006784, loss_dice: 0.164243
[11:26:02.333] TRAIN: iteration 22510 : loss : 0.237418, loss_ce: 0.002821, loss_dice: 0.472014
[11:26:02.540] TRAIN: iteration 22511 : loss : 0.081091, loss_ce: 0.003673, loss_dice: 0.158509
[11:26:02.749] TRAIN: iteration 22512 : loss : 0.250459, loss_ce: 0.000905, loss_dice: 0.500014
[11:26:02.957] TRAIN: iteration 22513 : loss : 0.053448, loss_ce: 0.001537, loss_dice: 0.105360
[11:26:03.164] TRAIN: iteration 22514 : loss : 0.102806, loss_ce: 0.002794, loss_dice: 0.202819
[11:26:03.374] TRAIN: iteration 22515 : loss : 0.078845, loss_ce: 0.002433, loss_dice: 0.155256
[11:26:11.754] TRAIN: iteration 22516 : loss : 0.251007, loss_ce: 0.002457, loss_dice: 0.499556
[11:26:11.961] TRAIN: iteration 22517 : loss : 0.036770, loss_ce: 0.003262, loss_dice: 0.070279
[11:26:12.169] TRAIN: iteration 22518 : loss : 0.157262, loss_ce: 0.002062, loss_dice: 0.312463
[11:26:12.376] TRAIN: iteration 22519 : loss : 0.073336, loss_ce: 0.002334, loss_dice: 0.144338
[11:26:12.584] TRAIN: iteration 22520 : loss : 0.050309, loss_ce: 0.004644, loss_dice: 0.095973
[11:26:12.815] TRAIN: iteration 22521 : loss : 0.127043, loss_ce: 0.002028, loss_dice: 0.252058
[11:26:13.022] TRAIN: iteration 22522 : loss : 0.217431, loss_ce: 0.002620, loss_dice: 0.432242
[11:26:13.228] TRAIN: iteration 22523 : loss : 0.049495, loss_ce: 0.006603, loss_dice: 0.092387
[11:26:19.353] TRAIN: iteration 22524 : loss : 0.191456, loss_ce: 0.006699, loss_dice: 0.376214
[11:26:19.560] TRAIN: iteration 22525 : loss : 0.042366, loss_ce: 0.008017, loss_dice: 0.076715
[11:26:19.768] TRAIN: iteration 22526 : loss : 0.119869, loss_ce: 0.007965, loss_dice: 0.231774
[11:26:19.976] TRAIN: iteration 22527 : loss : 0.092027, loss_ce: 0.007181, loss_dice: 0.176873
[11:26:20.184] TRAIN: iteration 22528 : loss : 0.250672, loss_ce: 0.001301, loss_dice: 0.500044
[11:26:20.391] TRAIN: iteration 22529 : loss : 0.100089, loss_ce: 0.002407, loss_dice: 0.197772
[11:26:20.598] TRAIN: iteration 22530 : loss : 0.047711, loss_ce: 0.001207, loss_dice: 0.094216
[11:26:20.807] TRAIN: iteration 22531 : loss : 0.249520, loss_ce: 0.003265, loss_dice: 0.495775
[11:26:28.032] TRAIN: iteration 22532 : loss : 0.251996, loss_ce: 0.003713, loss_dice: 0.500278
[11:26:28.240] TRAIN: iteration 22533 : loss : 0.081727, loss_ce: 0.001755, loss_dice: 0.161699
[11:26:28.449] TRAIN: iteration 22534 : loss : 0.132302, loss_ce: 0.006601, loss_dice: 0.258003
[11:26:28.660] TRAIN: iteration 22535 : loss : 0.023708, loss_ce: 0.003312, loss_dice: 0.044104
[11:26:28.867] TRAIN: iteration 22536 : loss : 0.065031, loss_ce: 0.001785, loss_dice: 0.128278
[11:26:29.077] TRAIN: iteration 22537 : loss : 0.203011, loss_ce: 0.002703, loss_dice: 0.403319
[11:26:29.285] TRAIN: iteration 22538 : loss : 0.093377, loss_ce: 0.004319, loss_dice: 0.182435
[11:26:29.495] TRAIN: iteration 22539 : loss : 0.207921, loss_ce: 0.002461, loss_dice: 0.413382
[11:26:37.655] TRAIN: iteration 22540 : loss : 0.061869, loss_ce: 0.008157, loss_dice: 0.115581
[11:26:37.893] TRAIN: iteration 22541 : loss : 0.175679, loss_ce: 0.002283, loss_dice: 0.349074
[11:26:38.100] TRAIN: iteration 22542 : loss : 0.068248, loss_ce: 0.002252, loss_dice: 0.134243
[11:26:38.308] TRAIN: iteration 22543 : loss : 0.160801, loss_ce: 0.004258, loss_dice: 0.317343
[11:26:38.519] TRAIN: iteration 22544 : loss : 0.109062, loss_ce: 0.001693, loss_dice: 0.216432
[11:26:38.726] TRAIN: iteration 22545 : loss : 0.082136, loss_ce: 0.002414, loss_dice: 0.161857
[11:26:38.932] TRAIN: iteration 22546 : loss : 0.142730, loss_ce: 0.001314, loss_dice: 0.284145
[11:26:39.140] TRAIN: iteration 22547 : loss : 0.102464, loss_ce: 0.001592, loss_dice: 0.203336
[11:26:46.517] TRAIN: iteration 22548 : loss : 0.147127, loss_ce: 0.007455, loss_dice: 0.286798
[11:26:48.253] TRAIN: iteration 22549 : loss : 0.086432, loss_ce: 0.004174, loss_dice: 0.168690
[11:26:48.460] TRAIN: iteration 22550 : loss : 0.147800, loss_ce: 0.003938, loss_dice: 0.291663
[11:26:48.668] TRAIN: iteration 22551 : loss : 0.091058, loss_ce: 0.004137, loss_dice: 0.177979
[11:26:48.875] TRAIN: iteration 22552 : loss : 0.078161, loss_ce: 0.005788, loss_dice: 0.150533
[11:26:49.083] TRAIN: iteration 22553 : loss : 0.140982, loss_ce: 0.002753, loss_dice: 0.279210
[11:26:49.290] TRAIN: iteration 22554 : loss : 0.098872, loss_ce: 0.012263, loss_dice: 0.185481
[11:26:49.386] TRAIN: iteration 22555 : loss : 0.250280, loss_ce: 0.000547, loss_dice: 0.500012
[11:32:14.570] VALIDATION: iteration 12 : loss : 0.117578, loss_ce: 0.004372, loss_dice: 0.230783
[11:32:15.288] TRAIN: iteration 22556 : loss : 0.132640, loss_ce: 0.003994, loss_dice: 0.261285
[11:32:16.635] TRAIN: iteration 22557 : loss : 0.043365, loss_ce: 0.008137, loss_dice: 0.078592
[11:32:16.846] TRAIN: iteration 22558 : loss : 0.060801, loss_ce: 0.002676, loss_dice: 0.118926
[11:32:17.056] TRAIN: iteration 22559 : loss : 0.050883, loss_ce: 0.001614, loss_dice: 0.100151
[11:32:17.273] TRAIN: iteration 22560 : loss : 0.159248, loss_ce: 0.004645, loss_dice: 0.313850
[11:32:18.086] TRAIN: iteration 22561 : loss : 0.070362, loss_ce: 0.002366, loss_dice: 0.138357
[11:32:18.297] TRAIN: iteration 22562 : loss : 0.173969, loss_ce: 0.006482, loss_dice: 0.341457
[11:32:18.507] TRAIN: iteration 22563 : loss : 0.037347, loss_ce: 0.001903, loss_dice: 0.072792
[11:32:18.717] TRAIN: iteration 22564 : loss : 0.141623, loss_ce: 0.001562, loss_dice: 0.281684
[11:32:18.927] TRAIN: iteration 22565 : loss : 0.233971, loss_ce: 0.003425, loss_dice: 0.464517
[11:32:19.140] TRAIN: iteration 22566 : loss : 0.108447, loss_ce: 0.001775, loss_dice: 0.215119
[11:32:19.351] TRAIN: iteration 22567 : loss : 0.030271, loss_ce: 0.001366, loss_dice: 0.059175
[11:32:19.570] TRAIN: iteration 22568 : loss : 0.149761, loss_ce: 0.002026, loss_dice: 0.297497
[11:32:19.778] TRAIN: iteration 22569 : loss : 0.107893, loss_ce: 0.001524, loss_dice: 0.214262
[11:32:19.986] TRAIN: iteration 22570 : loss : 0.066663, loss_ce: 0.005063, loss_dice: 0.128263
[11:32:20.195] TRAIN: iteration 22571 : loss : 0.079326, loss_ce: 0.003371, loss_dice: 0.155282
[11:32:20.408] TRAIN: iteration 22572 : loss : 0.058371, loss_ce: 0.001880, loss_dice: 0.114861
[11:32:20.621] TRAIN: iteration 22573 : loss : 0.247885, loss_ce: 0.001576, loss_dice: 0.494193
[11:32:20.831] TRAIN: iteration 22574 : loss : 0.077239, loss_ce: 0.007071, loss_dice: 0.147407
[11:32:21.136] TRAIN: iteration 22575 : loss : 0.049485, loss_ce: 0.002317, loss_dice: 0.096652
[11:32:21.348] TRAIN: iteration 22576 : loss : 0.183896, loss_ce: 0.001068, loss_dice: 0.366723
[11:32:21.557] TRAIN: iteration 22577 : loss : 0.032405, loss_ce: 0.000668, loss_dice: 0.064143
[11:32:21.768] TRAIN: iteration 22578 : loss : 0.250398, loss_ce: 0.000772, loss_dice: 0.500023
[11:32:21.983] TRAIN: iteration 22579 : loss : 0.080041, loss_ce: 0.001773, loss_dice: 0.158309
[11:32:22.200] TRAIN: iteration 22580 : loss : 0.087587, loss_ce: 0.001333, loss_dice: 0.173840
[11:32:22.435] TRAIN: iteration 22581 : loss : 0.185981, loss_ce: 0.022332, loss_dice: 0.349629
[11:32:22.647] TRAIN: iteration 22582 : loss : 0.149593, loss_ce: 0.005059, loss_dice: 0.294127
[11:32:22.856] TRAIN: iteration 22583 : loss : 0.125263, loss_ce: 0.008019, loss_dice: 0.242508
[11:32:23.065] TRAIN: iteration 22584 : loss : 0.070675, loss_ce: 0.006135, loss_dice: 0.135215
[11:32:23.276] TRAIN: iteration 22585 : loss : 0.047119, loss_ce: 0.001455, loss_dice: 0.092783
[11:32:23.489] TRAIN: iteration 22586 : loss : 0.189722, loss_ce: 0.002068, loss_dice: 0.377376
[11:32:23.700] TRAIN: iteration 22587 : loss : 0.117527, loss_ce: 0.006360, loss_dice: 0.228693
[11:32:23.914] TRAIN: iteration 22588 : loss : 0.027397, loss_ce: 0.001547, loss_dice: 0.053247
[11:32:24.127] TRAIN: iteration 22589 : loss : 0.136626, loss_ce: 0.005875, loss_dice: 0.267377
[11:32:24.336] TRAIN: iteration 22590 : loss : 0.174611, loss_ce: 0.002224, loss_dice: 0.346998
[11:32:24.544] TRAIN: iteration 22591 : loss : 0.084167, loss_ce: 0.003598, loss_dice: 0.164737
[11:32:24.753] TRAIN: iteration 22592 : loss : 0.082518, loss_ce: 0.004465, loss_dice: 0.160571
[11:32:24.962] TRAIN: iteration 22593 : loss : 0.070145, loss_ce: 0.005967, loss_dice: 0.134324
[11:32:25.177] TRAIN: iteration 22594 : loss : 0.091690, loss_ce: 0.001352, loss_dice: 0.182028
[11:32:25.396] TRAIN: iteration 22595 : loss : 0.029206, loss_ce: 0.000962, loss_dice: 0.057450
[11:32:25.605] TRAIN: iteration 22596 : loss : 0.250715, loss_ce: 0.003171, loss_dice: 0.498259
[11:32:25.813] TRAIN: iteration 22597 : loss : 0.185863, loss_ce: 0.002483, loss_dice: 0.369243
[11:32:26.035] TRAIN: iteration 22598 : loss : 0.121416, loss_ce: 0.001809, loss_dice: 0.241023
[11:32:26.246] TRAIN: iteration 22599 : loss : 0.133846, loss_ce: 0.009315, loss_dice: 0.258377
[11:32:26.462] TRAIN: iteration 22600 : loss : 0.036998, loss_ce: 0.001698, loss_dice: 0.072297
[11:32:26.705] TRAIN: iteration 22601 : loss : 0.064048, loss_ce: 0.004883, loss_dice: 0.123214
[11:32:26.914] TRAIN: iteration 22602 : loss : 0.250688, loss_ce: 0.001299, loss_dice: 0.500076
[11:32:27.125] TRAIN: iteration 22603 : loss : 0.071181, loss_ce: 0.004258, loss_dice: 0.138105
[11:32:27.342] TRAIN: iteration 22604 : loss : 0.085430, loss_ce: 0.001261, loss_dice: 0.169598
[11:32:27.551] TRAIN: iteration 22605 : loss : 0.123184, loss_ce: 0.006580, loss_dice: 0.239787
[11:32:27.760] TRAIN: iteration 22606 : loss : 0.039961, loss_ce: 0.001178, loss_dice: 0.078744
[11:32:28.056] TRAIN: iteration 22607 : loss : 0.186512, loss_ce: 0.003650, loss_dice: 0.369373
[11:32:28.264] TRAIN: iteration 22608 : loss : 0.054362, loss_ce: 0.002047, loss_dice: 0.106676
[11:32:28.475] TRAIN: iteration 22609 : loss : 0.126034, loss_ce: 0.007974, loss_dice: 0.244094
[11:32:28.687] TRAIN: iteration 22610 : loss : 0.153520, loss_ce: 0.009965, loss_dice: 0.297076
[11:32:28.896] TRAIN: iteration 22611 : loss : 0.170587, loss_ce: 0.003452, loss_dice: 0.337722
[11:32:29.107] TRAIN: iteration 22612 : loss : 0.244959, loss_ce: 0.008797, loss_dice: 0.481121
[11:32:29.317] TRAIN: iteration 22613 : loss : 0.128831, loss_ce: 0.004345, loss_dice: 0.253316
[11:32:29.527] TRAIN: iteration 22614 : loss : 0.143549, loss_ce: 0.001805, loss_dice: 0.285293
[11:32:29.736] TRAIN: iteration 22615 : loss : 0.179390, loss_ce: 0.002566, loss_dice: 0.356213
[11:32:29.948] TRAIN: iteration 22616 : loss : 0.086831, loss_ce: 0.002768, loss_dice: 0.170893
[11:32:30.161] TRAIN: iteration 22617 : loss : 0.184141, loss_ce: 0.007670, loss_dice: 0.360612
[11:32:30.375] TRAIN: iteration 22618 : loss : 0.048602, loss_ce: 0.002302, loss_dice: 0.094903
[11:32:30.588] TRAIN: iteration 22619 : loss : 0.251194, loss_ce: 0.002259, loss_dice: 0.500129
[11:32:30.899] TRAIN: iteration 22620 : loss : 0.115010, loss_ce: 0.005365, loss_dice: 0.224656
[11:32:31.145] TRAIN: iteration 22621 : loss : 0.040196, loss_ce: 0.001015, loss_dice: 0.079377
[11:32:31.356] TRAIN: iteration 22622 : loss : 0.033952, loss_ce: 0.001326, loss_dice: 0.066579
[11:32:31.565] TRAIN: iteration 22623 : loss : 0.154177, loss_ce: 0.010700, loss_dice: 0.297655
[11:32:31.774] TRAIN: iteration 22624 : loss : 0.157052, loss_ce: 0.005717, loss_dice: 0.308388
[11:32:31.984] TRAIN: iteration 22625 : loss : 0.219112, loss_ce: 0.020481, loss_dice: 0.417742
[11:32:32.197] TRAIN: iteration 22626 : loss : 0.096855, loss_ce: 0.004785, loss_dice: 0.188924
[11:32:32.407] TRAIN: iteration 22627 : loss : 0.114915, loss_ce: 0.002186, loss_dice: 0.227644
[11:32:32.616] TRAIN: iteration 22628 : loss : 0.251491, loss_ce: 0.002796, loss_dice: 0.500186
[11:32:32.825] TRAIN: iteration 22629 : loss : 0.097744, loss_ce: 0.003330, loss_dice: 0.192157
[11:32:33.033] TRAIN: iteration 22630 : loss : 0.249141, loss_ce: 0.001582, loss_dice: 0.496701
[11:32:33.243] TRAIN: iteration 22631 : loss : 0.097914, loss_ce: 0.006359, loss_dice: 0.189468
[11:32:33.451] TRAIN: iteration 22632 : loss : 0.076515, loss_ce: 0.001936, loss_dice: 0.151095
[11:32:33.660] TRAIN: iteration 22633 : loss : 0.113477, loss_ce: 0.005823, loss_dice: 0.221132
[11:32:33.868] TRAIN: iteration 22634 : loss : 0.100834, loss_ce: 0.011414, loss_dice: 0.190254
[11:32:34.080] TRAIN: iteration 22635 : loss : 0.251654, loss_ce: 0.003290, loss_dice: 0.500018
[11:32:34.290] TRAIN: iteration 22636 : loss : 0.251390, loss_ce: 0.002589, loss_dice: 0.500190
[11:32:34.500] TRAIN: iteration 22637 : loss : 0.054504, loss_ce: 0.002769, loss_dice: 0.106239
[11:32:34.710] TRAIN: iteration 22638 : loss : 0.164280, loss_ce: 0.030663, loss_dice: 0.297898
[11:32:34.921] TRAIN: iteration 22639 : loss : 0.251360, loss_ce: 0.003654, loss_dice: 0.499065
[11:32:35.132] TRAIN: iteration 22640 : loss : 0.081426, loss_ce: 0.003014, loss_dice: 0.159838
[11:32:35.373] TRAIN: iteration 22641 : loss : 0.085447, loss_ce: 0.001222, loss_dice: 0.169672
[11:32:35.583] TRAIN: iteration 22642 : loss : 0.082808, loss_ce: 0.004182, loss_dice: 0.161435
[11:32:35.879] TRAIN: iteration 22643 : loss : 0.221664, loss_ce: 0.002523, loss_dice: 0.440806
[11:32:36.090] TRAIN: iteration 22644 : loss : 0.045565, loss_ce: 0.001787, loss_dice: 0.089342
[11:32:36.300] TRAIN: iteration 22645 : loss : 0.049745, loss_ce: 0.001788, loss_dice: 0.097701
[11:32:36.509] TRAIN: iteration 22646 : loss : 0.195156, loss_ce: 0.002981, loss_dice: 0.387330
[11:32:36.720] TRAIN: iteration 22647 : loss : 0.087765, loss_ce: 0.003095, loss_dice: 0.172435
[11:32:36.937] TRAIN: iteration 22648 : loss : 0.055276, loss_ce: 0.003063, loss_dice: 0.107489
[11:32:37.146] TRAIN: iteration 22649 : loss : 0.149475, loss_ce: 0.006427, loss_dice: 0.292523
[11:32:37.354] TRAIN: iteration 22650 : loss : 0.069655, loss_ce: 0.009873, loss_dice: 0.129438
[11:32:37.565] TRAIN: iteration 22651 : loss : 0.086401, loss_ce: 0.004547, loss_dice: 0.168255
[11:32:37.779] TRAIN: iteration 22652 : loss : 0.259057, loss_ce: 0.017997, loss_dice: 0.500118
[11:32:37.989] TRAIN: iteration 22653 : loss : 0.116357, loss_ce: 0.004624, loss_dice: 0.228089
[11:32:38.202] TRAIN: iteration 22654 : loss : 0.250979, loss_ce: 0.001843, loss_dice: 0.500114
[11:32:38.411] TRAIN: iteration 22655 : loss : 0.250524, loss_ce: 0.001004, loss_dice: 0.500043
[11:32:38.625] TRAIN: iteration 22656 : loss : 0.054417, loss_ce: 0.002370, loss_dice: 0.106464
[11:32:38.836] TRAIN: iteration 22657 : loss : 0.087313, loss_ce: 0.003953, loss_dice: 0.170673
[11:32:39.045] TRAIN: iteration 22658 : loss : 0.035810, loss_ce: 0.003348, loss_dice: 0.068273
[11:32:39.260] TRAIN: iteration 22659 : loss : 0.043659, loss_ce: 0.004159, loss_dice: 0.083159
[11:32:39.476] TRAIN: iteration 22660 : loss : 0.114289, loss_ce: 0.001925, loss_dice: 0.226653
[11:32:39.728] TRAIN: iteration 22661 : loss : 0.207696, loss_ce: 0.004119, loss_dice: 0.411273
[11:32:39.937] TRAIN: iteration 22662 : loss : 0.043303, loss_ce: 0.004419, loss_dice: 0.082187
[11:32:40.147] TRAIN: iteration 22663 : loss : 0.102703, loss_ce: 0.010358, loss_dice: 0.195048
[11:32:40.359] TRAIN: iteration 22664 : loss : 0.105093, loss_ce: 0.001265, loss_dice: 0.208922
[11:32:40.570] TRAIN: iteration 22665 : loss : 0.105252, loss_ce: 0.001833, loss_dice: 0.208672
[11:32:40.779] TRAIN: iteration 22666 : loss : 0.055819, loss_ce: 0.001173, loss_dice: 0.110466
[11:32:40.988] TRAIN: iteration 22667 : loss : 0.126757, loss_ce: 0.003931, loss_dice: 0.249584
[11:32:41.197] TRAIN: iteration 22668 : loss : 0.060745, loss_ce: 0.001357, loss_dice: 0.120133
[11:32:41.406] TRAIN: iteration 22669 : loss : 0.056055, loss_ce: 0.006020, loss_dice: 0.106090
[11:32:41.621] TRAIN: iteration 22670 : loss : 0.067061, loss_ce: 0.001126, loss_dice: 0.132995
[11:32:41.831] TRAIN: iteration 22671 : loss : 0.131306, loss_ce: 0.011403, loss_dice: 0.251209
[11:32:42.044] TRAIN: iteration 22672 : loss : 0.083754, loss_ce: 0.002531, loss_dice: 0.164976
[11:32:42.253] TRAIN: iteration 22673 : loss : 0.147666, loss_ce: 0.003392, loss_dice: 0.291941
[11:32:42.463] TRAIN: iteration 22674 : loss : 0.129154, loss_ce: 0.003342, loss_dice: 0.254967
[11:32:42.673] TRAIN: iteration 22675 : loss : 0.250286, loss_ce: 0.003131, loss_dice: 0.497441
[11:32:42.883] TRAIN: iteration 22676 : loss : 0.120288, loss_ce: 0.003386, loss_dice: 0.237190
[11:32:43.092] TRAIN: iteration 22677 : loss : 0.193244, loss_ce: 0.002120, loss_dice: 0.384367
[11:32:43.302] TRAIN: iteration 22678 : loss : 0.249204, loss_ce: 0.001644, loss_dice: 0.496764
[11:32:43.510] TRAIN: iteration 22679 : loss : 0.059903, loss_ce: 0.002691, loss_dice: 0.117114
[11:32:43.718] TRAIN: iteration 22680 : loss : 0.071095, loss_ce: 0.004136, loss_dice: 0.138053
[11:32:43.955] TRAIN: iteration 22681 : loss : 0.145378, loss_ce: 0.002945, loss_dice: 0.287811
[11:32:44.165] TRAIN: iteration 22682 : loss : 0.145421, loss_ce: 0.004582, loss_dice: 0.286259
[11:32:44.373] TRAIN: iteration 22683 : loss : 0.085774, loss_ce: 0.007159, loss_dice: 0.164388
[11:32:44.581] TRAIN: iteration 22684 : loss : 0.070467, loss_ce: 0.007422, loss_dice: 0.133511
[11:32:44.797] TRAIN: iteration 22685 : loss : 0.127009, loss_ce: 0.007220, loss_dice: 0.246798
[11:32:45.006] TRAIN: iteration 22686 : loss : 0.251798, loss_ce: 0.012664, loss_dice: 0.490933
[11:32:45.226] TRAIN: iteration 22687 : loss : 0.078070, loss_ce: 0.002017, loss_dice: 0.154122
[11:32:45.434] TRAIN: iteration 22688 : loss : 0.067315, loss_ce: 0.009067, loss_dice: 0.125563
[11:32:45.650] TRAIN: iteration 22689 : loss : 0.163578, loss_ce: 0.024687, loss_dice: 0.302468
[11:32:45.859] TRAIN: iteration 22690 : loss : 0.060793, loss_ce: 0.008222, loss_dice: 0.113365
[11:32:46.071] TRAIN: iteration 22691 : loss : 0.039367, loss_ce: 0.001068, loss_dice: 0.077666
[11:32:46.280] TRAIN: iteration 22692 : loss : 0.039534, loss_ce: 0.002445, loss_dice: 0.076623
[11:32:46.489] TRAIN: iteration 22693 : loss : 0.027793, loss_ce: 0.002449, loss_dice: 0.053137
[11:32:46.702] TRAIN: iteration 22694 : loss : 0.066759, loss_ce: 0.003223, loss_dice: 0.130295
[11:32:46.912] TRAIN: iteration 22695 : loss : 0.069517, loss_ce: 0.003912, loss_dice: 0.135122
[11:32:47.124] TRAIN: iteration 22696 : loss : 0.162233, loss_ce: 0.003734, loss_dice: 0.320732
[11:32:47.332] TRAIN: iteration 22697 : loss : 0.091009, loss_ce: 0.004791, loss_dice: 0.177228
[11:32:47.917] TRAIN: iteration 22698 : loss : 0.120192, loss_ce: 0.009759, loss_dice: 0.230624
[11:32:48.131] TRAIN: iteration 22699 : loss : 0.121536, loss_ce: 0.002055, loss_dice: 0.241017
[11:32:48.340] TRAIN: iteration 22700 : loss : 0.131189, loss_ce: 0.003209, loss_dice: 0.259169
[11:32:48.578] TRAIN: iteration 22701 : loss : 0.069356, loss_ce: 0.001969, loss_dice: 0.136742
[11:32:48.788] TRAIN: iteration 22702 : loss : 0.246194, loss_ce: 0.002979, loss_dice: 0.489409
[11:32:48.998] TRAIN: iteration 22703 : loss : 0.051581, loss_ce: 0.004868, loss_dice: 0.098294
[11:32:49.213] TRAIN: iteration 22704 : loss : 0.075311, loss_ce: 0.004285, loss_dice: 0.146337
[11:32:49.422] TRAIN: iteration 22705 : loss : 0.134580, loss_ce: 0.001917, loss_dice: 0.267243
[11:32:49.632] TRAIN: iteration 22706 : loss : 0.037153, loss_ce: 0.003092, loss_dice: 0.071214
[11:32:49.841] TRAIN: iteration 22707 : loss : 0.045180, loss_ce: 0.006480, loss_dice: 0.083880
[11:32:50.053] TRAIN: iteration 22708 : loss : 0.046260, loss_ce: 0.001592, loss_dice: 0.090928
[11:32:50.263] TRAIN: iteration 22709 : loss : 0.250645, loss_ce: 0.001223, loss_dice: 0.500067
[11:32:50.477] TRAIN: iteration 22710 : loss : 0.250540, loss_ce: 0.001025, loss_dice: 0.500056
[11:32:50.687] TRAIN: iteration 22711 : loss : 0.089734, loss_ce: 0.001455, loss_dice: 0.178013
[11:32:50.896] TRAIN: iteration 22712 : loss : 0.102024, loss_ce: 0.010153, loss_dice: 0.193895
[11:32:51.106] TRAIN: iteration 22713 : loss : 0.050739, loss_ce: 0.001162, loss_dice: 0.100317
[11:32:51.317] TRAIN: iteration 22714 : loss : 0.086129, loss_ce: 0.002555, loss_dice: 0.169702
[11:32:51.536] TRAIN: iteration 22715 : loss : 0.056123, loss_ce: 0.003680, loss_dice: 0.108566
[11:32:51.744] TRAIN: iteration 22716 : loss : 0.187616, loss_ce: 0.005113, loss_dice: 0.370118
[11:32:51.952] TRAIN: iteration 22717 : loss : 0.077474, loss_ce: 0.001556, loss_dice: 0.153393
[11:32:52.163] TRAIN: iteration 22718 : loss : 0.147586, loss_ce: 0.005540, loss_dice: 0.289632
[11:32:52.380] TRAIN: iteration 22719 : loss : 0.041201, loss_ce: 0.002475, loss_dice: 0.079927
[11:32:52.601] TRAIN: iteration 22720 : loss : 0.250901, loss_ce: 0.001701, loss_dice: 0.500100
[11:32:52.848] TRAIN: iteration 22721 : loss : 0.046624, loss_ce: 0.002221, loss_dice: 0.091027
[11:32:53.059] TRAIN: iteration 22722 : loss : 0.154752, loss_ce: 0.002503, loss_dice: 0.307001
[11:32:53.269] TRAIN: iteration 22723 : loss : 0.168805, loss_ce: 0.002121, loss_dice: 0.335489
[11:32:53.478] TRAIN: iteration 22724 : loss : 0.249951, loss_ce: 0.004558, loss_dice: 0.495344
[11:32:53.688] TRAIN: iteration 22725 : loss : 0.047849, loss_ce: 0.001527, loss_dice: 0.094171
[11:32:53.897] TRAIN: iteration 22726 : loss : 0.051749, loss_ce: 0.002923, loss_dice: 0.100575
[11:32:54.107] TRAIN: iteration 22727 : loss : 0.163662, loss_ce: 0.012170, loss_dice: 0.315153
[11:32:54.318] TRAIN: iteration 22728 : loss : 0.039314, loss_ce: 0.000969, loss_dice: 0.077659
[11:32:54.528] TRAIN: iteration 22729 : loss : 0.244506, loss_ce: 0.002064, loss_dice: 0.486948
[11:32:54.737] TRAIN: iteration 22730 : loss : 0.251002, loss_ce: 0.001880, loss_dice: 0.500124
[11:32:54.948] TRAIN: iteration 22731 : loss : 0.129234, loss_ce: 0.003851, loss_dice: 0.254617
[11:32:55.164] TRAIN: iteration 22732 : loss : 0.118485, loss_ce: 0.005288, loss_dice: 0.231682
[11:32:55.401] TRAIN: iteration 22733 : loss : 0.044324, loss_ce: 0.005311, loss_dice: 0.083337
[11:32:55.609] TRAIN: iteration 22734 : loss : 0.250697, loss_ce: 0.001314, loss_dice: 0.500080
[11:32:55.821] TRAIN: iteration 22735 : loss : 0.184204, loss_ce: 0.001252, loss_dice: 0.367156
[11:32:56.030] TRAIN: iteration 22736 : loss : 0.147563, loss_ce: 0.001905, loss_dice: 0.293221
[11:32:56.239] TRAIN: iteration 22737 : loss : 0.135994, loss_ce: 0.002398, loss_dice: 0.269591
[11:32:56.448] TRAIN: iteration 22738 : loss : 0.120062, loss_ce: 0.006621, loss_dice: 0.233504
[11:32:56.658] TRAIN: iteration 22739 : loss : 0.044438, loss_ce: 0.001048, loss_dice: 0.087829
[11:32:56.870] TRAIN: iteration 22740 : loss : 0.049501, loss_ce: 0.005829, loss_dice: 0.093173
[11:32:57.118] TRAIN: iteration 22741 : loss : 0.101040, loss_ce: 0.004064, loss_dice: 0.198017
[11:32:57.329] TRAIN: iteration 22742 : loss : 0.226633, loss_ce: 0.007785, loss_dice: 0.445480
[11:32:57.537] TRAIN: iteration 22743 : loss : 0.018555, loss_ce: 0.002099, loss_dice: 0.035010
[11:32:57.749] TRAIN: iteration 22744 : loss : 0.249578, loss_ce: 0.002176, loss_dice: 0.496980
[11:32:57.959] TRAIN: iteration 22745 : loss : 0.055060, loss_ce: 0.001904, loss_dice: 0.108216
[11:32:58.170] TRAIN: iteration 22746 : loss : 0.086160, loss_ce: 0.006885, loss_dice: 0.165436
[11:32:58.381] TRAIN: iteration 22747 : loss : 0.245569, loss_ce: 0.002345, loss_dice: 0.488792
[11:32:58.590] TRAIN: iteration 22748 : loss : 0.246724, loss_ce: 0.008155, loss_dice: 0.485294
[11:32:58.799] TRAIN: iteration 22749 : loss : 0.071042, loss_ce: 0.002832, loss_dice: 0.139252
[11:32:59.009] TRAIN: iteration 22750 : loss : 0.044306, loss_ce: 0.003191, loss_dice: 0.085422
[11:32:59.220] TRAIN: iteration 22751 : loss : 0.250798, loss_ce: 0.001528, loss_dice: 0.500068
[11:32:59.428] TRAIN: iteration 22752 : loss : 0.116122, loss_ce: 0.009165, loss_dice: 0.223080
[11:32:59.671] TRAIN: iteration 22753 : loss : 0.064564, loss_ce: 0.001389, loss_dice: 0.127738
[11:32:59.885] TRAIN: iteration 22754 : loss : 0.087316, loss_ce: 0.004747, loss_dice: 0.169886
[11:33:00.097] TRAIN: iteration 22755 : loss : 0.246562, loss_ce: 0.002079, loss_dice: 0.491044
[11:33:00.306] TRAIN: iteration 22756 : loss : 0.062792, loss_ce: 0.003768, loss_dice: 0.121815
[11:33:00.520] TRAIN: iteration 22757 : loss : 0.251248, loss_ce: 0.002354, loss_dice: 0.500143
[11:33:00.735] TRAIN: iteration 22758 : loss : 0.068718, loss_ce: 0.004061, loss_dice: 0.133376
[11:33:00.943] TRAIN: iteration 22759 : loss : 0.138229, loss_ce: 0.003647, loss_dice: 0.272811
[11:33:01.153] TRAIN: iteration 22760 : loss : 0.242866, loss_ce: 0.006765, loss_dice: 0.478966
[11:33:01.397] TRAIN: iteration 22761 : loss : 0.059374, loss_ce: 0.007369, loss_dice: 0.111379
[11:33:01.642] TRAIN: iteration 22762 : loss : 0.094214, loss_ce: 0.003702, loss_dice: 0.184726
[11:33:01.851] TRAIN: iteration 22763 : loss : 0.093592, loss_ce: 0.004513, loss_dice: 0.182671
[11:33:02.063] TRAIN: iteration 22764 : loss : 0.068096, loss_ce: 0.007120, loss_dice: 0.129072
[11:33:02.271] TRAIN: iteration 22765 : loss : 0.060078, loss_ce: 0.004580, loss_dice: 0.115576
[11:33:02.484] TRAIN: iteration 22766 : loss : 0.127195, loss_ce: 0.006103, loss_dice: 0.248286
[11:33:02.694] TRAIN: iteration 22767 : loss : 0.145452, loss_ce: 0.002612, loss_dice: 0.288291
[11:33:02.903] TRAIN: iteration 22768 : loss : 0.070622, loss_ce: 0.005966, loss_dice: 0.135279
[11:33:03.115] TRAIN: iteration 22769 : loss : 0.164968, loss_ce: 0.005575, loss_dice: 0.324362
[11:33:03.325] TRAIN: iteration 22770 : loss : 0.250452, loss_ce: 0.003405, loss_dice: 0.497499
[11:33:03.534] TRAIN: iteration 22771 : loss : 0.065541, loss_ce: 0.003926, loss_dice: 0.127156
[11:33:03.743] TRAIN: iteration 22772 : loss : 0.114991, loss_ce: 0.002916, loss_dice: 0.227065
[11:33:03.953] TRAIN: iteration 22773 : loss : 0.182680, loss_ce: 0.001884, loss_dice: 0.363476
[11:33:04.163] TRAIN: iteration 22774 : loss : 0.081296, loss_ce: 0.006116, loss_dice: 0.156475
[11:33:04.380] TRAIN: iteration 22775 : loss : 0.250903, loss_ce: 0.001730, loss_dice: 0.500077
[11:33:04.589] TRAIN: iteration 22776 : loss : 0.067279, loss_ce: 0.001313, loss_dice: 0.133246
[11:33:04.797] TRAIN: iteration 22777 : loss : 0.240658, loss_ce: 0.003690, loss_dice: 0.477627
[11:33:05.006] TRAIN: iteration 22778 : loss : 0.077895, loss_ce: 0.011904, loss_dice: 0.143887
[11:33:05.221] TRAIN: iteration 22779 : loss : 0.074542, loss_ce: 0.007238, loss_dice: 0.141846
[11:33:05.434] TRAIN: iteration 22780 : loss : 0.118797, loss_ce: 0.006746, loss_dice: 0.230849
[11:33:05.672] TRAIN: iteration 22781 : loss : 0.049283, loss_ce: 0.002853, loss_dice: 0.095713
[11:33:05.880] TRAIN: iteration 22782 : loss : 0.162908, loss_ce: 0.001877, loss_dice: 0.323939
[11:33:06.105] TRAIN: iteration 22783 : loss : 0.145327, loss_ce: 0.002832, loss_dice: 0.287821
[11:33:06.319] TRAIN: iteration 22784 : loss : 0.250875, loss_ce: 0.001653, loss_dice: 0.500097
[11:33:06.531] TRAIN: iteration 22785 : loss : 0.215006, loss_ce: 0.004921, loss_dice: 0.425090
[11:33:06.751] TRAIN: iteration 22786 : loss : 0.040245, loss_ce: 0.002002, loss_dice: 0.078489
[11:33:06.962] TRAIN: iteration 22787 : loss : 0.053269, loss_ce: 0.000888, loss_dice: 0.105650
[11:33:07.174] TRAIN: iteration 22788 : loss : 0.045194, loss_ce: 0.003194, loss_dice: 0.087195
[11:33:07.384] TRAIN: iteration 22789 : loss : 0.060719, loss_ce: 0.003189, loss_dice: 0.118249
[11:33:07.597] TRAIN: iteration 22790 : loss : 0.132782, loss_ce: 0.005899, loss_dice: 0.259665
[11:33:07.809] TRAIN: iteration 22791 : loss : 0.125545, loss_ce: 0.002738, loss_dice: 0.248352
[11:33:08.019] TRAIN: iteration 22792 : loss : 0.250344, loss_ce: 0.000679, loss_dice: 0.500008
[11:33:08.228] TRAIN: iteration 22793 : loss : 0.120633, loss_ce: 0.003530, loss_dice: 0.237737
[11:33:08.447] TRAIN: iteration 22794 : loss : 0.022555, loss_ce: 0.001307, loss_dice: 0.043803
[11:33:08.656] TRAIN: iteration 22795 : loss : 0.038429, loss_ce: 0.003109, loss_dice: 0.073749
[11:33:08.867] TRAIN: iteration 22796 : loss : 0.099969, loss_ce: 0.003889, loss_dice: 0.196049
[11:33:09.075] TRAIN: iteration 22797 : loss : 0.034794, loss_ce: 0.001022, loss_dice: 0.068565
[11:33:09.288] TRAIN: iteration 22798 : loss : 0.222450, loss_ce: 0.035592, loss_dice: 0.409308
[11:33:09.496] TRAIN: iteration 22799 : loss : 0.118976, loss_ce: 0.002708, loss_dice: 0.235245
[11:33:09.705] TRAIN: iteration 22800 : loss : 0.250653, loss_ce: 0.001236, loss_dice: 0.500069
[11:33:09.942] TRAIN: iteration 22801 : loss : 0.148841, loss_ce: 0.011674, loss_dice: 0.286009
[11:33:10.151] TRAIN: iteration 22802 : loss : 0.113484, loss_ce: 0.002025, loss_dice: 0.224943
[11:33:10.363] TRAIN: iteration 22803 : loss : 0.067310, loss_ce: 0.002262, loss_dice: 0.132358
[11:33:10.572] TRAIN: iteration 22804 : loss : 0.044047, loss_ce: 0.002584, loss_dice: 0.085510
[11:33:10.789] TRAIN: iteration 22805 : loss : 0.040348, loss_ce: 0.004303, loss_dice: 0.076393
[11:33:11.000] TRAIN: iteration 22806 : loss : 0.094086, loss_ce: 0.005165, loss_dice: 0.183008
[11:33:11.210] TRAIN: iteration 22807 : loss : 0.080961, loss_ce: 0.008551, loss_dice: 0.153370
[11:33:11.420] TRAIN: iteration 22808 : loss : 0.039332, loss_ce: 0.003381, loss_dice: 0.075282
[11:33:11.629] TRAIN: iteration 22809 : loss : 0.237368, loss_ce: 0.002353, loss_dice: 0.472383
[11:33:11.838] TRAIN: iteration 22810 : loss : 0.251382, loss_ce: 0.002603, loss_dice: 0.500161
[11:33:12.054] TRAIN: iteration 22811 : loss : 0.251306, loss_ce: 0.002452, loss_dice: 0.500159
[11:33:12.268] TRAIN: iteration 22812 : loss : 0.167804, loss_ce: 0.006598, loss_dice: 0.329009
[11:33:12.484] TRAIN: iteration 22813 : loss : 0.047672, loss_ce: 0.008317, loss_dice: 0.087028
[11:33:12.697] TRAIN: iteration 22814 : loss : 0.250632, loss_ce: 0.001227, loss_dice: 0.500038
[11:33:12.908] TRAIN: iteration 22815 : loss : 0.157231, loss_ce: 0.002483, loss_dice: 0.311980
[11:33:13.117] TRAIN: iteration 22816 : loss : 0.095948, loss_ce: 0.004473, loss_dice: 0.187422
[11:33:13.328] TRAIN: iteration 22817 : loss : 0.069932, loss_ce: 0.006309, loss_dice: 0.133555
[11:33:13.539] TRAIN: iteration 22818 : loss : 0.250721, loss_ce: 0.001403, loss_dice: 0.500038
[11:33:13.754] TRAIN: iteration 22819 : loss : 0.068721, loss_ce: 0.005487, loss_dice: 0.131955
[11:33:13.963] TRAIN: iteration 22820 : loss : 0.250667, loss_ce: 0.001284, loss_dice: 0.500049
[11:33:14.199] TRAIN: iteration 22821 : loss : 0.146488, loss_ce: 0.023935, loss_dice: 0.269042
[11:33:14.411] TRAIN: iteration 22822 : loss : 0.126858, loss_ce: 0.006367, loss_dice: 0.247349
[11:33:14.623] TRAIN: iteration 22823 : loss : 0.250589, loss_ce: 0.001687, loss_dice: 0.499491
[11:33:14.840] TRAIN: iteration 22824 : loss : 0.085882, loss_ce: 0.004324, loss_dice: 0.167439
[11:33:15.051] TRAIN: iteration 22825 : loss : 0.049830, loss_ce: 0.001997, loss_dice: 0.097664
[11:33:15.260] TRAIN: iteration 22826 : loss : 0.038250, loss_ce: 0.000782, loss_dice: 0.075717
[11:33:15.470] TRAIN: iteration 22827 : loss : 0.250603, loss_ce: 0.001151, loss_dice: 0.500054
[11:33:15.685] TRAIN: iteration 22828 : loss : 0.041319, loss_ce: 0.002755, loss_dice: 0.079884
[11:33:15.901] TRAIN: iteration 22829 : loss : 0.100483, loss_ce: 0.001305, loss_dice: 0.199660
[11:33:16.110] TRAIN: iteration 22830 : loss : 0.249243, loss_ce: 0.002030, loss_dice: 0.496456
[11:33:16.319] TRAIN: iteration 22831 : loss : 0.250418, loss_ce: 0.000819, loss_dice: 0.500017
[11:33:16.529] TRAIN: iteration 22832 : loss : 0.059398, loss_ce: 0.001167, loss_dice: 0.117629
[11:33:16.737] TRAIN: iteration 22833 : loss : 0.163163, loss_ce: 0.001166, loss_dice: 0.325160
[11:33:16.946] TRAIN: iteration 22834 : loss : 0.111282, loss_ce: 0.002267, loss_dice: 0.220297
[11:33:17.156] TRAIN: iteration 22835 : loss : 0.075437, loss_ce: 0.002859, loss_dice: 0.148014
[11:33:17.368] TRAIN: iteration 22836 : loss : 0.088475, loss_ce: 0.001754, loss_dice: 0.175197
[11:33:17.578] TRAIN: iteration 22837 : loss : 0.030879, loss_ce: 0.000670, loss_dice: 0.061088
[11:33:17.791] TRAIN: iteration 22838 : loss : 0.127724, loss_ce: 0.012822, loss_dice: 0.242627
[11:33:18.006] TRAIN: iteration 22839 : loss : 0.073159, loss_ce: 0.001292, loss_dice: 0.145025
[11:33:18.217] TRAIN: iteration 22840 : loss : 0.073277, loss_ce: 0.003694, loss_dice: 0.142860
[11:33:18.452] TRAIN: iteration 22841 : loss : 0.084276, loss_ce: 0.005083, loss_dice: 0.163470
[11:33:18.666] TRAIN: iteration 22842 : loss : 0.065041, loss_ce: 0.000757, loss_dice: 0.129324
[11:33:18.878] TRAIN: iteration 22843 : loss : 0.161083, loss_ce: 0.002549, loss_dice: 0.319616
[11:33:19.095] TRAIN: iteration 22844 : loss : 0.064558, loss_ce: 0.000809, loss_dice: 0.128308
[11:33:19.304] TRAIN: iteration 22845 : loss : 0.220221, loss_ce: 0.001197, loss_dice: 0.439245
[11:33:19.515] TRAIN: iteration 22846 : loss : 0.059467, loss_ce: 0.006642, loss_dice: 0.112292
[11:33:19.724] TRAIN: iteration 22847 : loss : 0.250183, loss_ce: 0.000360, loss_dice: 0.500006
[11:33:19.933] TRAIN: iteration 22848 : loss : 0.192720, loss_ce: 0.011917, loss_dice: 0.373524
[11:33:20.143] TRAIN: iteration 22849 : loss : 0.121792, loss_ce: 0.005652, loss_dice: 0.237932
[11:33:20.354] TRAIN: iteration 22850 : loss : 0.250289, loss_ce: 0.002153, loss_dice: 0.498424
[11:33:20.562] TRAIN: iteration 22851 : loss : 0.250773, loss_ce: 0.001450, loss_dice: 0.500096
[11:33:20.771] TRAIN: iteration 22852 : loss : 0.241442, loss_ce: 0.008629, loss_dice: 0.474255
[11:33:20.980] TRAIN: iteration 22853 : loss : 0.251692, loss_ce: 0.003152, loss_dice: 0.500232
[11:33:21.189] TRAIN: iteration 22854 : loss : 0.027082, loss_ce: 0.002611, loss_dice: 0.051553
[11:33:21.398] TRAIN: iteration 22855 : loss : 0.131507, loss_ce: 0.004575, loss_dice: 0.258438
[11:33:21.607] TRAIN: iteration 22856 : loss : 0.179522, loss_ce: 0.002088, loss_dice: 0.356957
[11:33:21.816] TRAIN: iteration 22857 : loss : 0.057500, loss_ce: 0.006393, loss_dice: 0.108606
[11:33:22.029] TRAIN: iteration 22858 : loss : 0.060296, loss_ce: 0.001489, loss_dice: 0.119102
[11:33:22.241] TRAIN: iteration 22859 : loss : 0.131967, loss_ce: 0.004263, loss_dice: 0.259671
[11:33:22.452] TRAIN: iteration 22860 : loss : 0.180030, loss_ce: 0.002574, loss_dice: 0.357487
[11:33:22.694] TRAIN: iteration 22861 : loss : 0.167232, loss_ce: 0.003544, loss_dice: 0.330921
[11:33:22.903] TRAIN: iteration 22862 : loss : 0.108734, loss_ce: 0.004522, loss_dice: 0.212946
[11:33:23.114] TRAIN: iteration 22863 : loss : 0.043546, loss_ce: 0.007042, loss_dice: 0.080051
[11:33:23.341] TRAIN: iteration 22864 : loss : 0.054202, loss_ce: 0.002630, loss_dice: 0.105775
[11:33:23.550] TRAIN: iteration 22865 : loss : 0.047539, loss_ce: 0.003541, loss_dice: 0.091537
[11:33:23.759] TRAIN: iteration 22866 : loss : 0.254470, loss_ce: 0.009133, loss_dice: 0.499808
[11:33:23.975] TRAIN: iteration 22867 : loss : 0.076430, loss_ce: 0.004035, loss_dice: 0.148826
[11:33:24.184] TRAIN: iteration 22868 : loss : 0.250327, loss_ce: 0.000640, loss_dice: 0.500014
[11:33:24.393] TRAIN: iteration 22869 : loss : 0.234663, loss_ce: 0.005847, loss_dice: 0.463480
[11:33:24.604] TRAIN: iteration 22870 : loss : 0.124727, loss_ce: 0.008041, loss_dice: 0.241413
[11:33:24.812] TRAIN: iteration 22871 : loss : 0.039559, loss_ce: 0.001920, loss_dice: 0.077198
[11:33:25.022] TRAIN: iteration 22872 : loss : 0.045373, loss_ce: 0.000941, loss_dice: 0.089805
[11:33:25.230] TRAIN: iteration 22873 : loss : 0.250576, loss_ce: 0.001085, loss_dice: 0.500066
[11:33:25.439] TRAIN: iteration 22874 : loss : 0.135143, loss_ce: 0.003825, loss_dice: 0.266461
[11:33:25.647] TRAIN: iteration 22875 : loss : 0.027381, loss_ce: 0.000680, loss_dice: 0.054082
[11:33:25.856] TRAIN: iteration 22876 : loss : 0.081158, loss_ce: 0.007502, loss_dice: 0.154813
[11:33:26.066] TRAIN: iteration 22877 : loss : 0.228527, loss_ce: 0.001321, loss_dice: 0.455734
[11:33:26.275] TRAIN: iteration 22878 : loss : 0.063339, loss_ce: 0.002438, loss_dice: 0.124241
[11:33:26.491] TRAIN: iteration 22879 : loss : 0.046315, loss_ce: 0.002573, loss_dice: 0.090058
[11:33:26.703] TRAIN: iteration 22880 : loss : 0.232857, loss_ce: 0.002193, loss_dice: 0.463521
[11:33:26.942] TRAIN: iteration 22881 : loss : 0.251400, loss_ce: 0.002635, loss_dice: 0.500165
[11:33:27.158] TRAIN: iteration 22882 : loss : 0.051180, loss_ce: 0.001333, loss_dice: 0.101027
[11:33:27.369] TRAIN: iteration 22883 : loss : 0.022522, loss_ce: 0.002137, loss_dice: 0.042906
[11:33:27.578] TRAIN: iteration 22884 : loss : 0.070481, loss_ce: 0.007118, loss_dice: 0.133845
[11:33:27.796] TRAIN: iteration 22885 : loss : 0.131446, loss_ce: 0.005088, loss_dice: 0.257805
[11:33:28.005] TRAIN: iteration 22886 : loss : 0.062430, loss_ce: 0.003091, loss_dice: 0.121769
[11:33:28.216] TRAIN: iteration 22887 : loss : 0.130441, loss_ce: 0.007765, loss_dice: 0.253118
[11:33:28.424] TRAIN: iteration 22888 : loss : 0.250926, loss_ce: 0.001741, loss_dice: 0.500111
[11:33:28.635] TRAIN: iteration 22889 : loss : 0.041537, loss_ce: 0.004856, loss_dice: 0.078218
[11:33:28.851] TRAIN: iteration 22890 : loss : 0.251107, loss_ce: 0.007048, loss_dice: 0.495167
[11:33:29.063] TRAIN: iteration 22891 : loss : 0.084087, loss_ce: 0.002472, loss_dice: 0.165703
[11:33:29.276] TRAIN: iteration 22892 : loss : 0.063491, loss_ce: 0.002129, loss_dice: 0.124853
[11:33:29.487] TRAIN: iteration 22893 : loss : 0.252766, loss_ce: 0.010603, loss_dice: 0.494929
[11:33:29.695] TRAIN: iteration 22894 : loss : 0.164414, loss_ce: 0.006093, loss_dice: 0.322735
[11:33:29.915] TRAIN: iteration 22895 : loss : 0.047245, loss_ce: 0.006154, loss_dice: 0.088337
[11:33:30.128] TRAIN: iteration 22896 : loss : 0.052972, loss_ce: 0.004703, loss_dice: 0.101240
[11:33:30.339] TRAIN: iteration 22897 : loss : 0.239278, loss_ce: 0.005567, loss_dice: 0.472989
[11:33:30.549] TRAIN: iteration 22898 : loss : 0.252133, loss_ce: 0.004003, loss_dice: 0.500263
[11:33:30.762] TRAIN: iteration 22899 : loss : 0.251278, loss_ce: 0.002403, loss_dice: 0.500153
[11:33:30.973] TRAIN: iteration 22900 : loss : 0.027047, loss_ce: 0.001424, loss_dice: 0.052670
[11:33:31.227] TRAIN: iteration 22901 : loss : 0.119670, loss_ce: 0.003605, loss_dice: 0.235735
[11:33:31.440] TRAIN: iteration 22902 : loss : 0.119463, loss_ce: 0.004874, loss_dice: 0.234052
[11:33:31.650] TRAIN: iteration 22903 : loss : 0.115722, loss_ce: 0.005172, loss_dice: 0.226271
[11:33:31.863] TRAIN: iteration 22904 : loss : 0.151938, loss_ce: 0.017474, loss_dice: 0.286402
[11:33:32.081] TRAIN: iteration 22905 : loss : 0.068685, loss_ce: 0.004796, loss_dice: 0.132574
[11:33:32.292] TRAIN: iteration 22906 : loss : 0.113535, loss_ce: 0.002306, loss_dice: 0.224765
[11:33:32.505] TRAIN: iteration 22907 : loss : 0.146984, loss_ce: 0.004704, loss_dice: 0.289264
[11:33:32.722] TRAIN: iteration 22908 : loss : 0.246326, loss_ce: 0.003499, loss_dice: 0.489152
[11:33:32.935] TRAIN: iteration 22909 : loss : 0.243478, loss_ce: 0.001279, loss_dice: 0.485677
[11:33:33.151] TRAIN: iteration 22910 : loss : 0.049076, loss_ce: 0.002204, loss_dice: 0.095949
[11:33:33.360] TRAIN: iteration 22911 : loss : 0.029741, loss_ce: 0.001648, loss_dice: 0.057833
[11:33:33.574] TRAIN: iteration 22912 : loss : 0.161604, loss_ce: 0.001053, loss_dice: 0.322155
[11:33:33.784] TRAIN: iteration 22913 : loss : 0.072674, loss_ce: 0.001976, loss_dice: 0.143373
[11:33:33.993] TRAIN: iteration 22914 : loss : 0.135059, loss_ce: 0.003909, loss_dice: 0.266210
[11:33:34.204] TRAIN: iteration 22915 : loss : 0.250650, loss_ce: 0.001237, loss_dice: 0.500063
[11:33:34.414] TRAIN: iteration 22916 : loss : 0.050810, loss_ce: 0.002350, loss_dice: 0.099271
[11:33:34.644] TRAIN: iteration 22917 : loss : 0.211502, loss_ce: 0.002018, loss_dice: 0.420987
[11:33:34.857] TRAIN: iteration 22918 : loss : 0.107803, loss_ce: 0.001348, loss_dice: 0.214259
[11:33:35.073] TRAIN: iteration 22919 : loss : 0.238063, loss_ce: 0.001328, loss_dice: 0.474798
[11:33:35.284] TRAIN: iteration 22920 : loss : 0.058866, loss_ce: 0.001307, loss_dice: 0.116425
[11:33:35.532] TRAIN: iteration 22921 : loss : 0.126992, loss_ce: 0.005958, loss_dice: 0.248026
[11:33:35.744] TRAIN: iteration 22922 : loss : 0.082531, loss_ce: 0.000745, loss_dice: 0.164317
[11:33:35.958] TRAIN: iteration 22923 : loss : 0.048628, loss_ce: 0.000893, loss_dice: 0.096364
[11:33:36.168] TRAIN: iteration 22924 : loss : 0.207518, loss_ce: 0.037876, loss_dice: 0.377161
[11:33:36.377] TRAIN: iteration 22925 : loss : 0.049497, loss_ce: 0.004997, loss_dice: 0.093997
[11:33:36.586] TRAIN: iteration 22926 : loss : 0.250724, loss_ce: 0.001379, loss_dice: 0.500069
[11:33:36.794] TRAIN: iteration 22927 : loss : 0.034284, loss_ce: 0.003752, loss_dice: 0.064816
[11:33:37.003] TRAIN: iteration 22928 : loss : 0.219889, loss_ce: 0.005270, loss_dice: 0.434508
[11:33:37.211] TRAIN: iteration 22929 : loss : 0.251131, loss_ce: 0.002155, loss_dice: 0.500108
[11:33:37.422] TRAIN: iteration 22930 : loss : 0.190571, loss_ce: 0.002351, loss_dice: 0.378792
[11:33:37.633] TRAIN: iteration 22931 : loss : 0.041514, loss_ce: 0.002235, loss_dice: 0.080793
[11:33:37.842] TRAIN: iteration 22932 : loss : 0.251079, loss_ce: 0.002050, loss_dice: 0.500108
[11:33:38.242] TRAIN: iteration 22933 : loss : 0.046026, loss_ce: 0.004586, loss_dice: 0.087466
[11:33:38.451] TRAIN: iteration 22934 : loss : 0.059833, loss_ce: 0.010355, loss_dice: 0.109311
[11:33:38.659] TRAIN: iteration 22935 : loss : 0.251634, loss_ce: 0.003070, loss_dice: 0.500198
[11:33:38.868] TRAIN: iteration 22936 : loss : 0.044350, loss_ce: 0.008641, loss_dice: 0.080059
[11:33:39.077] TRAIN: iteration 22937 : loss : 0.110712, loss_ce: 0.007010, loss_dice: 0.214413
[11:33:39.286] TRAIN: iteration 22938 : loss : 0.030914, loss_ce: 0.002603, loss_dice: 0.059225
[11:33:41.696] TRAIN: iteration 22939 : loss : 0.037652, loss_ce: 0.002108, loss_dice: 0.073195
[11:33:41.906] TRAIN: iteration 22940 : loss : 0.165282, loss_ce: 0.011837, loss_dice: 0.318727
[11:33:42.152] TRAIN: iteration 22941 : loss : 0.214314, loss_ce: 0.004179, loss_dice: 0.424449
[11:33:42.364] TRAIN: iteration 22942 : loss : 0.093339, loss_ce: 0.002631, loss_dice: 0.184047
[11:33:42.576] TRAIN: iteration 22943 : loss : 0.234598, loss_ce: 0.002800, loss_dice: 0.466396
[11:33:42.784] TRAIN: iteration 22944 : loss : 0.107701, loss_ce: 0.002021, loss_dice: 0.213381
[11:33:42.997] TRAIN: iteration 22945 : loss : 0.146830, loss_ce: 0.008059, loss_dice: 0.285601
[11:33:43.214] TRAIN: iteration 22946 : loss : 0.041873, loss_ce: 0.002960, loss_dice: 0.080785
[11:33:43.762] TRAIN: iteration 22947 : loss : 0.142882, loss_ce: 0.004272, loss_dice: 0.281492
[11:33:43.979] TRAIN: iteration 22948 : loss : 0.147038, loss_ce: 0.006644, loss_dice: 0.287432
[11:33:44.187] TRAIN: iteration 22949 : loss : 0.039484, loss_ce: 0.007211, loss_dice: 0.071757
[11:33:44.399] TRAIN: iteration 22950 : loss : 0.251480, loss_ce: 0.002772, loss_dice: 0.500188
[11:33:44.610] TRAIN: iteration 22951 : loss : 0.175968, loss_ce: 0.001678, loss_dice: 0.350258
[11:33:44.820] TRAIN: iteration 22952 : loss : 0.092536, loss_ce: 0.005847, loss_dice: 0.179225
[11:33:45.037] TRAIN: iteration 22953 : loss : 0.124583, loss_ce: 0.004807, loss_dice: 0.244359
[11:33:45.249] TRAIN: iteration 22954 : loss : 0.088872, loss_ce: 0.004311, loss_dice: 0.173433
[11:33:46.030] TRAIN: iteration 22955 : loss : 0.141223, loss_ce: 0.011864, loss_dice: 0.270582
[11:33:46.239] TRAIN: iteration 22956 : loss : 0.036620, loss_ce: 0.002144, loss_dice: 0.071097
[11:33:46.448] TRAIN: iteration 22957 : loss : 0.192778, loss_ce: 0.002107, loss_dice: 0.383448
[11:33:46.659] TRAIN: iteration 22958 : loss : 0.076583, loss_ce: 0.002655, loss_dice: 0.150510
[11:33:46.870] TRAIN: iteration 22959 : loss : 0.223769, loss_ce: 0.001691, loss_dice: 0.445846
[11:33:47.085] TRAIN: iteration 22960 : loss : 0.237110, loss_ce: 0.002571, loss_dice: 0.471649
[11:33:47.324] TRAIN: iteration 22961 : loss : 0.077705, loss_ce: 0.001725, loss_dice: 0.153684
[11:33:47.533] TRAIN: iteration 22962 : loss : 0.090578, loss_ce: 0.004789, loss_dice: 0.176368
[11:33:47.742] TRAIN: iteration 22963 : loss : 0.224632, loss_ce: 0.003273, loss_dice: 0.445991
[11:33:47.950] TRAIN: iteration 22964 : loss : 0.142294, loss_ce: 0.004434, loss_dice: 0.280153
[11:33:48.160] TRAIN: iteration 22965 : loss : 0.075383, loss_ce: 0.001973, loss_dice: 0.148794
[11:33:48.369] TRAIN: iteration 22966 : loss : 0.089432, loss_ce: 0.004434, loss_dice: 0.174430
[11:33:48.579] TRAIN: iteration 22967 : loss : 0.044013, loss_ce: 0.004113, loss_dice: 0.083912
[11:33:48.796] TRAIN: iteration 22968 : loss : 0.093828, loss_ce: 0.003014, loss_dice: 0.184642
[11:33:49.010] TRAIN: iteration 22969 : loss : 0.090671, loss_ce: 0.005901, loss_dice: 0.175441
[11:33:49.219] TRAIN: iteration 22970 : loss : 0.121314, loss_ce: 0.004416, loss_dice: 0.238212
[11:33:50.738] TRAIN: iteration 22971 : loss : 0.103517, loss_ce: 0.014449, loss_dice: 0.192585
[11:33:50.949] TRAIN: iteration 22972 : loss : 0.124593, loss_ce: 0.002444, loss_dice: 0.246741
[11:33:51.164] TRAIN: iteration 22973 : loss : 0.248580, loss_ce: 0.002152, loss_dice: 0.495007
[11:33:51.373] TRAIN: iteration 22974 : loss : 0.150762, loss_ce: 0.001557, loss_dice: 0.299966
[11:33:51.589] TRAIN: iteration 22975 : loss : 0.236907, loss_ce: 0.002515, loss_dice: 0.471299
[11:33:51.801] TRAIN: iteration 22976 : loss : 0.167153, loss_ce: 0.003983, loss_dice: 0.330322
[11:33:52.011] TRAIN: iteration 22977 : loss : 0.250653, loss_ce: 0.001253, loss_dice: 0.500054
[11:33:52.229] TRAIN: iteration 22978 : loss : 0.069693, loss_ce: 0.003646, loss_dice: 0.135740
[11:33:52.443] TRAIN: iteration 22979 : loss : 0.130628, loss_ce: 0.004106, loss_dice: 0.257150
[11:33:52.652] TRAIN: iteration 22980 : loss : 0.183375, loss_ce: 0.003351, loss_dice: 0.363399
[11:33:52.897] TRAIN: iteration 22981 : loss : 0.029246, loss_ce: 0.001255, loss_dice: 0.057236
[11:33:53.111] TRAIN: iteration 22982 : loss : 0.063426, loss_ce: 0.005358, loss_dice: 0.121493
[11:33:53.328] TRAIN: iteration 22983 : loss : 0.071218, loss_ce: 0.004347, loss_dice: 0.138088
[11:33:53.539] TRAIN: iteration 22984 : loss : 0.027122, loss_ce: 0.000755, loss_dice: 0.053489
[11:33:53.751] TRAIN: iteration 22985 : loss : 0.058375, loss_ce: 0.002577, loss_dice: 0.114172
[11:33:53.962] TRAIN: iteration 22986 : loss : 0.051222, loss_ce: 0.002747, loss_dice: 0.099698
[11:33:54.171] TRAIN: iteration 22987 : loss : 0.145036, loss_ce: 0.002065, loss_dice: 0.288008
[11:33:54.381] TRAIN: iteration 22988 : loss : 0.250567, loss_ce: 0.001096, loss_dice: 0.500039
[11:33:54.591] TRAIN: iteration 22989 : loss : 0.127914, loss_ce: 0.001749, loss_dice: 0.254080
[11:33:54.806] TRAIN: iteration 22990 : loss : 0.062072, loss_ce: 0.002114, loss_dice: 0.122031
[11:33:55.026] TRAIN: iteration 22991 : loss : 0.251179, loss_ce: 0.002211, loss_dice: 0.500148
[11:33:55.236] TRAIN: iteration 22992 : loss : 0.037741, loss_ce: 0.005721, loss_dice: 0.069761
[11:33:55.446] TRAIN: iteration 22993 : loss : 0.155401, loss_ce: 0.011757, loss_dice: 0.299046
[11:33:55.663] TRAIN: iteration 22994 : loss : 0.250952, loss_ce: 0.001798, loss_dice: 0.500106
[11:33:55.883] TRAIN: iteration 22995 : loss : 0.188832, loss_ce: 0.003098, loss_dice: 0.374565
[11:33:56.101] TRAIN: iteration 22996 : loss : 0.167421, loss_ce: 0.005543, loss_dice: 0.329300
[11:33:56.312] TRAIN: iteration 22997 : loss : 0.048521, loss_ce: 0.003182, loss_dice: 0.093860
[11:33:56.522] TRAIN: iteration 22998 : loss : 0.088508, loss_ce: 0.003449, loss_dice: 0.173566
[11:33:56.733] TRAIN: iteration 22999 : loss : 0.254709, loss_ce: 0.010892, loss_dice: 0.498526
[11:33:56.943] TRAIN: iteration 23000 : loss : 0.118461, loss_ce: 0.005900, loss_dice: 0.231021
[11:33:57.185] TRAIN: iteration 23001 : loss : 0.186232, loss_ce: 0.002712, loss_dice: 0.369752
[11:33:57.396] TRAIN: iteration 23002 : loss : 0.251186, loss_ce: 0.002231, loss_dice: 0.500141
[11:33:57.605] TRAIN: iteration 23003 : loss : 0.195869, loss_ce: 0.002960, loss_dice: 0.388778
[11:33:57.815] TRAIN: iteration 23004 : loss : 0.093675, loss_ce: 0.006208, loss_dice: 0.181142
[11:33:58.024] TRAIN: iteration 23005 : loss : 0.012346, loss_ce: 0.000933, loss_dice: 0.023759
[11:33:58.234] TRAIN: iteration 23006 : loss : 0.090137, loss_ce: 0.004321, loss_dice: 0.175952
[11:33:58.445] TRAIN: iteration 23007 : loss : 0.088716, loss_ce: 0.004575, loss_dice: 0.172857
[11:33:58.653] TRAIN: iteration 23008 : loss : 0.089825, loss_ce: 0.004237, loss_dice: 0.175413
[11:33:58.864] TRAIN: iteration 23009 : loss : 0.050321, loss_ce: 0.006675, loss_dice: 0.093966
[11:33:59.078] TRAIN: iteration 23010 : loss : 0.153379, loss_ce: 0.007085, loss_dice: 0.299673
[11:33:59.287] TRAIN: iteration 23011 : loss : 0.133594, loss_ce: 0.007188, loss_dice: 0.260000
[11:33:59.498] TRAIN: iteration 23012 : loss : 0.101473, loss_ce: 0.014284, loss_dice: 0.188661
[11:33:59.709] TRAIN: iteration 23013 : loss : 0.151839, loss_ce: 0.003273, loss_dice: 0.300404
[11:33:59.918] TRAIN: iteration 23014 : loss : 0.049242, loss_ce: 0.001679, loss_dice: 0.096805
[11:34:00.128] TRAIN: iteration 23015 : loss : 0.108271, loss_ce: 0.005246, loss_dice: 0.211296
[11:34:00.336] TRAIN: iteration 23016 : loss : 0.109385, loss_ce: 0.005224, loss_dice: 0.213547
[11:34:00.546] TRAIN: iteration 23017 : loss : 0.039667, loss_ce: 0.004079, loss_dice: 0.075254
[11:34:00.758] TRAIN: iteration 23018 : loss : 0.095948, loss_ce: 0.007494, loss_dice: 0.184402
[11:34:00.969] TRAIN: iteration 23019 : loss : 0.069843, loss_ce: 0.005463, loss_dice: 0.134224
[11:34:01.178] TRAIN: iteration 23020 : loss : 0.251428, loss_ce: 0.002651, loss_dice: 0.500204
[11:34:01.419] TRAIN: iteration 23021 : loss : 0.050679, loss_ce: 0.006234, loss_dice: 0.095124
[11:34:01.631] TRAIN: iteration 23022 : loss : 0.248900, loss_ce: 0.003608, loss_dice: 0.494192
[11:34:01.848] TRAIN: iteration 23023 : loss : 0.028387, loss_ce: 0.001323, loss_dice: 0.055450
[11:34:02.057] TRAIN: iteration 23024 : loss : 0.068416, loss_ce: 0.005213, loss_dice: 0.131619
[11:34:02.302] TRAIN: iteration 23025 : loss : 0.104986, loss_ce: 0.005424, loss_dice: 0.204548
[11:34:02.511] TRAIN: iteration 23026 : loss : 0.053743, loss_ce: 0.007061, loss_dice: 0.100425
[11:34:02.719] TRAIN: iteration 23027 : loss : 0.057645, loss_ce: 0.004106, loss_dice: 0.111184
[11:34:02.935] TRAIN: iteration 23028 : loss : 0.103474, loss_ce: 0.002048, loss_dice: 0.204899
[11:34:03.149] TRAIN: iteration 23029 : loss : 0.053377, loss_ce: 0.010618, loss_dice: 0.096137
[11:34:03.371] TRAIN: iteration 23030 : loss : 0.071428, loss_ce: 0.004478, loss_dice: 0.138379
[11:34:03.679] TRAIN: iteration 23031 : loss : 0.250921, loss_ce: 0.001729, loss_dice: 0.500114
[11:34:03.888] TRAIN: iteration 23032 : loss : 0.109595, loss_ce: 0.004420, loss_dice: 0.214771
[11:34:04.217] TRAIN: iteration 23033 : loss : 0.251213, loss_ce: 0.002276, loss_dice: 0.500151
[11:34:04.426] TRAIN: iteration 23034 : loss : 0.056464, loss_ce: 0.002330, loss_dice: 0.110599
[11:34:04.637] TRAIN: iteration 23035 : loss : 0.130669, loss_ce: 0.002918, loss_dice: 0.258420
[11:34:04.849] TRAIN: iteration 23036 : loss : 0.159517, loss_ce: 0.003862, loss_dice: 0.315171
[11:34:05.063] TRAIN: iteration 23037 : loss : 0.185038, loss_ce: 0.002546, loss_dice: 0.367530
[11:34:05.274] TRAIN: iteration 23038 : loss : 0.059247, loss_ce: 0.008140, loss_dice: 0.110354
[11:34:07.189] TRAIN: iteration 23039 : loss : 0.250292, loss_ce: 0.000574, loss_dice: 0.500009
[11:34:07.405] TRAIN: iteration 23040 : loss : 0.250758, loss_ce: 0.001429, loss_dice: 0.500087
[11:34:07.640] TRAIN: iteration 23041 : loss : 0.139261, loss_ce: 0.005670, loss_dice: 0.272852
[11:34:07.852] TRAIN: iteration 23042 : loss : 0.096998, loss_ce: 0.005293, loss_dice: 0.188702
[11:34:08.071] TRAIN: iteration 23043 : loss : 0.205648, loss_ce: 0.001524, loss_dice: 0.409771
[11:34:08.281] TRAIN: iteration 23044 : loss : 0.070192, loss_ce: 0.004366, loss_dice: 0.136018
[11:34:08.490] TRAIN: iteration 23045 : loss : 0.173996, loss_ce: 0.008954, loss_dice: 0.339037
[11:34:08.699] TRAIN: iteration 23046 : loss : 0.104231, loss_ce: 0.007831, loss_dice: 0.200631
[11:34:08.909] TRAIN: iteration 23047 : loss : 0.029885, loss_ce: 0.001732, loss_dice: 0.058039
[11:34:09.126] TRAIN: iteration 23048 : loss : 0.061731, loss_ce: 0.004190, loss_dice: 0.119273
[11:34:09.342] TRAIN: iteration 23049 : loss : 0.030157, loss_ce: 0.001596, loss_dice: 0.058718
[11:34:09.552] TRAIN: iteration 23050 : loss : 0.149686, loss_ce: 0.002946, loss_dice: 0.296425
[11:34:09.762] TRAIN: iteration 23051 : loss : 0.082356, loss_ce: 0.005200, loss_dice: 0.159512
[11:34:09.976] TRAIN: iteration 23052 : loss : 0.024734, loss_ce: 0.004691, loss_dice: 0.044777
[11:34:10.186] TRAIN: iteration 23053 : loss : 0.043737, loss_ce: 0.003984, loss_dice: 0.083490
[11:34:10.397] TRAIN: iteration 23054 : loss : 0.250856, loss_ce: 0.001615, loss_dice: 0.500096
[11:34:10.605] TRAIN: iteration 23055 : loss : 0.084338, loss_ce: 0.001715, loss_dice: 0.166961
[11:34:10.814] TRAIN: iteration 23056 : loss : 0.058917, loss_ce: 0.008405, loss_dice: 0.109428
[11:34:11.024] TRAIN: iteration 23057 : loss : 0.239961, loss_ce: 0.006145, loss_dice: 0.473778
[11:34:11.234] TRAIN: iteration 23058 : loss : 0.222423, loss_ce: 0.002355, loss_dice: 0.442492
[11:34:11.444] TRAIN: iteration 23059 : loss : 0.069628, loss_ce: 0.007567, loss_dice: 0.131688
[11:34:11.656] TRAIN: iteration 23060 : loss : 0.069770, loss_ce: 0.004299, loss_dice: 0.135242
[11:34:11.897] TRAIN: iteration 23061 : loss : 0.033771, loss_ce: 0.002073, loss_dice: 0.065468
[11:34:12.106] TRAIN: iteration 23062 : loss : 0.042535, loss_ce: 0.007499, loss_dice: 0.077571
[11:34:12.317] TRAIN: iteration 23063 : loss : 0.250457, loss_ce: 0.000891, loss_dice: 0.500022
[11:34:12.526] TRAIN: iteration 23064 : loss : 0.037762, loss_ce: 0.004979, loss_dice: 0.070546
[11:34:12.741] TRAIN: iteration 23065 : loss : 0.050690, loss_ce: 0.005361, loss_dice: 0.096019
[11:34:12.949] TRAIN: iteration 23066 : loss : 0.206427, loss_ce: 0.006934, loss_dice: 0.405920
[11:34:13.159] TRAIN: iteration 23067 : loss : 0.203059, loss_ce: 0.007060, loss_dice: 0.399057
[11:34:13.374] TRAIN: iteration 23068 : loss : 0.105540, loss_ce: 0.005265, loss_dice: 0.205816
[11:34:13.590] TRAIN: iteration 23069 : loss : 0.043464, loss_ce: 0.001333, loss_dice: 0.085595
[11:34:13.798] TRAIN: iteration 23070 : loss : 0.140079, loss_ce: 0.001608, loss_dice: 0.278549
[11:34:14.007] TRAIN: iteration 23071 : loss : 0.250838, loss_ce: 0.001594, loss_dice: 0.500082
[11:34:14.217] TRAIN: iteration 23072 : loss : 0.051251, loss_ce: 0.010877, loss_dice: 0.091625
[11:34:14.426] TRAIN: iteration 23073 : loss : 0.057844, loss_ce: 0.002750, loss_dice: 0.112939
[11:34:14.635] TRAIN: iteration 23074 : loss : 0.071181, loss_ce: 0.009790, loss_dice: 0.132572
[11:34:14.846] TRAIN: iteration 23075 : loss : 0.116847, loss_ce: 0.003248, loss_dice: 0.230445
[11:34:15.074] TRAIN: iteration 23076 : loss : 0.141000, loss_ce: 0.016360, loss_dice: 0.265640
[11:34:15.283] TRAIN: iteration 23077 : loss : 0.009560, loss_ce: 0.001085, loss_dice: 0.018035
[11:34:15.492] TRAIN: iteration 23078 : loss : 0.252103, loss_ce: 0.003920, loss_dice: 0.500286
[11:34:15.706] TRAIN: iteration 23079 : loss : 0.068757, loss_ce: 0.003720, loss_dice: 0.133793
[11:34:15.914] TRAIN: iteration 23080 : loss : 0.250753, loss_ce: 0.001433, loss_dice: 0.500072
[11:34:15.915] NaN or Inf found in input tensor.
[11:34:16.133] TRAIN: iteration 23081 : loss : 0.074686, loss_ce: 0.008581, loss_dice: 0.140792
[11:34:16.343] TRAIN: iteration 23082 : loss : 0.144915, loss_ce: 0.002168, loss_dice: 0.287662
[11:34:16.551] TRAIN: iteration 23083 : loss : 0.046722, loss_ce: 0.006945, loss_dice: 0.086498
[11:34:16.763] TRAIN: iteration 23084 : loss : 0.036566, loss_ce: 0.002111, loss_dice: 0.071022
[11:34:16.973] TRAIN: iteration 23085 : loss : 0.248623, loss_ce: 0.008647, loss_dice: 0.488598
[11:34:17.190] TRAIN: iteration 23086 : loss : 0.035237, loss_ce: 0.006069, loss_dice: 0.064404
[11:34:17.399] TRAIN: iteration 23087 : loss : 0.051430, loss_ce: 0.005303, loss_dice: 0.097558
[11:34:17.854] TRAIN: iteration 23088 : loss : 0.157990, loss_ce: 0.012153, loss_dice: 0.303827
[11:34:18.063] TRAIN: iteration 23089 : loss : 0.150178, loss_ce: 0.003981, loss_dice: 0.296375
[11:34:18.273] TRAIN: iteration 23090 : loss : 0.250750, loss_ce: 0.001436, loss_dice: 0.500063
[11:34:18.483] TRAIN: iteration 23091 : loss : 0.041530, loss_ce: 0.002119, loss_dice: 0.080941
[11:34:18.694] TRAIN: iteration 23092 : loss : 0.251257, loss_ce: 0.002367, loss_dice: 0.500147
[11:34:18.911] TRAIN: iteration 23093 : loss : 0.157371, loss_ce: 0.005599, loss_dice: 0.309144
[11:34:19.121] TRAIN: iteration 23094 : loss : 0.184724, loss_ce: 0.001861, loss_dice: 0.367587
[11:34:19.331] TRAIN: iteration 23095 : loss : 0.047944, loss_ce: 0.002738, loss_dice: 0.093151
[11:34:19.547] TRAIN: iteration 23096 : loss : 0.184641, loss_ce: 0.002583, loss_dice: 0.366699
[11:34:19.757] TRAIN: iteration 23097 : loss : 0.177599, loss_ce: 0.003221, loss_dice: 0.351977
[11:34:19.969] TRAIN: iteration 23098 : loss : 0.097694, loss_ce: 0.002920, loss_dice: 0.192468
[11:34:20.178] TRAIN: iteration 23099 : loss : 0.121024, loss_ce: 0.001894, loss_dice: 0.240155
[11:34:20.390] TRAIN: iteration 23100 : loss : 0.101400, loss_ce: 0.002012, loss_dice: 0.200788
[11:34:20.631] TRAIN: iteration 23101 : loss : 0.065760, loss_ce: 0.002710, loss_dice: 0.128810
[11:34:20.842] TRAIN: iteration 23102 : loss : 0.079291, loss_ce: 0.001230, loss_dice: 0.157351
[11:34:21.059] TRAIN: iteration 23103 : loss : 0.047619, loss_ce: 0.009372, loss_dice: 0.085866
[11:34:21.271] TRAIN: iteration 23104 : loss : 0.039383, loss_ce: 0.005483, loss_dice: 0.073283
[11:34:21.481] TRAIN: iteration 23105 : loss : 0.041992, loss_ce: 0.002063, loss_dice: 0.081921
[11:34:21.694] TRAIN: iteration 23106 : loss : 0.046774, loss_ce: 0.004656, loss_dice: 0.088892
[11:34:21.903] TRAIN: iteration 23107 : loss : 0.082110, loss_ce: 0.002324, loss_dice: 0.161896
[11:34:22.139] TRAIN: iteration 23108 : loss : 0.092919, loss_ce: 0.008191, loss_dice: 0.177646
[11:34:22.357] TRAIN: iteration 23109 : loss : 0.053549, loss_ce: 0.001242, loss_dice: 0.105856
[11:34:22.565] TRAIN: iteration 23110 : loss : 0.037132, loss_ce: 0.002818, loss_dice: 0.071445
[11:34:22.775] TRAIN: iteration 23111 : loss : 0.029309, loss_ce: 0.001354, loss_dice: 0.057265
[11:34:22.984] TRAIN: iteration 23112 : loss : 0.088653, loss_ce: 0.001984, loss_dice: 0.175322
[11:34:23.200] TRAIN: iteration 23113 : loss : 0.092323, loss_ce: 0.005892, loss_dice: 0.178754
[11:34:23.410] TRAIN: iteration 23114 : loss : 0.250499, loss_ce: 0.000961, loss_dice: 0.500037
[11:34:23.625] TRAIN: iteration 23115 : loss : 0.250574, loss_ce: 0.001089, loss_dice: 0.500059
[11:34:23.842] TRAIN: iteration 23116 : loss : 0.104057, loss_ce: 0.003846, loss_dice: 0.204269
[11:34:24.050] TRAIN: iteration 23117 : loss : 0.081687, loss_ce: 0.005585, loss_dice: 0.157790
[11:34:24.260] TRAIN: iteration 23118 : loss : 0.059464, loss_ce: 0.001250, loss_dice: 0.117678
[11:34:24.477] TRAIN: iteration 23119 : loss : 0.072869, loss_ce: 0.005110, loss_dice: 0.140628
[11:34:24.693] TRAIN: iteration 23120 : loss : 0.097931, loss_ce: 0.001526, loss_dice: 0.194336
[11:34:24.946] TRAIN: iteration 23121 : loss : 0.242329, loss_ce: 0.002728, loss_dice: 0.481931
[11:34:25.154] TRAIN: iteration 23122 : loss : 0.075553, loss_ce: 0.001333, loss_dice: 0.149773
[11:34:25.363] TRAIN: iteration 23123 : loss : 0.058709, loss_ce: 0.002937, loss_dice: 0.114480
[11:34:25.572] TRAIN: iteration 23124 : loss : 0.086157, loss_ce: 0.001911, loss_dice: 0.170403
[11:34:25.783] TRAIN: iteration 23125 : loss : 0.122999, loss_ce: 0.002076, loss_dice: 0.243921
[11:34:25.993] TRAIN: iteration 23126 : loss : 0.102669, loss_ce: 0.004524, loss_dice: 0.200814
[11:34:26.204] TRAIN: iteration 23127 : loss : 0.114200, loss_ce: 0.006732, loss_dice: 0.221667
[11:34:26.413] TRAIN: iteration 23128 : loss : 0.250380, loss_ce: 0.000744, loss_dice: 0.500017
[11:34:26.623] TRAIN: iteration 23129 : loss : 0.245632, loss_ce: 0.001165, loss_dice: 0.490098
[11:34:26.833] TRAIN: iteration 23130 : loss : 0.032928, loss_ce: 0.003425, loss_dice: 0.062431
[11:34:27.050] TRAIN: iteration 23131 : loss : 0.069192, loss_ce: 0.003084, loss_dice: 0.135301
[11:34:27.259] TRAIN: iteration 23132 : loss : 0.033812, loss_ce: 0.001270, loss_dice: 0.066355
[11:34:27.468] TRAIN: iteration 23133 : loss : 0.237654, loss_ce: 0.010433, loss_dice: 0.464874
[11:34:27.676] TRAIN: iteration 23134 : loss : 0.168593, loss_ce: 0.002190, loss_dice: 0.334997
[11:34:27.885] TRAIN: iteration 23135 : loss : 0.250534, loss_ce: 0.001023, loss_dice: 0.500045
[11:34:28.097] TRAIN: iteration 23136 : loss : 0.079457, loss_ce: 0.004584, loss_dice: 0.154330
[11:34:28.306] TRAIN: iteration 23137 : loss : 0.223507, loss_ce: 0.021556, loss_dice: 0.425457
[11:34:30.136] TRAIN: iteration 23138 : loss : 0.068960, loss_ce: 0.004973, loss_dice: 0.132947
[11:34:30.351] TRAIN: iteration 23139 : loss : 0.250946, loss_ce: 0.001783, loss_dice: 0.500108
[11:34:30.561] TRAIN: iteration 23140 : loss : 0.078085, loss_ce: 0.004947, loss_dice: 0.151222
[11:34:30.799] TRAIN: iteration 23141 : loss : 0.167581, loss_ce: 0.001441, loss_dice: 0.333721
[11:34:31.011] TRAIN: iteration 23142 : loss : 0.037936, loss_ce: 0.003477, loss_dice: 0.072396
[11:34:31.221] TRAIN: iteration 23143 : loss : 0.066524, loss_ce: 0.000967, loss_dice: 0.132081
[11:34:31.433] TRAIN: iteration 23144 : loss : 0.062771, loss_ce: 0.001573, loss_dice: 0.123969
[11:34:31.647] TRAIN: iteration 23145 : loss : 0.250041, loss_ce: 0.003174, loss_dice: 0.496909
[11:34:32.283] TRAIN: iteration 23146 : loss : 0.073776, loss_ce: 0.010616, loss_dice: 0.136936
[11:34:32.495] TRAIN: iteration 23147 : loss : 0.185635, loss_ce: 0.001800, loss_dice: 0.369470
[11:34:32.714] TRAIN: iteration 23148 : loss : 0.062099, loss_ce: 0.002478, loss_dice: 0.121719
[11:34:32.926] TRAIN: iteration 23149 : loss : 0.249139, loss_ce: 0.001962, loss_dice: 0.496317
[11:34:33.137] TRAIN: iteration 23150 : loss : 0.172947, loss_ce: 0.005721, loss_dice: 0.340173
[11:34:33.345] TRAIN: iteration 23151 : loss : 0.201720, loss_ce: 0.001767, loss_dice: 0.401673
[11:34:33.553] TRAIN: iteration 23152 : loss : 0.056055, loss_ce: 0.001005, loss_dice: 0.111106
[11:34:33.763] TRAIN: iteration 23153 : loss : 0.082155, loss_ce: 0.002292, loss_dice: 0.162019
[11:34:33.975] TRAIN: iteration 23154 : loss : 0.127074, loss_ce: 0.002727, loss_dice: 0.251421
[11:34:34.186] TRAIN: iteration 23155 : loss : 0.183216, loss_ce: 0.006100, loss_dice: 0.360333
[11:34:34.394] TRAIN: iteration 23156 : loss : 0.072358, loss_ce: 0.003107, loss_dice: 0.141608
[11:34:34.603] TRAIN: iteration 23157 : loss : 0.022216, loss_ce: 0.001863, loss_dice: 0.042570
[11:34:34.814] TRAIN: iteration 23158 : loss : 0.071282, loss_ce: 0.010751, loss_dice: 0.131813
[11:34:35.024] TRAIN: iteration 23159 : loss : 0.053938, loss_ce: 0.003379, loss_dice: 0.104497
[11:34:35.236] TRAIN: iteration 23160 : loss : 0.250226, loss_ce: 0.000449, loss_dice: 0.500003
[11:34:35.479] TRAIN: iteration 23161 : loss : 0.030639, loss_ce: 0.001559, loss_dice: 0.059719
[11:34:35.688] TRAIN: iteration 23162 : loss : 0.115355, loss_ce: 0.003470, loss_dice: 0.227241
[11:34:35.898] TRAIN: iteration 23163 : loss : 0.072096, loss_ce: 0.005029, loss_dice: 0.139162
[11:34:36.114] TRAIN: iteration 23164 : loss : 0.062121, loss_ce: 0.004408, loss_dice: 0.119834
[11:34:36.325] TRAIN: iteration 23165 : loss : 0.250807, loss_ce: 0.001516, loss_dice: 0.500097
[11:34:36.541] TRAIN: iteration 23166 : loss : 0.069656, loss_ce: 0.002513, loss_dice: 0.136798
[11:34:36.754] TRAIN: iteration 23167 : loss : 0.108362, loss_ce: 0.005557, loss_dice: 0.211167
[11:34:36.964] TRAIN: iteration 23168 : loss : 0.206178, loss_ce: 0.000646, loss_dice: 0.411709
[11:34:37.175] TRAIN: iteration 23169 : loss : 0.240791, loss_ce: 0.001414, loss_dice: 0.480168
[11:34:37.385] TRAIN: iteration 23170 : loss : 0.049831, loss_ce: 0.004958, loss_dice: 0.094703
[11:34:37.742] TRAIN: iteration 23171 : loss : 0.083636, loss_ce: 0.002485, loss_dice: 0.164788
[11:34:37.951] TRAIN: iteration 23172 : loss : 0.147161, loss_ce: 0.004940, loss_dice: 0.289383
[11:34:38.168] TRAIN: iteration 23173 : loss : 0.082334, loss_ce: 0.004244, loss_dice: 0.160424
[11:34:38.383] TRAIN: iteration 23174 : loss : 0.115463, loss_ce: 0.009317, loss_dice: 0.221608
[11:34:38.593] TRAIN: iteration 23175 : loss : 0.113568, loss_ce: 0.002648, loss_dice: 0.224488
[11:34:38.803] TRAIN: iteration 23176 : loss : 0.110726, loss_ce: 0.003237, loss_dice: 0.218216
[11:34:39.013] TRAIN: iteration 23177 : loss : 0.143473, loss_ce: 0.002762, loss_dice: 0.284185
[11:34:39.230] TRAIN: iteration 23178 : loss : 0.166893, loss_ce: 0.030186, loss_dice: 0.303600
[11:34:39.445] TRAIN: iteration 23179 : loss : 0.183211, loss_ce: 0.002038, loss_dice: 0.364384
[11:34:39.654] TRAIN: iteration 23180 : loss : 0.053496, loss_ce: 0.004696, loss_dice: 0.102296
[11:34:39.891] TRAIN: iteration 23181 : loss : 0.060750, loss_ce: 0.004896, loss_dice: 0.116604
[11:34:40.109] TRAIN: iteration 23182 : loss : 0.157215, loss_ce: 0.005203, loss_dice: 0.309227
[11:34:40.324] TRAIN: iteration 23183 : loss : 0.042011, loss_ce: 0.001127, loss_dice: 0.082895
[11:34:40.533] TRAIN: iteration 23184 : loss : 0.251312, loss_ce: 0.002452, loss_dice: 0.500172
[11:34:40.744] TRAIN: iteration 23185 : loss : 0.061112, loss_ce: 0.004827, loss_dice: 0.117397
[11:34:40.957] TRAIN: iteration 23186 : loss : 0.078668, loss_ce: 0.004169, loss_dice: 0.153167
[11:34:41.167] TRAIN: iteration 23187 : loss : 0.107477, loss_ce: 0.001290, loss_dice: 0.213664
[11:34:41.376] TRAIN: iteration 23188 : loss : 0.131537, loss_ce: 0.004413, loss_dice: 0.258660
[11:34:41.585] TRAIN: iteration 23189 : loss : 0.179940, loss_ce: 0.003015, loss_dice: 0.356866
[11:34:41.795] TRAIN: iteration 23190 : loss : 0.158796, loss_ce: 0.004602, loss_dice: 0.312990
[11:34:42.009] TRAIN: iteration 23191 : loss : 0.201994, loss_ce: 0.002092, loss_dice: 0.401895
[11:34:42.220] TRAIN: iteration 23192 : loss : 0.236071, loss_ce: 0.006127, loss_dice: 0.466016
[11:34:42.798] TRAIN: iteration 23193 : loss : 0.029792, loss_ce: 0.000641, loss_dice: 0.058942
[11:34:43.012] TRAIN: iteration 23194 : loss : 0.118866, loss_ce: 0.016694, loss_dice: 0.221037
[11:34:43.223] TRAIN: iteration 23195 : loss : 0.080378, loss_ce: 0.004823, loss_dice: 0.155933
[11:34:43.434] TRAIN: iteration 23196 : loss : 0.131254, loss_ce: 0.002037, loss_dice: 0.260470
[11:34:43.647] TRAIN: iteration 23197 : loss : 0.077188, loss_ce: 0.005881, loss_dice: 0.148495
[11:34:43.864] TRAIN: iteration 23198 : loss : 0.077070, loss_ce: 0.004867, loss_dice: 0.149273
[11:34:44.080] TRAIN: iteration 23199 : loss : 0.012316, loss_ce: 0.000926, loss_dice: 0.023705
[11:34:44.290] TRAIN: iteration 23200 : loss : 0.250067, loss_ce: 0.002280, loss_dice: 0.497854
[11:34:44.532] TRAIN: iteration 23201 : loss : 0.114669, loss_ce: 0.002323, loss_dice: 0.227014
[11:34:44.743] TRAIN: iteration 23202 : loss : 0.098234, loss_ce: 0.003357, loss_dice: 0.193112
[11:34:44.957] TRAIN: iteration 23203 : loss : 0.252150, loss_ce: 0.021838, loss_dice: 0.482462
[11:34:45.174] TRAIN: iteration 23204 : loss : 0.144499, loss_ce: 0.018896, loss_dice: 0.270103
[11:34:45.383] TRAIN: iteration 23205 : loss : 0.063469, loss_ce: 0.002991, loss_dice: 0.123947
[11:34:45.595] TRAIN: iteration 23206 : loss : 0.223805, loss_ce: 0.001494, loss_dice: 0.446116
[11:34:45.806] TRAIN: iteration 23207 : loss : 0.250598, loss_ce: 0.002098, loss_dice: 0.499097
[11:34:46.018] TRAIN: iteration 23208 : loss : 0.070796, loss_ce: 0.003827, loss_dice: 0.137766
[11:34:46.229] TRAIN: iteration 23209 : loss : 0.251802, loss_ce: 0.003344, loss_dice: 0.500261
[11:34:46.438] TRAIN: iteration 23210 : loss : 0.091366, loss_ce: 0.002902, loss_dice: 0.179829
[11:34:46.647] TRAIN: iteration 23211 : loss : 0.132304, loss_ce: 0.015693, loss_dice: 0.248916
[11:34:46.860] TRAIN: iteration 23212 : loss : 0.086626, loss_ce: 0.002156, loss_dice: 0.171096
[11:34:47.069] TRAIN: iteration 23213 : loss : 0.058076, loss_ce: 0.001330, loss_dice: 0.114822
[11:34:47.279] TRAIN: iteration 23214 : loss : 0.030944, loss_ce: 0.002114, loss_dice: 0.059775
[11:34:47.488] TRAIN: iteration 23215 : loss : 0.101812, loss_ce: 0.004647, loss_dice: 0.198977
[11:34:47.697] TRAIN: iteration 23216 : loss : 0.059266, loss_ce: 0.000933, loss_dice: 0.117600
[11:34:47.906] TRAIN: iteration 23217 : loss : 0.142839, loss_ce: 0.001088, loss_dice: 0.284590
[11:34:48.115] TRAIN: iteration 23218 : loss : 0.150128, loss_ce: 0.006035, loss_dice: 0.294222
[11:34:48.323] TRAIN: iteration 23219 : loss : 0.216632, loss_ce: 0.001184, loss_dice: 0.432080
[11:34:48.532] TRAIN: iteration 23220 : loss : 0.200639, loss_ce: 0.015465, loss_dice: 0.385813
[11:34:48.775] TRAIN: iteration 23221 : loss : 0.143184, loss_ce: 0.002680, loss_dice: 0.283688
[11:34:48.983] TRAIN: iteration 23222 : loss : 0.074103, loss_ce: 0.002356, loss_dice: 0.145849
[11:34:49.193] TRAIN: iteration 23223 : loss : 0.030867, loss_ce: 0.000590, loss_dice: 0.061145
[11:34:49.402] TRAIN: iteration 23224 : loss : 0.090080, loss_ce: 0.001175, loss_dice: 0.178984
[11:34:49.612] TRAIN: iteration 23225 : loss : 0.201092, loss_ce: 0.001685, loss_dice: 0.400499
[11:34:49.822] TRAIN: iteration 23226 : loss : 0.155105, loss_ce: 0.007561, loss_dice: 0.302648
[11:34:50.031] TRAIN: iteration 23227 : loss : 0.139285, loss_ce: 0.003186, loss_dice: 0.275383
[11:34:50.240] TRAIN: iteration 23228 : loss : 0.088498, loss_ce: 0.002468, loss_dice: 0.174528
[11:34:51.003] TRAIN: iteration 23229 : loss : 0.032240, loss_ce: 0.003170, loss_dice: 0.061309
[11:34:51.212] TRAIN: iteration 23230 : loss : 0.146029, loss_ce: 0.003382, loss_dice: 0.288676
[11:34:51.423] TRAIN: iteration 23231 : loss : 0.250628, loss_ce: 0.001202, loss_dice: 0.500054
[11:34:51.633] TRAIN: iteration 23232 : loss : 0.240497, loss_ce: 0.006567, loss_dice: 0.474427
[11:34:51.841] TRAIN: iteration 23233 : loss : 0.214818, loss_ce: 0.002116, loss_dice: 0.427521
[11:34:52.049] TRAIN: iteration 23234 : loss : 0.159165, loss_ce: 0.003100, loss_dice: 0.315230
[11:34:52.260] TRAIN: iteration 23235 : loss : 0.085080, loss_ce: 0.012359, loss_dice: 0.157802
[11:34:52.473] TRAIN: iteration 23236 : loss : 0.024041, loss_ce: 0.003144, loss_dice: 0.044938
[11:34:52.684] TRAIN: iteration 23237 : loss : 0.066261, loss_ce: 0.006936, loss_dice: 0.125585
[11:34:52.912] TRAIN: iteration 23238 : loss : 0.045499, loss_ce: 0.001139, loss_dice: 0.089858
[11:34:53.131] TRAIN: iteration 23239 : loss : 0.250634, loss_ce: 0.002409, loss_dice: 0.498858
[11:34:53.346] TRAIN: iteration 23240 : loss : 0.099439, loss_ce: 0.004001, loss_dice: 0.194877
[11:34:53.589] TRAIN: iteration 23241 : loss : 0.077855, loss_ce: 0.005702, loss_dice: 0.150009
[11:34:53.799] TRAIN: iteration 23242 : loss : 0.150378, loss_ce: 0.003715, loss_dice: 0.297042
[11:34:54.018] TRAIN: iteration 23243 : loss : 0.138375, loss_ce: 0.006715, loss_dice: 0.270035
[11:34:54.230] TRAIN: iteration 23244 : loss : 0.251158, loss_ce: 0.002180, loss_dice: 0.500136
[11:34:54.440] TRAIN: iteration 23245 : loss : 0.067252, loss_ce: 0.006523, loss_dice: 0.127980
[11:34:54.649] TRAIN: iteration 23246 : loss : 0.093545, loss_ce: 0.002701, loss_dice: 0.184389
[11:34:54.861] TRAIN: iteration 23247 : loss : 0.251334, loss_ce: 0.002493, loss_dice: 0.500174
[11:34:55.070] TRAIN: iteration 23248 : loss : 0.066148, loss_ce: 0.003671, loss_dice: 0.128625
[11:34:55.281] TRAIN: iteration 23249 : loss : 0.192390, loss_ce: 0.006909, loss_dice: 0.377871
[11:34:55.491] TRAIN: iteration 23250 : loss : 0.066426, loss_ce: 0.002265, loss_dice: 0.130587
[11:34:55.702] TRAIN: iteration 23251 : loss : 0.056741, loss_ce: 0.008689, loss_dice: 0.104794
[11:34:55.915] TRAIN: iteration 23252 : loss : 0.250847, loss_ce: 0.001596, loss_dice: 0.500098
[11:34:56.125] TRAIN: iteration 23253 : loss : 0.023259, loss_ce: 0.003025, loss_dice: 0.043494
[11:34:56.337] TRAIN: iteration 23254 : loss : 0.142432, loss_ce: 0.001977, loss_dice: 0.282887
[11:34:56.546] TRAIN: iteration 23255 : loss : 0.061655, loss_ce: 0.004036, loss_dice: 0.119274
[11:34:56.755] TRAIN: iteration 23256 : loss : 0.146424, loss_ce: 0.003623, loss_dice: 0.289225
[11:34:56.966] TRAIN: iteration 23257 : loss : 0.041455, loss_ce: 0.005788, loss_dice: 0.077123
[11:34:57.177] TRAIN: iteration 23258 : loss : 0.066358, loss_ce: 0.000942, loss_dice: 0.131775
[11:34:57.387] TRAIN: iteration 23259 : loss : 0.071601, loss_ce: 0.007134, loss_dice: 0.136068
[11:34:57.597] TRAIN: iteration 23260 : loss : 0.057568, loss_ce: 0.006448, loss_dice: 0.108689
[11:34:57.598] NaN or Inf found in input tensor.
[11:34:57.814] TRAIN: iteration 23261 : loss : 0.238572, loss_ce: 0.000916, loss_dice: 0.476228
[11:34:58.031] TRAIN: iteration 23262 : loss : 0.162803, loss_ce: 0.011878, loss_dice: 0.313728
[11:34:58.244] TRAIN: iteration 23263 : loss : 0.129239, loss_ce: 0.002998, loss_dice: 0.255481
[11:34:58.454] TRAIN: iteration 23264 : loss : 0.063113, loss_ce: 0.003262, loss_dice: 0.122964
[11:34:58.663] TRAIN: iteration 23265 : loss : 0.250878, loss_ce: 0.001665, loss_dice: 0.500092
[11:34:58.872] TRAIN: iteration 23266 : loss : 0.045908, loss_ce: 0.005963, loss_dice: 0.085853
[11:34:59.082] TRAIN: iteration 23267 : loss : 0.032928, loss_ce: 0.003217, loss_dice: 0.062638
[11:34:59.295] TRAIN: iteration 23268 : loss : 0.105938, loss_ce: 0.011195, loss_dice: 0.200681
[11:34:59.506] TRAIN: iteration 23269 : loss : 0.122534, loss_ce: 0.001793, loss_dice: 0.243276
[11:34:59.714] TRAIN: iteration 23270 : loss : 0.037086, loss_ce: 0.002379, loss_dice: 0.071793
[11:34:59.932] TRAIN: iteration 23271 : loss : 0.078232, loss_ce: 0.001830, loss_dice: 0.154634
[11:35:00.148] TRAIN: iteration 23272 : loss : 0.138421, loss_ce: 0.005807, loss_dice: 0.271034
[11:35:00.357] TRAIN: iteration 23273 : loss : 0.101522, loss_ce: 0.002543, loss_dice: 0.200501
[11:35:00.566] TRAIN: iteration 23274 : loss : 0.210333, loss_ce: 0.024734, loss_dice: 0.395931
[11:35:00.776] TRAIN: iteration 23275 : loss : 0.222918, loss_ce: 0.002330, loss_dice: 0.443506
[11:35:00.984] TRAIN: iteration 23276 : loss : 0.071954, loss_ce: 0.002336, loss_dice: 0.141571
[11:35:01.195] TRAIN: iteration 23277 : loss : 0.028502, loss_ce: 0.000648, loss_dice: 0.056356
[11:35:01.409] TRAIN: iteration 23278 : loss : 0.230691, loss_ce: 0.001942, loss_dice: 0.459441
[11:35:01.623] TRAIN: iteration 23279 : loss : 0.029039, loss_ce: 0.004135, loss_dice: 0.053944
[11:35:01.834] TRAIN: iteration 23280 : loss : 0.248205, loss_ce: 0.001787, loss_dice: 0.494623
[11:35:02.081] TRAIN: iteration 23281 : loss : 0.078739, loss_ce: 0.005170, loss_dice: 0.152308
[11:35:02.304] TRAIN: iteration 23282 : loss : 0.112871, loss_ce: 0.002816, loss_dice: 0.222925
[11:35:02.514] TRAIN: iteration 23283 : loss : 0.208629, loss_ce: 0.001446, loss_dice: 0.415811
[11:35:02.730] TRAIN: iteration 23284 : loss : 0.250582, loss_ce: 0.001113, loss_dice: 0.500052
[11:35:03.285] TRAIN: iteration 23285 : loss : 0.085900, loss_ce: 0.001401, loss_dice: 0.170399
[11:35:03.496] TRAIN: iteration 23286 : loss : 0.035035, loss_ce: 0.003756, loss_dice: 0.066313
[11:35:03.713] TRAIN: iteration 23287 : loss : 0.122222, loss_ce: 0.019349, loss_dice: 0.225095
[11:35:03.923] TRAIN: iteration 23288 : loss : 0.052063, loss_ce: 0.000865, loss_dice: 0.103261
[11:35:04.134] TRAIN: iteration 23289 : loss : 0.186294, loss_ce: 0.001956, loss_dice: 0.370632
[11:35:04.344] TRAIN: iteration 23290 : loss : 0.187704, loss_ce: 0.004586, loss_dice: 0.370822
[11:35:04.556] TRAIN: iteration 23291 : loss : 0.091130, loss_ce: 0.001800, loss_dice: 0.180459
[11:35:04.767] TRAIN: iteration 23292 : loss : 0.251055, loss_ce: 0.001988, loss_dice: 0.500122
[11:35:04.983] TRAIN: iteration 23293 : loss : 0.077194, loss_ce: 0.004296, loss_dice: 0.150093
[11:35:05.192] TRAIN: iteration 23294 : loss : 0.046294, loss_ce: 0.001046, loss_dice: 0.091542
[11:35:05.402] TRAIN: iteration 23295 : loss : 0.053231, loss_ce: 0.001467, loss_dice: 0.104996
[11:35:05.611] TRAIN: iteration 23296 : loss : 0.052902, loss_ce: 0.001072, loss_dice: 0.104732
[11:35:05.821] TRAIN: iteration 23297 : loss : 0.214825, loss_ce: 0.000938, loss_dice: 0.428711
[11:35:06.240] TRAIN: iteration 23298 : loss : 0.127589, loss_ce: 0.002777, loss_dice: 0.252401
[11:35:06.452] TRAIN: iteration 23299 : loss : 0.079714, loss_ce: 0.006487, loss_dice: 0.152940
[11:35:06.661] TRAIN: iteration 23300 : loss : 0.012120, loss_ce: 0.001020, loss_dice: 0.023220
[11:35:06.903] TRAIN: iteration 23301 : loss : 0.251052, loss_ce: 0.002003, loss_dice: 0.500102
[11:35:07.118] TRAIN: iteration 23302 : loss : 0.061839, loss_ce: 0.001470, loss_dice: 0.122207
[11:35:07.327] TRAIN: iteration 23303 : loss : 0.090960, loss_ce: 0.003534, loss_dice: 0.178385
[11:35:07.537] TRAIN: iteration 23304 : loss : 0.154861, loss_ce: 0.002673, loss_dice: 0.307050
[11:35:07.756] TRAIN: iteration 23305 : loss : 0.073588, loss_ce: 0.008875, loss_dice: 0.138301
[11:35:07.971] TRAIN: iteration 23306 : loss : 0.044013, loss_ce: 0.001769, loss_dice: 0.086257
[11:35:08.182] TRAIN: iteration 23307 : loss : 0.034761, loss_ce: 0.001584, loss_dice: 0.067938
[11:35:08.436] TRAIN: iteration 23308 : loss : 0.035881, loss_ce: 0.002229, loss_dice: 0.069534
[11:35:08.645] TRAIN: iteration 23309 : loss : 0.064613, loss_ce: 0.002894, loss_dice: 0.126332
[11:35:08.863] TRAIN: iteration 23310 : loss : 0.065462, loss_ce: 0.002485, loss_dice: 0.128439
[11:35:09.082] TRAIN: iteration 23311 : loss : 0.055743, loss_ce: 0.002831, loss_dice: 0.108655
[11:35:09.299] TRAIN: iteration 23312 : loss : 0.100065, loss_ce: 0.002721, loss_dice: 0.197409
[11:35:09.509] TRAIN: iteration 23313 : loss : 0.129235, loss_ce: 0.003641, loss_dice: 0.254829
[11:35:09.718] TRAIN: iteration 23314 : loss : 0.059281, loss_ce: 0.005186, loss_dice: 0.113376
[11:35:09.982] TRAIN: iteration 23315 : loss : 0.148347, loss_ce: 0.006494, loss_dice: 0.290200
[11:35:10.191] TRAIN: iteration 23316 : loss : 0.198501, loss_ce: 0.008969, loss_dice: 0.388033
[11:35:10.401] TRAIN: iteration 23317 : loss : 0.079364, loss_ce: 0.007015, loss_dice: 0.151713
[11:35:10.617] TRAIN: iteration 23318 : loss : 0.103246, loss_ce: 0.004698, loss_dice: 0.201793
[11:35:10.830] TRAIN: iteration 23319 : loss : 0.080839, loss_ce: 0.002991, loss_dice: 0.158687
[11:35:11.043] TRAIN: iteration 23320 : loss : 0.062581, loss_ce: 0.002581, loss_dice: 0.122580
[11:35:11.281] TRAIN: iteration 23321 : loss : 0.120774, loss_ce: 0.000860, loss_dice: 0.240688
[11:35:11.495] TRAIN: iteration 23322 : loss : 0.037229, loss_ce: 0.007495, loss_dice: 0.066964
[11:35:12.601] TRAIN: iteration 23323 : loss : 0.250490, loss_ce: 0.000953, loss_dice: 0.500027
[11:35:13.545] TRAIN: iteration 23324 : loss : 0.060341, loss_ce: 0.006053, loss_dice: 0.114629
[11:35:13.763] TRAIN: iteration 23325 : loss : 0.181728, loss_ce: 0.002908, loss_dice: 0.360547
[11:35:13.974] TRAIN: iteration 23326 : loss : 0.156963, loss_ce: 0.004410, loss_dice: 0.309516
[11:35:14.182] TRAIN: iteration 23327 : loss : 0.241057, loss_ce: 0.002378, loss_dice: 0.479737
[11:35:14.391] TRAIN: iteration 23328 : loss : 0.087301, loss_ce: 0.005570, loss_dice: 0.169031
[11:35:14.600] TRAIN: iteration 23329 : loss : 0.056812, loss_ce: 0.002185, loss_dice: 0.111439
[11:35:14.810] TRAIN: iteration 23330 : loss : 0.250875, loss_ce: 0.001648, loss_dice: 0.500103
[11:35:15.021] TRAIN: iteration 23331 : loss : 0.032804, loss_ce: 0.001344, loss_dice: 0.064265
[11:35:15.230] TRAIN: iteration 23332 : loss : 0.039346, loss_ce: 0.002313, loss_dice: 0.076378
[11:35:15.442] TRAIN: iteration 23333 : loss : 0.097032, loss_ce: 0.001871, loss_dice: 0.192194
[11:35:15.652] TRAIN: iteration 23334 : loss : 0.080461, loss_ce: 0.001551, loss_dice: 0.159371
[11:35:15.865] TRAIN: iteration 23335 : loss : 0.250432, loss_ce: 0.000839, loss_dice: 0.500024
[11:35:16.081] TRAIN: iteration 23336 : loss : 0.251594, loss_ce: 0.002973, loss_dice: 0.500214
[11:35:16.292] TRAIN: iteration 23337 : loss : 0.117950, loss_ce: 0.002674, loss_dice: 0.233225
[11:35:16.507] TRAIN: iteration 23338 : loss : 0.250464, loss_ce: 0.000888, loss_dice: 0.500040
[11:35:16.715] TRAIN: iteration 23339 : loss : 0.251692, loss_ce: 0.003165, loss_dice: 0.500219
[11:35:17.884] TRAIN: iteration 23340 : loss : 0.239504, loss_ce: 0.002280, loss_dice: 0.476728
[11:35:18.128] TRAIN: iteration 23341 : loss : 0.119660, loss_ce: 0.020289, loss_dice: 0.219030
[11:35:18.340] TRAIN: iteration 23342 : loss : 0.248374, loss_ce: 0.001710, loss_dice: 0.495038
[11:35:18.549] TRAIN: iteration 23343 : loss : 0.200465, loss_ce: 0.003074, loss_dice: 0.397855
[11:35:18.759] TRAIN: iteration 23344 : loss : 0.063560, loss_ce: 0.001853, loss_dice: 0.125268
[11:35:18.975] TRAIN: iteration 23345 : loss : 0.047561, loss_ce: 0.002316, loss_dice: 0.092805
[11:35:19.187] TRAIN: iteration 23346 : loss : 0.116301, loss_ce: 0.002883, loss_dice: 0.229720
[11:35:19.396] TRAIN: iteration 23347 : loss : 0.244904, loss_ce: 0.004998, loss_dice: 0.484809
[11:35:19.605] TRAIN: iteration 23348 : loss : 0.042971, loss_ce: 0.002721, loss_dice: 0.083221
[11:35:19.815] TRAIN: iteration 23349 : loss : 0.172106, loss_ce: 0.019571, loss_dice: 0.324640
[11:35:20.114] TRAIN: iteration 23350 : loss : 0.250719, loss_ce: 0.001374, loss_dice: 0.500064
[11:35:20.325] TRAIN: iteration 23351 : loss : 0.170652, loss_ce: 0.002240, loss_dice: 0.339064
[11:35:20.539] TRAIN: iteration 23352 : loss : 0.073576, loss_ce: 0.003867, loss_dice: 0.143286
[11:35:20.753] TRAIN: iteration 23353 : loss : 0.117731, loss_ce: 0.005659, loss_dice: 0.229803
[11:35:20.969] TRAIN: iteration 23354 : loss : 0.067784, loss_ce: 0.011422, loss_dice: 0.124146
[11:35:21.182] TRAIN: iteration 23355 : loss : 0.132681, loss_ce: 0.005535, loss_dice: 0.259827
[11:35:21.394] TRAIN: iteration 23356 : loss : 0.251072, loss_ce: 0.002827, loss_dice: 0.499316
[11:35:21.606] TRAIN: iteration 23357 : loss : 0.119089, loss_ce: 0.001858, loss_dice: 0.236321
[11:35:21.817] TRAIN: iteration 23358 : loss : 0.251004, loss_ce: 0.001911, loss_dice: 0.500098
[11:35:22.027] TRAIN: iteration 23359 : loss : 0.123942, loss_ce: 0.004003, loss_dice: 0.243881
[11:35:22.237] TRAIN: iteration 23360 : loss : 0.177732, loss_ce: 0.002889, loss_dice: 0.352576
[11:35:22.875] TRAIN: iteration 23361 : loss : 0.224833, loss_ce: 0.002040, loss_dice: 0.447626
[11:35:23.087] TRAIN: iteration 23362 : loss : 0.121465, loss_ce: 0.003055, loss_dice: 0.239874
[11:35:23.299] TRAIN: iteration 23363 : loss : 0.075134, loss_ce: 0.002427, loss_dice: 0.147840
[11:35:23.508] TRAIN: iteration 23364 : loss : 0.048445, loss_ce: 0.005584, loss_dice: 0.091306
[11:35:23.718] TRAIN: iteration 23365 : loss : 0.069798, loss_ce: 0.002227, loss_dice: 0.137369
[11:35:23.927] TRAIN: iteration 23366 : loss : 0.075999, loss_ce: 0.002738, loss_dice: 0.149261
[11:35:24.137] TRAIN: iteration 23367 : loss : 0.020614, loss_ce: 0.001752, loss_dice: 0.039477
[11:35:24.346] TRAIN: iteration 23368 : loss : 0.251380, loss_ce: 0.002779, loss_dice: 0.499982
[11:35:24.555] TRAIN: iteration 23369 : loss : 0.042539, loss_ce: 0.000852, loss_dice: 0.084227
[11:35:24.810] TRAIN: iteration 23370 : loss : 0.187228, loss_ce: 0.002350, loss_dice: 0.372106
[11:35:25.024] TRAIN: iteration 23371 : loss : 0.091664, loss_ce: 0.002206, loss_dice: 0.181122
[11:35:25.235] TRAIN: iteration 23372 : loss : 0.066626, loss_ce: 0.001237, loss_dice: 0.132016
[11:35:25.444] TRAIN: iteration 23373 : loss : 0.067862, loss_ce: 0.003590, loss_dice: 0.132134
[11:35:25.655] TRAIN: iteration 23374 : loss : 0.122811, loss_ce: 0.003785, loss_dice: 0.241837
[11:35:25.865] TRAIN: iteration 23375 : loss : 0.043321, loss_ce: 0.001254, loss_dice: 0.085389
[11:35:26.081] TRAIN: iteration 23376 : loss : 0.120059, loss_ce: 0.001956, loss_dice: 0.238163
[11:35:26.293] TRAIN: iteration 23377 : loss : 0.045962, loss_ce: 0.005720, loss_dice: 0.086203
[11:35:26.507] TRAIN: iteration 23378 : loss : 0.149898, loss_ce: 0.005451, loss_dice: 0.294344
[11:35:26.720] TRAIN: iteration 23379 : loss : 0.251324, loss_ce: 0.002490, loss_dice: 0.500159
[11:35:26.931] TRAIN: iteration 23380 : loss : 0.200316, loss_ce: 0.004715, loss_dice: 0.395916
[11:35:27.166] TRAIN: iteration 23381 : loss : 0.110782, loss_ce: 0.002279, loss_dice: 0.219286
[11:35:27.381] TRAIN: iteration 23382 : loss : 0.088645, loss_ce: 0.007011, loss_dice: 0.170278
[11:35:27.589] TRAIN: iteration 23383 : loss : 0.034069, loss_ce: 0.001998, loss_dice: 0.066139
[11:35:27.801] TRAIN: iteration 23384 : loss : 0.047179, loss_ce: 0.002413, loss_dice: 0.091945
[11:35:28.012] TRAIN: iteration 23385 : loss : 0.100798, loss_ce: 0.004797, loss_dice: 0.196799
[11:35:28.222] TRAIN: iteration 23386 : loss : 0.082560, loss_ce: 0.006073, loss_dice: 0.159048
[11:35:28.436] TRAIN: iteration 23387 : loss : 0.139490, loss_ce: 0.002688, loss_dice: 0.276291
[11:35:28.648] TRAIN: iteration 23388 : loss : 0.110117, loss_ce: 0.005855, loss_dice: 0.214379
[11:35:28.859] TRAIN: iteration 23389 : loss : 0.221096, loss_ce: 0.004272, loss_dice: 0.437919
[11:35:29.069] TRAIN: iteration 23390 : loss : 0.231898, loss_ce: 0.002754, loss_dice: 0.461042
[11:35:29.280] TRAIN: iteration 23391 : loss : 0.132400, loss_ce: 0.009484, loss_dice: 0.255315
[11:35:29.490] TRAIN: iteration 23392 : loss : 0.044972, loss_ce: 0.002236, loss_dice: 0.087709
[11:35:29.699] TRAIN: iteration 23393 : loss : 0.140209, loss_ce: 0.009625, loss_dice: 0.270793
[11:35:29.908] TRAIN: iteration 23394 : loss : 0.072861, loss_ce: 0.001166, loss_dice: 0.144557
[11:35:30.116] TRAIN: iteration 23395 : loss : 0.081738, loss_ce: 0.002394, loss_dice: 0.161083
[11:35:30.334] TRAIN: iteration 23396 : loss : 0.251193, loss_ce: 0.002278, loss_dice: 0.500109
[11:35:30.544] TRAIN: iteration 23397 : loss : 0.052959, loss_ce: 0.001942, loss_dice: 0.103976
[11:35:30.783] TRAIN: iteration 23398 : loss : 0.200583, loss_ce: 0.011884, loss_dice: 0.389283
[11:35:30.992] TRAIN: iteration 23399 : loss : 0.250738, loss_ce: 0.001418, loss_dice: 0.500058
[11:35:31.346] TRAIN: iteration 23400 : loss : 0.245515, loss_ce: 0.001774, loss_dice: 0.489256
[11:35:31.583] TRAIN: iteration 23401 : loss : 0.103054, loss_ce: 0.002346, loss_dice: 0.203762
[11:35:31.796] TRAIN: iteration 23402 : loss : 0.251792, loss_ce: 0.003439, loss_dice: 0.500145
[11:35:32.006] TRAIN: iteration 23403 : loss : 0.233785, loss_ce: 0.003252, loss_dice: 0.464319
[11:35:32.223] TRAIN: iteration 23404 : loss : 0.195103, loss_ce: 0.001697, loss_dice: 0.388509
[11:35:32.441] TRAIN: iteration 23405 : loss : 0.091066, loss_ce: 0.005188, loss_dice: 0.176944
[11:35:32.654] TRAIN: iteration 23406 : loss : 0.154837, loss_ce: 0.030072, loss_dice: 0.279601
[11:35:33.003] TRAIN: iteration 23407 : loss : 0.137048, loss_ce: 0.018557, loss_dice: 0.255538
[11:35:34.731] TRAIN: iteration 23408 : loss : 0.036127, loss_ce: 0.002949, loss_dice: 0.069306
[11:35:34.940] TRAIN: iteration 23409 : loss : 0.251316, loss_ce: 0.002478, loss_dice: 0.500153
[11:35:35.151] TRAIN: iteration 23410 : loss : 0.218856, loss_ce: 0.002269, loss_dice: 0.435442
[11:35:35.359] TRAIN: iteration 23411 : loss : 0.191025, loss_ce: 0.007997, loss_dice: 0.374053
[11:35:35.569] TRAIN: iteration 23412 : loss : 0.072691, loss_ce: 0.004131, loss_dice: 0.141252
[11:35:35.778] TRAIN: iteration 23413 : loss : 0.201323, loss_ce: 0.003616, loss_dice: 0.399029
[11:35:35.990] TRAIN: iteration 23414 : loss : 0.160136, loss_ce: 0.007701, loss_dice: 0.312571
[11:35:36.197] TRAIN: iteration 23415 : loss : 0.047021, loss_ce: 0.003911, loss_dice: 0.090131
[11:35:38.663] TRAIN: iteration 23416 : loss : 0.174118, loss_ce: 0.005431, loss_dice: 0.342804
[11:35:38.877] TRAIN: iteration 23417 : loss : 0.126996, loss_ce: 0.009621, loss_dice: 0.244371
[11:35:39.086] TRAIN: iteration 23418 : loss : 0.065219, loss_ce: 0.010166, loss_dice: 0.120272
[11:35:39.300] TRAIN: iteration 23419 : loss : 0.252352, loss_ce: 0.004416, loss_dice: 0.500288
[11:35:39.509] TRAIN: iteration 23420 : loss : 0.099951, loss_ce: 0.006318, loss_dice: 0.193585
[11:35:39.755] TRAIN: iteration 23421 : loss : 0.112296, loss_ce: 0.004200, loss_dice: 0.220393
[11:35:39.965] TRAIN: iteration 23422 : loss : 0.234484, loss_ce: 0.002404, loss_dice: 0.466563
[11:35:40.180] TRAIN: iteration 23423 : loss : 0.104593, loss_ce: 0.006883, loss_dice: 0.202303
[11:35:41.095] TRAIN: iteration 23424 : loss : 0.042994, loss_ce: 0.003963, loss_dice: 0.082026
[11:35:41.305] TRAIN: iteration 23425 : loss : 0.061792, loss_ce: 0.001641, loss_dice: 0.121943
[11:35:41.516] TRAIN: iteration 23426 : loss : 0.055229, loss_ce: 0.005715, loss_dice: 0.104744
[11:35:41.726] TRAIN: iteration 23427 : loss : 0.025725, loss_ce: 0.000895, loss_dice: 0.050555
[11:35:41.937] TRAIN: iteration 23428 : loss : 0.251005, loss_ce: 0.001899, loss_dice: 0.500111
[11:35:42.148] TRAIN: iteration 23429 : loss : 0.081693, loss_ce: 0.003822, loss_dice: 0.159564
[11:35:42.359] TRAIN: iteration 23430 : loss : 0.242038, loss_ce: 0.004173, loss_dice: 0.479904
[11:35:42.571] TRAIN: iteration 23431 : loss : 0.163737, loss_ce: 0.004810, loss_dice: 0.322664
[11:35:44.601] TRAIN: iteration 23432 : loss : 0.250536, loss_ce: 0.001021, loss_dice: 0.500050
[11:35:44.822] TRAIN: iteration 23433 : loss : 0.120710, loss_ce: 0.001373, loss_dice: 0.240047
[11:35:45.032] TRAIN: iteration 23434 : loss : 0.247508, loss_ce: 0.002407, loss_dice: 0.492610
[11:35:45.241] TRAIN: iteration 23435 : loss : 0.040147, loss_ce: 0.001788, loss_dice: 0.078506
[11:35:45.452] TRAIN: iteration 23436 : loss : 0.090001, loss_ce: 0.001247, loss_dice: 0.178755
[11:35:45.662] TRAIN: iteration 23437 : loss : 0.106103, loss_ce: 0.003487, loss_dice: 0.208720
[11:35:45.875] TRAIN: iteration 23438 : loss : 0.240529, loss_ce: 0.001103, loss_dice: 0.479954
[11:35:46.088] TRAIN: iteration 23439 : loss : 0.142718, loss_ce: 0.008915, loss_dice: 0.276521
[11:35:46.302] TRAIN: iteration 23440 : loss : 0.212281, loss_ce: 0.004126, loss_dice: 0.420436
[11:35:46.540] TRAIN: iteration 23441 : loss : 0.127715, loss_ce: 0.003007, loss_dice: 0.252423
[11:35:46.751] TRAIN: iteration 23442 : loss : 0.250359, loss_ce: 0.000703, loss_dice: 0.500015
[11:35:46.962] TRAIN: iteration 23443 : loss : 0.063585, loss_ce: 0.001540, loss_dice: 0.125629
[11:35:47.172] TRAIN: iteration 23444 : loss : 0.250106, loss_ce: 0.001502, loss_dice: 0.498710
[11:35:47.381] TRAIN: iteration 23445 : loss : 0.039094, loss_ce: 0.001395, loss_dice: 0.076794
[11:35:47.591] TRAIN: iteration 23446 : loss : 0.041696, loss_ce: 0.001221, loss_dice: 0.082171
[11:35:47.800] TRAIN: iteration 23447 : loss : 0.147592, loss_ce: 0.002280, loss_dice: 0.292905
[11:35:48.010] TRAIN: iteration 23448 : loss : 0.039721, loss_ce: 0.002323, loss_dice: 0.077120
[11:35:48.222] TRAIN: iteration 23449 : loss : 0.069590, loss_ce: 0.005690, loss_dice: 0.133491
[11:35:48.432] TRAIN: iteration 23450 : loss : 0.106297, loss_ce: 0.008261, loss_dice: 0.204332
[11:35:48.641] TRAIN: iteration 23451 : loss : 0.251451, loss_ce: 0.002700, loss_dice: 0.500202
[11:35:48.853] TRAIN: iteration 23452 : loss : 0.250378, loss_ce: 0.000743, loss_dice: 0.500013
[11:35:49.065] TRAIN: iteration 23453 : loss : 0.226847, loss_ce: 0.000964, loss_dice: 0.452730
[11:35:49.275] TRAIN: iteration 23454 : loss : 0.214673, loss_ce: 0.003702, loss_dice: 0.425644
[11:35:49.484] TRAIN: iteration 23455 : loss : 0.179254, loss_ce: 0.007001, loss_dice: 0.351508
[11:35:49.695] TRAIN: iteration 23456 : loss : 0.137780, loss_ce: 0.009417, loss_dice: 0.266142
[11:35:49.906] TRAIN: iteration 23457 : loss : 0.029527, loss_ce: 0.001712, loss_dice: 0.057343
[11:35:51.747] TRAIN: iteration 23458 : loss : 0.249335, loss_ce: 0.002149, loss_dice: 0.496520
[11:35:51.957] TRAIN: iteration 23459 : loss : 0.064906, loss_ce: 0.003525, loss_dice: 0.126287
[11:35:52.167] TRAIN: iteration 23460 : loss : 0.130124, loss_ce: 0.003372, loss_dice: 0.256876
[11:35:52.407] TRAIN: iteration 23461 : loss : 0.160072, loss_ce: 0.002303, loss_dice: 0.317840
[11:35:52.618] TRAIN: iteration 23462 : loss : 0.250997, loss_ce: 0.001893, loss_dice: 0.500100
[11:35:52.830] TRAIN: iteration 23463 : loss : 0.046890, loss_ce: 0.003615, loss_dice: 0.090165
[11:35:53.043] TRAIN: iteration 23464 : loss : 0.177143, loss_ce: 0.009122, loss_dice: 0.345164
[11:35:53.254] TRAIN: iteration 23465 : loss : 0.209605, loss_ce: 0.004659, loss_dice: 0.414550
[11:35:53.859] TRAIN: iteration 23466 : loss : 0.073104, loss_ce: 0.004901, loss_dice: 0.141308
[11:35:54.069] TRAIN: iteration 23467 : loss : 0.161673, loss_ce: 0.004901, loss_dice: 0.318445
[11:35:54.286] TRAIN: iteration 23468 : loss : 0.070855, loss_ce: 0.002519, loss_dice: 0.139191
[11:35:54.497] TRAIN: iteration 23469 : loss : 0.138119, loss_ce: 0.002520, loss_dice: 0.273718
[11:35:54.707] TRAIN: iteration 23470 : loss : 0.210534, loss_ce: 0.003901, loss_dice: 0.417166
[11:35:54.916] TRAIN: iteration 23471 : loss : 0.050663, loss_ce: 0.001759, loss_dice: 0.099566
[11:35:55.127] TRAIN: iteration 23472 : loss : 0.055144, loss_ce: 0.003337, loss_dice: 0.106950
[11:35:55.337] TRAIN: iteration 23473 : loss : 0.047211, loss_ce: 0.002455, loss_dice: 0.091967
[11:35:55.557] TRAIN: iteration 23474 : loss : 0.060596, loss_ce: 0.001420, loss_dice: 0.119771
[11:35:55.768] TRAIN: iteration 23475 : loss : 0.237936, loss_ce: 0.009060, loss_dice: 0.466812
[11:35:55.978] TRAIN: iteration 23476 : loss : 0.165778, loss_ce: 0.008300, loss_dice: 0.323255
[11:35:56.188] TRAIN: iteration 23477 : loss : 0.023732, loss_ce: 0.001052, loss_dice: 0.046413
[11:35:56.399] TRAIN: iteration 23478 : loss : 0.152418, loss_ce: 0.001552, loss_dice: 0.303284
[11:35:56.610] TRAIN: iteration 23479 : loss : 0.078007, loss_ce: 0.002472, loss_dice: 0.153542
[11:35:56.820] TRAIN: iteration 23480 : loss : 0.136387, loss_ce: 0.001645, loss_dice: 0.271129
[11:35:57.064] TRAIN: iteration 23481 : loss : 0.104865, loss_ce: 0.006218, loss_dice: 0.203511
[11:35:57.363] TRAIN: iteration 23482 : loss : 0.021692, loss_ce: 0.001933, loss_dice: 0.041451
[11:35:57.584] TRAIN: iteration 23483 : loss : 0.250592, loss_ce: 0.001130, loss_dice: 0.500055
[11:35:59.188] TRAIN: iteration 23484 : loss : 0.250341, loss_ce: 0.000672, loss_dice: 0.500009
[11:35:59.396] TRAIN: iteration 23485 : loss : 0.060040, loss_ce: 0.003921, loss_dice: 0.116158
[11:35:59.605] TRAIN: iteration 23486 : loss : 0.070076, loss_ce: 0.004780, loss_dice: 0.135371
[11:35:59.813] TRAIN: iteration 23487 : loss : 0.119635, loss_ce: 0.003915, loss_dice: 0.235356
[11:36:00.023] TRAIN: iteration 23488 : loss : 0.244642, loss_ce: 0.001248, loss_dice: 0.488036
[11:36:00.235] TRAIN: iteration 23489 : loss : 0.030568, loss_ce: 0.001295, loss_dice: 0.059840
[11:36:00.446] TRAIN: iteration 23490 : loss : 0.038087, loss_ce: 0.002677, loss_dice: 0.073498
[11:36:00.654] TRAIN: iteration 23491 : loss : 0.045218, loss_ce: 0.005814, loss_dice: 0.084622
[11:36:01.382] TRAIN: iteration 23492 : loss : 0.078180, loss_ce: 0.005057, loss_dice: 0.151303
[11:36:01.591] TRAIN: iteration 23493 : loss : 0.098475, loss_ce: 0.004619, loss_dice: 0.192331
[11:36:01.799] TRAIN: iteration 23494 : loss : 0.082924, loss_ce: 0.002349, loss_dice: 0.163500
[11:36:02.007] TRAIN: iteration 23495 : loss : 0.133348, loss_ce: 0.004717, loss_dice: 0.261979
[11:36:02.217] TRAIN: iteration 23496 : loss : 0.035220, loss_ce: 0.002480, loss_dice: 0.067961
[11:36:02.426] TRAIN: iteration 23497 : loss : 0.039700, loss_ce: 0.005688, loss_dice: 0.073713
[11:36:02.633] TRAIN: iteration 23498 : loss : 0.228533, loss_ce: 0.008581, loss_dice: 0.448485
[11:36:02.846] TRAIN: iteration 23499 : loss : 0.024286, loss_ce: 0.000888, loss_dice: 0.047684
[11:36:03.056] TRAIN: iteration 23500 : loss : 0.143736, loss_ce: 0.004286, loss_dice: 0.283187
[11:36:03.294] TRAIN: iteration 23501 : loss : 0.038559, loss_ce: 0.005417, loss_dice: 0.071702
[11:36:03.504] TRAIN: iteration 23502 : loss : 0.069207, loss_ce: 0.006087, loss_dice: 0.132327
[11:36:03.712] TRAIN: iteration 23503 : loss : 0.137915, loss_ce: 0.007726, loss_dice: 0.268104
[11:36:03.925] TRAIN: iteration 23504 : loss : 0.065249, loss_ce: 0.003150, loss_dice: 0.127347
[11:36:04.134] TRAIN: iteration 23505 : loss : 0.250892, loss_ce: 0.001676, loss_dice: 0.500108
[11:36:04.345] TRAIN: iteration 23506 : loss : 0.036087, loss_ce: 0.001539, loss_dice: 0.070635
[11:36:04.554] TRAIN: iteration 23507 : loss : 0.250377, loss_ce: 0.000734, loss_dice: 0.500020
[11:36:04.769] TRAIN: iteration 23508 : loss : 0.213926, loss_ce: 0.020004, loss_dice: 0.407848
[11:36:04.979] TRAIN: iteration 23509 : loss : 0.101090, loss_ce: 0.003198, loss_dice: 0.198982
[11:36:05.189] TRAIN: iteration 23510 : loss : 0.142467, loss_ce: 0.001512, loss_dice: 0.283423
[11:36:05.398] TRAIN: iteration 23511 : loss : 0.133750, loss_ce: 0.008709, loss_dice: 0.258791
[11:36:05.609] TRAIN: iteration 23512 : loss : 0.038734, loss_ce: 0.002006, loss_dice: 0.075461
[11:36:05.821] TRAIN: iteration 23513 : loss : 0.048947, loss_ce: 0.005078, loss_dice: 0.092816
[11:36:06.049] TRAIN: iteration 23514 : loss : 0.099791, loss_ce: 0.007796, loss_dice: 0.191787
[11:36:06.262] TRAIN: iteration 23515 : loss : 0.097865, loss_ce: 0.004730, loss_dice: 0.191001
[11:36:06.480] TRAIN: iteration 23516 : loss : 0.210200, loss_ce: 0.001118, loss_dice: 0.419281
[11:36:06.689] TRAIN: iteration 23517 : loss : 0.048286, loss_ce: 0.005152, loss_dice: 0.091420
[11:36:06.898] TRAIN: iteration 23518 : loss : 0.036022, loss_ce: 0.000927, loss_dice: 0.071116
[11:36:07.107] TRAIN: iteration 23519 : loss : 0.044344, loss_ce: 0.001571, loss_dice: 0.087117
[11:36:07.316] TRAIN: iteration 23520 : loss : 0.054388, loss_ce: 0.007109, loss_dice: 0.101666
[11:36:08.287] TRAIN: iteration 23521 : loss : 0.263117, loss_ce: 0.025623, loss_dice: 0.500612
[11:36:08.498] TRAIN: iteration 23522 : loss : 0.095568, loss_ce: 0.005110, loss_dice: 0.186027
[11:36:08.913] TRAIN: iteration 23523 : loss : 0.065822, loss_ce: 0.001923, loss_dice: 0.129721
[11:36:09.122] TRAIN: iteration 23524 : loss : 0.116728, loss_ce: 0.001367, loss_dice: 0.232089
[11:36:09.338] TRAIN: iteration 23525 : loss : 0.250525, loss_ce: 0.001016, loss_dice: 0.500033
[11:36:09.580] TRAIN: iteration 23526 : loss : 0.022328, loss_ce: 0.001107, loss_dice: 0.043550
[11:36:09.789] TRAIN: iteration 23527 : loss : 0.083946, loss_ce: 0.003718, loss_dice: 0.164175
[11:36:09.999] TRAIN: iteration 23528 : loss : 0.136224, loss_ce: 0.006064, loss_dice: 0.266383
[11:36:10.209] TRAIN: iteration 23529 : loss : 0.061612, loss_ce: 0.006416, loss_dice: 0.116808
[11:36:10.419] TRAIN: iteration 23530 : loss : 0.053098, loss_ce: 0.001501, loss_dice: 0.104696
[11:36:11.321] TRAIN: iteration 23531 : loss : 0.107515, loss_ce: 0.013374, loss_dice: 0.201657
[11:36:11.531] TRAIN: iteration 23532 : loss : 0.085529, loss_ce: 0.009365, loss_dice: 0.161693
[11:36:12.070] TRAIN: iteration 23533 : loss : 0.058667, loss_ce: 0.003727, loss_dice: 0.113606
[11:36:12.280] TRAIN: iteration 23534 : loss : 0.070898, loss_ce: 0.001871, loss_dice: 0.139926
[11:36:12.488] TRAIN: iteration 23535 : loss : 0.180359, loss_ce: 0.001662, loss_dice: 0.359056
[11:36:12.698] TRAIN: iteration 23536 : loss : 0.180378, loss_ce: 0.007419, loss_dice: 0.353336
[11:36:12.907] TRAIN: iteration 23537 : loss : 0.041696, loss_ce: 0.001224, loss_dice: 0.082167
[11:36:13.116] TRAIN: iteration 23538 : loss : 0.101779, loss_ce: 0.011988, loss_dice: 0.191570
[11:36:13.647] TRAIN: iteration 23539 : loss : 0.043383, loss_ce: 0.002389, loss_dice: 0.084377
[11:36:13.858] TRAIN: iteration 23540 : loss : 0.067384, loss_ce: 0.003833, loss_dice: 0.130935
[11:36:15.119] TRAIN: iteration 23541 : loss : 0.184426, loss_ce: 0.008618, loss_dice: 0.360235
[11:36:15.327] TRAIN: iteration 23542 : loss : 0.050215, loss_ce: 0.003514, loss_dice: 0.096917
[11:36:15.546] TRAIN: iteration 23543 : loss : 0.115660, loss_ce: 0.003294, loss_dice: 0.228027
[11:36:15.757] TRAIN: iteration 23544 : loss : 0.123211, loss_ce: 0.002241, loss_dice: 0.244181
[11:36:15.967] TRAIN: iteration 23545 : loss : 0.074120, loss_ce: 0.002721, loss_dice: 0.145518
[11:36:16.180] TRAIN: iteration 23546 : loss : 0.118367, loss_ce: 0.003785, loss_dice: 0.232948
[11:36:16.395] TRAIN: iteration 23547 : loss : 0.035499, loss_ce: 0.002786, loss_dice: 0.068211
[11:36:16.604] TRAIN: iteration 23548 : loss : 0.149581, loss_ce: 0.007796, loss_dice: 0.291366
[11:36:17.171] TRAIN: iteration 23549 : loss : 0.052407, loss_ce: 0.003209, loss_dice: 0.101606
[11:36:17.380] TRAIN: iteration 23550 : loss : 0.208645, loss_ce: 0.002005, loss_dice: 0.415286
[11:36:17.590] TRAIN: iteration 23551 : loss : 0.029911, loss_ce: 0.001418, loss_dice: 0.058404
[11:36:17.800] TRAIN: iteration 23552 : loss : 0.186485, loss_ce: 0.007292, loss_dice: 0.365679
[11:36:18.011] TRAIN: iteration 23553 : loss : 0.176594, loss_ce: 0.007928, loss_dice: 0.345260
[11:36:18.225] TRAIN: iteration 23554 : loss : 0.075430, loss_ce: 0.004100, loss_dice: 0.146759
[11:36:18.434] TRAIN: iteration 23555 : loss : 0.040059, loss_ce: 0.001899, loss_dice: 0.078219
[11:36:18.644] TRAIN: iteration 23556 : loss : 0.049621, loss_ce: 0.007678, loss_dice: 0.091563
[11:36:18.853] TRAIN: iteration 23557 : loss : 0.038545, loss_ce: 0.003775, loss_dice: 0.073315
[11:36:19.062] TRAIN: iteration 23558 : loss : 0.042211, loss_ce: 0.004025, loss_dice: 0.080397
[11:36:19.278] TRAIN: iteration 23559 : loss : 0.250984, loss_ce: 0.001865, loss_dice: 0.500102
[11:36:19.487] TRAIN: iteration 23560 : loss : 0.046335, loss_ce: 0.009357, loss_dice: 0.083313
[11:36:19.734] TRAIN: iteration 23561 : loss : 0.063986, loss_ce: 0.010084, loss_dice: 0.117888
[11:36:19.945] TRAIN: iteration 23562 : loss : 0.088683, loss_ce: 0.002806, loss_dice: 0.174560
[11:36:20.154] TRAIN: iteration 23563 : loss : 0.250783, loss_ce: 0.001481, loss_dice: 0.500084
[11:36:20.364] TRAIN: iteration 23564 : loss : 0.048824, loss_ce: 0.002365, loss_dice: 0.095282
[11:36:20.575] TRAIN: iteration 23565 : loss : 0.241416, loss_ce: 0.003993, loss_dice: 0.478840
[11:36:20.785] TRAIN: iteration 23566 : loss : 0.124359, loss_ce: 0.004779, loss_dice: 0.243940
[11:36:20.993] TRAIN: iteration 23567 : loss : 0.120097, loss_ce: 0.001660, loss_dice: 0.238534
[11:36:21.206] TRAIN: iteration 23568 : loss : 0.073895, loss_ce: 0.008119, loss_dice: 0.139670
[11:36:21.414] TRAIN: iteration 23569 : loss : 0.019750, loss_ce: 0.001024, loss_dice: 0.038475
[11:36:21.623] TRAIN: iteration 23570 : loss : 0.087958, loss_ce: 0.004149, loss_dice: 0.171768
[11:36:22.644] TRAIN: iteration 23571 : loss : 0.028914, loss_ce: 0.002386, loss_dice: 0.055443
[11:36:22.861] TRAIN: iteration 23572 : loss : 0.188359, loss_ce: 0.002919, loss_dice: 0.373800
[11:36:23.072] TRAIN: iteration 23573 : loss : 0.048606, loss_ce: 0.004575, loss_dice: 0.092637
[11:36:23.283] TRAIN: iteration 23574 : loss : 0.048693, loss_ce: 0.001997, loss_dice: 0.095389
[11:36:23.495] TRAIN: iteration 23575 : loss : 0.093853, loss_ce: 0.004662, loss_dice: 0.183044
[11:36:23.704] TRAIN: iteration 23576 : loss : 0.038757, loss_ce: 0.002981, loss_dice: 0.074533
[11:36:24.103] TRAIN: iteration 23577 : loss : 0.118552, loss_ce: 0.003232, loss_dice: 0.233871
[11:36:24.314] TRAIN: iteration 23578 : loss : 0.043165, loss_ce: 0.001680, loss_dice: 0.084651
[11:36:25.715] TRAIN: iteration 23579 : loss : 0.174580, loss_ce: 0.001511, loss_dice: 0.347648
[11:36:25.934] TRAIN: iteration 23580 : loss : 0.102579, loss_ce: 0.006746, loss_dice: 0.198413
[11:36:26.183] TRAIN: iteration 23581 : loss : 0.067435, loss_ce: 0.004009, loss_dice: 0.130862
[11:36:26.394] TRAIN: iteration 23582 : loss : 0.250474, loss_ce: 0.000907, loss_dice: 0.500042
[11:36:26.603] TRAIN: iteration 23583 : loss : 0.122923, loss_ce: 0.001969, loss_dice: 0.243876
[11:36:26.813] TRAIN: iteration 23584 : loss : 0.138977, loss_ce: 0.004245, loss_dice: 0.273709
[11:36:27.022] TRAIN: iteration 23585 : loss : 0.250399, loss_ce: 0.000780, loss_dice: 0.500018
[11:36:27.233] TRAIN: iteration 23586 : loss : 0.154281, loss_ce: 0.006819, loss_dice: 0.301744
[11:36:28.972] TRAIN: iteration 23587 : loss : 0.041801, loss_ce: 0.004972, loss_dice: 0.078630
[11:36:29.182] TRAIN: iteration 23588 : loss : 0.153821, loss_ce: 0.002024, loss_dice: 0.305618
[11:36:29.393] TRAIN: iteration 23589 : loss : 0.042801, loss_ce: 0.002768, loss_dice: 0.082834
[11:36:29.610] TRAIN: iteration 23590 : loss : 0.070683, loss_ce: 0.008408, loss_dice: 0.132958
[11:36:29.820] TRAIN: iteration 23591 : loss : 0.060103, loss_ce: 0.003365, loss_dice: 0.116841
[11:36:30.033] TRAIN: iteration 23592 : loss : 0.250576, loss_ce: 0.001105, loss_dice: 0.500047
[11:36:30.246] TRAIN: iteration 23593 : loss : 0.246481, loss_ce: 0.001017, loss_dice: 0.491945
[11:36:30.455] TRAIN: iteration 23594 : loss : 0.027207, loss_ce: 0.004860, loss_dice: 0.049554
[11:36:31.270] TRAIN: iteration 23595 : loss : 0.245212, loss_ce: 0.002098, loss_dice: 0.488326
[11:36:31.482] TRAIN: iteration 23596 : loss : 0.098750, loss_ce: 0.003894, loss_dice: 0.193607
[11:36:31.693] TRAIN: iteration 23597 : loss : 0.130956, loss_ce: 0.002877, loss_dice: 0.259035
[11:36:31.910] TRAIN: iteration 23598 : loss : 0.030590, loss_ce: 0.002300, loss_dice: 0.058880
[11:36:32.119] TRAIN: iteration 23599 : loss : 0.199911, loss_ce: 0.004207, loss_dice: 0.395614
[11:36:32.329] TRAIN: iteration 23600 : loss : 0.060582, loss_ce: 0.001286, loss_dice: 0.119877
[11:36:32.602] TRAIN: iteration 23601 : loss : 0.173472, loss_ce: 0.005122, loss_dice: 0.341821
[11:36:32.813] TRAIN: iteration 23602 : loss : 0.055485, loss_ce: 0.000850, loss_dice: 0.110119
[11:36:34.443] TRAIN: iteration 23603 : loss : 0.105137, loss_ce: 0.002602, loss_dice: 0.207672
[11:36:34.653] TRAIN: iteration 23604 : loss : 0.052345, loss_ce: 0.004051, loss_dice: 0.100639
[11:36:34.865] TRAIN: iteration 23605 : loss : 0.209294, loss_ce: 0.004505, loss_dice: 0.414082
[11:36:35.077] TRAIN: iteration 23606 : loss : 0.193290, loss_ce: 0.001750, loss_dice: 0.384830
[11:36:35.289] TRAIN: iteration 23607 : loss : 0.251486, loss_ce: 0.002787, loss_dice: 0.500185
[11:36:35.499] TRAIN: iteration 23608 : loss : 0.169214, loss_ce: 0.021080, loss_dice: 0.317348
[11:36:35.708] TRAIN: iteration 23609 : loss : 0.251033, loss_ce: 0.001938, loss_dice: 0.500129
[11:36:35.921] TRAIN: iteration 23610 : loss : 0.080468, loss_ce: 0.004508, loss_dice: 0.156428
[11:36:38.067] TRAIN: iteration 23611 : loss : 0.065935, loss_ce: 0.002630, loss_dice: 0.129240
[11:36:38.279] TRAIN: iteration 23612 : loss : 0.047375, loss_ce: 0.001156, loss_dice: 0.093594
[11:36:38.488] TRAIN: iteration 23613 : loss : 0.068349, loss_ce: 0.004035, loss_dice: 0.132664
[11:36:38.697] TRAIN: iteration 23614 : loss : 0.043973, loss_ce: 0.001268, loss_dice: 0.086678
[11:36:38.915] TRAIN: iteration 23615 : loss : 0.087912, loss_ce: 0.003562, loss_dice: 0.172263
[11:36:39.124] TRAIN: iteration 23616 : loss : 0.250518, loss_ce: 0.001013, loss_dice: 0.500023
[11:36:39.338] TRAIN: iteration 23617 : loss : 0.232845, loss_ce: 0.000981, loss_dice: 0.464709
[11:36:39.547] TRAIN: iteration 23618 : loss : 0.166754, loss_ce: 0.003712, loss_dice: 0.329797
[11:36:40.239] TRAIN: iteration 23619 : loss : 0.201780, loss_ce: 0.002185, loss_dice: 0.401375
[11:36:40.455] TRAIN: iteration 23620 : loss : 0.045262, loss_ce: 0.001357, loss_dice: 0.089168
[11:36:40.700] TRAIN: iteration 23621 : loss : 0.047176, loss_ce: 0.005969, loss_dice: 0.088383
[11:36:40.909] TRAIN: iteration 23622 : loss : 0.067933, loss_ce: 0.004460, loss_dice: 0.131407
[11:36:41.119] TRAIN: iteration 23623 : loss : 0.060185, loss_ce: 0.006449, loss_dice: 0.113920
[11:36:41.328] TRAIN: iteration 23624 : loss : 0.078151, loss_ce: 0.004016, loss_dice: 0.152287
[11:36:41.540] TRAIN: iteration 23625 : loss : 0.110478, loss_ce: 0.018905, loss_dice: 0.202052
[11:36:41.752] TRAIN: iteration 23626 : loss : 0.053760, loss_ce: 0.004020, loss_dice: 0.103501
[11:36:43.908] TRAIN: iteration 23627 : loss : 0.088073, loss_ce: 0.002207, loss_dice: 0.173938
[11:36:44.122] TRAIN: iteration 23628 : loss : 0.073272, loss_ce: 0.004690, loss_dice: 0.141855
[11:36:44.333] TRAIN: iteration 23629 : loss : 0.075672, loss_ce: 0.002658, loss_dice: 0.148687
[11:36:44.542] TRAIN: iteration 23630 : loss : 0.182521, loss_ce: 0.004421, loss_dice: 0.360622
[11:36:44.751] TRAIN: iteration 23631 : loss : 0.221787, loss_ce: 0.001427, loss_dice: 0.442146
[11:36:44.960] TRAIN: iteration 23632 : loss : 0.029258, loss_ce: 0.002232, loss_dice: 0.056284
[11:36:45.174] TRAIN: iteration 23633 : loss : 0.039509, loss_ce: 0.002074, loss_dice: 0.076945
[11:36:45.382] TRAIN: iteration 23634 : loss : 0.092038, loss_ce: 0.001597, loss_dice: 0.182478
[11:36:45.592] TRAIN: iteration 23635 : loss : 0.045759, loss_ce: 0.003999, loss_dice: 0.087518
[11:36:45.802] TRAIN: iteration 23636 : loss : 0.050407, loss_ce: 0.009607, loss_dice: 0.091206
[11:36:46.014] TRAIN: iteration 23637 : loss : 0.054775, loss_ce: 0.003748, loss_dice: 0.105801
[11:36:46.223] TRAIN: iteration 23638 : loss : 0.073547, loss_ce: 0.001891, loss_dice: 0.145202
[11:36:46.434] TRAIN: iteration 23639 : loss : 0.096288, loss_ce: 0.002059, loss_dice: 0.190517
[11:36:46.649] TRAIN: iteration 23640 : loss : 0.153793, loss_ce: 0.003336, loss_dice: 0.304251
[11:36:46.889] TRAIN: iteration 23641 : loss : 0.059122, loss_ce: 0.004644, loss_dice: 0.113600
[11:36:47.100] TRAIN: iteration 23642 : loss : 0.075826, loss_ce: 0.003664, loss_dice: 0.147988
[11:36:47.309] TRAIN: iteration 23643 : loss : 0.144236, loss_ce: 0.003050, loss_dice: 0.285423
[11:36:47.521] TRAIN: iteration 23644 : loss : 0.249942, loss_ce: 0.001624, loss_dice: 0.498259
[11:36:47.730] TRAIN: iteration 23645 : loss : 0.027989, loss_ce: 0.005405, loss_dice: 0.050574
[11:36:47.943] TRAIN: iteration 23646 : loss : 0.035019, loss_ce: 0.001766, loss_dice: 0.068272
[11:36:48.154] TRAIN: iteration 23647 : loss : 0.152521, loss_ce: 0.000703, loss_dice: 0.304339
[11:36:48.362] TRAIN: iteration 23648 : loss : 0.095875, loss_ce: 0.001351, loss_dice: 0.190398
[11:36:49.268] TRAIN: iteration 23649 : loss : 0.076516, loss_ce: 0.001359, loss_dice: 0.151672
[11:36:49.477] TRAIN: iteration 23650 : loss : 0.250451, loss_ce: 0.000869, loss_dice: 0.500034
[11:36:49.686] TRAIN: iteration 23651 : loss : 0.034960, loss_ce: 0.003173, loss_dice: 0.066746
[11:36:49.896] TRAIN: iteration 23652 : loss : 0.249136, loss_ce: 0.003876, loss_dice: 0.494396
[11:36:50.122] TRAIN: iteration 23653 : loss : 0.069209, loss_ce: 0.001869, loss_dice: 0.136550
[11:36:51.472] TRAIN: iteration 23654 : loss : 0.036806, loss_ce: 0.005470, loss_dice: 0.068141
[11:36:51.681] TRAIN: iteration 23655 : loss : 0.095000, loss_ce: 0.004476, loss_dice: 0.185525
[11:36:51.891] TRAIN: iteration 23656 : loss : 0.114380, loss_ce: 0.002355, loss_dice: 0.226405
[11:36:52.101] TRAIN: iteration 23657 : loss : 0.045638, loss_ce: 0.001854, loss_dice: 0.089422
[11:36:52.320] TRAIN: iteration 23658 : loss : 0.247503, loss_ce: 0.001345, loss_dice: 0.493661
[11:36:52.528] TRAIN: iteration 23659 : loss : 0.043060, loss_ce: 0.006660, loss_dice: 0.079460
[11:36:52.742] TRAIN: iteration 23660 : loss : 0.250274, loss_ce: 0.000530, loss_dice: 0.500019
[11:36:52.983] TRAIN: iteration 23661 : loss : 0.084854, loss_ce: 0.021165, loss_dice: 0.148544
[11:36:53.198] TRAIN: iteration 23662 : loss : 0.095233, loss_ce: 0.009420, loss_dice: 0.181046
[11:36:53.408] TRAIN: iteration 23663 : loss : 0.230637, loss_ce: 0.000542, loss_dice: 0.460733
[11:36:53.617] TRAIN: iteration 23664 : loss : 0.056269, loss_ce: 0.002117, loss_dice: 0.110421
[11:36:54.237] TRAIN: iteration 23665 : loss : 0.083930, loss_ce: 0.003795, loss_dice: 0.164065
[11:36:54.449] TRAIN: iteration 23666 : loss : 0.048780, loss_ce: 0.001751, loss_dice: 0.095808
[11:36:54.657] TRAIN: iteration 23667 : loss : 0.055082, loss_ce: 0.006114, loss_dice: 0.104049
[11:36:54.868] TRAIN: iteration 23668 : loss : 0.053298, loss_ce: 0.004162, loss_dice: 0.102434
[11:36:55.089] TRAIN: iteration 23669 : loss : 0.071205, loss_ce: 0.002536, loss_dice: 0.139874
[11:36:55.299] TRAIN: iteration 23670 : loss : 0.250944, loss_ce: 0.001786, loss_dice: 0.500103
[11:36:55.511] TRAIN: iteration 23671 : loss : 0.066995, loss_ce: 0.003299, loss_dice: 0.130691
[11:36:55.721] TRAIN: iteration 23672 : loss : 0.073484, loss_ce: 0.002553, loss_dice: 0.144415
[11:36:56.357] TRAIN: iteration 23673 : loss : 0.072279, loss_ce: 0.002256, loss_dice: 0.142302
[11:36:56.567] TRAIN: iteration 23674 : loss : 0.165922, loss_ce: 0.005772, loss_dice: 0.326071
[11:36:56.778] TRAIN: iteration 23675 : loss : 0.250532, loss_ce: 0.001001, loss_dice: 0.500063
[11:36:56.988] TRAIN: iteration 23676 : loss : 0.088243, loss_ce: 0.003524, loss_dice: 0.172962
[11:36:57.590] TRAIN: iteration 23677 : loss : 0.039976, loss_ce: 0.001959, loss_dice: 0.077994
[11:36:57.808] TRAIN: iteration 23678 : loss : 0.083827, loss_ce: 0.003317, loss_dice: 0.164336
[11:36:58.019] TRAIN: iteration 23679 : loss : 0.056255, loss_ce: 0.005551, loss_dice: 0.106959
[11:36:58.232] TRAIN: iteration 23680 : loss : 0.250528, loss_ce: 0.001012, loss_dice: 0.500045
[11:36:58.480] TRAIN: iteration 23681 : loss : 0.077456, loss_ce: 0.008836, loss_dice: 0.146075
[11:36:59.841] TRAIN: iteration 23682 : loss : 0.049554, loss_ce: 0.005626, loss_dice: 0.093482
[11:37:00.053] TRAIN: iteration 23683 : loss : 0.119249, loss_ce: 0.005073, loss_dice: 0.233425
[11:37:00.273] TRAIN: iteration 23684 : loss : 0.048733, loss_ce: 0.003707, loss_dice: 0.093759
[11:37:00.485] TRAIN: iteration 23685 : loss : 0.019971, loss_ce: 0.002640, loss_dice: 0.037302
[11:37:00.701] TRAIN: iteration 23686 : loss : 0.049267, loss_ce: 0.001629, loss_dice: 0.096905
[11:37:00.910] TRAIN: iteration 23687 : loss : 0.103701, loss_ce: 0.004612, loss_dice: 0.202790
[11:37:01.124] TRAIN: iteration 23688 : loss : 0.027570, loss_ce: 0.000936, loss_dice: 0.054205
[11:37:01.335] TRAIN: iteration 23689 : loss : 0.175880, loss_ce: 0.004224, loss_dice: 0.347537
[11:37:01.544] TRAIN: iteration 23690 : loss : 0.097047, loss_ce: 0.001943, loss_dice: 0.192150
[11:37:01.753] TRAIN: iteration 23691 : loss : 0.120286, loss_ce: 0.004305, loss_dice: 0.236267
[11:37:01.962] TRAIN: iteration 23692 : loss : 0.195622, loss_ce: 0.004446, loss_dice: 0.386798
[11:37:02.172] TRAIN: iteration 23693 : loss : 0.099150, loss_ce: 0.003042, loss_dice: 0.195259
[11:37:02.388] TRAIN: iteration 23694 : loss : 0.106038, loss_ce: 0.003995, loss_dice: 0.208081
[11:37:02.604] TRAIN: iteration 23695 : loss : 0.047945, loss_ce: 0.004877, loss_dice: 0.091013
[11:37:04.459] TRAIN: iteration 23696 : loss : 0.250582, loss_ce: 0.001107, loss_dice: 0.500057
[11:37:04.674] TRAIN: iteration 23697 : loss : 0.073524, loss_ce: 0.002386, loss_dice: 0.144661
[11:37:06.060] TRAIN: iteration 23698 : loss : 0.032234, loss_ce: 0.004499, loss_dice: 0.059969
[11:37:06.282] TRAIN: iteration 23699 : loss : 0.170134, loss_ce: 0.005515, loss_dice: 0.334753
[11:37:06.493] TRAIN: iteration 23700 : loss : 0.106492, loss_ce: 0.004689, loss_dice: 0.208295
[11:37:06.737] TRAIN: iteration 23701 : loss : 0.081893, loss_ce: 0.004182, loss_dice: 0.159604
[11:37:06.950] TRAIN: iteration 23702 : loss : 0.057147, loss_ce: 0.003583, loss_dice: 0.110710
[11:37:07.159] TRAIN: iteration 23703 : loss : 0.241338, loss_ce: 0.028293, loss_dice: 0.454383
[11:37:07.367] TRAIN: iteration 23704 : loss : 0.128220, loss_ce: 0.002817, loss_dice: 0.253623
[11:37:07.581] TRAIN: iteration 23705 : loss : 0.096795, loss_ce: 0.002020, loss_dice: 0.191570
[11:37:07.918] TRAIN: iteration 23706 : loss : 0.069642, loss_ce: 0.011364, loss_dice: 0.127919
[11:37:08.129] TRAIN: iteration 23707 : loss : 0.140991, loss_ce: 0.003539, loss_dice: 0.278443
[11:37:08.337] TRAIN: iteration 23708 : loss : 0.120631, loss_ce: 0.003421, loss_dice: 0.237840
[11:37:08.547] TRAIN: iteration 23709 : loss : 0.083468, loss_ce: 0.001914, loss_dice: 0.165023
[11:37:08.758] TRAIN: iteration 23710 : loss : 0.192767, loss_ce: 0.003223, loss_dice: 0.382312
[11:37:08.966] TRAIN: iteration 23711 : loss : 0.166998, loss_ce: 0.001844, loss_dice: 0.332152
[11:37:09.468] TRAIN: iteration 23712 : loss : 0.031976, loss_ce: 0.004402, loss_dice: 0.059550
[11:37:09.680] TRAIN: iteration 23713 : loss : 0.186085, loss_ce: 0.001616, loss_dice: 0.370555
[11:37:10.148] TRAIN: iteration 23714 : loss : 0.109507, loss_ce: 0.006186, loss_dice: 0.212827
[11:37:10.358] TRAIN: iteration 23715 : loss : 0.074405, loss_ce: 0.001159, loss_dice: 0.147650
[11:37:10.576] TRAIN: iteration 23716 : loss : 0.044962, loss_ce: 0.001954, loss_dice: 0.087970
[11:37:11.906] TRAIN: iteration 23717 : loss : 0.189633, loss_ce: 0.005743, loss_dice: 0.373523
[11:37:12.120] TRAIN: iteration 23718 : loss : 0.119511, loss_ce: 0.004787, loss_dice: 0.234236
[11:37:12.332] TRAIN: iteration 23719 : loss : 0.246231, loss_ce: 0.001483, loss_dice: 0.490980
[11:37:12.546] TRAIN: iteration 23720 : loss : 0.104092, loss_ce: 0.001039, loss_dice: 0.207144
[11:37:12.547] NaN or Inf found in input tensor.
[11:37:12.768] TRAIN: iteration 23721 : loss : 0.094267, loss_ce: 0.003577, loss_dice: 0.184956
[11:37:12.983] TRAIN: iteration 23722 : loss : 0.062793, loss_ce: 0.001772, loss_dice: 0.123815
[11:37:13.193] TRAIN: iteration 23723 : loss : 0.094267, loss_ce: 0.001769, loss_dice: 0.186765
[11:37:13.402] TRAIN: iteration 23724 : loss : 0.060251, loss_ce: 0.002602, loss_dice: 0.117899
[11:37:13.689] TRAIN: iteration 23725 : loss : 0.042441, loss_ce: 0.004712, loss_dice: 0.080170
[11:37:13.899] TRAIN: iteration 23726 : loss : 0.039870, loss_ce: 0.001809, loss_dice: 0.077932
[11:37:14.110] TRAIN: iteration 23727 : loss : 0.115928, loss_ce: 0.007459, loss_dice: 0.224396
[11:37:14.320] TRAIN: iteration 23728 : loss : 0.039097, loss_ce: 0.000898, loss_dice: 0.077295
[11:37:14.536] TRAIN: iteration 23729 : loss : 0.067446, loss_ce: 0.004463, loss_dice: 0.130429
[11:37:14.745] TRAIN: iteration 23730 : loss : 0.210053, loss_ce: 0.007532, loss_dice: 0.412574
[11:37:15.156] TRAIN: iteration 23731 : loss : 0.256927, loss_ce: 0.016953, loss_dice: 0.496900
[11:37:18.297] TRAIN: iteration 23732 : loss : 0.088404, loss_ce: 0.005176, loss_dice: 0.171633
[11:37:18.512] TRAIN: iteration 23733 : loss : 0.070621, loss_ce: 0.002126, loss_dice: 0.139117
[11:37:18.721] TRAIN: iteration 23734 : loss : 0.117149, loss_ce: 0.002904, loss_dice: 0.231394
[11:37:18.930] TRAIN: iteration 23735 : loss : 0.057196, loss_ce: 0.007064, loss_dice: 0.107328
[11:37:19.140] TRAIN: iteration 23736 : loss : 0.118912, loss_ce: 0.005585, loss_dice: 0.232239
[11:37:19.349] TRAIN: iteration 23737 : loss : 0.176464, loss_ce: 0.007033, loss_dice: 0.345896
[11:37:19.557] TRAIN: iteration 23738 : loss : 0.223766, loss_ce: 0.004307, loss_dice: 0.443225
[11:37:19.766] TRAIN: iteration 23739 : loss : 0.056848, loss_ce: 0.002116, loss_dice: 0.111580
[11:37:20.221] TRAIN: iteration 23740 : loss : 0.080476, loss_ce: 0.004182, loss_dice: 0.156770
[11:37:20.460] TRAIN: iteration 23741 : loss : 0.117460, loss_ce: 0.008157, loss_dice: 0.226764
[11:37:20.672] TRAIN: iteration 23742 : loss : 0.250730, loss_ce: 0.001400, loss_dice: 0.500059
[11:37:20.882] TRAIN: iteration 23743 : loss : 0.117454, loss_ce: 0.003729, loss_dice: 0.231179
[11:37:21.098] TRAIN: iteration 23744 : loss : 0.038515, loss_ce: 0.001203, loss_dice: 0.075827
[11:37:21.307] TRAIN: iteration 23745 : loss : 0.091416, loss_ce: 0.011185, loss_dice: 0.171646
[11:37:22.264] TRAIN: iteration 23746 : loss : 0.250981, loss_ce: 0.001873, loss_dice: 0.500089
[11:37:22.473] TRAIN: iteration 23747 : loss : 0.044775, loss_ce: 0.001688, loss_dice: 0.087862
[11:37:22.683] TRAIN: iteration 23748 : loss : 0.089000, loss_ce: 0.009882, loss_dice: 0.168119
[11:37:22.895] TRAIN: iteration 23749 : loss : 0.065963, loss_ce: 0.008609, loss_dice: 0.123318
[11:37:23.104] TRAIN: iteration 23750 : loss : 0.076129, loss_ce: 0.004501, loss_dice: 0.147757
[11:37:23.314] TRAIN: iteration 23751 : loss : 0.251522, loss_ce: 0.002884, loss_dice: 0.500160
[11:37:23.525] TRAIN: iteration 23752 : loss : 0.056249, loss_ce: 0.003867, loss_dice: 0.108630
[11:37:23.738] TRAIN: iteration 23753 : loss : 0.102619, loss_ce: 0.004712, loss_dice: 0.200525
[11:37:26.272] TRAIN: iteration 23754 : loss : 0.114451, loss_ce: 0.005011, loss_dice: 0.223891
[11:37:26.485] TRAIN: iteration 23755 : loss : 0.060474, loss_ce: 0.003334, loss_dice: 0.117615
[11:37:26.695] TRAIN: iteration 23756 : loss : 0.167779, loss_ce: 0.002630, loss_dice: 0.332927
[11:37:26.904] TRAIN: iteration 23757 : loss : 0.060290, loss_ce: 0.002161, loss_dice: 0.118420
[11:37:27.114] TRAIN: iteration 23758 : loss : 0.050416, loss_ce: 0.001774, loss_dice: 0.099059
[11:37:27.326] TRAIN: iteration 23759 : loss : 0.127733, loss_ce: 0.002810, loss_dice: 0.252655
[11:37:27.535] TRAIN: iteration 23760 : loss : 0.030379, loss_ce: 0.001760, loss_dice: 0.058999
[11:37:27.777] TRAIN: iteration 23761 : loss : 0.088044, loss_ce: 0.004674, loss_dice: 0.171415
[11:37:28.860] TRAIN: iteration 23762 : loss : 0.251412, loss_ce: 0.002648, loss_dice: 0.500177
[11:37:29.068] TRAIN: iteration 23763 : loss : 0.049781, loss_ce: 0.002508, loss_dice: 0.097055
[11:37:29.278] TRAIN: iteration 23764 : loss : 0.254374, loss_ce: 0.012220, loss_dice: 0.496528
[11:37:29.490] TRAIN: iteration 23765 : loss : 0.119541, loss_ce: 0.002987, loss_dice: 0.236094
[11:37:29.699] TRAIN: iteration 23766 : loss : 0.068076, loss_ce: 0.001186, loss_dice: 0.134966
[11:37:29.908] TRAIN: iteration 23767 : loss : 0.138920, loss_ce: 0.007951, loss_dice: 0.269889
[11:37:30.116] TRAIN: iteration 23768 : loss : 0.250927, loss_ce: 0.001766, loss_dice: 0.500089
[11:37:30.327] TRAIN: iteration 23769 : loss : 0.217636, loss_ce: 0.001159, loss_dice: 0.434112
[11:37:33.227] TRAIN: iteration 23770 : loss : 0.043062, loss_ce: 0.002529, loss_dice: 0.083595
[11:37:33.436] TRAIN: iteration 23771 : loss : 0.031408, loss_ce: 0.002494, loss_dice: 0.060322
[11:37:33.648] TRAIN: iteration 23772 : loss : 0.047328, loss_ce: 0.001231, loss_dice: 0.093425
[11:37:33.862] TRAIN: iteration 23773 : loss : 0.087778, loss_ce: 0.006008, loss_dice: 0.169547
[11:37:34.074] TRAIN: iteration 23774 : loss : 0.126710, loss_ce: 0.008466, loss_dice: 0.244954
[11:37:34.284] TRAIN: iteration 23775 : loss : 0.199802, loss_ce: 0.005695, loss_dice: 0.393909
[11:37:34.494] TRAIN: iteration 23776 : loss : 0.076413, loss_ce: 0.003428, loss_dice: 0.149398
[11:37:34.706] TRAIN: iteration 23777 : loss : 0.046171, loss_ce: 0.003258, loss_dice: 0.089085
[11:37:36.526] TRAIN: iteration 23778 : loss : 0.151621, loss_ce: 0.003680, loss_dice: 0.299563
[11:37:36.734] TRAIN: iteration 23779 : loss : 0.140074, loss_ce: 0.003495, loss_dice: 0.276654
[11:37:36.947] TRAIN: iteration 23780 : loss : 0.070587, loss_ce: 0.003168, loss_dice: 0.138006
[11:37:37.189] TRAIN: iteration 23781 : loss : 0.052511, loss_ce: 0.001140, loss_dice: 0.103882
[11:37:37.399] TRAIN: iteration 23782 : loss : 0.216844, loss_ce: 0.003084, loss_dice: 0.430603
[11:37:37.611] TRAIN: iteration 23783 : loss : 0.078318, loss_ce: 0.004460, loss_dice: 0.152175
[11:37:37.823] TRAIN: iteration 23784 : loss : 0.218744, loss_ce: 0.002011, loss_dice: 0.435477
[11:37:38.032] TRAIN: iteration 23785 : loss : 0.115961, loss_ce: 0.006102, loss_dice: 0.225821
[11:37:40.691] TRAIN: iteration 23786 : loss : 0.251247, loss_ce: 0.002367, loss_dice: 0.500127
[11:37:40.907] TRAIN: iteration 23787 : loss : 0.066102, loss_ce: 0.001777, loss_dice: 0.130427
[11:37:41.119] TRAIN: iteration 23788 : loss : 0.166924, loss_ce: 0.003621, loss_dice: 0.330228
[11:37:41.328] TRAIN: iteration 23789 : loss : 0.115006, loss_ce: 0.001705, loss_dice: 0.228306
[11:37:41.537] TRAIN: iteration 23790 : loss : 0.063648, loss_ce: 0.009222, loss_dice: 0.118073
[11:37:41.747] TRAIN: iteration 23791 : loss : 0.082526, loss_ce: 0.002911, loss_dice: 0.162140
[11:37:41.960] TRAIN: iteration 23792 : loss : 0.032121, loss_ce: 0.002912, loss_dice: 0.061329
[11:37:42.168] TRAIN: iteration 23793 : loss : 0.205734, loss_ce: 0.010508, loss_dice: 0.400959
[11:37:44.859] TRAIN: iteration 23794 : loss : 0.074542, loss_ce: 0.003589, loss_dice: 0.145495
[11:37:45.074] TRAIN: iteration 23795 : loss : 0.091641, loss_ce: 0.003736, loss_dice: 0.179545
[11:37:45.283] TRAIN: iteration 23796 : loss : 0.131213, loss_ce: 0.005388, loss_dice: 0.257037
[11:37:45.492] TRAIN: iteration 23797 : loss : 0.061985, loss_ce: 0.002272, loss_dice: 0.121698
[11:37:45.701] TRAIN: iteration 23798 : loss : 0.053773, loss_ce: 0.003441, loss_dice: 0.104104
[11:37:45.910] TRAIN: iteration 23799 : loss : 0.250660, loss_ce: 0.001283, loss_dice: 0.500037
[11:37:46.124] TRAIN: iteration 23800 : loss : 0.070218, loss_ce: 0.003446, loss_dice: 0.136989
[11:37:46.375] TRAIN: iteration 23801 : loss : 0.219244, loss_ce: 0.002288, loss_dice: 0.436200
[11:37:46.584] TRAIN: iteration 23802 : loss : 0.235853, loss_ce: 0.002079, loss_dice: 0.469626
[11:37:46.794] TRAIN: iteration 23803 : loss : 0.215304, loss_ce: 0.006209, loss_dice: 0.424399
[11:37:47.009] TRAIN: iteration 23804 : loss : 0.055458, loss_ce: 0.003169, loss_dice: 0.107748
[11:37:47.219] TRAIN: iteration 23805 : loss : 0.077396, loss_ce: 0.004248, loss_dice: 0.150543
[11:37:47.429] TRAIN: iteration 23806 : loss : 0.086662, loss_ce: 0.001307, loss_dice: 0.172018
[11:37:47.644] TRAIN: iteration 23807 : loss : 0.253166, loss_ce: 0.006902, loss_dice: 0.499429
[11:37:47.856] TRAIN: iteration 23808 : loss : 0.070002, loss_ce: 0.002733, loss_dice: 0.137272
[11:37:48.067] TRAIN: iteration 23809 : loss : 0.250817, loss_ce: 0.001575, loss_dice: 0.500059
[11:37:48.484] TRAIN: iteration 23810 : loss : 0.123819, loss_ce: 0.001785, loss_dice: 0.245853
[11:37:51.611] TRAIN: iteration 23811 : loss : 0.059967, loss_ce: 0.016449, loss_dice: 0.103485
[11:37:51.819] TRAIN: iteration 23812 : loss : 0.052512, loss_ce: 0.000791, loss_dice: 0.104232
[11:37:52.033] TRAIN: iteration 23813 : loss : 0.250642, loss_ce: 0.001225, loss_dice: 0.500059
[11:37:52.241] TRAIN: iteration 23814 : loss : 0.030921, loss_ce: 0.001132, loss_dice: 0.060710
[11:37:52.451] TRAIN: iteration 23815 : loss : 0.042237, loss_ce: 0.006114, loss_dice: 0.078359
[11:37:52.659] TRAIN: iteration 23816 : loss : 0.046148, loss_ce: 0.000530, loss_dice: 0.091766
[11:37:52.869] TRAIN: iteration 23817 : loss : 0.181650, loss_ce: 0.007557, loss_dice: 0.355743
[11:37:53.078] TRAIN: iteration 23818 : loss : 0.147475, loss_ce: 0.002312, loss_dice: 0.292639
[11:37:55.972] TRAIN: iteration 23819 : loss : 0.049127, loss_ce: 0.001202, loss_dice: 0.097052
[11:37:56.181] TRAIN: iteration 23820 : loss : 0.026398, loss_ce: 0.003665, loss_dice: 0.049131
[11:37:56.425] TRAIN: iteration 23821 : loss : 0.235919, loss_ce: 0.022318, loss_dice: 0.449521
[11:37:56.635] TRAIN: iteration 23822 : loss : 0.085190, loss_ce: 0.002072, loss_dice: 0.168309
[11:37:56.843] TRAIN: iteration 23823 : loss : 0.080759, loss_ce: 0.001329, loss_dice: 0.160188
[11:37:57.053] TRAIN: iteration 23824 : loss : 0.172981, loss_ce: 0.006332, loss_dice: 0.339630
[11:37:57.264] TRAIN: iteration 23825 : loss : 0.088385, loss_ce: 0.002873, loss_dice: 0.173897
[11:37:57.472] TRAIN: iteration 23826 : loss : 0.250467, loss_ce: 0.000877, loss_dice: 0.500057
[11:37:57.681] TRAIN: iteration 23827 : loss : 0.252269, loss_ce: 0.004635, loss_dice: 0.499903
[11:37:57.892] TRAIN: iteration 23828 : loss : 0.080395, loss_ce: 0.003448, loss_dice: 0.157343
[11:37:58.104] TRAIN: iteration 23829 : loss : 0.057513, loss_ce: 0.002557, loss_dice: 0.112469
[11:37:58.314] TRAIN: iteration 23830 : loss : 0.250548, loss_ce: 0.001047, loss_dice: 0.500049
[11:37:58.524] TRAIN: iteration 23831 : loss : 0.073942, loss_ce: 0.002528, loss_dice: 0.145356
[11:38:00.215] TRAIN: iteration 23832 : loss : 0.073223, loss_ce: 0.000918, loss_dice: 0.145528
[11:38:00.426] TRAIN: iteration 23833 : loss : 0.156739, loss_ce: 0.001863, loss_dice: 0.311615
[11:38:00.636] TRAIN: iteration 23834 : loss : 0.119347, loss_ce: 0.005185, loss_dice: 0.233509
[11:38:00.849] TRAIN: iteration 23835 : loss : 0.085702, loss_ce: 0.004525, loss_dice: 0.166878
[11:38:01.060] TRAIN: iteration 23836 : loss : 0.049608, loss_ce: 0.000891, loss_dice: 0.098325
[11:38:01.269] TRAIN: iteration 23837 : loss : 0.162133, loss_ce: 0.003589, loss_dice: 0.320677
[11:38:01.479] TRAIN: iteration 23838 : loss : 0.107918, loss_ce: 0.004717, loss_dice: 0.211119
[11:38:01.688] TRAIN: iteration 23839 : loss : 0.084752, loss_ce: 0.002366, loss_dice: 0.167139
[11:38:03.561] TRAIN: iteration 23840 : loss : 0.213298, loss_ce: 0.000978, loss_dice: 0.425618
[11:38:04.178] TRAIN: iteration 23841 : loss : 0.140265, loss_ce: 0.003123, loss_dice: 0.277406
[11:38:04.388] TRAIN: iteration 23842 : loss : 0.231254, loss_ce: 0.002124, loss_dice: 0.460384
[11:38:04.598] TRAIN: iteration 23843 : loss : 0.011890, loss_ce: 0.001025, loss_dice: 0.022755
[11:38:04.806] TRAIN: iteration 23844 : loss : 0.243623, loss_ce: 0.001477, loss_dice: 0.485769
[11:38:05.015] TRAIN: iteration 23845 : loss : 0.246718, loss_ce: 0.001719, loss_dice: 0.491717
[11:38:05.227] TRAIN: iteration 23846 : loss : 0.102812, loss_ce: 0.007260, loss_dice: 0.198364
[11:38:05.437] TRAIN: iteration 23847 : loss : 0.085998, loss_ce: 0.003359, loss_dice: 0.168638
[11:38:06.125] TRAIN: iteration 23848 : loss : 0.075725, loss_ce: 0.004368, loss_dice: 0.147082
[11:38:07.617] TRAIN: iteration 23849 : loss : 0.050187, loss_ce: 0.005018, loss_dice: 0.095356
[11:38:07.826] TRAIN: iteration 23850 : loss : 0.042315, loss_ce: 0.003597, loss_dice: 0.081033
[11:38:08.040] TRAIN: iteration 23851 : loss : 0.166727, loss_ce: 0.006184, loss_dice: 0.327269
[11:38:08.257] TRAIN: iteration 23852 : loss : 0.019664, loss_ce: 0.001830, loss_dice: 0.037498
[11:38:08.466] TRAIN: iteration 23853 : loss : 0.170741, loss_ce: 0.001914, loss_dice: 0.339568
[11:38:08.675] TRAIN: iteration 23854 : loss : 0.091022, loss_ce: 0.002485, loss_dice: 0.179559
[11:38:08.888] TRAIN: iteration 23855 : loss : 0.250927, loss_ce: 0.001745, loss_dice: 0.500110
[11:38:09.097] TRAIN: iteration 23856 : loss : 0.047260, loss_ce: 0.003323, loss_dice: 0.091197
[11:38:11.049] TRAIN: iteration 23857 : loss : 0.024205, loss_ce: 0.001255, loss_dice: 0.047154
[11:38:11.261] TRAIN: iteration 23858 : loss : 0.072888, loss_ce: 0.011017, loss_dice: 0.134759
[11:38:11.471] TRAIN: iteration 23859 : loss : 0.246807, loss_ce: 0.009274, loss_dice: 0.484341
[11:38:11.689] TRAIN: iteration 23860 : loss : 0.260829, loss_ce: 0.021564, loss_dice: 0.500095
[11:38:11.933] TRAIN: iteration 23861 : loss : 0.085162, loss_ce: 0.004740, loss_dice: 0.165585
[11:38:12.142] TRAIN: iteration 23862 : loss : 0.251051, loss_ce: 0.001976, loss_dice: 0.500127
[11:38:12.357] TRAIN: iteration 23863 : loss : 0.045416, loss_ce: 0.005131, loss_dice: 0.085702
[11:38:12.568] TRAIN: iteration 23864 : loss : 0.057236, loss_ce: 0.007814, loss_dice: 0.106659
[11:38:13.207] TRAIN: iteration 23865 : loss : 0.248118, loss_ce: 0.003190, loss_dice: 0.493046
[11:38:13.416] TRAIN: iteration 23866 : loss : 0.230738, loss_ce: 0.003512, loss_dice: 0.457964
[11:38:13.625] TRAIN: iteration 23867 : loss : 0.252263, loss_ce: 0.005127, loss_dice: 0.499399
[11:38:13.961] TRAIN: iteration 23868 : loss : 0.230148, loss_ce: 0.002061, loss_dice: 0.458236
[11:38:14.170] TRAIN: iteration 23869 : loss : 0.167450, loss_ce: 0.004857, loss_dice: 0.330042
[11:38:14.615] TRAIN: iteration 23870 : loss : 0.070444, loss_ce: 0.002661, loss_dice: 0.138228
[11:38:15.284] TRAIN: iteration 23871 : loss : 0.121453, loss_ce: 0.003844, loss_dice: 0.239061
[11:38:15.493] TRAIN: iteration 23872 : loss : 0.077874, loss_ce: 0.002400, loss_dice: 0.153348
[11:38:16.867] TRAIN: iteration 23873 : loss : 0.044899, loss_ce: 0.002207, loss_dice: 0.087592
[11:38:17.075] TRAIN: iteration 23874 : loss : 0.042687, loss_ce: 0.004579, loss_dice: 0.080795
[11:38:17.287] TRAIN: iteration 23875 : loss : 0.078502, loss_ce: 0.004446, loss_dice: 0.152557
[11:38:17.500] TRAIN: iteration 23876 : loss : 0.251360, loss_ce: 0.003493, loss_dice: 0.499227
[11:38:17.711] TRAIN: iteration 23877 : loss : 0.038299, loss_ce: 0.004754, loss_dice: 0.071845
[11:38:17.975] TRAIN: iteration 23878 : loss : 0.056632, loss_ce: 0.002554, loss_dice: 0.110709
[11:38:18.184] TRAIN: iteration 23879 : loss : 0.182480, loss_ce: 0.001676, loss_dice: 0.363284
[11:38:18.393] TRAIN: iteration 23880 : loss : 0.078757, loss_ce: 0.007867, loss_dice: 0.149648
[11:38:19.980] TRAIN: iteration 23881 : loss : 0.068801, loss_ce: 0.003062, loss_dice: 0.134539
[11:38:20.188] TRAIN: iteration 23882 : loss : 0.234400, loss_ce: 0.002827, loss_dice: 0.465973
[11:38:20.395] TRAIN: iteration 23883 : loss : 0.088961, loss_ce: 0.005113, loss_dice: 0.172809
[11:38:20.603] TRAIN: iteration 23884 : loss : 0.186864, loss_ce: 0.002955, loss_dice: 0.370773
[11:38:20.814] TRAIN: iteration 23885 : loss : 0.113925, loss_ce: 0.004740, loss_dice: 0.223110
[11:38:21.022] TRAIN: iteration 23886 : loss : 0.075623, loss_ce: 0.001728, loss_dice: 0.149518
[11:38:21.230] TRAIN: iteration 23887 : loss : 0.017820, loss_ce: 0.001664, loss_dice: 0.033976
[11:38:21.678] TRAIN: iteration 23888 : loss : 0.047066, loss_ce: 0.002996, loss_dice: 0.091135
[11:38:22.420] TRAIN: iteration 23889 : loss : 0.089104, loss_ce: 0.001904, loss_dice: 0.176304
[11:38:22.627] TRAIN: iteration 23890 : loss : 0.037441, loss_ce: 0.003258, loss_dice: 0.071625
[11:38:22.834] TRAIN: iteration 23891 : loss : 0.099162, loss_ce: 0.003541, loss_dice: 0.194782
[11:38:23.044] TRAIN: iteration 23892 : loss : 0.048077, loss_ce: 0.002384, loss_dice: 0.093770
[11:38:23.659] TRAIN: iteration 23893 : loss : 0.146965, loss_ce: 0.001272, loss_dice: 0.292657
[11:38:24.949] TRAIN: iteration 23894 : loss : 0.120545, loss_ce: 0.001386, loss_dice: 0.239705
[11:38:27.871] TRAIN: iteration 23895 : loss : 0.071237, loss_ce: 0.004434, loss_dice: 0.138040
[11:38:28.078] TRAIN: iteration 23896 : loss : 0.121241, loss_ce: 0.004047, loss_dice: 0.238435
[11:38:28.287] TRAIN: iteration 23897 : loss : 0.141048, loss_ce: 0.002899, loss_dice: 0.279196
[11:38:28.495] TRAIN: iteration 23898 : loss : 0.190005, loss_ce: 0.009433, loss_dice: 0.370578
[11:38:28.704] TRAIN: iteration 23899 : loss : 0.212399, loss_ce: 0.002350, loss_dice: 0.422447
[11:38:28.912] TRAIN: iteration 23900 : loss : 0.250998, loss_ce: 0.001864, loss_dice: 0.500132
[11:38:29.155] TRAIN: iteration 23901 : loss : 0.127921, loss_ce: 0.006552, loss_dice: 0.249290
[11:38:29.363] TRAIN: iteration 23902 : loss : 0.134923, loss_ce: 0.003185, loss_dice: 0.266660
[11:38:30.809] TRAIN: iteration 23903 : loss : 0.050752, loss_ce: 0.001390, loss_dice: 0.100113
[11:38:31.018] TRAIN: iteration 23904 : loss : 0.157847, loss_ce: 0.000604, loss_dice: 0.315090
[11:38:31.226] TRAIN: iteration 23905 : loss : 0.089719, loss_ce: 0.001448, loss_dice: 0.177990
[11:38:31.434] TRAIN: iteration 23906 : loss : 0.029650, loss_ce: 0.000860, loss_dice: 0.058440
[11:38:31.643] TRAIN: iteration 23907 : loss : 0.055440, loss_ce: 0.010973, loss_dice: 0.099907
[11:38:31.851] TRAIN: iteration 23908 : loss : 0.125733, loss_ce: 0.003986, loss_dice: 0.247479
[11:38:32.061] TRAIN: iteration 23909 : loss : 0.019529, loss_ce: 0.004383, loss_dice: 0.034675
[11:38:32.270] TRAIN: iteration 23910 : loss : 0.123386, loss_ce: 0.009071, loss_dice: 0.237702
[11:38:34.227] TRAIN: iteration 23911 : loss : 0.036448, loss_ce: 0.002513, loss_dice: 0.070383
[11:38:34.437] TRAIN: iteration 23912 : loss : 0.035742, loss_ce: 0.001694, loss_dice: 0.069790
[11:38:34.644] TRAIN: iteration 23913 : loss : 0.166017, loss_ce: 0.010019, loss_dice: 0.322015
[11:38:34.852] TRAIN: iteration 23914 : loss : 0.044674, loss_ce: 0.002606, loss_dice: 0.086741
[11:38:35.060] TRAIN: iteration 23915 : loss : 0.062245, loss_ce: 0.002980, loss_dice: 0.121511
[11:38:35.272] TRAIN: iteration 23916 : loss : 0.111217, loss_ce: 0.005350, loss_dice: 0.217085
[11:38:35.480] TRAIN: iteration 23917 : loss : 0.136622, loss_ce: 0.002725, loss_dice: 0.270519
[11:38:35.687] TRAIN: iteration 23918 : loss : 0.110606, loss_ce: 0.004630, loss_dice: 0.216583
[11:38:37.274] TRAIN: iteration 23919 : loss : 0.127234, loss_ce: 0.004574, loss_dice: 0.249893
[11:38:37.485] TRAIN: iteration 23920 : loss : 0.251402, loss_ce: 0.002614, loss_dice: 0.500190
[11:38:37.725] TRAIN: iteration 23921 : loss : 0.041077, loss_ce: 0.004620, loss_dice: 0.077535
[11:38:37.938] TRAIN: iteration 23922 : loss : 0.105875, loss_ce: 0.002019, loss_dice: 0.209731
[11:38:38.147] TRAIN: iteration 23923 : loss : 0.037960, loss_ce: 0.000968, loss_dice: 0.074952
[11:38:38.355] TRAIN: iteration 23924 : loss : 0.194530, loss_ce: 0.003900, loss_dice: 0.385159
[11:38:38.562] TRAIN: iteration 23925 : loss : 0.058188, loss_ce: 0.006582, loss_dice: 0.109794
[11:38:39.417] TRAIN: iteration 23926 : loss : 0.072258, loss_ce: 0.001495, loss_dice: 0.143021
[11:38:39.624] TRAIN: iteration 23927 : loss : 0.163470, loss_ce: 0.002073, loss_dice: 0.324868
[11:38:39.832] TRAIN: iteration 23928 : loss : 0.111135, loss_ce: 0.003895, loss_dice: 0.218375
[11:38:40.145] TRAIN: iteration 23929 : loss : 0.089468, loss_ce: 0.002408, loss_dice: 0.176528
[11:38:40.997] TRAIN: iteration 23930 : loss : 0.045313, loss_ce: 0.013061, loss_dice: 0.077565
[11:38:41.206] TRAIN: iteration 23931 : loss : 0.133725, loss_ce: 0.006917, loss_dice: 0.260533
[11:38:41.414] TRAIN: iteration 23932 : loss : 0.029893, loss_ce: 0.003834, loss_dice: 0.055953
[11:38:41.622] TRAIN: iteration 23933 : loss : 0.249362, loss_ce: 0.002898, loss_dice: 0.495826
[11:38:43.241] TRAIN: iteration 23934 : loss : 0.251063, loss_ce: 0.002206, loss_dice: 0.499920
[11:38:43.894] TRAIN: iteration 23935 : loss : 0.121639, loss_ce: 0.004030, loss_dice: 0.239248
[11:38:44.101] TRAIN: iteration 23936 : loss : 0.042776, loss_ce: 0.002116, loss_dice: 0.083435
[11:38:44.308] TRAIN: iteration 23937 : loss : 0.065259, loss_ce: 0.003121, loss_dice: 0.127398
[11:38:46.188] TRAIN: iteration 23938 : loss : 0.078018, loss_ce: 0.006248, loss_dice: 0.149788
[11:38:46.396] TRAIN: iteration 23939 : loss : 0.062398, loss_ce: 0.006227, loss_dice: 0.118570
[11:38:46.604] TRAIN: iteration 23940 : loss : 0.090362, loss_ce: 0.006509, loss_dice: 0.174215
[11:38:46.844] TRAIN: iteration 23941 : loss : 0.045634, loss_ce: 0.001003, loss_dice: 0.090265
[11:38:47.051] TRAIN: iteration 23942 : loss : 0.111632, loss_ce: 0.002086, loss_dice: 0.221178
[11:38:47.805] TRAIN: iteration 23943 : loss : 0.247055, loss_ce: 0.002820, loss_dice: 0.491291
[11:38:48.014] TRAIN: iteration 23944 : loss : 0.105036, loss_ce: 0.003617, loss_dice: 0.206455
[11:38:48.224] TRAIN: iteration 23945 : loss : 0.053539, loss_ce: 0.003894, loss_dice: 0.103183
[11:38:49.850] TRAIN: iteration 23946 : loss : 0.049851, loss_ce: 0.002885, loss_dice: 0.096818
[11:38:50.058] TRAIN: iteration 23947 : loss : 0.065645, loss_ce: 0.000958, loss_dice: 0.130333
[11:38:50.266] TRAIN: iteration 23948 : loss : 0.253732, loss_ce: 0.007406, loss_dice: 0.500059
[11:38:50.474] TRAIN: iteration 23949 : loss : 0.202641, loss_ce: 0.011302, loss_dice: 0.393979
[11:38:50.683] TRAIN: iteration 23950 : loss : 0.177103, loss_ce: 0.000907, loss_dice: 0.353300
[11:38:52.686] TRAIN: iteration 23951 : loss : 0.154085, loss_ce: 0.007361, loss_dice: 0.300810
[11:38:52.899] TRAIN: iteration 23952 : loss : 0.245460, loss_ce: 0.001802, loss_dice: 0.489119
[11:38:53.107] TRAIN: iteration 23953 : loss : 0.054719, loss_ce: 0.004169, loss_dice: 0.105270
[11:38:53.318] TRAIN: iteration 23954 : loss : 0.109020, loss_ce: 0.005779, loss_dice: 0.212261
[11:38:53.527] TRAIN: iteration 23955 : loss : 0.084314, loss_ce: 0.003211, loss_dice: 0.165417
[11:38:53.735] TRAIN: iteration 23956 : loss : 0.022613, loss_ce: 0.000503, loss_dice: 0.044722
[11:38:55.426] TRAIN: iteration 23957 : loss : 0.051920, loss_ce: 0.000973, loss_dice: 0.102867
[11:38:55.636] TRAIN: iteration 23958 : loss : 0.057599, loss_ce: 0.001646, loss_dice: 0.113553
[11:38:55.844] TRAIN: iteration 23959 : loss : 0.041596, loss_ce: 0.002730, loss_dice: 0.080461
[11:38:56.054] TRAIN: iteration 23960 : loss : 0.149332, loss_ce: 0.002474, loss_dice: 0.296190
[11:38:56.292] TRAIN: iteration 23961 : loss : 0.089784, loss_ce: 0.002015, loss_dice: 0.177552
[11:38:57.020] TRAIN: iteration 23962 : loss : 0.115671, loss_ce: 0.000642, loss_dice: 0.230701
[11:38:57.228] TRAIN: iteration 23963 : loss : 0.251574, loss_ce: 0.005368, loss_dice: 0.497780
[11:38:57.436] TRAIN: iteration 23964 : loss : 0.098073, loss_ce: 0.006328, loss_dice: 0.189819
[11:39:01.081] TRAIN: iteration 23965 : loss : 0.054808, loss_ce: 0.002150, loss_dice: 0.107466
[11:39:01.293] TRAIN: iteration 23966 : loss : 0.097535, loss_ce: 0.003536, loss_dice: 0.191534
[11:39:01.502] TRAIN: iteration 23967 : loss : 0.026383, loss_ce: 0.001281, loss_dice: 0.051485
[11:39:01.712] TRAIN: iteration 23968 : loss : 0.100335, loss_ce: 0.003924, loss_dice: 0.196746
[11:39:01.923] TRAIN: iteration 23969 : loss : 0.080480, loss_ce: 0.002875, loss_dice: 0.158085
[11:39:02.146] TRAIN: iteration 23970 : loss : 0.075471, loss_ce: 0.003873, loss_dice: 0.147069
[11:39:02.354] TRAIN: iteration 23971 : loss : 0.125103, loss_ce: 0.003498, loss_dice: 0.246707
[11:39:02.561] TRAIN: iteration 23972 : loss : 0.205367, loss_ce: 0.001400, loss_dice: 0.409333
[11:39:05.750] TRAIN: iteration 23973 : loss : 0.104344, loss_ce: 0.004743, loss_dice: 0.203946
[11:39:05.964] TRAIN: iteration 23974 : loss : 0.045458, loss_ce: 0.002695, loss_dice: 0.088221
[11:39:06.173] TRAIN: iteration 23975 : loss : 0.250479, loss_ce: 0.000900, loss_dice: 0.500058
[11:39:06.380] TRAIN: iteration 23976 : loss : 0.079086, loss_ce: 0.005677, loss_dice: 0.152495
[11:39:06.588] TRAIN: iteration 23977 : loss : 0.047653, loss_ce: 0.003149, loss_dice: 0.092157
[11:39:06.795] TRAIN: iteration 23978 : loss : 0.066102, loss_ce: 0.005567, loss_dice: 0.126638
[11:39:07.004] TRAIN: iteration 23979 : loss : 0.054861, loss_ce: 0.003684, loss_dice: 0.106038
[11:39:07.216] TRAIN: iteration 23980 : loss : 0.247653, loss_ce: 0.003775, loss_dice: 0.491531
[11:39:10.457] TRAIN: iteration 23981 : loss : 0.080393, loss_ce: 0.003551, loss_dice: 0.157236
[11:39:10.665] TRAIN: iteration 23982 : loss : 0.061843, loss_ce: 0.003862, loss_dice: 0.119824
[11:39:10.873] TRAIN: iteration 23983 : loss : 0.050602, loss_ce: 0.005459, loss_dice: 0.095745
[11:39:11.090] TRAIN: iteration 23984 : loss : 0.063093, loss_ce: 0.002438, loss_dice: 0.123748
[11:39:11.300] TRAIN: iteration 23985 : loss : 0.245423, loss_ce: 0.002831, loss_dice: 0.488015
[11:39:11.508] TRAIN: iteration 23986 : loss : 0.059139, loss_ce: 0.001772, loss_dice: 0.116506
[11:39:11.718] TRAIN: iteration 23987 : loss : 0.128510, loss_ce: 0.015382, loss_dice: 0.241637
[11:39:11.927] TRAIN: iteration 23988 : loss : 0.149787, loss_ce: 0.004585, loss_dice: 0.294989
[11:39:16.111] TRAIN: iteration 23989 : loss : 0.054259, loss_ce: 0.001185, loss_dice: 0.107333
[11:39:16.319] TRAIN: iteration 23990 : loss : 0.109438, loss_ce: 0.001446, loss_dice: 0.217430
[11:39:16.527] TRAIN: iteration 23991 : loss : 0.126477, loss_ce: 0.004687, loss_dice: 0.248266
[11:39:16.742] TRAIN: iteration 23992 : loss : 0.050850, loss_ce: 0.002966, loss_dice: 0.098735
[11:39:16.954] TRAIN: iteration 23993 : loss : 0.023804, loss_ce: 0.001002, loss_dice: 0.046607
[11:39:17.164] TRAIN: iteration 23994 : loss : 0.250674, loss_ce: 0.001261, loss_dice: 0.500087
[11:39:17.373] TRAIN: iteration 23995 : loss : 0.250235, loss_ce: 0.000454, loss_dice: 0.500015
[11:39:17.581] TRAIN: iteration 23996 : loss : 0.135538, loss_ce: 0.001233, loss_dice: 0.269843
[11:39:17.947] TRAIN: iteration 23997 : loss : 0.124316, loss_ce: 0.008017, loss_dice: 0.240615
[11:39:18.156] TRAIN: iteration 23998 : loss : 0.135759, loss_ce: 0.002516, loss_dice: 0.269001
[11:39:18.364] TRAIN: iteration 23999 : loss : 0.137228, loss_ce: 0.001652, loss_dice: 0.272803
[11:39:18.572] TRAIN: iteration 24000 : loss : 0.055545, loss_ce: 0.002584, loss_dice: 0.108506
[11:39:18.806] TRAIN: iteration 24001 : loss : 0.126762, loss_ce: 0.001961, loss_dice: 0.251563
[11:39:19.013] TRAIN: iteration 24002 : loss : 0.229870, loss_ce: 0.014431, loss_dice: 0.445308
[11:39:19.223] TRAIN: iteration 24003 : loss : 0.148189, loss_ce: 0.006442, loss_dice: 0.289937
[11:39:19.431] TRAIN: iteration 24004 : loss : 0.251124, loss_ce: 0.002117, loss_dice: 0.500132
[11:39:22.211] TRAIN: iteration 24005 : loss : 0.144887, loss_ce: 0.003119, loss_dice: 0.286656
[11:39:22.428] TRAIN: iteration 24006 : loss : 0.052158, loss_ce: 0.005864, loss_dice: 0.098453
[11:39:22.638] TRAIN: iteration 24007 : loss : 0.080903, loss_ce: 0.007316, loss_dice: 0.154490
[11:39:22.845] TRAIN: iteration 24008 : loss : 0.039585, loss_ce: 0.006609, loss_dice: 0.072560
[11:39:23.589] TRAIN: iteration 24009 : loss : 0.100641, loss_ce: 0.004399, loss_dice: 0.196883
[11:39:23.800] TRAIN: iteration 24010 : loss : 0.154755, loss_ce: 0.005696, loss_dice: 0.303814
[11:39:24.009] TRAIN: iteration 24011 : loss : 0.087187, loss_ce: 0.003513, loss_dice: 0.170861
[11:39:24.838] TRAIN: iteration 24012 : loss : 0.251273, loss_ce: 0.002417, loss_dice: 0.500129
[11:39:27.191] TRAIN: iteration 24013 : loss : 0.037117, loss_ce: 0.004965, loss_dice: 0.069269
[11:39:27.398] TRAIN: iteration 24014 : loss : 0.134126, loss_ce: 0.003363, loss_dice: 0.264890
[11:39:27.606] TRAIN: iteration 24015 : loss : 0.047139, loss_ce: 0.006758, loss_dice: 0.087520
[11:39:27.813] TRAIN: iteration 24016 : loss : 0.102851, loss_ce: 0.004303, loss_dice: 0.201399
[11:39:29.463] TRAIN: iteration 24017 : loss : 0.034126, loss_ce: 0.003995, loss_dice: 0.064258
[11:39:29.670] TRAIN: iteration 24018 : loss : 0.039028, loss_ce: 0.004190, loss_dice: 0.073866
[11:39:29.878] TRAIN: iteration 24019 : loss : 0.130276, loss_ce: 0.002286, loss_dice: 0.258267
[11:39:30.088] TRAIN: iteration 24020 : loss : 0.251063, loss_ce: 0.002076, loss_dice: 0.500050
[11:39:33.271] TRAIN: iteration 24021 : loss : 0.038422, loss_ce: 0.001184, loss_dice: 0.075660
[11:39:33.481] TRAIN: iteration 24022 : loss : 0.118156, loss_ce: 0.006910, loss_dice: 0.229402
[11:39:33.688] TRAIN: iteration 24023 : loss : 0.241745, loss_ce: 0.000950, loss_dice: 0.482539
[11:39:33.896] TRAIN: iteration 24024 : loss : 0.102033, loss_ce: 0.005135, loss_dice: 0.198931
[11:39:34.104] TRAIN: iteration 24025 : loss : 0.069925, loss_ce: 0.003672, loss_dice: 0.136177
[11:39:34.315] TRAIN: iteration 24026 : loss : 0.047517, loss_ce: 0.003046, loss_dice: 0.091988
[11:39:34.525] TRAIN: iteration 24027 : loss : 0.152068, loss_ce: 0.003169, loss_dice: 0.300967
[11:39:34.733] TRAIN: iteration 24028 : loss : 0.134957, loss_ce: 0.002651, loss_dice: 0.267262
[11:39:38.414] TRAIN: iteration 24029 : loss : 0.115563, loss_ce: 0.001248, loss_dice: 0.229879
[11:39:38.654] TRAIN: iteration 24030 : loss : 0.047140, loss_ce: 0.003978, loss_dice: 0.090302
[11:39:38.863] TRAIN: iteration 24031 : loss : 0.240531, loss_ce: 0.004715, loss_dice: 0.476347
[11:39:39.076] TRAIN: iteration 24032 : loss : 0.033474, loss_ce: 0.000829, loss_dice: 0.066119
[11:39:39.286] TRAIN: iteration 24033 : loss : 0.250190, loss_ce: 0.000380, loss_dice: 0.500000
[11:39:39.875] TRAIN: iteration 24034 : loss : 0.250407, loss_ce: 0.000794, loss_dice: 0.500021
[11:39:40.083] TRAIN: iteration 24035 : loss : 0.048291, loss_ce: 0.002949, loss_dice: 0.093633
[11:39:40.290] TRAIN: iteration 24036 : loss : 0.050110, loss_ce: 0.005355, loss_dice: 0.094865
[11:39:44.350] TRAIN: iteration 24037 : loss : 0.235355, loss_ce: 0.006610, loss_dice: 0.464099
[11:39:44.560] TRAIN: iteration 24038 : loss : 0.067230, loss_ce: 0.001939, loss_dice: 0.132521
[11:39:44.769] TRAIN: iteration 24039 : loss : 0.250491, loss_ce: 0.000934, loss_dice: 0.500048
[11:39:44.978] TRAIN: iteration 24040 : loss : 0.119922, loss_ce: 0.006818, loss_dice: 0.233027
[11:39:44.979] NaN or Inf found in input tensor.
[11:39:45.962] TRAIN: iteration 24041 : loss : 0.213407, loss_ce: 0.009055, loss_dice: 0.417758
[11:39:46.170] TRAIN: iteration 24042 : loss : 0.021233, loss_ce: 0.001526, loss_dice: 0.040940
[11:39:46.379] TRAIN: iteration 24043 : loss : 0.227561, loss_ce: 0.003061, loss_dice: 0.452061
[11:39:46.587] TRAIN: iteration 24044 : loss : 0.043449, loss_ce: 0.002353, loss_dice: 0.084545
[11:39:51.902] TRAIN: iteration 24045 : loss : 0.100922, loss_ce: 0.006371, loss_dice: 0.195473
[11:39:52.110] TRAIN: iteration 24046 : loss : 0.249818, loss_ce: 0.005094, loss_dice: 0.494543
[11:39:52.321] TRAIN: iteration 24047 : loss : 0.204970, loss_ce: 0.002179, loss_dice: 0.407761
[11:39:52.529] TRAIN: iteration 24048 : loss : 0.109467, loss_ce: 0.004440, loss_dice: 0.214494
[11:39:52.737] TRAIN: iteration 24049 : loss : 0.250809, loss_ce: 0.001564, loss_dice: 0.500055
[11:39:52.947] TRAIN: iteration 24050 : loss : 0.210464, loss_ce: 0.002404, loss_dice: 0.418523
[11:39:53.155] TRAIN: iteration 24051 : loss : 0.148304, loss_ce: 0.010897, loss_dice: 0.285712
[11:39:53.363] TRAIN: iteration 24052 : loss : 0.080847, loss_ce: 0.003007, loss_dice: 0.158687
[11:39:58.435] TRAIN: iteration 24053 : loss : 0.077639, loss_ce: 0.004443, loss_dice: 0.150834
[11:39:58.643] TRAIN: iteration 24054 : loss : 0.106865, loss_ce: 0.009042, loss_dice: 0.204688
[11:39:58.851] TRAIN: iteration 24055 : loss : 0.087536, loss_ce: 0.003063, loss_dice: 0.172009
[11:39:59.059] TRAIN: iteration 24056 : loss : 0.071360, loss_ce: 0.002091, loss_dice: 0.140629
[11:39:59.268] TRAIN: iteration 24057 : loss : 0.182188, loss_ce: 0.002877, loss_dice: 0.361499
[11:39:59.476] TRAIN: iteration 24058 : loss : 0.189597, loss_ce: 0.002700, loss_dice: 0.376494
[11:39:59.684] TRAIN: iteration 24059 : loss : 0.043262, loss_ce: 0.002870, loss_dice: 0.083655
[11:39:59.893] TRAIN: iteration 24060 : loss : 0.231993, loss_ce: 0.011035, loss_dice: 0.452951
[11:40:03.943] TRAIN: iteration 24061 : loss : 0.250915, loss_ce: 0.001769, loss_dice: 0.500062
[11:40:04.157] TRAIN: iteration 24062 : loss : 0.245294, loss_ce: 0.001773, loss_dice: 0.488815
[11:40:04.365] TRAIN: iteration 24063 : loss : 0.114301, loss_ce: 0.001786, loss_dice: 0.226815
[11:40:04.573] TRAIN: iteration 24064 : loss : 0.212967, loss_ce: 0.002365, loss_dice: 0.423569
[11:40:04.781] TRAIN: iteration 24065 : loss : 0.046345, loss_ce: 0.001492, loss_dice: 0.091199
[11:40:04.989] TRAIN: iteration 24066 : loss : 0.128993, loss_ce: 0.001840, loss_dice: 0.256145
[11:40:05.199] TRAIN: iteration 24067 : loss : 0.048056, loss_ce: 0.010295, loss_dice: 0.085817
[11:40:05.407] TRAIN: iteration 24068 : loss : 0.097404, loss_ce: 0.004471, loss_dice: 0.190337
[11:40:07.659] TRAIN: iteration 24069 : loss : 0.250242, loss_ce: 0.000481, loss_dice: 0.500003
[11:40:07.866] TRAIN: iteration 24070 : loss : 0.251154, loss_ce: 0.002168, loss_dice: 0.500140
[11:40:08.130] TRAIN: iteration 24071 : loss : 0.087224, loss_ce: 0.001583, loss_dice: 0.172865
[11:40:08.339] TRAIN: iteration 24072 : loss : 0.250521, loss_ce: 0.001001, loss_dice: 0.500040
[11:40:08.549] TRAIN: iteration 24073 : loss : 0.110102, loss_ce: 0.001886, loss_dice: 0.218319
[11:40:08.758] TRAIN: iteration 24074 : loss : 0.169853, loss_ce: 0.009815, loss_dice: 0.329890
[11:40:08.967] TRAIN: iteration 24075 : loss : 0.022572, loss_ce: 0.000567, loss_dice: 0.044578
[11:40:09.178] TRAIN: iteration 24076 : loss : 0.023113, loss_ce: 0.001271, loss_dice: 0.044954
[11:40:11.031] TRAIN: iteration 24077 : loss : 0.236497, loss_ce: 0.001320, loss_dice: 0.471673
[11:40:11.238] TRAIN: iteration 24078 : loss : 0.127141, loss_ce: 0.002134, loss_dice: 0.252147
[11:40:12.188] TRAIN: iteration 24079 : loss : 0.243068, loss_ce: 0.001112, loss_dice: 0.485024
[11:40:12.396] TRAIN: iteration 24080 : loss : 0.250701, loss_ce: 0.001332, loss_dice: 0.500070
[11:40:13.162] TRAIN: iteration 24081 : loss : 0.066698, loss_ce: 0.001951, loss_dice: 0.131445
[11:40:13.370] TRAIN: iteration 24082 : loss : 0.215791, loss_ce: 0.024316, loss_dice: 0.407267
[11:40:13.579] TRAIN: iteration 24083 : loss : 0.131217, loss_ce: 0.001373, loss_dice: 0.261061
[11:40:13.787] TRAIN: iteration 24084 : loss : 0.250406, loss_ce: 0.000779, loss_dice: 0.500033
[11:40:16.346] TRAIN: iteration 24085 : loss : 0.061821, loss_ce: 0.001668, loss_dice: 0.121974
[11:40:16.555] TRAIN: iteration 24086 : loss : 0.137982, loss_ce: 0.007843, loss_dice: 0.268121
[11:40:16.764] TRAIN: iteration 24087 : loss : 0.105434, loss_ce: 0.001825, loss_dice: 0.209043
[11:40:16.971] TRAIN: iteration 24088 : loss : 0.033923, loss_ce: 0.004277, loss_dice: 0.063569
[11:40:20.117] TRAIN: iteration 24089 : loss : 0.134418, loss_ce: 0.001198, loss_dice: 0.267638
[11:40:20.325] TRAIN: iteration 24090 : loss : 0.127530, loss_ce: 0.005616, loss_dice: 0.249444
[11:40:20.533] TRAIN: iteration 24091 : loss : 0.191846, loss_ce: 0.005221, loss_dice: 0.378470
[11:40:20.741] TRAIN: iteration 24092 : loss : 0.035912, loss_ce: 0.006318, loss_dice: 0.065506
[11:40:24.085] TRAIN: iteration 24093 : loss : 0.234790, loss_ce: 0.006559, loss_dice: 0.463021
[11:40:24.293] TRAIN: iteration 24094 : loss : 0.081947, loss_ce: 0.005992, loss_dice: 0.157902
[11:40:24.502] TRAIN: iteration 24095 : loss : 0.144132, loss_ce: 0.002840, loss_dice: 0.285425
[11:40:24.710] TRAIN: iteration 24096 : loss : 0.076011, loss_ce: 0.009571, loss_dice: 0.142452
[11:40:26.310] TRAIN: iteration 24097 : loss : 0.125205, loss_ce: 0.001282, loss_dice: 0.249128
[11:40:26.519] TRAIN: iteration 24098 : loss : 0.067829, loss_ce: 0.005315, loss_dice: 0.130343
[11:40:26.730] TRAIN: iteration 24099 : loss : 0.181177, loss_ce: 0.005634, loss_dice: 0.356720
[11:40:26.939] TRAIN: iteration 24100 : loss : 0.073847, loss_ce: 0.002230, loss_dice: 0.145463
[11:40:30.127] TRAIN: iteration 24101 : loss : 0.032443, loss_ce: 0.003380, loss_dice: 0.061505
[11:40:30.335] TRAIN: iteration 24102 : loss : 0.251463, loss_ce: 0.002726, loss_dice: 0.500200
[11:40:30.543] TRAIN: iteration 24103 : loss : 0.066451, loss_ce: 0.003817, loss_dice: 0.129086
[11:40:30.752] TRAIN: iteration 24104 : loss : 0.250796, loss_ce: 0.001491, loss_dice: 0.500100
[11:40:31.694] TRAIN: iteration 24105 : loss : 0.037469, loss_ce: 0.002554, loss_dice: 0.072383
[11:40:32.170] TRAIN: iteration 24106 : loss : 0.054628, loss_ce: 0.004695, loss_dice: 0.104561
[11:40:32.378] TRAIN: iteration 24107 : loss : 0.235963, loss_ce: 0.002275, loss_dice: 0.469652
[11:40:32.585] TRAIN: iteration 24108 : loss : 0.046189, loss_ce: 0.001693, loss_dice: 0.090686
[11:40:37.683] TRAIN: iteration 24109 : loss : 0.166596, loss_ce: 0.002521, loss_dice: 0.330671
[11:40:37.891] TRAIN: iteration 24110 : loss : 0.179151, loss_ce: 0.006931, loss_dice: 0.351371
[11:40:38.101] TRAIN: iteration 24111 : loss : 0.045744, loss_ce: 0.003622, loss_dice: 0.087866
[11:40:38.311] TRAIN: iteration 24112 : loss : 0.040433, loss_ce: 0.006741, loss_dice: 0.074125
[11:40:38.519] TRAIN: iteration 24113 : loss : 0.062870, loss_ce: 0.003775, loss_dice: 0.121965
[11:40:38.728] TRAIN: iteration 24114 : loss : 0.068361, loss_ce: 0.012472, loss_dice: 0.124250
[11:40:38.942] TRAIN: iteration 24115 : loss : 0.072265, loss_ce: 0.001767, loss_dice: 0.142762
[11:40:39.152] TRAIN: iteration 24116 : loss : 0.242765, loss_ce: 0.001952, loss_dice: 0.483578
[11:40:43.131] TRAIN: iteration 24117 : loss : 0.194023, loss_ce: 0.004849, loss_dice: 0.383198
[11:40:43.344] TRAIN: iteration 24118 : loss : 0.035316, loss_ce: 0.002730, loss_dice: 0.067903
[11:40:43.551] TRAIN: iteration 24119 : loss : 0.149907, loss_ce: 0.003281, loss_dice: 0.296533
[11:40:43.759] TRAIN: iteration 24120 : loss : 0.079865, loss_ce: 0.001398, loss_dice: 0.158332
[11:40:43.997] TRAIN: iteration 24121 : loss : 0.184657, loss_ce: 0.004013, loss_dice: 0.365302
[11:40:44.207] TRAIN: iteration 24122 : loss : 0.067583, loss_ce: 0.004587, loss_dice: 0.130578
[11:40:44.418] TRAIN: iteration 24123 : loss : 0.251246, loss_ce: 0.002334, loss_dice: 0.500158
[11:40:44.627] TRAIN: iteration 24124 : loss : 0.082912, loss_ce: 0.004792, loss_dice: 0.161032
[11:40:52.164] TRAIN: iteration 24125 : loss : 0.250390, loss_ce: 0.000765, loss_dice: 0.500014
[11:40:52.372] TRAIN: iteration 24126 : loss : 0.108676, loss_ce: 0.001893, loss_dice: 0.215458
[11:40:52.588] TRAIN: iteration 24127 : loss : 0.197022, loss_ce: 0.001070, loss_dice: 0.392975
[11:40:52.798] TRAIN: iteration 24128 : loss : 0.057288, loss_ce: 0.002213, loss_dice: 0.112363
[11:40:53.007] TRAIN: iteration 24129 : loss : 0.250350, loss_ce: 0.003720, loss_dice: 0.496981
[11:40:53.214] TRAIN: iteration 24130 : loss : 0.144361, loss_ce: 0.001595, loss_dice: 0.287128
[11:40:53.424] TRAIN: iteration 24131 : loss : 0.076330, loss_ce: 0.002297, loss_dice: 0.150363
[11:40:53.632] TRAIN: iteration 24132 : loss : 0.184052, loss_ce: 0.005404, loss_dice: 0.362700
[11:40:58.276] TRAIN: iteration 24133 : loss : 0.019875, loss_ce: 0.002692, loss_dice: 0.037058
[11:40:58.485] TRAIN: iteration 24134 : loss : 0.092776, loss_ce: 0.001873, loss_dice: 0.183679
[11:40:58.695] TRAIN: iteration 24135 : loss : 0.107533, loss_ce: 0.004699, loss_dice: 0.210367
[11:40:58.970] TRAIN: iteration 24136 : loss : 0.026670, loss_ce: 0.001131, loss_dice: 0.052209
[11:40:59.839] TRAIN: iteration 24137 : loss : 0.074758, loss_ce: 0.001612, loss_dice: 0.147905
[11:41:00.048] TRAIN: iteration 24138 : loss : 0.085091, loss_ce: 0.002293, loss_dice: 0.167888
[11:41:00.257] TRAIN: iteration 24139 : loss : 0.089984, loss_ce: 0.005190, loss_dice: 0.174778
[11:41:00.466] TRAIN: iteration 24140 : loss : 0.160145, loss_ce: 0.008002, loss_dice: 0.312287
[11:41:05.989] TRAIN: iteration 24141 : loss : 0.136311, loss_ce: 0.005408, loss_dice: 0.267214
[11:41:06.196] TRAIN: iteration 24142 : loss : 0.038718, loss_ce: 0.001619, loss_dice: 0.075816
[11:41:06.405] TRAIN: iteration 24143 : loss : 0.250798, loss_ce: 0.001510, loss_dice: 0.500087
[11:41:06.612] TRAIN: iteration 24144 : loss : 0.024437, loss_ce: 0.001237, loss_dice: 0.047636
[11:41:07.862] TRAIN: iteration 24145 : loss : 0.013942, loss_ce: 0.001042, loss_dice: 0.026842
[11:41:08.070] TRAIN: iteration 24146 : loss : 0.098865, loss_ce: 0.004401, loss_dice: 0.193329
[11:41:08.278] TRAIN: iteration 24147 : loss : 0.049314, loss_ce: 0.006114, loss_dice: 0.092514
[11:41:08.490] TRAIN: iteration 24148 : loss : 0.039654, loss_ce: 0.002503, loss_dice: 0.076805
[11:41:11.137] TRAIN: iteration 24149 : loss : 0.251138, loss_ce: 0.002149, loss_dice: 0.500127
[11:41:11.347] TRAIN: iteration 24150 : loss : 0.218872, loss_ce: 0.007300, loss_dice: 0.430444
[11:41:11.555] TRAIN: iteration 24151 : loss : 0.126680, loss_ce: 0.002563, loss_dice: 0.250798
[11:41:11.763] TRAIN: iteration 24152 : loss : 0.250793, loss_ce: 0.001514, loss_dice: 0.500072
[11:41:14.573] TRAIN: iteration 24153 : loss : 0.064469, loss_ce: 0.002609, loss_dice: 0.126329
[11:41:14.783] TRAIN: iteration 24154 : loss : 0.056210, loss_ce: 0.001983, loss_dice: 0.110438
[11:41:14.994] TRAIN: iteration 24155 : loss : 0.177694, loss_ce: 0.001687, loss_dice: 0.353701
[11:41:15.204] TRAIN: iteration 24156 : loss : 0.177008, loss_ce: 0.004017, loss_dice: 0.349999
[11:41:20.374] TRAIN: iteration 24157 : loss : 0.186476, loss_ce: 0.003480, loss_dice: 0.369472
[11:41:20.588] TRAIN: iteration 24158 : loss : 0.042351, loss_ce: 0.003759, loss_dice: 0.080942
[11:41:20.798] TRAIN: iteration 24159 : loss : 0.092151, loss_ce: 0.002962, loss_dice: 0.181341
[11:41:21.006] TRAIN: iteration 24160 : loss : 0.072610, loss_ce: 0.002429, loss_dice: 0.142790
[11:41:21.802] TRAIN: iteration 24161 : loss : 0.175223, loss_ce: 0.005019, loss_dice: 0.345426
[11:41:23.038] TRAIN: iteration 24162 : loss : 0.069313, loss_ce: 0.001462, loss_dice: 0.137164
[11:41:23.246] TRAIN: iteration 24163 : loss : 0.043311, loss_ce: 0.004836, loss_dice: 0.081786
[11:41:23.455] TRAIN: iteration 24164 : loss : 0.239392, loss_ce: 0.011377, loss_dice: 0.467407
[11:41:26.072] TRAIN: iteration 24165 : loss : 0.229002, loss_ce: 0.001130, loss_dice: 0.456874
[11:41:26.279] TRAIN: iteration 24166 : loss : 0.134923, loss_ce: 0.007051, loss_dice: 0.262795
[11:41:26.489] TRAIN: iteration 24167 : loss : 0.070078, loss_ce: 0.003257, loss_dice: 0.136899
[11:41:27.608] TRAIN: iteration 24168 : loss : 0.060357, loss_ce: 0.001546, loss_dice: 0.119168
[11:41:29.316] TRAIN: iteration 24169 : loss : 0.168293, loss_ce: 0.011772, loss_dice: 0.324813
[11:41:31.353] TRAIN: iteration 24170 : loss : 0.036799, loss_ce: 0.002348, loss_dice: 0.071250
[11:41:31.561] TRAIN: iteration 24171 : loss : 0.123305, loss_ce: 0.009058, loss_dice: 0.237551
[11:41:31.775] TRAIN: iteration 24172 : loss : 0.250499, loss_ce: 0.000975, loss_dice: 0.500023
[11:41:33.933] TRAIN: iteration 24173 : loss : 0.112806, loss_ce: 0.004341, loss_dice: 0.221271
[11:41:34.142] TRAIN: iteration 24174 : loss : 0.151044, loss_ce: 0.005024, loss_dice: 0.297064
[11:41:34.357] TRAIN: iteration 24175 : loss : 0.250894, loss_ce: 0.001716, loss_dice: 0.500071
[11:41:36.125] TRAIN: iteration 24176 : loss : 0.251123, loss_ce: 0.002118, loss_dice: 0.500128
[11:41:37.385] TRAIN: iteration 24177 : loss : 0.095592, loss_ce: 0.003632, loss_dice: 0.187552
[11:41:40.386] TRAIN: iteration 24178 : loss : 0.087083, loss_ce: 0.002582, loss_dice: 0.171584
[11:41:40.601] TRAIN: iteration 24179 : loss : 0.250834, loss_ce: 0.001592, loss_dice: 0.500077
[11:41:40.808] TRAIN: iteration 24180 : loss : 0.021153, loss_ce: 0.002024, loss_dice: 0.040283
[11:41:41.529] TRAIN: iteration 24181 : loss : 0.133787, loss_ce: 0.005059, loss_dice: 0.262516
[11:41:41.739] TRAIN: iteration 24182 : loss : 0.058812, loss_ce: 0.002640, loss_dice: 0.114984
[11:41:41.947] TRAIN: iteration 24183 : loss : 0.214349, loss_ce: 0.002665, loss_dice: 0.426033
[11:41:42.604] TRAIN: iteration 24184 : loss : 0.088648, loss_ce: 0.002226, loss_dice: 0.175069
[11:41:45.355] TRAIN: iteration 24185 : loss : 0.089453, loss_ce: 0.004256, loss_dice: 0.174651
[11:41:49.285] TRAIN: iteration 24186 : loss : 0.040678, loss_ce: 0.001613, loss_dice: 0.079743
[11:41:49.494] TRAIN: iteration 24187 : loss : 0.079263, loss_ce: 0.009890, loss_dice: 0.148637
[11:41:49.701] TRAIN: iteration 24188 : loss : 0.099791, loss_ce: 0.002355, loss_dice: 0.197227
[11:41:50.365] TRAIN: iteration 24189 : loss : 0.080352, loss_ce: 0.003217, loss_dice: 0.157488
[11:41:50.574] TRAIN: iteration 24190 : loss : 0.060001, loss_ce: 0.004416, loss_dice: 0.115586
[11:41:50.784] TRAIN: iteration 24191 : loss : 0.035587, loss_ce: 0.003288, loss_dice: 0.067885
[11:41:50.992] TRAIN: iteration 24192 : loss : 0.128273, loss_ce: 0.007043, loss_dice: 0.249504
[11:41:52.794] TRAIN: iteration 24193 : loss : 0.156502, loss_ce: 0.002672, loss_dice: 0.310332
[11:41:57.078] TRAIN: iteration 24194 : loss : 0.244993, loss_ce: 0.009315, loss_dice: 0.480672
[11:41:57.290] TRAIN: iteration 24195 : loss : 0.110099, loss_ce: 0.003024, loss_dice: 0.217173
[11:41:57.498] TRAIN: iteration 24196 : loss : 0.250768, loss_ce: 0.001458, loss_dice: 0.500078
[11:42:00.990] TRAIN: iteration 24197 : loss : 0.105908, loss_ce: 0.003784, loss_dice: 0.208032
[11:42:01.200] TRAIN: iteration 24198 : loss : 0.049429, loss_ce: 0.002694, loss_dice: 0.096164
[11:42:01.408] TRAIN: iteration 24199 : loss : 0.250529, loss_ce: 0.001017, loss_dice: 0.500041
[11:42:01.616] TRAIN: iteration 24200 : loss : 0.055779, loss_ce: 0.007844, loss_dice: 0.103714
[11:42:01.859] TRAIN: iteration 24201 : loss : 0.073062, loss_ce: 0.003188, loss_dice: 0.142936
[11:42:05.791] TRAIN: iteration 24202 : loss : 0.049175, loss_ce: 0.006639, loss_dice: 0.091711
[11:42:05.999] TRAIN: iteration 24203 : loss : 0.053023, loss_ce: 0.001171, loss_dice: 0.104876
[11:42:06.208] TRAIN: iteration 24204 : loss : 0.135280, loss_ce: 0.003821, loss_dice: 0.266739
[11:42:09.129] TRAIN: iteration 24205 : loss : 0.250728, loss_ce: 0.001368, loss_dice: 0.500088
[11:42:09.337] TRAIN: iteration 24206 : loss : 0.035601, loss_ce: 0.001124, loss_dice: 0.070078
[11:42:09.546] TRAIN: iteration 24207 : loss : 0.125105, loss_ce: 0.001717, loss_dice: 0.248493
[11:42:09.754] TRAIN: iteration 24208 : loss : 0.124296, loss_ce: 0.005644, loss_dice: 0.242949
[11:42:09.962] TRAIN: iteration 24209 : loss : 0.158741, loss_ce: 0.004844, loss_dice: 0.312638
[11:42:14.048] TRAIN: iteration 24210 : loss : 0.108103, loss_ce: 0.003931, loss_dice: 0.212274
[11:42:14.256] TRAIN: iteration 24211 : loss : 0.021034, loss_ce: 0.003710, loss_dice: 0.038358
[11:42:14.465] TRAIN: iteration 24212 : loss : 0.054449, loss_ce: 0.002247, loss_dice: 0.106650
[11:42:17.775] TRAIN: iteration 24213 : loss : 0.075297, loss_ce: 0.005304, loss_dice: 0.145290
[11:42:17.986] TRAIN: iteration 24214 : loss : 0.048911, loss_ce: 0.001456, loss_dice: 0.096366
[11:42:18.193] TRAIN: iteration 24215 : loss : 0.101770, loss_ce: 0.002237, loss_dice: 0.201303
[11:42:18.404] TRAIN: iteration 24216 : loss : 0.081049, loss_ce: 0.008113, loss_dice: 0.153985
[11:42:18.613] TRAIN: iteration 24217 : loss : 0.107132, loss_ce: 0.003873, loss_dice: 0.210391
[11:42:22.007] TRAIN: iteration 24218 : loss : 0.250462, loss_ce: 0.000901, loss_dice: 0.500023
[11:42:22.221] TRAIN: iteration 24219 : loss : 0.038951, loss_ce: 0.002782, loss_dice: 0.075119
[11:42:22.431] TRAIN: iteration 24220 : loss : 0.245993, loss_ce: 0.001950, loss_dice: 0.490036
[11:42:27.585] TRAIN: iteration 24221 : loss : 0.119594, loss_ce: 0.001536, loss_dice: 0.237653
[11:42:27.799] TRAIN: iteration 24222 : loss : 0.033234, loss_ce: 0.003615, loss_dice: 0.062853
[11:42:28.008] TRAIN: iteration 24223 : loss : 0.068116, loss_ce: 0.006067, loss_dice: 0.130164
[11:42:28.219] TRAIN: iteration 24224 : loss : 0.032888, loss_ce: 0.002415, loss_dice: 0.063361
[11:42:28.435] TRAIN: iteration 24225 : loss : 0.054625, loss_ce: 0.001201, loss_dice: 0.108050
[11:42:32.141] TRAIN: iteration 24226 : loss : 0.161200, loss_ce: 0.008081, loss_dice: 0.314319
[11:42:32.348] TRAIN: iteration 24227 : loss : 0.071614, loss_ce: 0.001371, loss_dice: 0.141856
[11:42:32.559] TRAIN: iteration 24228 : loss : 0.237581, loss_ce: 0.007200, loss_dice: 0.467963
[11:42:37.140] TRAIN: iteration 24229 : loss : 0.168780, loss_ce: 0.011897, loss_dice: 0.325663
[11:42:37.347] TRAIN: iteration 24230 : loss : 0.188837, loss_ce: 0.023449, loss_dice: 0.354225
[11:42:37.555] TRAIN: iteration 24231 : loss : 0.092037, loss_ce: 0.002150, loss_dice: 0.181924
[11:42:37.761] TRAIN: iteration 24232 : loss : 0.206645, loss_ce: 0.004729, loss_dice: 0.408560
[11:42:37.969] TRAIN: iteration 24233 : loss : 0.072478, loss_ce: 0.004034, loss_dice: 0.140922
[11:42:42.778] TRAIN: iteration 24234 : loss : 0.046095, loss_ce: 0.002249, loss_dice: 0.089942
[11:42:42.986] TRAIN: iteration 24235 : loss : 0.245855, loss_ce: 0.001516, loss_dice: 0.490194
[11:42:43.197] TRAIN: iteration 24236 : loss : 0.058475, loss_ce: 0.002986, loss_dice: 0.113964
[11:42:45.821] TRAIN: iteration 24237 : loss : 0.114221, loss_ce: 0.001799, loss_dice: 0.226642
[11:42:46.031] TRAIN: iteration 24238 : loss : 0.036001, loss_ce: 0.001332, loss_dice: 0.070670
[11:42:46.239] TRAIN: iteration 24239 : loss : 0.037851, loss_ce: 0.003926, loss_dice: 0.071777
[11:42:46.448] TRAIN: iteration 24240 : loss : 0.078345, loss_ce: 0.001516, loss_dice: 0.155174
[11:42:46.684] TRAIN: iteration 24241 : loss : 0.143600, loss_ce: 0.003566, loss_dice: 0.283634
[11:42:52.046] TRAIN: iteration 24242 : loss : 0.071952, loss_ce: 0.001571, loss_dice: 0.142334
[11:42:52.253] TRAIN: iteration 24243 : loss : 0.234801, loss_ce: 0.002121, loss_dice: 0.467481
[11:42:52.462] TRAIN: iteration 24244 : loss : 0.093830, loss_ce: 0.002120, loss_dice: 0.185541
[11:42:53.362] TRAIN: iteration 24245 : loss : 0.072764, loss_ce: 0.001511, loss_dice: 0.144017
[11:42:53.569] TRAIN: iteration 24246 : loss : 0.115260, loss_ce: 0.005309, loss_dice: 0.225211
[11:42:53.777] TRAIN: iteration 24247 : loss : 0.137255, loss_ce: 0.009710, loss_dice: 0.264800
[11:42:53.985] TRAIN: iteration 24248 : loss : 0.101389, loss_ce: 0.012777, loss_dice: 0.190001
[11:42:54.193] TRAIN: iteration 24249 : loss : 0.123485, loss_ce: 0.001121, loss_dice: 0.245849
[11:43:01.013] TRAIN: iteration 24250 : loss : 0.045217, loss_ce: 0.001469, loss_dice: 0.088966
[11:43:01.220] TRAIN: iteration 24251 : loss : 0.024362, loss_ce: 0.000490, loss_dice: 0.048234
[11:43:01.428] TRAIN: iteration 24252 : loss : 0.033077, loss_ce: 0.001715, loss_dice: 0.064440
[11:43:02.143] TRAIN: iteration 24253 : loss : 0.250331, loss_ce: 0.000647, loss_dice: 0.500015
[11:43:02.351] TRAIN: iteration 24254 : loss : 0.064200, loss_ce: 0.010122, loss_dice: 0.118278
[11:43:02.559] TRAIN: iteration 24255 : loss : 0.201832, loss_ce: 0.008001, loss_dice: 0.395663
[11:43:02.771] TRAIN: iteration 24256 : loss : 0.070840, loss_ce: 0.005026, loss_dice: 0.136655
[11:43:02.980] TRAIN: iteration 24257 : loss : 0.147013, loss_ce: 0.002904, loss_dice: 0.291123
[11:43:11.056] TRAIN: iteration 24258 : loss : 0.044007, loss_ce: 0.000664, loss_dice: 0.087351
[11:43:11.264] TRAIN: iteration 24259 : loss : 0.208512, loss_ce: 0.001717, loss_dice: 0.415307
[11:43:11.473] TRAIN: iteration 24260 : loss : 0.118859, loss_ce: 0.005790, loss_dice: 0.231929
[11:43:11.709] TRAIN: iteration 24261 : loss : 0.221189, loss_ce: 0.009862, loss_dice: 0.432515
[11:43:11.917] TRAIN: iteration 24262 : loss : 0.250380, loss_ce: 0.000744, loss_dice: 0.500015
[11:43:12.126] TRAIN: iteration 24263 : loss : 0.067007, loss_ce: 0.002695, loss_dice: 0.131320
[11:43:12.334] TRAIN: iteration 24264 : loss : 0.069087, loss_ce: 0.004439, loss_dice: 0.133735
[11:43:12.542] TRAIN: iteration 24265 : loss : 0.138009, loss_ce: 0.001693, loss_dice: 0.274325
[11:43:18.076] TRAIN: iteration 24266 : loss : 0.233180, loss_ce: 0.001427, loss_dice: 0.464934
[11:43:18.291] TRAIN: iteration 24267 : loss : 0.102880, loss_ce: 0.003421, loss_dice: 0.202339
[11:43:18.499] TRAIN: iteration 24268 : loss : 0.035015, loss_ce: 0.004591, loss_dice: 0.065438
[11:43:18.992] TRAIN: iteration 24269 : loss : 0.122369, loss_ce: 0.013385, loss_dice: 0.231354
[11:43:19.200] TRAIN: iteration 24270 : loss : 0.214095, loss_ce: 0.003849, loss_dice: 0.424341
[11:43:19.410] TRAIN: iteration 24271 : loss : 0.046058, loss_ce: 0.005519, loss_dice: 0.086598
[11:43:19.618] TRAIN: iteration 24272 : loss : 0.250300, loss_ce: 0.000590, loss_dice: 0.500010
[11:43:20.013] TRAIN: iteration 24273 : loss : 0.054146, loss_ce: 0.003584, loss_dice: 0.104708
[11:43:24.980] TRAIN: iteration 24274 : loss : 0.013249, loss_ce: 0.000650, loss_dice: 0.025849
[11:43:25.187] TRAIN: iteration 24275 : loss : 0.013466, loss_ce: 0.000980, loss_dice: 0.025953
[11:43:25.395] TRAIN: iteration 24276 : loss : 0.099753, loss_ce: 0.003754, loss_dice: 0.195751
[11:43:28.379] TRAIN: iteration 24277 : loss : 0.034564, loss_ce: 0.000834, loss_dice: 0.068295
[11:43:28.588] TRAIN: iteration 24278 : loss : 0.119212, loss_ce: 0.007111, loss_dice: 0.231313
[11:43:28.799] TRAIN: iteration 24279 : loss : 0.203396, loss_ce: 0.002512, loss_dice: 0.404279
[11:43:29.008] TRAIN: iteration 24280 : loss : 0.174680, loss_ce: 0.008703, loss_dice: 0.340658
[11:43:29.476] TRAIN: iteration 24281 : loss : 0.227257, loss_ce: 0.002479, loss_dice: 0.452035
[11:43:33.264] TRAIN: iteration 24282 : loss : 0.094502, loss_ce: 0.004634, loss_dice: 0.184370
[11:43:33.471] TRAIN: iteration 24283 : loss : 0.071705, loss_ce: 0.003097, loss_dice: 0.140313
[11:43:33.679] TRAIN: iteration 24284 : loss : 0.085031, loss_ce: 0.002516, loss_dice: 0.167546
[11:43:38.839] TRAIN: iteration 24285 : loss : 0.020116, loss_ce: 0.000684, loss_dice: 0.039547
[11:43:39.046] TRAIN: iteration 24286 : loss : 0.251078, loss_ce: 0.002033, loss_dice: 0.500124
[11:43:39.253] TRAIN: iteration 24287 : loss : 0.023999, loss_ce: 0.002145, loss_dice: 0.045852
[11:43:39.461] TRAIN: iteration 24288 : loss : 0.105419, loss_ce: 0.004096, loss_dice: 0.206743
[11:43:39.670] TRAIN: iteration 24289 : loss : 0.213688, loss_ce: 0.001376, loss_dice: 0.425999
[11:43:39.767] TRAIN: iteration 24290 : loss : 0.250537, loss_ce: 0.001045, loss_dice: 0.500028
[11:49:09.292] VALIDATION: iteration 13 : loss : 0.121119, loss_ce: 0.004281, loss_dice: 0.237956
[11:49:10.061] TRAIN: iteration 24291 : loss : 0.076072, loss_ce: 0.006134, loss_dice: 0.146010
[11:49:12.749] TRAIN: iteration 24292 : loss : 0.089276, loss_ce: 0.001769, loss_dice: 0.176782
[11:49:12.970] TRAIN: iteration 24293 : loss : 0.071788, loss_ce: 0.003411, loss_dice: 0.140164
[11:49:13.181] TRAIN: iteration 24294 : loss : 0.245814, loss_ce: 0.002473, loss_dice: 0.489156
[11:49:13.388] TRAIN: iteration 24295 : loss : 0.050440, loss_ce: 0.003604, loss_dice: 0.097275
[11:49:13.599] TRAIN: iteration 24296 : loss : 0.052288, loss_ce: 0.002615, loss_dice: 0.101962
[11:49:13.814] TRAIN: iteration 24297 : loss : 0.077312, loss_ce: 0.006859, loss_dice: 0.147766
[11:49:14.030] TRAIN: iteration 24298 : loss : 0.088889, loss_ce: 0.001666, loss_dice: 0.176112
[11:49:14.240] TRAIN: iteration 24299 : loss : 0.035835, loss_ce: 0.005393, loss_dice: 0.066276
[11:49:14.450] TRAIN: iteration 24300 : loss : 0.251284, loss_ce: 0.002433, loss_dice: 0.500135
[11:49:14.702] TRAIN: iteration 24301 : loss : 0.043120, loss_ce: 0.003118, loss_dice: 0.083121
[11:49:14.909] TRAIN: iteration 24302 : loss : 0.174061, loss_ce: 0.006510, loss_dice: 0.341612
[11:49:15.117] TRAIN: iteration 24303 : loss : 0.033535, loss_ce: 0.002080, loss_dice: 0.064990
[11:49:15.325] TRAIN: iteration 24304 : loss : 0.140954, loss_ce: 0.002365, loss_dice: 0.279544
[11:49:15.533] TRAIN: iteration 24305 : loss : 0.041780, loss_ce: 0.000963, loss_dice: 0.082597
[11:49:15.742] TRAIN: iteration 24306 : loss : 0.057144, loss_ce: 0.002545, loss_dice: 0.111742
[11:49:15.952] TRAIN: iteration 24307 : loss : 0.040198, loss_ce: 0.000724, loss_dice: 0.079671
[11:49:16.161] TRAIN: iteration 24308 : loss : 0.250536, loss_ce: 0.001044, loss_dice: 0.500028
[11:49:16.371] TRAIN: iteration 24309 : loss : 0.040546, loss_ce: 0.000758, loss_dice: 0.080335
[11:49:16.579] TRAIN: iteration 24310 : loss : 0.235673, loss_ce: 0.003017, loss_dice: 0.468328
[11:49:16.787] TRAIN: iteration 24311 : loss : 0.035310, loss_ce: 0.002491, loss_dice: 0.068128
[11:49:16.995] TRAIN: iteration 24312 : loss : 0.135491, loss_ce: 0.005558, loss_dice: 0.265424
[11:49:17.203] TRAIN: iteration 24313 : loss : 0.117303, loss_ce: 0.009128, loss_dice: 0.225479
[11:49:17.411] TRAIN: iteration 24314 : loss : 0.112810, loss_ce: 0.002941, loss_dice: 0.222679
[11:49:17.620] TRAIN: iteration 24315 : loss : 0.091489, loss_ce: 0.013334, loss_dice: 0.169645
[11:49:17.830] TRAIN: iteration 24316 : loss : 0.074131, loss_ce: 0.005781, loss_dice: 0.142480
[11:49:18.038] TRAIN: iteration 24317 : loss : 0.042983, loss_ce: 0.001829, loss_dice: 0.084138
[11:49:18.251] TRAIN: iteration 24318 : loss : 0.133202, loss_ce: 0.003683, loss_dice: 0.262722
[11:49:18.462] TRAIN: iteration 24319 : loss : 0.079747, loss_ce: 0.003216, loss_dice: 0.156278
[11:49:18.677] TRAIN: iteration 24320 : loss : 0.069113, loss_ce: 0.004832, loss_dice: 0.133393
[11:49:18.916] TRAIN: iteration 24321 : loss : 0.114992, loss_ce: 0.003188, loss_dice: 0.226796
[11:49:19.125] TRAIN: iteration 24322 : loss : 0.056881, loss_ce: 0.004795, loss_dice: 0.108966
[11:49:19.333] TRAIN: iteration 24323 : loss : 0.049883, loss_ce: 0.002891, loss_dice: 0.096875
[11:49:19.543] TRAIN: iteration 24324 : loss : 0.030966, loss_ce: 0.003562, loss_dice: 0.058371
[11:49:19.750] TRAIN: iteration 24325 : loss : 0.170033, loss_ce: 0.001748, loss_dice: 0.338317
[11:49:19.965] TRAIN: iteration 24326 : loss : 0.226901, loss_ce: 0.002513, loss_dice: 0.451290
[11:49:20.180] TRAIN: iteration 24327 : loss : 0.075416, loss_ce: 0.001458, loss_dice: 0.149374
[11:49:20.393] TRAIN: iteration 24328 : loss : 0.095609, loss_ce: 0.001928, loss_dice: 0.189291
[11:49:20.602] TRAIN: iteration 24329 : loss : 0.044558, loss_ce: 0.005529, loss_dice: 0.083587
[11:49:20.815] TRAIN: iteration 24330 : loss : 0.030734, loss_ce: 0.001074, loss_dice: 0.060394
[11:49:21.032] TRAIN: iteration 24331 : loss : 0.046613, loss_ce: 0.001224, loss_dice: 0.092003
[11:49:21.243] TRAIN: iteration 24332 : loss : 0.059877, loss_ce: 0.003703, loss_dice: 0.116051
[11:49:21.451] TRAIN: iteration 24333 : loss : 0.186474, loss_ce: 0.008546, loss_dice: 0.364403
[11:49:21.660] TRAIN: iteration 24334 : loss : 0.035967, loss_ce: 0.004489, loss_dice: 0.067444
[11:49:21.869] TRAIN: iteration 24335 : loss : 0.050768, loss_ce: 0.003828, loss_dice: 0.097709
[11:49:22.085] TRAIN: iteration 24336 : loss : 0.117182, loss_ce: 0.004675, loss_dice: 0.229690
[11:49:22.296] TRAIN: iteration 24337 : loss : 0.088642, loss_ce: 0.005749, loss_dice: 0.171535
[11:49:22.507] TRAIN: iteration 24338 : loss : 0.044829, loss_ce: 0.002584, loss_dice: 0.087074
[11:49:22.715] TRAIN: iteration 24339 : loss : 0.250879, loss_ce: 0.001665, loss_dice: 0.500093
[11:49:22.925] TRAIN: iteration 24340 : loss : 0.053300, loss_ce: 0.001349, loss_dice: 0.105251
[11:49:23.160] TRAIN: iteration 24341 : loss : 0.163185, loss_ce: 0.002209, loss_dice: 0.324161
[11:49:23.370] TRAIN: iteration 24342 : loss : 0.042765, loss_ce: 0.001081, loss_dice: 0.084449
[11:49:23.599] TRAIN: iteration 24343 : loss : 0.096166, loss_ce: 0.001621, loss_dice: 0.190712
[11:49:23.810] TRAIN: iteration 24344 : loss : 0.068614, loss_ce: 0.005152, loss_dice: 0.132076
[11:49:24.020] TRAIN: iteration 24345 : loss : 0.250642, loss_ce: 0.001229, loss_dice: 0.500055
[11:49:24.228] TRAIN: iteration 24346 : loss : 0.125134, loss_ce: 0.002568, loss_dice: 0.247701
[11:49:24.437] TRAIN: iteration 24347 : loss : 0.087558, loss_ce: 0.003784, loss_dice: 0.171333
[11:49:24.647] TRAIN: iteration 24348 : loss : 0.164268, loss_ce: 0.009934, loss_dice: 0.318601
[11:49:24.855] TRAIN: iteration 24349 : loss : 0.117939, loss_ce: 0.007250, loss_dice: 0.228628
[11:49:25.064] TRAIN: iteration 24350 : loss : 0.248992, loss_ce: 0.004647, loss_dice: 0.493336
[11:49:25.273] TRAIN: iteration 24351 : loss : 0.250395, loss_ce: 0.000778, loss_dice: 0.500011
[11:49:25.489] TRAIN: iteration 24352 : loss : 0.060219, loss_ce: 0.002377, loss_dice: 0.118060
[11:49:25.701] TRAIN: iteration 24353 : loss : 0.063733, loss_ce: 0.005835, loss_dice: 0.121632
[11:49:25.909] TRAIN: iteration 24354 : loss : 0.229017, loss_ce: 0.002409, loss_dice: 0.455625
[11:49:26.126] TRAIN: iteration 24355 : loss : 0.080754, loss_ce: 0.002222, loss_dice: 0.159287
[11:49:26.335] TRAIN: iteration 24356 : loss : 0.054281, loss_ce: 0.003044, loss_dice: 0.105518
[11:49:26.652] TRAIN: iteration 24357 : loss : 0.082537, loss_ce: 0.001416, loss_dice: 0.163657
[11:49:26.860] TRAIN: iteration 24358 : loss : 0.028698, loss_ce: 0.001778, loss_dice: 0.055617
[11:49:27.075] TRAIN: iteration 24359 : loss : 0.111278, loss_ce: 0.004892, loss_dice: 0.217663
[11:49:27.288] TRAIN: iteration 24360 : loss : 0.093333, loss_ce: 0.003263, loss_dice: 0.183403
[11:49:27.526] TRAIN: iteration 24361 : loss : 0.186821, loss_ce: 0.001972, loss_dice: 0.371669
[11:49:27.742] TRAIN: iteration 24362 : loss : 0.044677, loss_ce: 0.001835, loss_dice: 0.087519
[11:49:27.951] TRAIN: iteration 24363 : loss : 0.156168, loss_ce: 0.006803, loss_dice: 0.305532
[11:49:28.169] TRAIN: iteration 24364 : loss : 0.037699, loss_ce: 0.002512, loss_dice: 0.072886
[11:49:28.376] TRAIN: iteration 24365 : loss : 0.119873, loss_ce: 0.002974, loss_dice: 0.236771
[11:49:28.584] TRAIN: iteration 24366 : loss : 0.214986, loss_ce: 0.002845, loss_dice: 0.427127
[11:49:28.793] TRAIN: iteration 24367 : loss : 0.071135, loss_ce: 0.005025, loss_dice: 0.137245
[11:49:29.005] TRAIN: iteration 24368 : loss : 0.118515, loss_ce: 0.014908, loss_dice: 0.222121
[11:49:29.213] TRAIN: iteration 24369 : loss : 0.251113, loss_ce: 0.005435, loss_dice: 0.496791
[11:49:29.446] TRAIN: iteration 24370 : loss : 0.103512, loss_ce: 0.001619, loss_dice: 0.205404
[11:49:29.655] TRAIN: iteration 24371 : loss : 0.041634, loss_ce: 0.007726, loss_dice: 0.075542
[11:49:29.865] TRAIN: iteration 24372 : loss : 0.058646, loss_ce: 0.001741, loss_dice: 0.115551
[11:49:30.076] TRAIN: iteration 24373 : loss : 0.164782, loss_ce: 0.005365, loss_dice: 0.324199
[11:49:30.284] TRAIN: iteration 24374 : loss : 0.173715, loss_ce: 0.005540, loss_dice: 0.341890
[11:49:30.492] TRAIN: iteration 24375 : loss : 0.055993, loss_ce: 0.002654, loss_dice: 0.109331
[11:49:30.714] TRAIN: iteration 24376 : loss : 0.029886, loss_ce: 0.001645, loss_dice: 0.058127
[11:49:30.924] TRAIN: iteration 24377 : loss : 0.205334, loss_ce: 0.009353, loss_dice: 0.401315
[11:49:31.133] TRAIN: iteration 24378 : loss : 0.037225, loss_ce: 0.001863, loss_dice: 0.072587
[11:49:31.340] TRAIN: iteration 24379 : loss : 0.087962, loss_ce: 0.005851, loss_dice: 0.170073
[11:49:31.551] TRAIN: iteration 24380 : loss : 0.102829, loss_ce: 0.003904, loss_dice: 0.201753
[11:49:31.873] TRAIN: iteration 24381 : loss : 0.049942, loss_ce: 0.005060, loss_dice: 0.094824
[11:49:32.082] TRAIN: iteration 24382 : loss : 0.038807, loss_ce: 0.003191, loss_dice: 0.074422
[11:49:32.291] TRAIN: iteration 24383 : loss : 0.050664, loss_ce: 0.002547, loss_dice: 0.098780
[11:49:32.513] TRAIN: iteration 24384 : loss : 0.031852, loss_ce: 0.002161, loss_dice: 0.061543
[11:49:32.721] TRAIN: iteration 24385 : loss : 0.144428, loss_ce: 0.005912, loss_dice: 0.282944
[11:49:32.935] TRAIN: iteration 24386 : loss : 0.110102, loss_ce: 0.004604, loss_dice: 0.215600
[11:49:33.146] TRAIN: iteration 24387 : loss : 0.081584, loss_ce: 0.003071, loss_dice: 0.160097
[11:49:33.361] TRAIN: iteration 24388 : loss : 0.162685, loss_ce: 0.001266, loss_dice: 0.324105
[11:49:33.575] TRAIN: iteration 24389 : loss : 0.062490, loss_ce: 0.001458, loss_dice: 0.123523
[11:49:33.784] TRAIN: iteration 24390 : loss : 0.250419, loss_ce: 0.000822, loss_dice: 0.500016
[11:49:33.996] TRAIN: iteration 24391 : loss : 0.100836, loss_ce: 0.003199, loss_dice: 0.198472
[11:49:34.206] TRAIN: iteration 24392 : loss : 0.085705, loss_ce: 0.002663, loss_dice: 0.168746
[11:49:34.416] TRAIN: iteration 24393 : loss : 0.238689, loss_ce: 0.002499, loss_dice: 0.474880
[11:49:34.631] TRAIN: iteration 24394 : loss : 0.239875, loss_ce: 0.005717, loss_dice: 0.474032
[11:49:34.840] TRAIN: iteration 24395 : loss : 0.136752, loss_ce: 0.005102, loss_dice: 0.268401
[11:49:35.048] TRAIN: iteration 24396 : loss : 0.052241, loss_ce: 0.002080, loss_dice: 0.102403
[11:49:35.259] TRAIN: iteration 24397 : loss : 0.111702, loss_ce: 0.010693, loss_dice: 0.212711
[11:49:35.469] TRAIN: iteration 24398 : loss : 0.246165, loss_ce: 0.002731, loss_dice: 0.489599
[11:49:35.678] TRAIN: iteration 24399 : loss : 0.064388, loss_ce: 0.002144, loss_dice: 0.126633
[11:49:35.886] TRAIN: iteration 24400 : loss : 0.179582, loss_ce: 0.002010, loss_dice: 0.357153
[11:49:36.125] TRAIN: iteration 24401 : loss : 0.152515, loss_ce: 0.002401, loss_dice: 0.302628
[11:49:36.333] TRAIN: iteration 24402 : loss : 0.068775, loss_ce: 0.009303, loss_dice: 0.128247
[11:49:36.541] TRAIN: iteration 24403 : loss : 0.105504, loss_ce: 0.009034, loss_dice: 0.201974
[11:49:36.751] TRAIN: iteration 24404 : loss : 0.168894, loss_ce: 0.004303, loss_dice: 0.333485
[11:49:36.961] TRAIN: iteration 24405 : loss : 0.198348, loss_ce: 0.007509, loss_dice: 0.389188
[11:49:37.170] TRAIN: iteration 24406 : loss : 0.227788, loss_ce: 0.003605, loss_dice: 0.451971
[11:49:37.378] TRAIN: iteration 24407 : loss : 0.212292, loss_ce: 0.001980, loss_dice: 0.422603
[11:49:37.587] TRAIN: iteration 24408 : loss : 0.025294, loss_ce: 0.001032, loss_dice: 0.049557
[11:49:37.797] TRAIN: iteration 24409 : loss : 0.050933, loss_ce: 0.001921, loss_dice: 0.099945
[11:49:38.006] TRAIN: iteration 24410 : loss : 0.105421, loss_ce: 0.002215, loss_dice: 0.208626
[11:49:38.216] TRAIN: iteration 24411 : loss : 0.251820, loss_ce: 0.003412, loss_dice: 0.500228
[11:49:38.424] TRAIN: iteration 24412 : loss : 0.182611, loss_ce: 0.003548, loss_dice: 0.361674
[11:49:38.633] TRAIN: iteration 24413 : loss : 0.149986, loss_ce: 0.003499, loss_dice: 0.296473
[11:49:38.843] TRAIN: iteration 24414 : loss : 0.166717, loss_ce: 0.005756, loss_dice: 0.327678
[11:49:39.055] TRAIN: iteration 24415 : loss : 0.069368, loss_ce: 0.005201, loss_dice: 0.133535
[11:49:39.269] TRAIN: iteration 24416 : loss : 0.054470, loss_ce: 0.006940, loss_dice: 0.101999
[11:49:39.479] TRAIN: iteration 24417 : loss : 0.252689, loss_ce: 0.008686, loss_dice: 0.496691
[11:49:39.687] TRAIN: iteration 24418 : loss : 0.096063, loss_ce: 0.002660, loss_dice: 0.189466
[11:49:39.895] TRAIN: iteration 24419 : loss : 0.250876, loss_ce: 0.001676, loss_dice: 0.500077
[11:49:40.110] TRAIN: iteration 24420 : loss : 0.080265, loss_ce: 0.002945, loss_dice: 0.157586
[11:49:40.346] TRAIN: iteration 24421 : loss : 0.120623, loss_ce: 0.001075, loss_dice: 0.240172
[11:49:40.555] TRAIN: iteration 24422 : loss : 0.087424, loss_ce: 0.003486, loss_dice: 0.171363
[11:49:40.765] TRAIN: iteration 24423 : loss : 0.117022, loss_ce: 0.012057, loss_dice: 0.221987
[11:49:40.973] TRAIN: iteration 24424 : loss : 0.046240, loss_ce: 0.002954, loss_dice: 0.089525
[11:49:41.183] TRAIN: iteration 24425 : loss : 0.244001, loss_ce: 0.004990, loss_dice: 0.483011
[11:49:41.392] TRAIN: iteration 24426 : loss : 0.097834, loss_ce: 0.001374, loss_dice: 0.194294
[11:49:41.601] TRAIN: iteration 24427 : loss : 0.184181, loss_ce: 0.006777, loss_dice: 0.361585
[11:49:41.816] TRAIN: iteration 24428 : loss : 0.129117, loss_ce: 0.003383, loss_dice: 0.254851
[11:49:42.026] TRAIN: iteration 24429 : loss : 0.131806, loss_ce: 0.004318, loss_dice: 0.259295
[11:49:42.234] TRAIN: iteration 24430 : loss : 0.250545, loss_ce: 0.001032, loss_dice: 0.500058
[11:49:42.450] TRAIN: iteration 24431 : loss : 0.056302, loss_ce: 0.002178, loss_dice: 0.110427
[11:49:42.664] TRAIN: iteration 24432 : loss : 0.038504, loss_ce: 0.007194, loss_dice: 0.069814
[11:49:42.880] TRAIN: iteration 24433 : loss : 0.134410, loss_ce: 0.012645, loss_dice: 0.256175
[11:49:43.087] TRAIN: iteration 24434 : loss : 0.249357, loss_ce: 0.001720, loss_dice: 0.496994
[11:49:43.295] TRAIN: iteration 24435 : loss : 0.172287, loss_ce: 0.002812, loss_dice: 0.341761
[11:49:43.502] TRAIN: iteration 24436 : loss : 0.100117, loss_ce: 0.000985, loss_dice: 0.199249
[11:49:43.816] TRAIN: iteration 24437 : loss : 0.035734, loss_ce: 0.001428, loss_dice: 0.070039
[11:49:44.026] TRAIN: iteration 24438 : loss : 0.076665, loss_ce: 0.003451, loss_dice: 0.149880
[11:49:44.236] TRAIN: iteration 24439 : loss : 0.163559, loss_ce: 0.007984, loss_dice: 0.319133
[11:49:44.448] TRAIN: iteration 24440 : loss : 0.130891, loss_ce: 0.005755, loss_dice: 0.256028
[11:49:44.697] TRAIN: iteration 24441 : loss : 0.089577, loss_ce: 0.004059, loss_dice: 0.175094
[11:49:44.906] TRAIN: iteration 24442 : loss : 0.048127, loss_ce: 0.003075, loss_dice: 0.093179
[11:49:45.115] TRAIN: iteration 24443 : loss : 0.029606, loss_ce: 0.004426, loss_dice: 0.054787
[11:49:45.324] TRAIN: iteration 24444 : loss : 0.250677, loss_ce: 0.001270, loss_dice: 0.500084
[11:49:45.533] TRAIN: iteration 24445 : loss : 0.052442, loss_ce: 0.010455, loss_dice: 0.094428
[11:49:45.748] TRAIN: iteration 24446 : loss : 0.250532, loss_ce: 0.001017, loss_dice: 0.500047
[11:49:45.957] TRAIN: iteration 24447 : loss : 0.071291, loss_ce: 0.009309, loss_dice: 0.133272
[11:49:46.167] TRAIN: iteration 24448 : loss : 0.051843, loss_ce: 0.003322, loss_dice: 0.100365
[11:49:46.376] TRAIN: iteration 24449 : loss : 0.112589, loss_ce: 0.004670, loss_dice: 0.220509
[11:49:46.603] TRAIN: iteration 24450 : loss : 0.250206, loss_ce: 0.001037, loss_dice: 0.499376
[11:49:46.813] TRAIN: iteration 24451 : loss : 0.069660, loss_ce: 0.000756, loss_dice: 0.138563
[11:49:47.021] TRAIN: iteration 24452 : loss : 0.250429, loss_ce: 0.000823, loss_dice: 0.500035
[11:49:47.230] TRAIN: iteration 24453 : loss : 0.186905, loss_ce: 0.007946, loss_dice: 0.365864
[11:49:47.438] TRAIN: iteration 24454 : loss : 0.130379, loss_ce: 0.005807, loss_dice: 0.254951
[11:49:47.647] TRAIN: iteration 24455 : loss : 0.062204, loss_ce: 0.002049, loss_dice: 0.122358
[11:49:47.859] TRAIN: iteration 24456 : loss : 0.028418, loss_ce: 0.001393, loss_dice: 0.055443
[11:49:48.067] TRAIN: iteration 24457 : loss : 0.099545, loss_ce: 0.001808, loss_dice: 0.197282
[11:49:48.279] TRAIN: iteration 24458 : loss : 0.101550, loss_ce: 0.010149, loss_dice: 0.192951
[11:49:48.487] TRAIN: iteration 24459 : loss : 0.074239, loss_ce: 0.003374, loss_dice: 0.145104
[11:49:48.699] TRAIN: iteration 24460 : loss : 0.183293, loss_ce: 0.005695, loss_dice: 0.360890
[11:49:48.700] NaN or Inf found in input tensor.
[11:49:48.915] TRAIN: iteration 24461 : loss : 0.105148, loss_ce: 0.002365, loss_dice: 0.207932
[11:49:49.126] TRAIN: iteration 24462 : loss : 0.045345, loss_ce: 0.002745, loss_dice: 0.087946
[11:49:49.336] TRAIN: iteration 24463 : loss : 0.036965, loss_ce: 0.002628, loss_dice: 0.071303
[11:49:49.545] TRAIN: iteration 24464 : loss : 0.155713, loss_ce: 0.003460, loss_dice: 0.307967
[11:49:49.759] TRAIN: iteration 24465 : loss : 0.142426, loss_ce: 0.003615, loss_dice: 0.281237
[11:49:49.967] TRAIN: iteration 24466 : loss : 0.033925, loss_ce: 0.007908, loss_dice: 0.059943
[11:49:50.179] TRAIN: iteration 24467 : loss : 0.060672, loss_ce: 0.004878, loss_dice: 0.116466
[11:49:50.387] TRAIN: iteration 24468 : loss : 0.196901, loss_ce: 0.003668, loss_dice: 0.390135
[11:49:50.625] TRAIN: iteration 24469 : loss : 0.033153, loss_ce: 0.001186, loss_dice: 0.065120
[11:49:50.837] TRAIN: iteration 24470 : loss : 0.250754, loss_ce: 0.001442, loss_dice: 0.500067
[11:49:51.048] TRAIN: iteration 24471 : loss : 0.134109, loss_ce: 0.003271, loss_dice: 0.264946
[11:49:51.259] TRAIN: iteration 24472 : loss : 0.078703, loss_ce: 0.001272, loss_dice: 0.156134
[11:49:51.475] TRAIN: iteration 24473 : loss : 0.251538, loss_ce: 0.002887, loss_dice: 0.500189
[11:49:51.686] TRAIN: iteration 24474 : loss : 0.250339, loss_ce: 0.019195, loss_dice: 0.481483
[11:49:51.902] TRAIN: iteration 24475 : loss : 0.192644, loss_ce: 0.011779, loss_dice: 0.373510
[11:49:52.116] TRAIN: iteration 24476 : loss : 0.126467, loss_ce: 0.004323, loss_dice: 0.248611
[11:49:52.324] TRAIN: iteration 24477 : loss : 0.251291, loss_ce: 0.002431, loss_dice: 0.500151
[11:49:52.532] TRAIN: iteration 24478 : loss : 0.154215, loss_ce: 0.002244, loss_dice: 0.306186
[11:49:52.738] TRAIN: iteration 24479 : loss : 0.165034, loss_ce: 0.004292, loss_dice: 0.325776
[11:49:52.949] TRAIN: iteration 24480 : loss : 0.251283, loss_ce: 0.002431, loss_dice: 0.500135
[11:49:53.192] TRAIN: iteration 24481 : loss : 0.250742, loss_ce: 0.001432, loss_dice: 0.500053
[11:49:53.400] TRAIN: iteration 24482 : loss : 0.103772, loss_ce: 0.017126, loss_dice: 0.190419
[11:49:54.171] TRAIN: iteration 24483 : loss : 0.137325, loss_ce: 0.003762, loss_dice: 0.270889
[11:49:54.379] TRAIN: iteration 24484 : loss : 0.147998, loss_ce: 0.002385, loss_dice: 0.293610
[11:49:54.588] TRAIN: iteration 24485 : loss : 0.006630, loss_ce: 0.000868, loss_dice: 0.012392
[11:49:54.796] TRAIN: iteration 24486 : loss : 0.030511, loss_ce: 0.002000, loss_dice: 0.059022
[11:49:55.006] TRAIN: iteration 24487 : loss : 0.036089, loss_ce: 0.002320, loss_dice: 0.069857
[11:49:55.213] TRAIN: iteration 24488 : loss : 0.251216, loss_ce: 0.002270, loss_dice: 0.500162
[11:49:55.422] TRAIN: iteration 24489 : loss : 0.250517, loss_ce: 0.000995, loss_dice: 0.500038
[11:49:55.629] TRAIN: iteration 24490 : loss : 0.251045, loss_ce: 0.001974, loss_dice: 0.500117
[11:49:55.837] TRAIN: iteration 24491 : loss : 0.036644, loss_ce: 0.001959, loss_dice: 0.071329
[11:49:56.046] TRAIN: iteration 24492 : loss : 0.114005, loss_ce: 0.008900, loss_dice: 0.219110
[11:49:56.259] TRAIN: iteration 24493 : loss : 0.076783, loss_ce: 0.002569, loss_dice: 0.150996
[11:49:56.466] TRAIN: iteration 24494 : loss : 0.021536, loss_ce: 0.000550, loss_dice: 0.042522
[11:49:56.674] TRAIN: iteration 24495 : loss : 0.140875, loss_ce: 0.002451, loss_dice: 0.279300
[11:49:56.882] TRAIN: iteration 24496 : loss : 0.246045, loss_ce: 0.001668, loss_dice: 0.490421
[11:49:57.089] TRAIN: iteration 24497 : loss : 0.187693, loss_ce: 0.002389, loss_dice: 0.372996
[11:49:57.300] TRAIN: iteration 24498 : loss : 0.250871, loss_ce: 0.001627, loss_dice: 0.500115
[11:49:57.509] TRAIN: iteration 24499 : loss : 0.041790, loss_ce: 0.001043, loss_dice: 0.082536
[11:49:57.718] TRAIN: iteration 24500 : loss : 0.250120, loss_ce: 0.001373, loss_dice: 0.498867
[11:49:57.963] TRAIN: iteration 24501 : loss : 0.094365, loss_ce: 0.005277, loss_dice: 0.183453
[11:49:58.171] TRAIN: iteration 24502 : loss : 0.056804, loss_ce: 0.001126, loss_dice: 0.112482
[11:49:58.380] TRAIN: iteration 24503 : loss : 0.249706, loss_ce: 0.002831, loss_dice: 0.496580
[11:49:58.588] TRAIN: iteration 24504 : loss : 0.043026, loss_ce: 0.002653, loss_dice: 0.083400
[11:49:58.796] TRAIN: iteration 24505 : loss : 0.250381, loss_ce: 0.000723, loss_dice: 0.500040
[11:49:59.004] TRAIN: iteration 24506 : loss : 0.114107, loss_ce: 0.003650, loss_dice: 0.224564
[11:49:59.213] TRAIN: iteration 24507 : loss : 0.129226, loss_ce: 0.003806, loss_dice: 0.254647
[11:49:59.428] TRAIN: iteration 24508 : loss : 0.250470, loss_ce: 0.000899, loss_dice: 0.500040
[11:49:59.639] TRAIN: iteration 24509 : loss : 0.073004, loss_ce: 0.007819, loss_dice: 0.138188
[11:49:59.847] TRAIN: iteration 24510 : loss : 0.248867, loss_ce: 0.001861, loss_dice: 0.495873
[11:50:00.057] TRAIN: iteration 24511 : loss : 0.080477, loss_ce: 0.001126, loss_dice: 0.159827
[11:50:00.267] TRAIN: iteration 24512 : loss : 0.250168, loss_ce: 0.000335, loss_dice: 0.500002
[11:50:00.478] TRAIN: iteration 24513 : loss : 0.250156, loss_ce: 0.000717, loss_dice: 0.499594
[11:50:00.686] TRAIN: iteration 24514 : loss : 0.048582, loss_ce: 0.001136, loss_dice: 0.096027
[11:50:00.894] TRAIN: iteration 24515 : loss : 0.085650, loss_ce: 0.000801, loss_dice: 0.170499
[11:50:01.104] TRAIN: iteration 24516 : loss : 0.251070, loss_ce: 0.001997, loss_dice: 0.500142
[11:50:01.312] TRAIN: iteration 24517 : loss : 0.104737, loss_ce: 0.000712, loss_dice: 0.208762
[11:50:01.519] TRAIN: iteration 24518 : loss : 0.035522, loss_ce: 0.003103, loss_dice: 0.067940
[11:50:01.727] TRAIN: iteration 24519 : loss : 0.056617, loss_ce: 0.006343, loss_dice: 0.106891
[11:50:01.935] TRAIN: iteration 24520 : loss : 0.052597, loss_ce: 0.003984, loss_dice: 0.101211
[11:50:02.176] TRAIN: iteration 24521 : loss : 0.172966, loss_ce: 0.003066, loss_dice: 0.342867
[11:50:02.386] TRAIN: iteration 24522 : loss : 0.065989, loss_ce: 0.005322, loss_dice: 0.126656
[11:50:02.594] TRAIN: iteration 24523 : loss : 0.038897, loss_ce: 0.004589, loss_dice: 0.073206
[11:50:02.801] TRAIN: iteration 24524 : loss : 0.047417, loss_ce: 0.000732, loss_dice: 0.094101
[11:50:03.009] TRAIN: iteration 24525 : loss : 0.247950, loss_ce: 0.002401, loss_dice: 0.493498
[11:50:03.218] TRAIN: iteration 24526 : loss : 0.181544, loss_ce: 0.005405, loss_dice: 0.357683
[11:50:03.426] TRAIN: iteration 24527 : loss : 0.092546, loss_ce: 0.001748, loss_dice: 0.183345
[11:50:03.635] TRAIN: iteration 24528 : loss : 0.057060, loss_ce: 0.006557, loss_dice: 0.107563
[11:50:03.844] TRAIN: iteration 24529 : loss : 0.097382, loss_ce: 0.003697, loss_dice: 0.191066
[11:50:04.055] TRAIN: iteration 24530 : loss : 0.072331, loss_ce: 0.006119, loss_dice: 0.138543
[11:50:04.265] TRAIN: iteration 24531 : loss : 0.059697, loss_ce: 0.002068, loss_dice: 0.117327
[11:50:04.473] TRAIN: iteration 24532 : loss : 0.132531, loss_ce: 0.006480, loss_dice: 0.258582
[11:50:04.680] TRAIN: iteration 24533 : loss : 0.251154, loss_ce: 0.002154, loss_dice: 0.500154
[11:50:04.889] TRAIN: iteration 24534 : loss : 0.157348, loss_ce: 0.005925, loss_dice: 0.308771
[11:50:05.097] TRAIN: iteration 24535 : loss : 0.115582, loss_ce: 0.000862, loss_dice: 0.230302
[11:50:05.305] TRAIN: iteration 24536 : loss : 0.071326, loss_ce: 0.003115, loss_dice: 0.139536
[11:50:05.514] TRAIN: iteration 24537 : loss : 0.072993, loss_ce: 0.002488, loss_dice: 0.143498
[11:50:05.723] TRAIN: iteration 24538 : loss : 0.057725, loss_ce: 0.001691, loss_dice: 0.113759
[11:50:05.931] TRAIN: iteration 24539 : loss : 0.093008, loss_ce: 0.001492, loss_dice: 0.184525
[11:50:06.139] TRAIN: iteration 24540 : loss : 0.039981, loss_ce: 0.000875, loss_dice: 0.079088
[11:50:06.374] TRAIN: iteration 24541 : loss : 0.023108, loss_ce: 0.000427, loss_dice: 0.045788
[11:50:06.590] TRAIN: iteration 24542 : loss : 0.150920, loss_ce: 0.008069, loss_dice: 0.293771
[11:50:06.798] TRAIN: iteration 24543 : loss : 0.022514, loss_ce: 0.000722, loss_dice: 0.044307
[11:50:07.026] TRAIN: iteration 24544 : loss : 0.207311, loss_ce: 0.001841, loss_dice: 0.412781
[11:50:07.235] TRAIN: iteration 24545 : loss : 0.156804, loss_ce: 0.005615, loss_dice: 0.307993
[11:50:07.444] TRAIN: iteration 24546 : loss : 0.127153, loss_ce: 0.004596, loss_dice: 0.249709
[11:50:07.653] TRAIN: iteration 24547 : loss : 0.250169, loss_ce: 0.000334, loss_dice: 0.500004
[11:50:07.883] TRAIN: iteration 24548 : loss : 0.142072, loss_ce: 0.004874, loss_dice: 0.279270
[11:50:08.092] TRAIN: iteration 24549 : loss : 0.083786, loss_ce: 0.000820, loss_dice: 0.166752
[11:50:08.303] TRAIN: iteration 24550 : loss : 0.007994, loss_ce: 0.000813, loss_dice: 0.015176
[11:50:08.515] TRAIN: iteration 24551 : loss : 0.051538, loss_ce: 0.000763, loss_dice: 0.102313
[11:50:08.726] TRAIN: iteration 24552 : loss : 0.162394, loss_ce: 0.007902, loss_dice: 0.316886
[11:50:08.935] TRAIN: iteration 24553 : loss : 0.214170, loss_ce: 0.001933, loss_dice: 0.426408
[11:50:09.148] TRAIN: iteration 24554 : loss : 0.074815, loss_ce: 0.004760, loss_dice: 0.144871
[11:50:09.356] TRAIN: iteration 24555 : loss : 0.077158, loss_ce: 0.006553, loss_dice: 0.147764
[11:50:09.564] TRAIN: iteration 24556 : loss : 0.159412, loss_ce: 0.003641, loss_dice: 0.315183
[11:50:09.976] TRAIN: iteration 24557 : loss : 0.114381, loss_ce: 0.003078, loss_dice: 0.225685
[11:50:10.187] TRAIN: iteration 24558 : loss : 0.106546, loss_ce: 0.002872, loss_dice: 0.210220
[11:50:10.402] TRAIN: iteration 24559 : loss : 0.211830, loss_ce: 0.007120, loss_dice: 0.416539
[11:50:10.619] TRAIN: iteration 24560 : loss : 0.072648, loss_ce: 0.001959, loss_dice: 0.143337
[11:50:10.860] TRAIN: iteration 24561 : loss : 0.018264, loss_ce: 0.001112, loss_dice: 0.035415
[11:50:11.099] TRAIN: iteration 24562 : loss : 0.218511, loss_ce: 0.008325, loss_dice: 0.428697
[11:50:11.311] TRAIN: iteration 24563 : loss : 0.202925, loss_ce: 0.005156, loss_dice: 0.400695
[11:50:11.520] TRAIN: iteration 24564 : loss : 0.178077, loss_ce: 0.001742, loss_dice: 0.354413
[11:50:11.730] TRAIN: iteration 24565 : loss : 0.078842, loss_ce: 0.007105, loss_dice: 0.150578
[11:50:11.939] TRAIN: iteration 24566 : loss : 0.110250, loss_ce: 0.004892, loss_dice: 0.215607
[11:50:12.148] TRAIN: iteration 24567 : loss : 0.134840, loss_ce: 0.008863, loss_dice: 0.260817
[11:50:12.363] TRAIN: iteration 24568 : loss : 0.091336, loss_ce: 0.008474, loss_dice: 0.174198
[11:50:12.574] TRAIN: iteration 24569 : loss : 0.087058, loss_ce: 0.006984, loss_dice: 0.167132
[11:50:12.793] TRAIN: iteration 24570 : loss : 0.068731, loss_ce: 0.003497, loss_dice: 0.133966
[11:50:13.003] TRAIN: iteration 24571 : loss : 0.022301, loss_ce: 0.002760, loss_dice: 0.041843
[11:50:13.214] TRAIN: iteration 24572 : loss : 0.190226, loss_ce: 0.004792, loss_dice: 0.375660
[11:50:13.426] TRAIN: iteration 24573 : loss : 0.075492, loss_ce: 0.002686, loss_dice: 0.148298
[11:50:13.637] TRAIN: iteration 24574 : loss : 0.071258, loss_ce: 0.002831, loss_dice: 0.139684
[11:50:13.855] TRAIN: iteration 24575 : loss : 0.084506, loss_ce: 0.004059, loss_dice: 0.164953
[11:50:14.066] TRAIN: iteration 24576 : loss : 0.213960, loss_ce: 0.003980, loss_dice: 0.423940
[11:50:14.274] TRAIN: iteration 24577 : loss : 0.169355, loss_ce: 0.003109, loss_dice: 0.335601
[11:50:14.482] TRAIN: iteration 24578 : loss : 0.251946, loss_ce: 0.003696, loss_dice: 0.500197
[11:50:14.689] TRAIN: iteration 24579 : loss : 0.059099, loss_ce: 0.003662, loss_dice: 0.114537
[11:50:14.897] TRAIN: iteration 24580 : loss : 0.089422, loss_ce: 0.006574, loss_dice: 0.172271
[11:50:15.136] TRAIN: iteration 24581 : loss : 0.150889, loss_ce: 0.003601, loss_dice: 0.298177
[11:50:15.346] TRAIN: iteration 24582 : loss : 0.102287, loss_ce: 0.004575, loss_dice: 0.200000
[11:50:15.559] TRAIN: iteration 24583 : loss : 0.067414, loss_ce: 0.004270, loss_dice: 0.130557
[11:50:15.768] TRAIN: iteration 24584 : loss : 0.079243, loss_ce: 0.002710, loss_dice: 0.155776
[11:50:15.976] TRAIN: iteration 24585 : loss : 0.251171, loss_ce: 0.002239, loss_dice: 0.500102
[11:50:16.187] TRAIN: iteration 24586 : loss : 0.090831, loss_ce: 0.002027, loss_dice: 0.179634
[11:50:16.402] TRAIN: iteration 24587 : loss : 0.063165, loss_ce: 0.002540, loss_dice: 0.123790
[11:50:16.612] TRAIN: iteration 24588 : loss : 0.110462, loss_ce: 0.004190, loss_dice: 0.216735
[11:50:16.819] TRAIN: iteration 24589 : loss : 0.089387, loss_ce: 0.002582, loss_dice: 0.176193
[11:50:17.029] TRAIN: iteration 24590 : loss : 0.201710, loss_ce: 0.003735, loss_dice: 0.399685
[11:50:17.276] TRAIN: iteration 24591 : loss : 0.082793, loss_ce: 0.006402, loss_dice: 0.159184
[11:50:17.489] TRAIN: iteration 24592 : loss : 0.048790, loss_ce: 0.006463, loss_dice: 0.091117
[11:50:17.697] TRAIN: iteration 24593 : loss : 0.092631, loss_ce: 0.001385, loss_dice: 0.183878
[11:50:17.905] TRAIN: iteration 24594 : loss : 0.075335, loss_ce: 0.004655, loss_dice: 0.146015
[11:50:18.115] TRAIN: iteration 24595 : loss : 0.068454, loss_ce: 0.001116, loss_dice: 0.135792
[11:50:18.324] TRAIN: iteration 24596 : loss : 0.052293, loss_ce: 0.001516, loss_dice: 0.103070
[11:50:18.534] TRAIN: iteration 24597 : loss : 0.120196, loss_ce: 0.003536, loss_dice: 0.236857
[11:50:18.743] TRAIN: iteration 24598 : loss : 0.048033, loss_ce: 0.003359, loss_dice: 0.092707
[11:50:18.953] TRAIN: iteration 24599 : loss : 0.150794, loss_ce: 0.001314, loss_dice: 0.300274
[11:50:19.161] TRAIN: iteration 24600 : loss : 0.102444, loss_ce: 0.001849, loss_dice: 0.203040
[11:50:19.399] TRAIN: iteration 24601 : loss : 0.088795, loss_ce: 0.007798, loss_dice: 0.169793
[11:50:19.607] TRAIN: iteration 24602 : loss : 0.115650, loss_ce: 0.012102, loss_dice: 0.219197
[11:50:19.821] TRAIN: iteration 24603 : loss : 0.015818, loss_ce: 0.001094, loss_dice: 0.030543
[11:50:20.029] TRAIN: iteration 24604 : loss : 0.168082, loss_ce: 0.005413, loss_dice: 0.330750
[11:50:20.237] TRAIN: iteration 24605 : loss : 0.250379, loss_ce: 0.000724, loss_dice: 0.500033
[11:50:20.447] TRAIN: iteration 24606 : loss : 0.030042, loss_ce: 0.002312, loss_dice: 0.057772
[11:50:20.659] TRAIN: iteration 24607 : loss : 0.186885, loss_ce: 0.004974, loss_dice: 0.368796
[11:50:20.867] TRAIN: iteration 24608 : loss : 0.087212, loss_ce: 0.002475, loss_dice: 0.171949
[11:50:21.078] TRAIN: iteration 24609 : loss : 0.041860, loss_ce: 0.000784, loss_dice: 0.082935
[11:50:21.289] TRAIN: iteration 24610 : loss : 0.030190, loss_ce: 0.001923, loss_dice: 0.058456
[11:50:21.498] TRAIN: iteration 24611 : loss : 0.030961, loss_ce: 0.002303, loss_dice: 0.059620
[11:50:21.707] TRAIN: iteration 24612 : loss : 0.052185, loss_ce: 0.005003, loss_dice: 0.099367
[11:50:21.922] TRAIN: iteration 24613 : loss : 0.111361, loss_ce: 0.004598, loss_dice: 0.218124
[11:50:22.134] TRAIN: iteration 24614 : loss : 0.071616, loss_ce: 0.003116, loss_dice: 0.140116
[11:50:22.342] TRAIN: iteration 24615 : loss : 0.077159, loss_ce: 0.003531, loss_dice: 0.150787
[11:50:22.550] TRAIN: iteration 24616 : loss : 0.038217, loss_ce: 0.002701, loss_dice: 0.073733
[11:50:22.758] TRAIN: iteration 24617 : loss : 0.094242, loss_ce: 0.003278, loss_dice: 0.185206
[11:50:22.970] TRAIN: iteration 24618 : loss : 0.124697, loss_ce: 0.002507, loss_dice: 0.246887
[11:50:23.186] TRAIN: iteration 24619 : loss : 0.250772, loss_ce: 0.001462, loss_dice: 0.500082
[11:50:23.396] TRAIN: iteration 24620 : loss : 0.061974, loss_ce: 0.006007, loss_dice: 0.117941
[11:50:23.632] TRAIN: iteration 24621 : loss : 0.204448, loss_ce: 0.002253, loss_dice: 0.406643
[11:50:23.841] TRAIN: iteration 24622 : loss : 0.251099, loss_ce: 0.002064, loss_dice: 0.500134
[11:50:24.048] TRAIN: iteration 24623 : loss : 0.186546, loss_ce: 0.006022, loss_dice: 0.367069
[11:50:24.257] TRAIN: iteration 24624 : loss : 0.250289, loss_ce: 0.003096, loss_dice: 0.497481
[11:50:24.472] TRAIN: iteration 24625 : loss : 0.250620, loss_ce: 0.001184, loss_dice: 0.500056
[11:50:24.680] TRAIN: iteration 24626 : loss : 0.126784, loss_ce: 0.003308, loss_dice: 0.250259
[11:50:24.897] TRAIN: iteration 24627 : loss : 0.111199, loss_ce: 0.001605, loss_dice: 0.220794
[11:50:25.105] TRAIN: iteration 24628 : loss : 0.082186, loss_ce: 0.027763, loss_dice: 0.136609
[11:50:25.320] TRAIN: iteration 24629 : loss : 0.064175, loss_ce: 0.004063, loss_dice: 0.124287
[11:50:25.528] TRAIN: iteration 24630 : loss : 0.204573, loss_ce: 0.004812, loss_dice: 0.404333
[11:50:25.736] TRAIN: iteration 24631 : loss : 0.051000, loss_ce: 0.006949, loss_dice: 0.095051
[11:50:25.947] TRAIN: iteration 24632 : loss : 0.105028, loss_ce: 0.003492, loss_dice: 0.206564
[11:50:26.163] TRAIN: iteration 24633 : loss : 0.034652, loss_ce: 0.002939, loss_dice: 0.066366
[11:50:26.370] TRAIN: iteration 24634 : loss : 0.067986, loss_ce: 0.008723, loss_dice: 0.127249
[11:50:26.578] TRAIN: iteration 24635 : loss : 0.090758, loss_ce: 0.004648, loss_dice: 0.176868
[11:50:26.786] TRAIN: iteration 24636 : loss : 0.079259, loss_ce: 0.002656, loss_dice: 0.155863
[11:50:27.001] TRAIN: iteration 24637 : loss : 0.041885, loss_ce: 0.003041, loss_dice: 0.080730
[11:50:27.210] TRAIN: iteration 24638 : loss : 0.066099, loss_ce: 0.002594, loss_dice: 0.129604
[11:50:27.419] TRAIN: iteration 24639 : loss : 0.208129, loss_ce: 0.003317, loss_dice: 0.412941
[11:50:27.633] TRAIN: iteration 24640 : loss : 0.161514, loss_ce: 0.002297, loss_dice: 0.320730
[11:50:27.871] TRAIN: iteration 24641 : loss : 0.250690, loss_ce: 0.001331, loss_dice: 0.500050
[11:50:28.080] TRAIN: iteration 24642 : loss : 0.099571, loss_ce: 0.002227, loss_dice: 0.196915
[11:50:28.291] TRAIN: iteration 24643 : loss : 0.129742, loss_ce: 0.007612, loss_dice: 0.251873
[11:50:28.501] TRAIN: iteration 24644 : loss : 0.251117, loss_ce: 0.002097, loss_dice: 0.500137
[11:50:28.712] TRAIN: iteration 24645 : loss : 0.054242, loss_ce: 0.006461, loss_dice: 0.102022
[11:50:28.928] TRAIN: iteration 24646 : loss : 0.026398, loss_ce: 0.000889, loss_dice: 0.051908
[11:50:29.137] TRAIN: iteration 24647 : loss : 0.192755, loss_ce: 0.004982, loss_dice: 0.380527
[11:50:29.344] TRAIN: iteration 24648 : loss : 0.061643, loss_ce: 0.001494, loss_dice: 0.121791
[11:50:29.552] TRAIN: iteration 24649 : loss : 0.133850, loss_ce: 0.001004, loss_dice: 0.266696
[11:50:29.762] TRAIN: iteration 24650 : loss : 0.119604, loss_ce: 0.002703, loss_dice: 0.236506
[11:50:29.970] TRAIN: iteration 24651 : loss : 0.033855, loss_ce: 0.005735, loss_dice: 0.061975
[11:50:30.183] TRAIN: iteration 24652 : loss : 0.125615, loss_ce: 0.016740, loss_dice: 0.234489
[11:50:30.391] TRAIN: iteration 24653 : loss : 0.110952, loss_ce: 0.002747, loss_dice: 0.219156
[11:50:30.599] TRAIN: iteration 24654 : loss : 0.042866, loss_ce: 0.002970, loss_dice: 0.082763
[11:50:30.808] TRAIN: iteration 24655 : loss : 0.129363, loss_ce: 0.021842, loss_dice: 0.236884
[11:50:31.018] TRAIN: iteration 24656 : loss : 0.142899, loss_ce: 0.001663, loss_dice: 0.284136
[11:50:31.228] TRAIN: iteration 24657 : loss : 0.044152, loss_ce: 0.003487, loss_dice: 0.084817
[11:50:31.435] TRAIN: iteration 24658 : loss : 0.112539, loss_ce: 0.006181, loss_dice: 0.218897
[11:50:31.642] TRAIN: iteration 24659 : loss : 0.180954, loss_ce: 0.002219, loss_dice: 0.359689
[11:50:31.850] TRAIN: iteration 24660 : loss : 0.078675, loss_ce: 0.002158, loss_dice: 0.155192
[11:50:32.085] TRAIN: iteration 24661 : loss : 0.056211, loss_ce: 0.001934, loss_dice: 0.110488
[11:50:32.293] TRAIN: iteration 24662 : loss : 0.042188, loss_ce: 0.004326, loss_dice: 0.080050
[11:50:32.501] TRAIN: iteration 24663 : loss : 0.100722, loss_ce: 0.004411, loss_dice: 0.197033
[11:50:32.709] TRAIN: iteration 24664 : loss : 0.250169, loss_ce: 0.000336, loss_dice: 0.500001
[11:50:32.918] TRAIN: iteration 24665 : loss : 0.156463, loss_ce: 0.005776, loss_dice: 0.307149
[11:50:33.126] TRAIN: iteration 24666 : loss : 0.250972, loss_ce: 0.001820, loss_dice: 0.500124
[11:50:33.335] TRAIN: iteration 24667 : loss : 0.249174, loss_ce: 0.001212, loss_dice: 0.497137
[11:50:33.543] TRAIN: iteration 24668 : loss : 0.208648, loss_ce: 0.004319, loss_dice: 0.412978
[11:50:33.750] TRAIN: iteration 24669 : loss : 0.042789, loss_ce: 0.004150, loss_dice: 0.081429
[11:50:33.961] TRAIN: iteration 24670 : loss : 0.250426, loss_ce: 0.000810, loss_dice: 0.500043
[11:50:34.170] TRAIN: iteration 24671 : loss : 0.233928, loss_ce: 0.000905, loss_dice: 0.466952
[11:50:34.382] TRAIN: iteration 24672 : loss : 0.109948, loss_ce: 0.008464, loss_dice: 0.211432
[11:50:34.592] TRAIN: iteration 24673 : loss : 0.215748, loss_ce: 0.002772, loss_dice: 0.428724
[11:50:34.799] TRAIN: iteration 24674 : loss : 0.140925, loss_ce: 0.005823, loss_dice: 0.276026
[11:50:35.008] TRAIN: iteration 24675 : loss : 0.119607, loss_ce: 0.004834, loss_dice: 0.234379
[11:50:35.224] TRAIN: iteration 24676 : loss : 0.243944, loss_ce: 0.001897, loss_dice: 0.485992
[11:50:35.437] TRAIN: iteration 24677 : loss : 0.088340, loss_ce: 0.004757, loss_dice: 0.171923
[11:50:35.652] TRAIN: iteration 24678 : loss : 0.034097, loss_ce: 0.003229, loss_dice: 0.064965
[11:50:35.861] TRAIN: iteration 24679 : loss : 0.082322, loss_ce: 0.002051, loss_dice: 0.162592
[11:50:36.069] TRAIN: iteration 24680 : loss : 0.044814, loss_ce: 0.007652, loss_dice: 0.081976
[11:50:36.301] TRAIN: iteration 24681 : loss : 0.040941, loss_ce: 0.001573, loss_dice: 0.080309
[11:50:36.511] TRAIN: iteration 24682 : loss : 0.219833, loss_ce: 0.001471, loss_dice: 0.438195
[11:50:36.719] TRAIN: iteration 24683 : loss : 0.064472, loss_ce: 0.010754, loss_dice: 0.118190
[11:50:36.929] TRAIN: iteration 24684 : loss : 0.189930, loss_ce: 0.002198, loss_dice: 0.377663
[11:50:37.140] TRAIN: iteration 24685 : loss : 0.056598, loss_ce: 0.004310, loss_dice: 0.108885
[11:50:37.350] TRAIN: iteration 24686 : loss : 0.251812, loss_ce: 0.003374, loss_dice: 0.500251
[11:50:37.559] TRAIN: iteration 24687 : loss : 0.051623, loss_ce: 0.003797, loss_dice: 0.099449
[11:50:37.773] TRAIN: iteration 24688 : loss : 0.089160, loss_ce: 0.002405, loss_dice: 0.175915
[11:50:37.981] TRAIN: iteration 24689 : loss : 0.101676, loss_ce: 0.002850, loss_dice: 0.200502
[11:50:38.190] TRAIN: iteration 24690 : loss : 0.057866, loss_ce: 0.005359, loss_dice: 0.110373
[11:50:38.398] TRAIN: iteration 24691 : loss : 0.091899, loss_ce: 0.003739, loss_dice: 0.180059
[11:50:38.607] TRAIN: iteration 24692 : loss : 0.055607, loss_ce: 0.001161, loss_dice: 0.110053
[11:50:38.816] TRAIN: iteration 24693 : loss : 0.085329, loss_ce: 0.002289, loss_dice: 0.168369
[11:50:39.023] TRAIN: iteration 24694 : loss : 0.123284, loss_ce: 0.001575, loss_dice: 0.244993
[11:50:39.233] TRAIN: iteration 24695 : loss : 0.033140, loss_ce: 0.001021, loss_dice: 0.065259
[11:50:39.446] TRAIN: iteration 24696 : loss : 0.135495, loss_ce: 0.010711, loss_dice: 0.260280
[11:50:39.657] TRAIN: iteration 24697 : loss : 0.042667, loss_ce: 0.006857, loss_dice: 0.078477
[11:50:39.868] TRAIN: iteration 24698 : loss : 0.134837, loss_ce: 0.001164, loss_dice: 0.268510
[11:50:40.078] TRAIN: iteration 24699 : loss : 0.207566, loss_ce: 0.010885, loss_dice: 0.404247
[11:50:40.287] TRAIN: iteration 24700 : loss : 0.105428, loss_ce: 0.003805, loss_dice: 0.207050
[11:50:40.530] TRAIN: iteration 24701 : loss : 0.106710, loss_ce: 0.009128, loss_dice: 0.204291
[11:50:40.743] TRAIN: iteration 24702 : loss : 0.250302, loss_ce: 0.000585, loss_dice: 0.500019
[11:50:40.952] TRAIN: iteration 24703 : loss : 0.068358, loss_ce: 0.001500, loss_dice: 0.135217
[11:50:41.161] TRAIN: iteration 24704 : loss : 0.180897, loss_ce: 0.008047, loss_dice: 0.353747
[11:50:41.378] TRAIN: iteration 24705 : loss : 0.185384, loss_ce: 0.005133, loss_dice: 0.365634
[11:50:41.586] TRAIN: iteration 24706 : loss : 0.061043, loss_ce: 0.001940, loss_dice: 0.120145
[11:50:42.240] TRAIN: iteration 24707 : loss : 0.174087, loss_ce: 0.001176, loss_dice: 0.346999
[11:50:42.447] TRAIN: iteration 24708 : loss : 0.188751, loss_ce: 0.004008, loss_dice: 0.373493
[11:50:42.680] TRAIN: iteration 24709 : loss : 0.066691, loss_ce: 0.001522, loss_dice: 0.131861
[11:50:42.895] TRAIN: iteration 24710 : loss : 0.083051, loss_ce: 0.003145, loss_dice: 0.162957
[11:50:43.103] TRAIN: iteration 24711 : loss : 0.026357, loss_ce: 0.001047, loss_dice: 0.051668
[11:50:43.311] TRAIN: iteration 24712 : loss : 0.064510, loss_ce: 0.002664, loss_dice: 0.126357
[11:50:43.527] TRAIN: iteration 24713 : loss : 0.033535, loss_ce: 0.001619, loss_dice: 0.065452
[11:50:43.736] TRAIN: iteration 24714 : loss : 0.194007, loss_ce: 0.006296, loss_dice: 0.381717
[11:50:43.948] TRAIN: iteration 24715 : loss : 0.110724, loss_ce: 0.003591, loss_dice: 0.217857
[11:50:44.157] TRAIN: iteration 24716 : loss : 0.122406, loss_ce: 0.002351, loss_dice: 0.242462
[11:50:44.365] TRAIN: iteration 24717 : loss : 0.100120, loss_ce: 0.007056, loss_dice: 0.193184
[11:50:44.574] TRAIN: iteration 24718 : loss : 0.060753, loss_ce: 0.007568, loss_dice: 0.113938
[11:50:44.792] TRAIN: iteration 24719 : loss : 0.078015, loss_ce: 0.005316, loss_dice: 0.150713
[11:50:45.003] TRAIN: iteration 24720 : loss : 0.252331, loss_ce: 0.004355, loss_dice: 0.500307
[11:50:45.241] TRAIN: iteration 24721 : loss : 0.107039, loss_ce: 0.006712, loss_dice: 0.207366
[11:50:45.457] TRAIN: iteration 24722 : loss : 0.206112, loss_ce: 0.004167, loss_dice: 0.408057
[11:50:45.665] TRAIN: iteration 24723 : loss : 0.072581, loss_ce: 0.009438, loss_dice: 0.135724
[11:50:45.883] TRAIN: iteration 24724 : loss : 0.050728, loss_ce: 0.003529, loss_dice: 0.097928
[11:50:46.095] TRAIN: iteration 24725 : loss : 0.069023, loss_ce: 0.007261, loss_dice: 0.130784
[11:50:46.510] TRAIN: iteration 24726 : loss : 0.036209, loss_ce: 0.003183, loss_dice: 0.069235
[11:50:46.719] TRAIN: iteration 24727 : loss : 0.158589, loss_ce: 0.003402, loss_dice: 0.313775
[11:50:46.929] TRAIN: iteration 24728 : loss : 0.079496, loss_ce: 0.001687, loss_dice: 0.157304
[11:50:47.141] TRAIN: iteration 24729 : loss : 0.105689, loss_ce: 0.003239, loss_dice: 0.208140
[11:50:47.358] TRAIN: iteration 24730 : loss : 0.250935, loss_ce: 0.001773, loss_dice: 0.500097
[11:50:47.566] TRAIN: iteration 24731 : loss : 0.065739, loss_ce: 0.005814, loss_dice: 0.125665
[11:50:47.783] TRAIN: iteration 24732 : loss : 0.251103, loss_ce: 0.002060, loss_dice: 0.500145
[11:50:47.994] TRAIN: iteration 24733 : loss : 0.073220, loss_ce: 0.002683, loss_dice: 0.143757
[11:50:48.206] TRAIN: iteration 24734 : loss : 0.113305, loss_ce: 0.006294, loss_dice: 0.220315
[11:50:48.417] TRAIN: iteration 24735 : loss : 0.102644, loss_ce: 0.002933, loss_dice: 0.202354
[11:50:48.625] TRAIN: iteration 24736 : loss : 0.095823, loss_ce: 0.005538, loss_dice: 0.186108
[11:50:48.833] TRAIN: iteration 24737 : loss : 0.061099, loss_ce: 0.001106, loss_dice: 0.121093
[11:50:49.050] TRAIN: iteration 24738 : loss : 0.074915, loss_ce: 0.003778, loss_dice: 0.146053
[11:50:49.262] TRAIN: iteration 24739 : loss : 0.078269, loss_ce: 0.003039, loss_dice: 0.153499
[11:50:49.471] TRAIN: iteration 24740 : loss : 0.189489, loss_ce: 0.002979, loss_dice: 0.375999
[11:50:49.707] TRAIN: iteration 24741 : loss : 0.130884, loss_ce: 0.003001, loss_dice: 0.258768
[11:50:49.916] TRAIN: iteration 24742 : loss : 0.065627, loss_ce: 0.002565, loss_dice: 0.128688
[11:50:50.124] TRAIN: iteration 24743 : loss : 0.234783, loss_ce: 0.001041, loss_dice: 0.468524
[11:50:50.333] TRAIN: iteration 24744 : loss : 0.091707, loss_ce: 0.018384, loss_dice: 0.165031
[11:50:50.542] TRAIN: iteration 24745 : loss : 0.036921, loss_ce: 0.001411, loss_dice: 0.072432
[11:50:50.755] TRAIN: iteration 24746 : loss : 0.050242, loss_ce: 0.001342, loss_dice: 0.099141
[11:50:50.966] TRAIN: iteration 24747 : loss : 0.047243, loss_ce: 0.002526, loss_dice: 0.091960
[11:50:51.180] TRAIN: iteration 24748 : loss : 0.019485, loss_ce: 0.001272, loss_dice: 0.037697
[11:50:51.392] TRAIN: iteration 24749 : loss : 0.167633, loss_ce: 0.007808, loss_dice: 0.327458
[11:50:51.600] TRAIN: iteration 24750 : loss : 0.160429, loss_ce: 0.002057, loss_dice: 0.318801
[11:50:51.813] TRAIN: iteration 24751 : loss : 0.064320, loss_ce: 0.003920, loss_dice: 0.124719
[11:50:52.022] TRAIN: iteration 24752 : loss : 0.084553, loss_ce: 0.002489, loss_dice: 0.166617
[11:50:52.230] TRAIN: iteration 24753 : loss : 0.115881, loss_ce: 0.005465, loss_dice: 0.226296
[11:50:52.440] TRAIN: iteration 24754 : loss : 0.099939, loss_ce: 0.005200, loss_dice: 0.194678
[11:50:52.652] TRAIN: iteration 24755 : loss : 0.105089, loss_ce: 0.004720, loss_dice: 0.205457
[11:50:52.863] TRAIN: iteration 24756 : loss : 0.085168, loss_ce: 0.004956, loss_dice: 0.165380
[11:50:53.073] TRAIN: iteration 24757 : loss : 0.101590, loss_ce: 0.004206, loss_dice: 0.198973
[11:50:53.284] TRAIN: iteration 24758 : loss : 0.232781, loss_ce: 0.004844, loss_dice: 0.460718
[11:50:53.498] TRAIN: iteration 24759 : loss : 0.064998, loss_ce: 0.002733, loss_dice: 0.127263
[11:50:53.706] TRAIN: iteration 24760 : loss : 0.132313, loss_ce: 0.002346, loss_dice: 0.262280
[11:50:53.945] TRAIN: iteration 24761 : loss : 0.078425, loss_ce: 0.001215, loss_dice: 0.155635
[11:50:54.154] TRAIN: iteration 24762 : loss : 0.041070, loss_ce: 0.001346, loss_dice: 0.080795
[11:50:54.363] TRAIN: iteration 24763 : loss : 0.064794, loss_ce: 0.004760, loss_dice: 0.124827
[11:50:54.569] TRAIN: iteration 24764 : loss : 0.251405, loss_ce: 0.002639, loss_dice: 0.500170
[11:50:54.776] TRAIN: iteration 24765 : loss : 0.071908, loss_ce: 0.003991, loss_dice: 0.139825
[11:50:54.984] TRAIN: iteration 24766 : loss : 0.048269, loss_ce: 0.004741, loss_dice: 0.091796
[11:50:55.192] TRAIN: iteration 24767 : loss : 0.088050, loss_ce: 0.002465, loss_dice: 0.173635
[11:50:55.403] TRAIN: iteration 24768 : loss : 0.051469, loss_ce: 0.002875, loss_dice: 0.100063
[11:50:55.612] TRAIN: iteration 24769 : loss : 0.201774, loss_ce: 0.001820, loss_dice: 0.401729
[11:50:55.825] TRAIN: iteration 24770 : loss : 0.042467, loss_ce: 0.001049, loss_dice: 0.083886
[11:50:56.045] TRAIN: iteration 24771 : loss : 0.036752, loss_ce: 0.001187, loss_dice: 0.072318
[11:50:56.255] TRAIN: iteration 24772 : loss : 0.153888, loss_ce: 0.004599, loss_dice: 0.303177
[11:50:56.464] TRAIN: iteration 24773 : loss : 0.044586, loss_ce: 0.001910, loss_dice: 0.087263
[11:50:56.672] TRAIN: iteration 24774 : loss : 0.077847, loss_ce: 0.002294, loss_dice: 0.153401
[11:50:56.882] TRAIN: iteration 24775 : loss : 0.250537, loss_ce: 0.001035, loss_dice: 0.500039
[11:50:57.094] TRAIN: iteration 24776 : loss : 0.235234, loss_ce: 0.007151, loss_dice: 0.463317
[11:50:57.304] TRAIN: iteration 24777 : loss : 0.047833, loss_ce: 0.001224, loss_dice: 0.094442
[11:50:57.513] TRAIN: iteration 24778 : loss : 0.070342, loss_ce: 0.001820, loss_dice: 0.138864
[11:50:57.722] TRAIN: iteration 24779 : loss : 0.149110, loss_ce: 0.001683, loss_dice: 0.296538
[11:50:57.938] TRAIN: iteration 24780 : loss : 0.132578, loss_ce: 0.014672, loss_dice: 0.250484
[11:50:58.216] TRAIN: iteration 24781 : loss : 0.030207, loss_ce: 0.004193, loss_dice: 0.056220
[11:50:58.424] TRAIN: iteration 24782 : loss : 0.131704, loss_ce: 0.023856, loss_dice: 0.239552
[11:50:58.632] TRAIN: iteration 24783 : loss : 0.084403, loss_ce: 0.004445, loss_dice: 0.164361
[11:50:58.841] TRAIN: iteration 24784 : loss : 0.248189, loss_ce: 0.001397, loss_dice: 0.494981
[11:50:59.051] TRAIN: iteration 24785 : loss : 0.093733, loss_ce: 0.005031, loss_dice: 0.182435
[11:50:59.260] TRAIN: iteration 24786 : loss : 0.198264, loss_ce: 0.021834, loss_dice: 0.374693
[11:50:59.470] TRAIN: iteration 24787 : loss : 0.113347, loss_ce: 0.012549, loss_dice: 0.214145
[11:50:59.681] TRAIN: iteration 24788 : loss : 0.044473, loss_ce: 0.002882, loss_dice: 0.086063
[11:50:59.897] TRAIN: iteration 24789 : loss : 0.241751, loss_ce: 0.001082, loss_dice: 0.482420
[11:51:00.107] TRAIN: iteration 24790 : loss : 0.227733, loss_ce: 0.002446, loss_dice: 0.453021
[11:51:00.315] TRAIN: iteration 24791 : loss : 0.059895, loss_ce: 0.003102, loss_dice: 0.116688
[11:51:00.531] TRAIN: iteration 24792 : loss : 0.083674, loss_ce: 0.004493, loss_dice: 0.162854
[11:51:00.738] TRAIN: iteration 24793 : loss : 0.056358, loss_ce: 0.003360, loss_dice: 0.109357
[11:51:00.945] TRAIN: iteration 24794 : loss : 0.072257, loss_ce: 0.005285, loss_dice: 0.139230
[11:51:01.153] TRAIN: iteration 24795 : loss : 0.134460, loss_ce: 0.003247, loss_dice: 0.265674
[11:51:01.363] TRAIN: iteration 24796 : loss : 0.110605, loss_ce: 0.004898, loss_dice: 0.216312
[11:51:01.577] TRAIN: iteration 24797 : loss : 0.100240, loss_ce: 0.003875, loss_dice: 0.196605
[11:51:01.786] TRAIN: iteration 24798 : loss : 0.026227, loss_ce: 0.001158, loss_dice: 0.051295
[11:51:01.993] TRAIN: iteration 24799 : loss : 0.028237, loss_ce: 0.002015, loss_dice: 0.054458
[11:51:02.202] TRAIN: iteration 24800 : loss : 0.034595, loss_ce: 0.001459, loss_dice: 0.067731
[11:51:02.442] TRAIN: iteration 24801 : loss : 0.243046, loss_ce: 0.004163, loss_dice: 0.481928
[11:51:02.654] TRAIN: iteration 24802 : loss : 0.070854, loss_ce: 0.011112, loss_dice: 0.130596
[11:51:02.863] TRAIN: iteration 24803 : loss : 0.149578, loss_ce: 0.003922, loss_dice: 0.295235
[11:51:03.071] TRAIN: iteration 24804 : loss : 0.114471, loss_ce: 0.003726, loss_dice: 0.225217
[11:51:03.285] TRAIN: iteration 24805 : loss : 0.140232, loss_ce: 0.001415, loss_dice: 0.279049
[11:51:03.495] TRAIN: iteration 24806 : loss : 0.253153, loss_ce: 0.005863, loss_dice: 0.500443
[11:51:03.705] TRAIN: iteration 24807 : loss : 0.158032, loss_ce: 0.002112, loss_dice: 0.313953
[11:51:03.915] TRAIN: iteration 24808 : loss : 0.251739, loss_ce: 0.003320, loss_dice: 0.500158
[11:51:04.123] TRAIN: iteration 24809 : loss : 0.043788, loss_ce: 0.001328, loss_dice: 0.086248
[11:51:04.340] TRAIN: iteration 24810 : loss : 0.087594, loss_ce: 0.001588, loss_dice: 0.173601
[11:51:04.548] TRAIN: iteration 24811 : loss : 0.038911, loss_ce: 0.003949, loss_dice: 0.073873
[11:51:04.755] TRAIN: iteration 24812 : loss : 0.041600, loss_ce: 0.008157, loss_dice: 0.075043
[11:51:04.963] TRAIN: iteration 24813 : loss : 0.091228, loss_ce: 0.003687, loss_dice: 0.178769
[11:51:05.171] TRAIN: iteration 24814 : loss : 0.149715, loss_ce: 0.003347, loss_dice: 0.296084
[11:51:05.379] TRAIN: iteration 24815 : loss : 0.028238, loss_ce: 0.004360, loss_dice: 0.052116
[11:51:05.595] TRAIN: iteration 24816 : loss : 0.053996, loss_ce: 0.003338, loss_dice: 0.104654
[11:51:05.802] TRAIN: iteration 24817 : loss : 0.146787, loss_ce: 0.017721, loss_dice: 0.275853
[11:51:06.012] TRAIN: iteration 24818 : loss : 0.089287, loss_ce: 0.001473, loss_dice: 0.177100
[11:51:06.221] TRAIN: iteration 24819 : loss : 0.128930, loss_ce: 0.003993, loss_dice: 0.253868
[11:51:06.429] TRAIN: iteration 24820 : loss : 0.050848, loss_ce: 0.004311, loss_dice: 0.097386
[11:51:06.681] TRAIN: iteration 24821 : loss : 0.131518, loss_ce: 0.003178, loss_dice: 0.259859
[11:51:06.889] TRAIN: iteration 24822 : loss : 0.115414, loss_ce: 0.002218, loss_dice: 0.228610
[11:51:07.103] TRAIN: iteration 24823 : loss : 0.235796, loss_ce: 0.001201, loss_dice: 0.470391
[11:51:07.312] TRAIN: iteration 24824 : loss : 0.034513, loss_ce: 0.000672, loss_dice: 0.068353
[11:51:07.558] TRAIN: iteration 24825 : loss : 0.048253, loss_ce: 0.003788, loss_dice: 0.092719
[11:51:07.766] TRAIN: iteration 24826 : loss : 0.057481, loss_ce: 0.006009, loss_dice: 0.108953
[11:51:08.343] TRAIN: iteration 24827 : loss : 0.060837, loss_ce: 0.002258, loss_dice: 0.119416
[11:51:08.550] TRAIN: iteration 24828 : loss : 0.034067, loss_ce: 0.000927, loss_dice: 0.067206
[11:51:08.758] TRAIN: iteration 24829 : loss : 0.106373, loss_ce: 0.014252, loss_dice: 0.198494
[11:51:08.967] TRAIN: iteration 24830 : loss : 0.184357, loss_ce: 0.016772, loss_dice: 0.351942
[11:51:09.181] TRAIN: iteration 24831 : loss : 0.200615, loss_ce: 0.001950, loss_dice: 0.399280
[11:51:09.389] TRAIN: iteration 24832 : loss : 0.245926, loss_ce: 0.000920, loss_dice: 0.490932
[11:51:09.597] TRAIN: iteration 24833 : loss : 0.127441, loss_ce: 0.007171, loss_dice: 0.247711
[11:51:09.807] TRAIN: iteration 24834 : loss : 0.048916, loss_ce: 0.001157, loss_dice: 0.096676
[11:51:10.015] TRAIN: iteration 24835 : loss : 0.219056, loss_ce: 0.003204, loss_dice: 0.434908
[11:51:10.224] TRAIN: iteration 24836 : loss : 0.250225, loss_ce: 0.000442, loss_dice: 0.500007
[11:51:10.434] TRAIN: iteration 24837 : loss : 0.228905, loss_ce: 0.001513, loss_dice: 0.456297
[11:51:10.651] TRAIN: iteration 24838 : loss : 0.052146, loss_ce: 0.003041, loss_dice: 0.101252
[11:51:10.861] TRAIN: iteration 24839 : loss : 0.071653, loss_ce: 0.009991, loss_dice: 0.133316
[11:51:11.072] TRAIN: iteration 24840 : loss : 0.084441, loss_ce: 0.010534, loss_dice: 0.158348
[11:51:11.317] TRAIN: iteration 24841 : loss : 0.025934, loss_ce: 0.004526, loss_dice: 0.047342
[11:51:11.526] TRAIN: iteration 24842 : loss : 0.097348, loss_ce: 0.005205, loss_dice: 0.189491
[11:51:12.439] TRAIN: iteration 24843 : loss : 0.250547, loss_ce: 0.003906, loss_dice: 0.497188
[11:51:12.647] TRAIN: iteration 24844 : loss : 0.062606, loss_ce: 0.002142, loss_dice: 0.123069
[11:51:12.861] TRAIN: iteration 24845 : loss : 0.222583, loss_ce: 0.003780, loss_dice: 0.441387
[11:51:13.070] TRAIN: iteration 24846 : loss : 0.250740, loss_ce: 0.001420, loss_dice: 0.500059
[11:51:13.279] TRAIN: iteration 24847 : loss : 0.061852, loss_ce: 0.010478, loss_dice: 0.113226
[11:51:13.486] TRAIN: iteration 24848 : loss : 0.144420, loss_ce: 0.004612, loss_dice: 0.284228
[11:51:13.694] TRAIN: iteration 24849 : loss : 0.024210, loss_ce: 0.001616, loss_dice: 0.046805
[11:51:13.904] TRAIN: iteration 24850 : loss : 0.021085, loss_ce: 0.001455, loss_dice: 0.040716
[11:51:14.126] TRAIN: iteration 24851 : loss : 0.192682, loss_ce: 0.024519, loss_dice: 0.360844
[11:51:14.335] TRAIN: iteration 24852 : loss : 0.038014, loss_ce: 0.005884, loss_dice: 0.070144
[11:51:14.543] TRAIN: iteration 24853 : loss : 0.148851, loss_ce: 0.002005, loss_dice: 0.295696
[11:51:14.752] TRAIN: iteration 24854 : loss : 0.123692, loss_ce: 0.002674, loss_dice: 0.244710
[11:51:14.971] TRAIN: iteration 24855 : loss : 0.142735, loss_ce: 0.005984, loss_dice: 0.279485
[11:51:15.180] TRAIN: iteration 24856 : loss : 0.066032, loss_ce: 0.006719, loss_dice: 0.125344
[11:51:15.389] TRAIN: iteration 24857 : loss : 0.036945, loss_ce: 0.003114, loss_dice: 0.070776
[11:51:15.597] TRAIN: iteration 24858 : loss : 0.251707, loss_ce: 0.003198, loss_dice: 0.500216
[11:51:15.806] TRAIN: iteration 24859 : loss : 0.077683, loss_ce: 0.001247, loss_dice: 0.154120
[11:51:16.014] TRAIN: iteration 24860 : loss : 0.080128, loss_ce: 0.004120, loss_dice: 0.156136
[11:51:16.253] TRAIN: iteration 24861 : loss : 0.243296, loss_ce: 0.003072, loss_dice: 0.483521
[11:51:16.468] TRAIN: iteration 24862 : loss : 0.229421, loss_ce: 0.024208, loss_dice: 0.434634
[11:51:16.682] TRAIN: iteration 24863 : loss : 0.079436, loss_ce: 0.002066, loss_dice: 0.156807
[11:51:16.891] TRAIN: iteration 24864 : loss : 0.098090, loss_ce: 0.007829, loss_dice: 0.188352
[11:51:17.104] TRAIN: iteration 24865 : loss : 0.161754, loss_ce: 0.006312, loss_dice: 0.317195
[11:51:17.319] TRAIN: iteration 24866 : loss : 0.052201, loss_ce: 0.002716, loss_dice: 0.101686
[11:51:17.534] TRAIN: iteration 24867 : loss : 0.060318, loss_ce: 0.002664, loss_dice: 0.117973
[11:51:17.741] TRAIN: iteration 24868 : loss : 0.047200, loss_ce: 0.004132, loss_dice: 0.090267
[11:51:17.951] TRAIN: iteration 24869 : loss : 0.072131, loss_ce: 0.005375, loss_dice: 0.138887
[11:51:18.161] TRAIN: iteration 24870 : loss : 0.048014, loss_ce: 0.003435, loss_dice: 0.092593
[11:51:18.370] TRAIN: iteration 24871 : loss : 0.025552, loss_ce: 0.001872, loss_dice: 0.049231
[11:51:18.578] TRAIN: iteration 24872 : loss : 0.078429, loss_ce: 0.004495, loss_dice: 0.152364
[11:51:18.789] TRAIN: iteration 24873 : loss : 0.052324, loss_ce: 0.000986, loss_dice: 0.103662
[11:51:18.999] TRAIN: iteration 24874 : loss : 0.240408, loss_ce: 0.001014, loss_dice: 0.479802
[11:51:19.208] TRAIN: iteration 24875 : loss : 0.091800, loss_ce: 0.004840, loss_dice: 0.178760
[11:51:19.417] TRAIN: iteration 24876 : loss : 0.139542, loss_ce: 0.005345, loss_dice: 0.273740
[11:51:20.738] TRAIN: iteration 24877 : loss : 0.094002, loss_ce: 0.008707, loss_dice: 0.179296
[11:51:20.954] TRAIN: iteration 24878 : loss : 0.064854, loss_ce: 0.001729, loss_dice: 0.127979
[11:51:21.164] TRAIN: iteration 24879 : loss : 0.034455, loss_ce: 0.001426, loss_dice: 0.067484
[11:51:21.374] TRAIN: iteration 24880 : loss : 0.085919, loss_ce: 0.001907, loss_dice: 0.169932
[11:51:21.612] TRAIN: iteration 24881 : loss : 0.249230, loss_ce: 0.001339, loss_dice: 0.497121
[11:51:21.824] TRAIN: iteration 24882 : loss : 0.088705, loss_ce: 0.002034, loss_dice: 0.175375
[11:51:22.032] TRAIN: iteration 24883 : loss : 0.250493, loss_ce: 0.000963, loss_dice: 0.500024
[11:51:22.250] TRAIN: iteration 24884 : loss : 0.054713, loss_ce: 0.001223, loss_dice: 0.108203
[11:51:22.472] TRAIN: iteration 24885 : loss : 0.250595, loss_ce: 0.001138, loss_dice: 0.500053
[11:51:22.682] TRAIN: iteration 24886 : loss : 0.086202, loss_ce: 0.005969, loss_dice: 0.166434
[11:51:22.897] TRAIN: iteration 24887 : loss : 0.043762, loss_ce: 0.003393, loss_dice: 0.084132
[11:51:23.112] TRAIN: iteration 24888 : loss : 0.156698, loss_ce: 0.003270, loss_dice: 0.310127
[11:51:23.320] TRAIN: iteration 24889 : loss : 0.250624, loss_ce: 0.001199, loss_dice: 0.500049
[11:51:23.537] TRAIN: iteration 24890 : loss : 0.033661, loss_ce: 0.000780, loss_dice: 0.066541
[11:51:23.749] TRAIN: iteration 24891 : loss : 0.148608, loss_ce: 0.027157, loss_dice: 0.270058
[11:51:23.957] TRAIN: iteration 24892 : loss : 0.120123, loss_ce: 0.003115, loss_dice: 0.237131
[11:51:24.172] TRAIN: iteration 24893 : loss : 0.144855, loss_ce: 0.002707, loss_dice: 0.287003
[11:51:24.381] TRAIN: iteration 24894 : loss : 0.082888, loss_ce: 0.006156, loss_dice: 0.159621
[11:51:24.594] TRAIN: iteration 24895 : loss : 0.250563, loss_ce: 0.001082, loss_dice: 0.500045
[11:51:24.805] TRAIN: iteration 24896 : loss : 0.060032, loss_ce: 0.000868, loss_dice: 0.119197
[11:51:25.014] TRAIN: iteration 24897 : loss : 0.017274, loss_ce: 0.000543, loss_dice: 0.034005
[11:51:25.228] TRAIN: iteration 24898 : loss : 0.250840, loss_ce: 0.001593, loss_dice: 0.500087
[11:51:25.436] TRAIN: iteration 24899 : loss : 0.211708, loss_ce: 0.006913, loss_dice: 0.416502
[11:51:25.645] TRAIN: iteration 24900 : loss : 0.074587, loss_ce: 0.001431, loss_dice: 0.147743
[11:51:25.886] TRAIN: iteration 24901 : loss : 0.242692, loss_ce: 0.005268, loss_dice: 0.480115
[11:51:26.103] TRAIN: iteration 24902 : loss : 0.081170, loss_ce: 0.010179, loss_dice: 0.152160
[11:51:26.311] TRAIN: iteration 24903 : loss : 0.251150, loss_ce: 0.002161, loss_dice: 0.500138
[11:51:26.519] TRAIN: iteration 24904 : loss : 0.158012, loss_ce: 0.002395, loss_dice: 0.313629
[11:51:26.727] TRAIN: iteration 24905 : loss : 0.188075, loss_ce: 0.003786, loss_dice: 0.372363
[11:51:26.939] TRAIN: iteration 24906 : loss : 0.252814, loss_ce: 0.006452, loss_dice: 0.499177
[11:51:27.150] TRAIN: iteration 24907 : loss : 0.049041, loss_ce: 0.006632, loss_dice: 0.091450
[11:51:27.358] TRAIN: iteration 24908 : loss : 0.250915, loss_ce: 0.001728, loss_dice: 0.500102
[11:51:27.569] TRAIN: iteration 24909 : loss : 0.203113, loss_ce: 0.002066, loss_dice: 0.404160
[11:51:27.778] TRAIN: iteration 24910 : loss : 0.104886, loss_ce: 0.003277, loss_dice: 0.206494
[11:51:27.986] TRAIN: iteration 24911 : loss : 0.069957, loss_ce: 0.001144, loss_dice: 0.138769
[11:51:28.196] TRAIN: iteration 24912 : loss : 0.152660, loss_ce: 0.001798, loss_dice: 0.303522
[11:51:28.404] TRAIN: iteration 24913 : loss : 0.075579, loss_ce: 0.004470, loss_dice: 0.146687
[11:51:28.629] TRAIN: iteration 24914 : loss : 0.048119, loss_ce: 0.002594, loss_dice: 0.093645
[11:51:28.838] TRAIN: iteration 24915 : loss : 0.100500, loss_ce: 0.001420, loss_dice: 0.199581
[11:51:29.069] TRAIN: iteration 24916 : loss : 0.066355, loss_ce: 0.006691, loss_dice: 0.126018
[11:51:29.277] TRAIN: iteration 24917 : loss : 0.149540, loss_ce: 0.004406, loss_dice: 0.294675
[11:51:29.486] TRAIN: iteration 24918 : loss : 0.140734, loss_ce: 0.001608, loss_dice: 0.279860
[11:51:29.694] TRAIN: iteration 24919 : loss : 0.076094, loss_ce: 0.002332, loss_dice: 0.149857
[11:51:29.903] TRAIN: iteration 24920 : loss : 0.087178, loss_ce: 0.003959, loss_dice: 0.170398
[11:51:30.143] TRAIN: iteration 24921 : loss : 0.098783, loss_ce: 0.003688, loss_dice: 0.193878
[11:51:30.351] TRAIN: iteration 24922 : loss : 0.042422, loss_ce: 0.001333, loss_dice: 0.083512
[11:51:30.575] TRAIN: iteration 24923 : loss : 0.194877, loss_ce: 0.002693, loss_dice: 0.387061
[11:51:30.785] TRAIN: iteration 24924 : loss : 0.079815, loss_ce: 0.003342, loss_dice: 0.156288
[11:51:30.995] TRAIN: iteration 24925 : loss : 0.250332, loss_ce: 0.000644, loss_dice: 0.500020
[11:51:31.205] TRAIN: iteration 24926 : loss : 0.053095, loss_ce: 0.005606, loss_dice: 0.100583
[11:51:31.415] TRAIN: iteration 24927 : loss : 0.085910, loss_ce: 0.002081, loss_dice: 0.169740
[11:51:31.625] TRAIN: iteration 24928 : loss : 0.179979, loss_ce: 0.005472, loss_dice: 0.354486
[11:51:31.834] TRAIN: iteration 24929 : loss : 0.172954, loss_ce: 0.006402, loss_dice: 0.339507
[11:51:32.050] TRAIN: iteration 24930 : loss : 0.250129, loss_ce: 0.003128, loss_dice: 0.497130
[11:51:32.261] TRAIN: iteration 24931 : loss : 0.251183, loss_ce: 0.002211, loss_dice: 0.500156
[11:51:32.473] TRAIN: iteration 24932 : loss : 0.026992, loss_ce: 0.002323, loss_dice: 0.051662
[11:51:32.685] TRAIN: iteration 24933 : loss : 0.160064, loss_ce: 0.007549, loss_dice: 0.312579
[11:51:32.896] TRAIN: iteration 24934 : loss : 0.237093, loss_ce: 0.001335, loss_dice: 0.472850
[11:51:33.105] TRAIN: iteration 24935 : loss : 0.250649, loss_ce: 0.001248, loss_dice: 0.500050
[11:51:33.314] TRAIN: iteration 24936 : loss : 0.117155, loss_ce: 0.004314, loss_dice: 0.229995
[11:51:33.522] TRAIN: iteration 24937 : loss : 0.200438, loss_ce: 0.005324, loss_dice: 0.395553
[11:51:34.965] TRAIN: iteration 24938 : loss : 0.055974, loss_ce: 0.003450, loss_dice: 0.108497
[11:51:35.174] TRAIN: iteration 24939 : loss : 0.085280, loss_ce: 0.008935, loss_dice: 0.161625
[11:51:35.385] TRAIN: iteration 24940 : loss : 0.078436, loss_ce: 0.002418, loss_dice: 0.154453
[11:51:35.386] NaN or Inf found in input tensor.
[11:51:35.601] TRAIN: iteration 24941 : loss : 0.232937, loss_ce: 0.001446, loss_dice: 0.464429
[11:51:35.810] TRAIN: iteration 24942 : loss : 0.250724, loss_ce: 0.001369, loss_dice: 0.500080
[11:51:36.018] TRAIN: iteration 24943 : loss : 0.052598, loss_ce: 0.004241, loss_dice: 0.100955
[11:51:36.231] TRAIN: iteration 24944 : loss : 0.250416, loss_ce: 0.005100, loss_dice: 0.495732
[11:51:36.441] TRAIN: iteration 24945 : loss : 0.077870, loss_ce: 0.002234, loss_dice: 0.153505
[11:51:37.262] TRAIN: iteration 24946 : loss : 0.092314, loss_ce: 0.004094, loss_dice: 0.180534
[11:51:37.470] TRAIN: iteration 24947 : loss : 0.249329, loss_ce: 0.001698, loss_dice: 0.496959
[11:51:37.678] TRAIN: iteration 24948 : loss : 0.236248, loss_ce: 0.002734, loss_dice: 0.469762
[11:51:37.885] TRAIN: iteration 24949 : loss : 0.150550, loss_ce: 0.002630, loss_dice: 0.298469
[11:51:38.094] TRAIN: iteration 24950 : loss : 0.250808, loss_ce: 0.001529, loss_dice: 0.500087
[11:51:38.304] TRAIN: iteration 24951 : loss : 0.072957, loss_ce: 0.001204, loss_dice: 0.144710
[11:51:38.511] TRAIN: iteration 24952 : loss : 0.250345, loss_ce: 0.000672, loss_dice: 0.500018
[11:51:38.718] TRAIN: iteration 24953 : loss : 0.024253, loss_ce: 0.000838, loss_dice: 0.047668
[11:51:39.218] TRAIN: iteration 24954 : loss : 0.022700, loss_ce: 0.001258, loss_dice: 0.044143
[11:51:39.428] TRAIN: iteration 24955 : loss : 0.019931, loss_ce: 0.001007, loss_dice: 0.038856
[11:51:39.642] TRAIN: iteration 24956 : loss : 0.113133, loss_ce: 0.004214, loss_dice: 0.222052
[11:51:39.856] TRAIN: iteration 24957 : loss : 0.130313, loss_ce: 0.003592, loss_dice: 0.257033
[11:51:40.065] TRAIN: iteration 24958 : loss : 0.081400, loss_ce: 0.003265, loss_dice: 0.159534
[11:51:40.273] TRAIN: iteration 24959 : loss : 0.033595, loss_ce: 0.003450, loss_dice: 0.063740
[11:51:40.481] TRAIN: iteration 24960 : loss : 0.051669, loss_ce: 0.003231, loss_dice: 0.100108
[11:51:40.720] TRAIN: iteration 24961 : loss : 0.071368, loss_ce: 0.003811, loss_dice: 0.138924
[11:51:40.927] TRAIN: iteration 24962 : loss : 0.070201, loss_ce: 0.006829, loss_dice: 0.133572
[11:51:41.135] TRAIN: iteration 24963 : loss : 0.250154, loss_ce: 0.001180, loss_dice: 0.499128
[11:51:41.351] TRAIN: iteration 24964 : loss : 0.046262, loss_ce: 0.002320, loss_dice: 0.090205
[11:51:41.564] TRAIN: iteration 24965 : loss : 0.116087, loss_ce: 0.006866, loss_dice: 0.225307
[11:51:41.774] TRAIN: iteration 24966 : loss : 0.043505, loss_ce: 0.003387, loss_dice: 0.083623
[11:51:41.983] TRAIN: iteration 24967 : loss : 0.098835, loss_ce: 0.006867, loss_dice: 0.190804
[11:51:42.201] TRAIN: iteration 24968 : loss : 0.095262, loss_ce: 0.004613, loss_dice: 0.185911
[11:51:42.410] TRAIN: iteration 24969 : loss : 0.109493, loss_ce: 0.002165, loss_dice: 0.216820
[11:51:42.619] TRAIN: iteration 24970 : loss : 0.090567, loss_ce: 0.003972, loss_dice: 0.177162
[11:51:42.829] TRAIN: iteration 24971 : loss : 0.128254, loss_ce: 0.003294, loss_dice: 0.253215
[11:51:43.039] TRAIN: iteration 24972 : loss : 0.250636, loss_ce: 0.001236, loss_dice: 0.500037
[11:51:43.250] TRAIN: iteration 24973 : loss : 0.137687, loss_ce: 0.002384, loss_dice: 0.272990
[11:51:43.460] TRAIN: iteration 24974 : loss : 0.033337, loss_ce: 0.004972, loss_dice: 0.061702
[11:51:43.669] TRAIN: iteration 24975 : loss : 0.058949, loss_ce: 0.001086, loss_dice: 0.116812
[11:51:43.885] TRAIN: iteration 24976 : loss : 0.040563, loss_ce: 0.006480, loss_dice: 0.074646
[11:51:44.094] TRAIN: iteration 24977 : loss : 0.250517, loss_ce: 0.000993, loss_dice: 0.500041
[11:51:44.305] TRAIN: iteration 24978 : loss : 0.063727, loss_ce: 0.004265, loss_dice: 0.123190
[11:51:44.514] TRAIN: iteration 24979 : loss : 0.230601, loss_ce: 0.002872, loss_dice: 0.458329
[11:51:44.721] TRAIN: iteration 24980 : loss : 0.039357, loss_ce: 0.001994, loss_dice: 0.076720
[11:51:44.966] TRAIN: iteration 24981 : loss : 0.082566, loss_ce: 0.001260, loss_dice: 0.163872
[11:51:45.174] TRAIN: iteration 24982 : loss : 0.250725, loss_ce: 0.001389, loss_dice: 0.500061
[11:51:45.385] TRAIN: iteration 24983 : loss : 0.034916, loss_ce: 0.001494, loss_dice: 0.068339
[11:51:45.593] TRAIN: iteration 24984 : loss : 0.077478, loss_ce: 0.007414, loss_dice: 0.147541
[11:51:45.802] TRAIN: iteration 24985 : loss : 0.209364, loss_ce: 0.001305, loss_dice: 0.417424
[11:51:46.009] TRAIN: iteration 24986 : loss : 0.109933, loss_ce: 0.001736, loss_dice: 0.218129
[11:51:46.217] TRAIN: iteration 24987 : loss : 0.153760, loss_ce: 0.003476, loss_dice: 0.304043
[11:51:46.427] TRAIN: iteration 24988 : loss : 0.250636, loss_ce: 0.001225, loss_dice: 0.500047
[11:51:46.634] TRAIN: iteration 24989 : loss : 0.091788, loss_ce: 0.003844, loss_dice: 0.179733
[11:51:46.842] TRAIN: iteration 24990 : loss : 0.103907, loss_ce: 0.003879, loss_dice: 0.203935
[11:51:47.056] TRAIN: iteration 24991 : loss : 0.049464, loss_ce: 0.004721, loss_dice: 0.094206
[11:51:47.275] TRAIN: iteration 24992 : loss : 0.071950, loss_ce: 0.001888, loss_dice: 0.142013
[11:51:47.485] TRAIN: iteration 24993 : loss : 0.185671, loss_ce: 0.002427, loss_dice: 0.368916
[11:51:47.694] TRAIN: iteration 24994 : loss : 0.035454, loss_ce: 0.005404, loss_dice: 0.065504
[11:51:47.907] TRAIN: iteration 24995 : loss : 0.081881, loss_ce: 0.006114, loss_dice: 0.157649
[11:51:48.116] TRAIN: iteration 24996 : loss : 0.244267, loss_ce: 0.003583, loss_dice: 0.484951
[11:51:48.325] TRAIN: iteration 24997 : loss : 0.190809, loss_ce: 0.004903, loss_dice: 0.376716
[11:51:48.535] TRAIN: iteration 24998 : loss : 0.108756, loss_ce: 0.004120, loss_dice: 0.213392
[11:51:48.743] TRAIN: iteration 24999 : loss : 0.250682, loss_ce: 0.001319, loss_dice: 0.500046
[11:51:48.951] TRAIN: iteration 25000 : loss : 0.198451, loss_ce: 0.003375, loss_dice: 0.393526
[11:51:49.203] TRAIN: iteration 25001 : loss : 0.251003, loss_ce: 0.001885, loss_dice: 0.500121
[11:51:49.414] TRAIN: iteration 25002 : loss : 0.045044, loss_ce: 0.004984, loss_dice: 0.085103
[11:51:49.627] TRAIN: iteration 25003 : loss : 0.076675, loss_ce: 0.007024, loss_dice: 0.146326
[11:51:49.838] TRAIN: iteration 25004 : loss : 0.144014, loss_ce: 0.006156, loss_dice: 0.281872
[11:51:50.046] TRAIN: iteration 25005 : loss : 0.118386, loss_ce: 0.004252, loss_dice: 0.232519
[11:51:50.256] TRAIN: iteration 25006 : loss : 0.150929, loss_ce: 0.005459, loss_dice: 0.296399
[11:51:50.477] TRAIN: iteration 25007 : loss : 0.128980, loss_ce: 0.004776, loss_dice: 0.253184
[11:51:50.685] TRAIN: iteration 25008 : loss : 0.251291, loss_ce: 0.002455, loss_dice: 0.500128
[11:51:50.893] TRAIN: iteration 25009 : loss : 0.033092, loss_ce: 0.002831, loss_dice: 0.063354
[11:51:51.102] TRAIN: iteration 25010 : loss : 0.065132, loss_ce: 0.002640, loss_dice: 0.127623
[11:51:51.320] TRAIN: iteration 25011 : loss : 0.025915, loss_ce: 0.001621, loss_dice: 0.050209
[11:51:51.527] TRAIN: iteration 25012 : loss : 0.052266, loss_ce: 0.003452, loss_dice: 0.101080
[11:51:51.735] TRAIN: iteration 25013 : loss : 0.024852, loss_ce: 0.003464, loss_dice: 0.046239
[11:51:51.948] TRAIN: iteration 25014 : loss : 0.043272, loss_ce: 0.002246, loss_dice: 0.084298
[11:51:52.163] TRAIN: iteration 25015 : loss : 0.107127, loss_ce: 0.002248, loss_dice: 0.212007
[11:51:52.371] TRAIN: iteration 25016 : loss : 0.018666, loss_ce: 0.003745, loss_dice: 0.033586
[11:51:52.581] TRAIN: iteration 25017 : loss : 0.082559, loss_ce: 0.003100, loss_dice: 0.162019
[11:51:52.790] TRAIN: iteration 25018 : loss : 0.074163, loss_ce: 0.005390, loss_dice: 0.142936
[11:51:53.001] TRAIN: iteration 25019 : loss : 0.016074, loss_ce: 0.001231, loss_dice: 0.030917
[11:51:53.213] TRAIN: iteration 25020 : loss : 0.060453, loss_ce: 0.003226, loss_dice: 0.117679
[11:51:53.452] TRAIN: iteration 25021 : loss : 0.025885, loss_ce: 0.001813, loss_dice: 0.049957
[11:51:53.665] TRAIN: iteration 25022 : loss : 0.131698, loss_ce: 0.019110, loss_dice: 0.244286
[11:51:53.874] TRAIN: iteration 25023 : loss : 0.142622, loss_ce: 0.004566, loss_dice: 0.280677
[11:51:54.083] TRAIN: iteration 25024 : loss : 0.251135, loss_ce: 0.002150, loss_dice: 0.500119
[11:51:54.294] TRAIN: iteration 25025 : loss : 0.045434, loss_ce: 0.004305, loss_dice: 0.086563
[11:51:54.507] TRAIN: iteration 25026 : loss : 0.139293, loss_ce: 0.002664, loss_dice: 0.275921
[11:51:54.716] TRAIN: iteration 25027 : loss : 0.234269, loss_ce: 0.002918, loss_dice: 0.465621
[11:51:54.932] TRAIN: iteration 25028 : loss : 0.076407, loss_ce: 0.007500, loss_dice: 0.145313
[11:51:55.141] TRAIN: iteration 25029 : loss : 0.117591, loss_ce: 0.004378, loss_dice: 0.230805
[11:51:55.350] TRAIN: iteration 25030 : loss : 0.142248, loss_ce: 0.010899, loss_dice: 0.273596
[11:51:55.560] TRAIN: iteration 25031 : loss : 0.047105, loss_ce: 0.002248, loss_dice: 0.091962
[11:51:55.769] TRAIN: iteration 25032 : loss : 0.042038, loss_ce: 0.005815, loss_dice: 0.078261
[11:51:55.977] TRAIN: iteration 25033 : loss : 0.252405, loss_ce: 0.006171, loss_dice: 0.498640
[11:51:56.186] TRAIN: iteration 25034 : loss : 0.145667, loss_ce: 0.003161, loss_dice: 0.288174
[11:51:56.395] TRAIN: iteration 25035 : loss : 0.109209, loss_ce: 0.002837, loss_dice: 0.215581
[11:51:56.604] TRAIN: iteration 25036 : loss : 0.248216, loss_ce: 0.003441, loss_dice: 0.492992
[11:51:57.063] TRAIN: iteration 25037 : loss : 0.057690, loss_ce: 0.002880, loss_dice: 0.112500
[11:51:57.272] TRAIN: iteration 25038 : loss : 0.055901, loss_ce: 0.001448, loss_dice: 0.110353
[11:51:57.480] TRAIN: iteration 25039 : loss : 0.251888, loss_ce: 0.003549, loss_dice: 0.500226
[11:51:57.689] TRAIN: iteration 25040 : loss : 0.059671, loss_ce: 0.008186, loss_dice: 0.111156
[11:51:57.925] TRAIN: iteration 25041 : loss : 0.251977, loss_ce: 0.004207, loss_dice: 0.499748
[11:51:58.134] TRAIN: iteration 25042 : loss : 0.053540, loss_ce: 0.001422, loss_dice: 0.105657
[11:51:58.345] TRAIN: iteration 25043 : loss : 0.251148, loss_ce: 0.002176, loss_dice: 0.500121
[11:51:58.553] TRAIN: iteration 25044 : loss : 0.074129, loss_ce: 0.003739, loss_dice: 0.144519
[11:51:58.760] TRAIN: iteration 25045 : loss : 0.031725, loss_ce: 0.007433, loss_dice: 0.056016
[11:51:58.968] TRAIN: iteration 25046 : loss : 0.030308, loss_ce: 0.006011, loss_dice: 0.054605
[11:51:59.180] TRAIN: iteration 25047 : loss : 0.095682, loss_ce: 0.008941, loss_dice: 0.182422
[11:51:59.394] TRAIN: iteration 25048 : loss : 0.071846, loss_ce: 0.003778, loss_dice: 0.139914
[11:51:59.602] TRAIN: iteration 25049 : loss : 0.036116, loss_ce: 0.005904, loss_dice: 0.066328
[11:51:59.811] TRAIN: iteration 25050 : loss : 0.023315, loss_ce: 0.001170, loss_dice: 0.045460
[11:52:00.020] TRAIN: iteration 25051 : loss : 0.186021, loss_ce: 0.002496, loss_dice: 0.369547
[11:52:00.229] TRAIN: iteration 25052 : loss : 0.103022, loss_ce: 0.002563, loss_dice: 0.203482
[11:52:00.438] TRAIN: iteration 25053 : loss : 0.167379, loss_ce: 0.011146, loss_dice: 0.323612
[11:52:00.648] TRAIN: iteration 25054 : loss : 0.251628, loss_ce: 0.003040, loss_dice: 0.500216
[11:52:01.133] TRAIN: iteration 25055 : loss : 0.228587, loss_ce: 0.003070, loss_dice: 0.454103
[11:52:01.343] TRAIN: iteration 25056 : loss : 0.102712, loss_ce: 0.006372, loss_dice: 0.199052
[11:52:02.743] TRAIN: iteration 25057 : loss : 0.071866, loss_ce: 0.001101, loss_dice: 0.142632
[11:52:02.953] TRAIN: iteration 25058 : loss : 0.082747, loss_ce: 0.001367, loss_dice: 0.164127
[11:52:03.162] TRAIN: iteration 25059 : loss : 0.227841, loss_ce: 0.001389, loss_dice: 0.454293
[11:52:03.373] TRAIN: iteration 25060 : loss : 0.066648, loss_ce: 0.002891, loss_dice: 0.130404
[11:52:03.617] TRAIN: iteration 25061 : loss : 0.162197, loss_ce: 0.002825, loss_dice: 0.321569
[11:52:03.826] TRAIN: iteration 25062 : loss : 0.010351, loss_ce: 0.000881, loss_dice: 0.019820
[11:52:04.035] TRAIN: iteration 25063 : loss : 0.093485, loss_ce: 0.003717, loss_dice: 0.183252
[11:52:04.244] TRAIN: iteration 25064 : loss : 0.250517, loss_ce: 0.001000, loss_dice: 0.500035
[11:52:04.452] TRAIN: iteration 25065 : loss : 0.060236, loss_ce: 0.016393, loss_dice: 0.104079
[11:52:04.738] TRAIN: iteration 25066 : loss : 0.147262, loss_ce: 0.000863, loss_dice: 0.293661
[11:52:04.950] TRAIN: iteration 25067 : loss : 0.250607, loss_ce: 0.001151, loss_dice: 0.500062
[11:52:05.161] TRAIN: iteration 25068 : loss : 0.063560, loss_ce: 0.004688, loss_dice: 0.122432
[11:52:05.373] TRAIN: iteration 25069 : loss : 0.250465, loss_ce: 0.000889, loss_dice: 0.500041
[11:52:05.582] TRAIN: iteration 25070 : loss : 0.163589, loss_ce: 0.002302, loss_dice: 0.324876
[11:52:05.791] TRAIN: iteration 25071 : loss : 0.104903, loss_ce: 0.009468, loss_dice: 0.200337
[11:52:06.000] TRAIN: iteration 25072 : loss : 0.059288, loss_ce: 0.000672, loss_dice: 0.117904
[11:52:06.216] TRAIN: iteration 25073 : loss : 0.024316, loss_ce: 0.001245, loss_dice: 0.047387
[11:52:06.428] TRAIN: iteration 25074 : loss : 0.080038, loss_ce: 0.002424, loss_dice: 0.157652
[11:52:06.638] TRAIN: iteration 25075 : loss : 0.071165, loss_ce: 0.009085, loss_dice: 0.133245
[11:52:06.853] TRAIN: iteration 25076 : loss : 0.030415, loss_ce: 0.002503, loss_dice: 0.058326
[11:52:07.062] TRAIN: iteration 25077 : loss : 0.034585, loss_ce: 0.002541, loss_dice: 0.066629
[11:52:07.274] TRAIN: iteration 25078 : loss : 0.070117, loss_ce: 0.001294, loss_dice: 0.138940
[11:52:07.485] TRAIN: iteration 25079 : loss : 0.065822, loss_ce: 0.002394, loss_dice: 0.129250
[11:52:07.693] TRAIN: iteration 25080 : loss : 0.187295, loss_ce: 0.003379, loss_dice: 0.371212
[11:52:07.928] TRAIN: iteration 25081 : loss : 0.214362, loss_ce: 0.001374, loss_dice: 0.427350
[11:52:08.137] TRAIN: iteration 25082 : loss : 0.050002, loss_ce: 0.001526, loss_dice: 0.098478
[11:52:08.345] TRAIN: iteration 25083 : loss : 0.039061, loss_ce: 0.001286, loss_dice: 0.076837
[11:52:08.553] TRAIN: iteration 25084 : loss : 0.099237, loss_ce: 0.001011, loss_dice: 0.197462
[11:52:08.761] TRAIN: iteration 25085 : loss : 0.027509, loss_ce: 0.000559, loss_dice: 0.054459
[11:52:08.971] TRAIN: iteration 25086 : loss : 0.092531, loss_ce: 0.003222, loss_dice: 0.181839
[11:52:09.185] TRAIN: iteration 25087 : loss : 0.036707, loss_ce: 0.003872, loss_dice: 0.069542
[11:52:09.394] TRAIN: iteration 25088 : loss : 0.084301, loss_ce: 0.004067, loss_dice: 0.164535
[11:52:09.602] TRAIN: iteration 25089 : loss : 0.135530, loss_ce: 0.001792, loss_dice: 0.269268
[11:52:09.809] TRAIN: iteration 25090 : loss : 0.082202, loss_ce: 0.005512, loss_dice: 0.158892
[11:52:10.096] TRAIN: iteration 25091 : loss : 0.250427, loss_ce: 0.000829, loss_dice: 0.500025
[11:52:10.303] TRAIN: iteration 25092 : loss : 0.240773, loss_ce: 0.009080, loss_dice: 0.472466
[11:52:10.511] TRAIN: iteration 25093 : loss : 0.128508, loss_ce: 0.003023, loss_dice: 0.253993
[11:52:10.728] TRAIN: iteration 25094 : loss : 0.146747, loss_ce: 0.007422, loss_dice: 0.286073
[11:52:10.936] TRAIN: iteration 25095 : loss : 0.137828, loss_ce: 0.003882, loss_dice: 0.271775
[11:52:11.147] TRAIN: iteration 25096 : loss : 0.068009, loss_ce: 0.001828, loss_dice: 0.134190
[11:52:11.462] TRAIN: iteration 25097 : loss : 0.102538, loss_ce: 0.002487, loss_dice: 0.202589
[11:52:11.670] TRAIN: iteration 25098 : loss : 0.081638, loss_ce: 0.004391, loss_dice: 0.158885
[11:52:11.880] TRAIN: iteration 25099 : loss : 0.058573, loss_ce: 0.003511, loss_dice: 0.113636
[11:52:12.093] TRAIN: iteration 25100 : loss : 0.175695, loss_ce: 0.003210, loss_dice: 0.348179
[11:52:12.332] TRAIN: iteration 25101 : loss : 0.090633, loss_ce: 0.001810, loss_dice: 0.179457
[11:52:12.539] TRAIN: iteration 25102 : loss : 0.259673, loss_ce: 0.018933, loss_dice: 0.500413
[11:52:12.779] TRAIN: iteration 25103 : loss : 0.043838, loss_ce: 0.002736, loss_dice: 0.084940
[11:52:12.986] TRAIN: iteration 25104 : loss : 0.085979, loss_ce: 0.005733, loss_dice: 0.166226
[11:52:13.195] TRAIN: iteration 25105 : loss : 0.095454, loss_ce: 0.001397, loss_dice: 0.189512
[11:52:13.407] TRAIN: iteration 25106 : loss : 0.031562, loss_ce: 0.003907, loss_dice: 0.059217
[11:52:13.618] TRAIN: iteration 25107 : loss : 0.058090, loss_ce: 0.004981, loss_dice: 0.111199
[11:52:13.827] TRAIN: iteration 25108 : loss : 0.127320, loss_ce: 0.001556, loss_dice: 0.253085
[11:52:14.035] TRAIN: iteration 25109 : loss : 0.030400, loss_ce: 0.002609, loss_dice: 0.058190
[11:52:14.245] TRAIN: iteration 25110 : loss : 0.036530, loss_ce: 0.004908, loss_dice: 0.068153
[11:52:14.455] TRAIN: iteration 25111 : loss : 0.076536, loss_ce: 0.006450, loss_dice: 0.146621
[11:52:14.664] TRAIN: iteration 25112 : loss : 0.091117, loss_ce: 0.002758, loss_dice: 0.179475
[11:52:14.873] TRAIN: iteration 25113 : loss : 0.079400, loss_ce: 0.003351, loss_dice: 0.155449
[11:52:15.177] TRAIN: iteration 25114 : loss : 0.028687, loss_ce: 0.001621, loss_dice: 0.055753
[11:52:15.385] TRAIN: iteration 25115 : loss : 0.109673, loss_ce: 0.014314, loss_dice: 0.205033
[11:52:15.594] TRAIN: iteration 25116 : loss : 0.009968, loss_ce: 0.001228, loss_dice: 0.018709
[11:52:15.802] TRAIN: iteration 25117 : loss : 0.034489, loss_ce: 0.001265, loss_dice: 0.067713
[11:52:16.025] TRAIN: iteration 25118 : loss : 0.226999, loss_ce: 0.001201, loss_dice: 0.452797
[11:52:16.233] TRAIN: iteration 25119 : loss : 0.097469, loss_ce: 0.002538, loss_dice: 0.192400
[11:52:16.444] TRAIN: iteration 25120 : loss : 0.130027, loss_ce: 0.012312, loss_dice: 0.247741
[11:52:16.445] NaN or Inf found in input tensor.
[11:52:16.662] TRAIN: iteration 25121 : loss : 0.250607, loss_ce: 0.001166, loss_dice: 0.500048
[11:52:16.891] TRAIN: iteration 25122 : loss : 0.139560, loss_ce: 0.004184, loss_dice: 0.274936
[11:52:17.333] TRAIN: iteration 25123 : loss : 0.250216, loss_ce: 0.000425, loss_dice: 0.500006
[11:52:17.541] TRAIN: iteration 25124 : loss : 0.079386, loss_ce: 0.004216, loss_dice: 0.154555
[11:52:17.999] TRAIN: iteration 25125 : loss : 0.251111, loss_ce: 0.002077, loss_dice: 0.500144
[11:52:18.215] TRAIN: iteration 25126 : loss : 0.130215, loss_ce: 0.001671, loss_dice: 0.258759
[11:52:18.422] TRAIN: iteration 25127 : loss : 0.071022, loss_ce: 0.003008, loss_dice: 0.139037
[11:52:18.630] TRAIN: iteration 25128 : loss : 0.014724, loss_ce: 0.000668, loss_dice: 0.028780
[11:52:18.844] TRAIN: iteration 25129 : loss : 0.054179, loss_ce: 0.003466, loss_dice: 0.104892
[11:52:19.055] TRAIN: iteration 25130 : loss : 0.153117, loss_ce: 0.004110, loss_dice: 0.302123
[11:52:19.572] TRAIN: iteration 25131 : loss : 0.067716, loss_ce: 0.002814, loss_dice: 0.132617
[11:52:19.780] TRAIN: iteration 25132 : loss : 0.222476, loss_ce: 0.001292, loss_dice: 0.443660
[11:52:19.990] TRAIN: iteration 25133 : loss : 0.023057, loss_ce: 0.001521, loss_dice: 0.044593
[11:52:20.199] TRAIN: iteration 25134 : loss : 0.033940, loss_ce: 0.001702, loss_dice: 0.066178
[11:52:20.413] TRAIN: iteration 25135 : loss : 0.066868, loss_ce: 0.005705, loss_dice: 0.128030
[11:52:20.628] TRAIN: iteration 25136 : loss : 0.198678, loss_ce: 0.002327, loss_dice: 0.395029
[11:52:20.836] TRAIN: iteration 25137 : loss : 0.250298, loss_ce: 0.003125, loss_dice: 0.497471
[11:52:21.052] TRAIN: iteration 25138 : loss : 0.074349, loss_ce: 0.002054, loss_dice: 0.146644
[11:52:21.267] TRAIN: iteration 25139 : loss : 0.250405, loss_ce: 0.003093, loss_dice: 0.497717
[11:52:21.476] TRAIN: iteration 25140 : loss : 0.040233, loss_ce: 0.002004, loss_dice: 0.078463
[11:52:21.717] TRAIN: iteration 25141 : loss : 0.064110, loss_ce: 0.003280, loss_dice: 0.124940
[11:52:21.925] TRAIN: iteration 25142 : loss : 0.056256, loss_ce: 0.001161, loss_dice: 0.111351
[11:52:22.134] TRAIN: iteration 25143 : loss : 0.223216, loss_ce: 0.002600, loss_dice: 0.443833
[11:52:22.345] TRAIN: iteration 25144 : loss : 0.171156, loss_ce: 0.000904, loss_dice: 0.341409
[11:52:22.554] TRAIN: iteration 25145 : loss : 0.019287, loss_ce: 0.000512, loss_dice: 0.038063
[11:52:22.764] TRAIN: iteration 25146 : loss : 0.113507, loss_ce: 0.005483, loss_dice: 0.221530
[11:52:22.975] TRAIN: iteration 25147 : loss : 0.083728, loss_ce: 0.004111, loss_dice: 0.163345
[11:52:23.190] TRAIN: iteration 25148 : loss : 0.061246, loss_ce: 0.003717, loss_dice: 0.118775
[11:52:23.803] TRAIN: iteration 25149 : loss : 0.069042, loss_ce: 0.001078, loss_dice: 0.137007
[11:52:24.015] TRAIN: iteration 25150 : loss : 0.072237, loss_ce: 0.001468, loss_dice: 0.143007
[11:52:24.225] TRAIN: iteration 25151 : loss : 0.048098, loss_ce: 0.004018, loss_dice: 0.092177
[11:52:24.433] TRAIN: iteration 25152 : loss : 0.250235, loss_ce: 0.000466, loss_dice: 0.500004
[11:52:24.647] TRAIN: iteration 25153 : loss : 0.039257, loss_ce: 0.001826, loss_dice: 0.076688
[11:52:24.859] TRAIN: iteration 25154 : loss : 0.108736, loss_ce: 0.002001, loss_dice: 0.215471
[11:52:25.072] TRAIN: iteration 25155 : loss : 0.055802, loss_ce: 0.010843, loss_dice: 0.100761
[11:52:25.312] TRAIN: iteration 25156 : loss : 0.043808, loss_ce: 0.001786, loss_dice: 0.085830
[11:52:25.522] TRAIN: iteration 25157 : loss : 0.228829, loss_ce: 0.006924, loss_dice: 0.450733
[11:52:25.730] TRAIN: iteration 25158 : loss : 0.081897, loss_ce: 0.024662, loss_dice: 0.139132
[11:52:25.944] TRAIN: iteration 25159 : loss : 0.059009, loss_ce: 0.000991, loss_dice: 0.117027
[11:52:26.153] TRAIN: iteration 25160 : loss : 0.037105, loss_ce: 0.002041, loss_dice: 0.072169
[11:52:26.386] TRAIN: iteration 25161 : loss : 0.022147, loss_ce: 0.001194, loss_dice: 0.043100
[11:52:26.594] TRAIN: iteration 25162 : loss : 0.061956, loss_ce: 0.004697, loss_dice: 0.119216
[11:52:26.837] TRAIN: iteration 25163 : loss : 0.105019, loss_ce: 0.003834, loss_dice: 0.206204
[11:52:27.053] TRAIN: iteration 25164 : loss : 0.147594, loss_ce: 0.002344, loss_dice: 0.292844
[11:52:27.280] TRAIN: iteration 25165 : loss : 0.200701, loss_ce: 0.020164, loss_dice: 0.381237
[11:52:27.491] TRAIN: iteration 25166 : loss : 0.040771, loss_ce: 0.000523, loss_dice: 0.081018
[11:52:27.700] TRAIN: iteration 25167 : loss : 0.250365, loss_ce: 0.000709, loss_dice: 0.500021
[11:52:27.911] TRAIN: iteration 25168 : loss : 0.249692, loss_ce: 0.000696, loss_dice: 0.498688
[11:52:28.126] TRAIN: iteration 25169 : loss : 0.053946, loss_ce: 0.002435, loss_dice: 0.105457
[11:52:28.428] TRAIN: iteration 25170 : loss : 0.072534, loss_ce: 0.004692, loss_dice: 0.140376
[11:52:28.635] TRAIN: iteration 25171 : loss : 0.250166, loss_ce: 0.000331, loss_dice: 0.500001
[11:52:28.843] TRAIN: iteration 25172 : loss : 0.069612, loss_ce: 0.008754, loss_dice: 0.130469
[11:52:29.051] TRAIN: iteration 25173 : loss : 0.250326, loss_ce: 0.000633, loss_dice: 0.500018
[11:52:29.259] TRAIN: iteration 25174 : loss : 0.205968, loss_ce: 0.007277, loss_dice: 0.404660
[11:52:29.467] TRAIN: iteration 25175 : loss : 0.069320, loss_ce: 0.002062, loss_dice: 0.136577
[11:52:29.678] TRAIN: iteration 25176 : loss : 0.047484, loss_ce: 0.001181, loss_dice: 0.093787
[11:52:29.886] TRAIN: iteration 25177 : loss : 0.097711, loss_ce: 0.001217, loss_dice: 0.194205
[11:52:30.922] TRAIN: iteration 25178 : loss : 0.031769, loss_ce: 0.003009, loss_dice: 0.060530
[11:52:31.137] TRAIN: iteration 25179 : loss : 0.078901, loss_ce: 0.017879, loss_dice: 0.139923
[11:52:31.345] TRAIN: iteration 25180 : loss : 0.144493, loss_ce: 0.003263, loss_dice: 0.285724
[11:52:31.594] TRAIN: iteration 25181 : loss : 0.044898, loss_ce: 0.002225, loss_dice: 0.087570
[11:52:31.809] TRAIN: iteration 25182 : loss : 0.061416, loss_ce: 0.003612, loss_dice: 0.119219
[11:52:32.018] TRAIN: iteration 25183 : loss : 0.044606, loss_ce: 0.004109, loss_dice: 0.085104
[11:52:32.225] TRAIN: iteration 25184 : loss : 0.128664, loss_ce: 0.005682, loss_dice: 0.251647
[11:52:32.432] TRAIN: iteration 25185 : loss : 0.039624, loss_ce: 0.001129, loss_dice: 0.078119
[11:52:32.640] TRAIN: iteration 25186 : loss : 0.249536, loss_ce: 0.001318, loss_dice: 0.497754
[11:52:32.847] TRAIN: iteration 25187 : loss : 0.251033, loss_ce: 0.001929, loss_dice: 0.500137
[11:52:33.057] TRAIN: iteration 25188 : loss : 0.174941, loss_ce: 0.001511, loss_dice: 0.348371
[11:52:33.268] TRAIN: iteration 25189 : loss : 0.052240, loss_ce: 0.002443, loss_dice: 0.102037
[11:52:33.475] TRAIN: iteration 25190 : loss : 0.047968, loss_ce: 0.001912, loss_dice: 0.094023
[11:52:33.684] TRAIN: iteration 25191 : loss : 0.178769, loss_ce: 0.002292, loss_dice: 0.355246
[11:52:33.894] TRAIN: iteration 25192 : loss : 0.106596, loss_ce: 0.001021, loss_dice: 0.212172
[11:52:34.102] TRAIN: iteration 25193 : loss : 0.086844, loss_ce: 0.008230, loss_dice: 0.165458
[11:52:34.310] TRAIN: iteration 25194 : loss : 0.251105, loss_ce: 0.002071, loss_dice: 0.500139
[11:52:34.518] TRAIN: iteration 25195 : loss : 0.072371, loss_ce: 0.001769, loss_dice: 0.142972
[11:52:34.726] TRAIN: iteration 25196 : loss : 0.166196, loss_ce: 0.002039, loss_dice: 0.330354
[11:52:34.936] TRAIN: iteration 25197 : loss : 0.047712, loss_ce: 0.001116, loss_dice: 0.094308
[11:52:35.144] TRAIN: iteration 25198 : loss : 0.085141, loss_ce: 0.007583, loss_dice: 0.162700
[11:52:35.358] TRAIN: iteration 25199 : loss : 0.049607, loss_ce: 0.004206, loss_dice: 0.095008
[11:52:35.565] TRAIN: iteration 25200 : loss : 0.055396, loss_ce: 0.003956, loss_dice: 0.106835
[11:52:35.808] TRAIN: iteration 25201 : loss : 0.250493, loss_ce: 0.000940, loss_dice: 0.500046
[11:52:37.491] TRAIN: iteration 25202 : loss : 0.076842, loss_ce: 0.009883, loss_dice: 0.143800
[11:52:37.700] TRAIN: iteration 25203 : loss : 0.044315, loss_ce: 0.004509, loss_dice: 0.084120
[11:52:37.912] TRAIN: iteration 25204 : loss : 0.061971, loss_ce: 0.007008, loss_dice: 0.116933
[11:52:38.125] TRAIN: iteration 25205 : loss : 0.125845, loss_ce: 0.007386, loss_dice: 0.244303
[11:52:38.334] TRAIN: iteration 25206 : loss : 0.034290, loss_ce: 0.001354, loss_dice: 0.067227
[11:52:38.545] TRAIN: iteration 25207 : loss : 0.250785, loss_ce: 0.001492, loss_dice: 0.500077
[11:52:38.760] TRAIN: iteration 25208 : loss : 0.059175, loss_ce: 0.001749, loss_dice: 0.116602
[11:52:38.968] TRAIN: iteration 25209 : loss : 0.053858, loss_ce: 0.008794, loss_dice: 0.098922
[11:52:39.412] TRAIN: iteration 25210 : loss : 0.046978, loss_ce: 0.002551, loss_dice: 0.091405
[11:52:39.621] TRAIN: iteration 25211 : loss : 0.058002, loss_ce: 0.004944, loss_dice: 0.111061
[11:52:39.829] TRAIN: iteration 25212 : loss : 0.244363, loss_ce: 0.007042, loss_dice: 0.481684
[11:52:40.046] TRAIN: iteration 25213 : loss : 0.251125, loss_ce: 0.002110, loss_dice: 0.500141
[11:52:40.255] TRAIN: iteration 25214 : loss : 0.055543, loss_ce: 0.002162, loss_dice: 0.108923
[11:52:40.463] TRAIN: iteration 25215 : loss : 0.045860, loss_ce: 0.005977, loss_dice: 0.085743
[11:52:40.672] TRAIN: iteration 25216 : loss : 0.049438, loss_ce: 0.005566, loss_dice: 0.093310
[11:52:40.882] TRAIN: iteration 25217 : loss : 0.050438, loss_ce: 0.002703, loss_dice: 0.098173
[11:52:41.135] TRAIN: iteration 25218 : loss : 0.096745, loss_ce: 0.005024, loss_dice: 0.188465
[11:52:41.344] TRAIN: iteration 25219 : loss : 0.209625, loss_ce: 0.001725, loss_dice: 0.417525
[11:52:41.554] TRAIN: iteration 25220 : loss : 0.056000, loss_ce: 0.002138, loss_dice: 0.109862
[11:52:41.794] TRAIN: iteration 25221 : loss : 0.244805, loss_ce: 0.004331, loss_dice: 0.485279
[11:52:42.002] TRAIN: iteration 25222 : loss : 0.031892, loss_ce: 0.000842, loss_dice: 0.062942
[11:52:42.210] TRAIN: iteration 25223 : loss : 0.029941, loss_ce: 0.003763, loss_dice: 0.056119
[11:52:42.419] TRAIN: iteration 25224 : loss : 0.105512, loss_ce: 0.006602, loss_dice: 0.204423
[11:52:42.630] TRAIN: iteration 25225 : loss : 0.185608, loss_ce: 0.004504, loss_dice: 0.366712
[11:52:43.153] TRAIN: iteration 25226 : loss : 0.046266, loss_ce: 0.004269, loss_dice: 0.088262
[11:52:43.364] TRAIN: iteration 25227 : loss : 0.234561, loss_ce: 0.002820, loss_dice: 0.466302
[11:52:43.573] TRAIN: iteration 25228 : loss : 0.190816, loss_ce: 0.009650, loss_dice: 0.371982
[11:52:43.782] TRAIN: iteration 25229 : loss : 0.032913, loss_ce: 0.006017, loss_dice: 0.059810
[11:52:43.991] TRAIN: iteration 25230 : loss : 0.034127, loss_ce: 0.002142, loss_dice: 0.066112
[11:52:44.200] TRAIN: iteration 25231 : loss : 0.059950, loss_ce: 0.000779, loss_dice: 0.119120
[11:52:44.408] TRAIN: iteration 25232 : loss : 0.048113, loss_ce: 0.002816, loss_dice: 0.093410
[11:52:44.616] TRAIN: iteration 25233 : loss : 0.092697, loss_ce: 0.001646, loss_dice: 0.183749
[11:52:45.858] TRAIN: iteration 25234 : loss : 0.130949, loss_ce: 0.004355, loss_dice: 0.257543
[11:52:46.395] TRAIN: iteration 25235 : loss : 0.250888, loss_ce: 0.001689, loss_dice: 0.500086
[11:52:46.602] TRAIN: iteration 25236 : loss : 0.133010, loss_ce: 0.002580, loss_dice: 0.263439
[11:52:46.815] TRAIN: iteration 25237 : loss : 0.251168, loss_ce: 0.002203, loss_dice: 0.500132
[11:52:47.024] TRAIN: iteration 25238 : loss : 0.238470, loss_ce: 0.002012, loss_dice: 0.474929
[11:52:47.231] TRAIN: iteration 25239 : loss : 0.239266, loss_ce: 0.002011, loss_dice: 0.476520
[11:52:47.439] TRAIN: iteration 25240 : loss : 0.108742, loss_ce: 0.001934, loss_dice: 0.215549
[11:52:47.456] NaN or Inf found in input tensor.
[11:52:47.673] TRAIN: iteration 25241 : loss : 0.040542, loss_ce: 0.003449, loss_dice: 0.077635
[11:52:48.878] TRAIN: iteration 25242 : loss : 0.069761, loss_ce: 0.001274, loss_dice: 0.138248
[11:52:49.089] TRAIN: iteration 25243 : loss : 0.065024, loss_ce: 0.001877, loss_dice: 0.128172
[11:52:49.298] TRAIN: iteration 25244 : loss : 0.045028, loss_ce: 0.002554, loss_dice: 0.087501
[11:52:49.507] TRAIN: iteration 25245 : loss : 0.092470, loss_ce: 0.004201, loss_dice: 0.180739
[11:52:49.722] TRAIN: iteration 25246 : loss : 0.160688, loss_ce: 0.006920, loss_dice: 0.314456
[11:52:49.937] TRAIN: iteration 25247 : loss : 0.136016, loss_ce: 0.002431, loss_dice: 0.269600
[11:52:50.146] TRAIN: iteration 25248 : loss : 0.048025, loss_ce: 0.003964, loss_dice: 0.092087
[11:52:50.361] TRAIN: iteration 25249 : loss : 0.057118, loss_ce: 0.002714, loss_dice: 0.111522
[11:52:52.301] TRAIN: iteration 25250 : loss : 0.161794, loss_ce: 0.002326, loss_dice: 0.321263
[11:52:52.515] TRAIN: iteration 25251 : loss : 0.235605, loss_ce: 0.001974, loss_dice: 0.469236
[11:52:52.733] TRAIN: iteration 25252 : loss : 0.079517, loss_ce: 0.002516, loss_dice: 0.156517
[11:52:52.943] TRAIN: iteration 25253 : loss : 0.086479, loss_ce: 0.002265, loss_dice: 0.170693
[11:52:53.152] TRAIN: iteration 25254 : loss : 0.071004, loss_ce: 0.002363, loss_dice: 0.139645
[11:52:53.360] TRAIN: iteration 25255 : loss : 0.145612, loss_ce: 0.004743, loss_dice: 0.286481
[11:52:53.568] TRAIN: iteration 25256 : loss : 0.131255, loss_ce: 0.003531, loss_dice: 0.258980
[11:52:53.777] TRAIN: iteration 25257 : loss : 0.024390, loss_ce: 0.004892, loss_dice: 0.043888
[11:52:53.986] TRAIN: iteration 25258 : loss : 0.088948, loss_ce: 0.003107, loss_dice: 0.174790
[11:52:54.200] TRAIN: iteration 25259 : loss : 0.203571, loss_ce: 0.002048, loss_dice: 0.405093
[11:52:54.407] TRAIN: iteration 25260 : loss : 0.065496, loss_ce: 0.001804, loss_dice: 0.129189
[11:52:54.646] TRAIN: iteration 25261 : loss : 0.131177, loss_ce: 0.000888, loss_dice: 0.261467
[11:52:54.862] TRAIN: iteration 25262 : loss : 0.077287, loss_ce: 0.002462, loss_dice: 0.152111
[11:52:55.071] TRAIN: iteration 25263 : loss : 0.090510, loss_ce: 0.000857, loss_dice: 0.180163
[11:52:55.279] TRAIN: iteration 25264 : loss : 0.143136, loss_ce: 0.001306, loss_dice: 0.284967
[11:52:55.490] TRAIN: iteration 25265 : loss : 0.250124, loss_ce: 0.000249, loss_dice: 0.499999
[11:52:55.710] TRAIN: iteration 25266 : loss : 0.246716, loss_ce: 0.014086, loss_dice: 0.479346
[11:52:55.918] TRAIN: iteration 25267 : loss : 0.047954, loss_ce: 0.000885, loss_dice: 0.095023
[11:52:56.127] TRAIN: iteration 25268 : loss : 0.129840, loss_ce: 0.002486, loss_dice: 0.257194
[11:52:56.341] TRAIN: iteration 25269 : loss : 0.048679, loss_ce: 0.005430, loss_dice: 0.091927
[11:52:56.549] TRAIN: iteration 25270 : loss : 0.161195, loss_ce: 0.007750, loss_dice: 0.314639
[11:52:56.758] TRAIN: iteration 25271 : loss : 0.198109, loss_ce: 0.001089, loss_dice: 0.395129
[11:52:56.967] TRAIN: iteration 25272 : loss : 0.016105, loss_ce: 0.001713, loss_dice: 0.030497
[11:52:57.796] TRAIN: iteration 25273 : loss : 0.066747, loss_ce: 0.002215, loss_dice: 0.131278
[11:52:58.004] TRAIN: iteration 25274 : loss : 0.250387, loss_ce: 0.000738, loss_dice: 0.500036
[11:52:58.213] TRAIN: iteration 25275 : loss : 0.029834, loss_ce: 0.001398, loss_dice: 0.058269
[11:52:58.421] TRAIN: iteration 25276 : loss : 0.253705, loss_ce: 0.012317, loss_dice: 0.495092
[11:52:58.635] TRAIN: iteration 25277 : loss : 0.243584, loss_ce: 0.002250, loss_dice: 0.484918
[11:52:58.843] TRAIN: iteration 25278 : loss : 0.039209, loss_ce: 0.001499, loss_dice: 0.076918
[11:52:59.054] TRAIN: iteration 25279 : loss : 0.068037, loss_ce: 0.004868, loss_dice: 0.131207
[11:52:59.269] TRAIN: iteration 25280 : loss : 0.051410, loss_ce: 0.002275, loss_dice: 0.100546
[11:52:59.507] TRAIN: iteration 25281 : loss : 0.250551, loss_ce: 0.001040, loss_dice: 0.500062
[11:52:59.715] TRAIN: iteration 25282 : loss : 0.122212, loss_ce: 0.002984, loss_dice: 0.241441
[11:52:59.924] TRAIN: iteration 25283 : loss : 0.227154, loss_ce: 0.001384, loss_dice: 0.452925
[11:53:00.134] TRAIN: iteration 25284 : loss : 0.047101, loss_ce: 0.003002, loss_dice: 0.091200
[11:53:00.343] TRAIN: iteration 25285 : loss : 0.146287, loss_ce: 0.002066, loss_dice: 0.290507
[11:53:00.552] TRAIN: iteration 25286 : loss : 0.251281, loss_ce: 0.002394, loss_dice: 0.500167
[11:53:00.763] TRAIN: iteration 25287 : loss : 0.088323, loss_ce: 0.010709, loss_dice: 0.165937
[11:53:00.972] TRAIN: iteration 25288 : loss : 0.079724, loss_ce: 0.004302, loss_dice: 0.155145
[11:53:01.686] TRAIN: iteration 25289 : loss : 0.249352, loss_ce: 0.001389, loss_dice: 0.497314
[11:53:01.893] TRAIN: iteration 25290 : loss : 0.156662, loss_ce: 0.006168, loss_dice: 0.307155
[11:53:02.129] TRAIN: iteration 25291 : loss : 0.085888, loss_ce: 0.003521, loss_dice: 0.168255
[11:53:02.339] TRAIN: iteration 25292 : loss : 0.044320, loss_ce: 0.003885, loss_dice: 0.084754
[11:53:02.556] TRAIN: iteration 25293 : loss : 0.077139, loss_ce: 0.002238, loss_dice: 0.152041
[11:53:02.767] TRAIN: iteration 25294 : loss : 0.059495, loss_ce: 0.001244, loss_dice: 0.117746
[11:53:02.980] TRAIN: iteration 25295 : loss : 0.069266, loss_ce: 0.001418, loss_dice: 0.137114
[11:53:03.190] TRAIN: iteration 25296 : loss : 0.093904, loss_ce: 0.001593, loss_dice: 0.186215
[11:53:03.397] TRAIN: iteration 25297 : loss : 0.046601, loss_ce: 0.002264, loss_dice: 0.090937
[11:53:03.605] TRAIN: iteration 25298 : loss : 0.054972, loss_ce: 0.004649, loss_dice: 0.105296
[11:53:03.813] TRAIN: iteration 25299 : loss : 0.072562, loss_ce: 0.003121, loss_dice: 0.142003
[11:53:04.021] TRAIN: iteration 25300 : loss : 0.055625, loss_ce: 0.002290, loss_dice: 0.108960
[11:53:04.313] TRAIN: iteration 25301 : loss : 0.165956, loss_ce: 0.006073, loss_dice: 0.325840
[11:53:04.521] TRAIN: iteration 25302 : loss : 0.046366, loss_ce: 0.001080, loss_dice: 0.091652
[11:53:05.257] TRAIN: iteration 25303 : loss : 0.045114, loss_ce: 0.002644, loss_dice: 0.087584
[11:53:05.464] TRAIN: iteration 25304 : loss : 0.050990, loss_ce: 0.002010, loss_dice: 0.099970
[11:53:05.672] TRAIN: iteration 25305 : loss : 0.099490, loss_ce: 0.001616, loss_dice: 0.197364
[11:53:05.881] TRAIN: iteration 25306 : loss : 0.059652, loss_ce: 0.002054, loss_dice: 0.117249
[11:53:06.984] TRAIN: iteration 25307 : loss : 0.176260, loss_ce: 0.007196, loss_dice: 0.345324
[11:53:07.192] TRAIN: iteration 25308 : loss : 0.243029, loss_ce: 0.002304, loss_dice: 0.483754
[11:53:07.401] TRAIN: iteration 25309 : loss : 0.072930, loss_ce: 0.002957, loss_dice: 0.142903
[11:53:07.609] TRAIN: iteration 25310 : loss : 0.048175, loss_ce: 0.002464, loss_dice: 0.093886
[11:53:07.817] TRAIN: iteration 25311 : loss : 0.183023, loss_ce: 0.007868, loss_dice: 0.358179
[11:53:08.028] TRAIN: iteration 25312 : loss : 0.093604, loss_ce: 0.000818, loss_dice: 0.186389
[11:53:08.235] TRAIN: iteration 25313 : loss : 0.092118, loss_ce: 0.003710, loss_dice: 0.180526
[11:53:08.443] TRAIN: iteration 25314 : loss : 0.055425, loss_ce: 0.001271, loss_dice: 0.109579
[11:53:09.158] TRAIN: iteration 25315 : loss : 0.207100, loss_ce: 0.002168, loss_dice: 0.412033
[11:53:09.374] TRAIN: iteration 25316 : loss : 0.244210, loss_ce: 0.000962, loss_dice: 0.487458
[11:53:09.583] TRAIN: iteration 25317 : loss : 0.250927, loss_ce: 0.001743, loss_dice: 0.500111
[11:53:09.790] TRAIN: iteration 25318 : loss : 0.250399, loss_ce: 0.000782, loss_dice: 0.500016
[11:53:09.999] TRAIN: iteration 25319 : loss : 0.053383, loss_ce: 0.001346, loss_dice: 0.105421
[11:53:10.213] TRAIN: iteration 25320 : loss : 0.036123, loss_ce: 0.001089, loss_dice: 0.071156
[11:53:10.455] TRAIN: iteration 25321 : loss : 0.040470, loss_ce: 0.001220, loss_dice: 0.079719
[11:53:10.665] TRAIN: iteration 25322 : loss : 0.140069, loss_ce: 0.001954, loss_dice: 0.278184
[11:53:10.876] TRAIN: iteration 25323 : loss : 0.082170, loss_ce: 0.003000, loss_dice: 0.161339
[11:53:11.089] TRAIN: iteration 25324 : loss : 0.240194, loss_ce: 0.001440, loss_dice: 0.478948
[11:53:11.297] TRAIN: iteration 25325 : loss : 0.052687, loss_ce: 0.008904, loss_dice: 0.096471
[11:53:11.507] TRAIN: iteration 25326 : loss : 0.079360, loss_ce: 0.002757, loss_dice: 0.155962
[11:53:11.715] TRAIN: iteration 25327 : loss : 0.182454, loss_ce: 0.002486, loss_dice: 0.362422
[11:53:12.013] TRAIN: iteration 25328 : loss : 0.077979, loss_ce: 0.004905, loss_dice: 0.151053
[11:53:12.221] TRAIN: iteration 25329 : loss : 0.040494, loss_ce: 0.001379, loss_dice: 0.079609
[11:53:12.430] TRAIN: iteration 25330 : loss : 0.031349, loss_ce: 0.001288, loss_dice: 0.061409
[11:53:12.639] TRAIN: iteration 25331 : loss : 0.250469, loss_ce: 0.000901, loss_dice: 0.500038
[11:53:12.848] TRAIN: iteration 25332 : loss : 0.090577, loss_ce: 0.004342, loss_dice: 0.176812
[11:53:13.056] TRAIN: iteration 25333 : loss : 0.250452, loss_ce: 0.000870, loss_dice: 0.500034
[11:53:13.267] TRAIN: iteration 25334 : loss : 0.137762, loss_ce: 0.002056, loss_dice: 0.273468
[11:53:13.479] TRAIN: iteration 25335 : loss : 0.025354, loss_ce: 0.000535, loss_dice: 0.050174
[11:53:13.693] TRAIN: iteration 25336 : loss : 0.244754, loss_ce: 0.010476, loss_dice: 0.479031
[11:53:13.900] TRAIN: iteration 25337 : loss : 0.103281, loss_ce: 0.001964, loss_dice: 0.204598
[11:53:14.117] TRAIN: iteration 25338 : loss : 0.196720, loss_ce: 0.002958, loss_dice: 0.390482
[11:53:14.611] TRAIN: iteration 25339 : loss : 0.078003, loss_ce: 0.004027, loss_dice: 0.151979
[11:53:14.819] TRAIN: iteration 25340 : loss : 0.038462, loss_ce: 0.001150, loss_dice: 0.075774
[11:53:15.057] TRAIN: iteration 25341 : loss : 0.164527, loss_ce: 0.002418, loss_dice: 0.326636
[11:53:15.269] TRAIN: iteration 25342 : loss : 0.144188, loss_ce: 0.001593, loss_dice: 0.286783
[11:53:15.757] TRAIN: iteration 25343 : loss : 0.168734, loss_ce: 0.006851, loss_dice: 0.330618
[11:53:15.965] TRAIN: iteration 25344 : loss : 0.232485, loss_ce: 0.000851, loss_dice: 0.464118
[11:53:16.179] TRAIN: iteration 25345 : loss : 0.061663, loss_ce: 0.001644, loss_dice: 0.121682
[11:53:17.499] TRAIN: iteration 25346 : loss : 0.220949, loss_ce: 0.004321, loss_dice: 0.437578
[11:53:17.707] TRAIN: iteration 25347 : loss : 0.052260, loss_ce: 0.006710, loss_dice: 0.097810
[11:53:17.915] TRAIN: iteration 25348 : loss : 0.143519, loss_ce: 0.007710, loss_dice: 0.279328
[11:53:18.128] TRAIN: iteration 25349 : loss : 0.127326, loss_ce: 0.003604, loss_dice: 0.251048
[11:53:18.336] TRAIN: iteration 25350 : loss : 0.050250, loss_ce: 0.001907, loss_dice: 0.098593
[11:53:19.310] TRAIN: iteration 25351 : loss : 0.096615, loss_ce: 0.002372, loss_dice: 0.190858
[11:53:19.519] TRAIN: iteration 25352 : loss : 0.135413, loss_ce: 0.002351, loss_dice: 0.268474
[11:53:19.730] TRAIN: iteration 25353 : loss : 0.056428, loss_ce: 0.002128, loss_dice: 0.110728
[11:53:19.940] TRAIN: iteration 25354 : loss : 0.132969, loss_ce: 0.007957, loss_dice: 0.257980
[11:53:20.147] TRAIN: iteration 25355 : loss : 0.084111, loss_ce: 0.005214, loss_dice: 0.163008
[11:53:20.360] TRAIN: iteration 25356 : loss : 0.030232, loss_ce: 0.000837, loss_dice: 0.059626
[11:53:20.568] TRAIN: iteration 25357 : loss : 0.146415, loss_ce: 0.003627, loss_dice: 0.289203
[11:53:21.306] TRAIN: iteration 25358 : loss : 0.024542, loss_ce: 0.004339, loss_dice: 0.044744
[11:53:21.513] TRAIN: iteration 25359 : loss : 0.216852, loss_ce: 0.003898, loss_dice: 0.429807
[11:53:21.722] TRAIN: iteration 25360 : loss : 0.251326, loss_ce: 0.002494, loss_dice: 0.500158
[11:53:22.139] TRAIN: iteration 25361 : loss : 0.157381, loss_ce: 0.002533, loss_dice: 0.312230
[11:53:22.347] TRAIN: iteration 25362 : loss : 0.110376, loss_ce: 0.002497, loss_dice: 0.218255
[11:53:22.555] TRAIN: iteration 25363 : loss : 0.251610, loss_ce: 0.003020, loss_dice: 0.500199
[11:53:22.764] TRAIN: iteration 25364 : loss : 0.031065, loss_ce: 0.000815, loss_dice: 0.061314
[11:53:22.973] TRAIN: iteration 25365 : loss : 0.061422, loss_ce: 0.005127, loss_dice: 0.117718
[11:53:24.710] TRAIN: iteration 25366 : loss : 0.027541, loss_ce: 0.003275, loss_dice: 0.051808
[11:53:24.921] TRAIN: iteration 25367 : loss : 0.037434, loss_ce: 0.003664, loss_dice: 0.071203
[11:53:25.130] TRAIN: iteration 25368 : loss : 0.037471, loss_ce: 0.003009, loss_dice: 0.071933
[11:53:25.338] TRAIN: iteration 25369 : loss : 0.054444, loss_ce: 0.002089, loss_dice: 0.106798
[11:53:25.545] TRAIN: iteration 25370 : loss : 0.046946, loss_ce: 0.006018, loss_dice: 0.087873
[11:53:25.753] TRAIN: iteration 25371 : loss : 0.039976, loss_ce: 0.002686, loss_dice: 0.077266
[11:53:25.961] TRAIN: iteration 25372 : loss : 0.113177, loss_ce: 0.003880, loss_dice: 0.222475
[11:53:26.176] TRAIN: iteration 25373 : loss : 0.155839, loss_ce: 0.002718, loss_dice: 0.308959
[11:53:26.394] TRAIN: iteration 25374 : loss : 0.038232, loss_ce: 0.006208, loss_dice: 0.070255
[11:53:26.604] TRAIN: iteration 25375 : loss : 0.091549, loss_ce: 0.008205, loss_dice: 0.174893
[11:53:26.821] TRAIN: iteration 25376 : loss : 0.065073, loss_ce: 0.005505, loss_dice: 0.124642
[11:53:27.585] TRAIN: iteration 25377 : loss : 0.053577, loss_ce: 0.002735, loss_dice: 0.104419
[11:53:28.859] TRAIN: iteration 25378 : loss : 0.081573, loss_ce: 0.015376, loss_dice: 0.147770
[11:53:29.071] TRAIN: iteration 25379 : loss : 0.113459, loss_ce: 0.003222, loss_dice: 0.223695
[11:53:29.282] TRAIN: iteration 25380 : loss : 0.050582, loss_ce: 0.001673, loss_dice: 0.099492
[11:53:29.525] TRAIN: iteration 25381 : loss : 0.084863, loss_ce: 0.004497, loss_dice: 0.165229
[11:53:30.123] TRAIN: iteration 25382 : loss : 0.105254, loss_ce: 0.002541, loss_dice: 0.207968
[11:53:30.341] TRAIN: iteration 25383 : loss : 0.025079, loss_ce: 0.005420, loss_dice: 0.044738
[11:53:30.549] TRAIN: iteration 25384 : loss : 0.026554, loss_ce: 0.000666, loss_dice: 0.052442
[11:53:30.759] TRAIN: iteration 25385 : loss : 0.059300, loss_ce: 0.003567, loss_dice: 0.115033
[11:53:30.973] TRAIN: iteration 25386 : loss : 0.240153, loss_ce: 0.003219, loss_dice: 0.477087
[11:53:31.182] TRAIN: iteration 25387 : loss : 0.027651, loss_ce: 0.004576, loss_dice: 0.050726
[11:53:31.391] TRAIN: iteration 25388 : loss : 0.251332, loss_ce: 0.002510, loss_dice: 0.500154
[11:53:31.598] TRAIN: iteration 25389 : loss : 0.205482, loss_ce: 0.003231, loss_dice: 0.407732
[11:53:31.806] TRAIN: iteration 25390 : loss : 0.237240, loss_ce: 0.005254, loss_dice: 0.469226
[11:53:32.138] TRAIN: iteration 25391 : loss : 0.113529, loss_ce: 0.006697, loss_dice: 0.220361
[11:53:32.345] TRAIN: iteration 25392 : loss : 0.128153, loss_ce: 0.004526, loss_dice: 0.251781
[11:53:32.722] TRAIN: iteration 25393 : loss : 0.083812, loss_ce: 0.007160, loss_dice: 0.160464
[11:53:32.931] TRAIN: iteration 25394 : loss : 0.108816, loss_ce: 0.005055, loss_dice: 0.212578
[11:53:33.141] TRAIN: iteration 25395 : loss : 0.251482, loss_ce: 0.002774, loss_dice: 0.500189
[11:53:33.955] TRAIN: iteration 25396 : loss : 0.027834, loss_ce: 0.005978, loss_dice: 0.049690
[11:53:34.163] TRAIN: iteration 25397 : loss : 0.126861, loss_ce: 0.004507, loss_dice: 0.249214
[11:53:34.511] TRAIN: iteration 25398 : loss : 0.190214, loss_ce: 0.014150, loss_dice: 0.366279
[11:53:34.720] TRAIN: iteration 25399 : loss : 0.149444, loss_ce: 0.002503, loss_dice: 0.296385
[11:53:34.927] TRAIN: iteration 25400 : loss : 0.096523, loss_ce: 0.001835, loss_dice: 0.191210
[11:53:35.169] TRAIN: iteration 25401 : loss : 0.139240, loss_ce: 0.007443, loss_dice: 0.271037
[11:53:35.377] TRAIN: iteration 25402 : loss : 0.114535, loss_ce: 0.005245, loss_dice: 0.223825
[11:53:35.585] TRAIN: iteration 25403 : loss : 0.250590, loss_ce: 0.001133, loss_dice: 0.500047
[11:53:35.793] TRAIN: iteration 25404 : loss : 0.250647, loss_ce: 0.001242, loss_dice: 0.500053
[11:53:36.000] TRAIN: iteration 25405 : loss : 0.038664, loss_ce: 0.007965, loss_dice: 0.069364
[11:53:36.212] TRAIN: iteration 25406 : loss : 0.051040, loss_ce: 0.002435, loss_dice: 0.099646
[11:53:36.426] TRAIN: iteration 25407 : loss : 0.058276, loss_ce: 0.008039, loss_dice: 0.108514
[11:53:36.756] TRAIN: iteration 25408 : loss : 0.036581, loss_ce: 0.003997, loss_dice: 0.069166
[11:53:36.969] TRAIN: iteration 25409 : loss : 0.086278, loss_ce: 0.001917, loss_dice: 0.170639
[11:53:37.177] TRAIN: iteration 25410 : loss : 0.097852, loss_ce: 0.008594, loss_dice: 0.187111
[11:53:37.390] TRAIN: iteration 25411 : loss : 0.062548, loss_ce: 0.002038, loss_dice: 0.123057
[11:53:37.913] TRAIN: iteration 25412 : loss : 0.029280, loss_ce: 0.004424, loss_dice: 0.054136
[11:53:38.579] TRAIN: iteration 25413 : loss : 0.168816, loss_ce: 0.001479, loss_dice: 0.336152
[11:53:38.788] TRAIN: iteration 25414 : loss : 0.061318, loss_ce: 0.003718, loss_dice: 0.118917
[11:53:38.996] TRAIN: iteration 25415 : loss : 0.057909, loss_ce: 0.001903, loss_dice: 0.113915
[11:53:39.205] TRAIN: iteration 25416 : loss : 0.087876, loss_ce: 0.003377, loss_dice: 0.172375
[11:53:39.415] TRAIN: iteration 25417 : loss : 0.075770, loss_ce: 0.002150, loss_dice: 0.149391
[11:53:39.624] TRAIN: iteration 25418 : loss : 0.047416, loss_ce: 0.007984, loss_dice: 0.086848
[11:53:39.833] TRAIN: iteration 25419 : loss : 0.080832, loss_ce: 0.003021, loss_dice: 0.158642
[11:53:41.402] TRAIN: iteration 25420 : loss : 0.039071, loss_ce: 0.003006, loss_dice: 0.075135
[11:53:41.638] TRAIN: iteration 25421 : loss : 0.143765, loss_ce: 0.006413, loss_dice: 0.281117
[11:53:41.846] TRAIN: iteration 25422 : loss : 0.242932, loss_ce: 0.003759, loss_dice: 0.482104
[11:53:42.054] TRAIN: iteration 25423 : loss : 0.250808, loss_ce: 0.001521, loss_dice: 0.500096
[11:53:42.262] TRAIN: iteration 25424 : loss : 0.097199, loss_ce: 0.003381, loss_dice: 0.191018
[11:53:42.470] TRAIN: iteration 25425 : loss : 0.039255, loss_ce: 0.002212, loss_dice: 0.076298
[11:53:42.685] TRAIN: iteration 25426 : loss : 0.251239, loss_ce: 0.002326, loss_dice: 0.500153
[11:53:42.896] TRAIN: iteration 25427 : loss : 0.251090, loss_ce: 0.002066, loss_dice: 0.500113
[11:53:43.421] TRAIN: iteration 25428 : loss : 0.092568, loss_ce: 0.006342, loss_dice: 0.178794
[11:53:43.628] TRAIN: iteration 25429 : loss : 0.227651, loss_ce: 0.002771, loss_dice: 0.452531
[11:53:43.836] TRAIN: iteration 25430 : loss : 0.159087, loss_ce: 0.002224, loss_dice: 0.315949
[11:53:44.043] TRAIN: iteration 25431 : loss : 0.067034, loss_ce: 0.004854, loss_dice: 0.129214
[11:53:44.251] TRAIN: iteration 25432 : loss : 0.143566, loss_ce: 0.011774, loss_dice: 0.275358
[11:53:44.459] TRAIN: iteration 25433 : loss : 0.215448, loss_ce: 0.003351, loss_dice: 0.427545
[11:53:44.945] TRAIN: iteration 25434 : loss : 0.124607, loss_ce: 0.005096, loss_dice: 0.244119
[11:53:45.155] TRAIN: iteration 25435 : loss : 0.026491, loss_ce: 0.002328, loss_dice: 0.050655
[11:53:47.564] TRAIN: iteration 25436 : loss : 0.059847, loss_ce: 0.003309, loss_dice: 0.116385
[11:53:47.772] TRAIN: iteration 25437 : loss : 0.250899, loss_ce: 0.001685, loss_dice: 0.500112
[11:53:47.980] TRAIN: iteration 25438 : loss : 0.019283, loss_ce: 0.000662, loss_dice: 0.037904
[11:53:48.190] TRAIN: iteration 25439 : loss : 0.130542, loss_ce: 0.001659, loss_dice: 0.259425
[11:53:48.398] TRAIN: iteration 25440 : loss : 0.035114, loss_ce: 0.002495, loss_dice: 0.067734
[11:53:48.637] TRAIN: iteration 25441 : loss : 0.062233, loss_ce: 0.001501, loss_dice: 0.122965
[11:53:48.846] TRAIN: iteration 25442 : loss : 0.034338, loss_ce: 0.002067, loss_dice: 0.066609
[11:53:49.055] TRAIN: iteration 25443 : loss : 0.033138, loss_ce: 0.002772, loss_dice: 0.063503
[11:53:52.187] TRAIN: iteration 25444 : loss : 0.189467, loss_ce: 0.005078, loss_dice: 0.373856
[11:53:52.396] TRAIN: iteration 25445 : loss : 0.137075, loss_ce: 0.003793, loss_dice: 0.270357
[11:53:52.605] TRAIN: iteration 25446 : loss : 0.139699, loss_ce: 0.000935, loss_dice: 0.278463
[11:53:52.816] TRAIN: iteration 25447 : loss : 0.055627, loss_ce: 0.005115, loss_dice: 0.106139
[11:53:53.028] TRAIN: iteration 25448 : loss : 0.072241, loss_ce: 0.000999, loss_dice: 0.143483
[11:53:53.237] TRAIN: iteration 25449 : loss : 0.015577, loss_ce: 0.001217, loss_dice: 0.029937
[11:53:53.447] TRAIN: iteration 25450 : loss : 0.034546, loss_ce: 0.001426, loss_dice: 0.067667
[11:53:53.658] TRAIN: iteration 25451 : loss : 0.250425, loss_ce: 0.000800, loss_dice: 0.500049
[11:53:53.868] TRAIN: iteration 25452 : loss : 0.251587, loss_ce: 0.002957, loss_dice: 0.500218
[11:53:54.076] TRAIN: iteration 25453 : loss : 0.207811, loss_ce: 0.009700, loss_dice: 0.405923
[11:53:54.289] TRAIN: iteration 25454 : loss : 0.022556, loss_ce: 0.001706, loss_dice: 0.043406
[11:53:54.500] TRAIN: iteration 25455 : loss : 0.111533, loss_ce: 0.002828, loss_dice: 0.220239
[11:53:54.712] TRAIN: iteration 25456 : loss : 0.144112, loss_ce: 0.004617, loss_dice: 0.283608
[11:53:54.923] TRAIN: iteration 25457 : loss : 0.078274, loss_ce: 0.012202, loss_dice: 0.144346
[11:53:55.137] TRAIN: iteration 25458 : loss : 0.071774, loss_ce: 0.002616, loss_dice: 0.140932
[11:53:55.364] TRAIN: iteration 25459 : loss : 0.120251, loss_ce: 0.010862, loss_dice: 0.229640
[11:53:55.580] TRAIN: iteration 25460 : loss : 0.081750, loss_ce: 0.001174, loss_dice: 0.162326
[11:53:55.581] NaN or Inf found in input tensor.
[11:53:55.795] TRAIN: iteration 25461 : loss : 0.205571, loss_ce: 0.001440, loss_dice: 0.409702
[11:53:56.006] TRAIN: iteration 25462 : loss : 0.035645, loss_ce: 0.002350, loss_dice: 0.068940
[11:53:56.216] TRAIN: iteration 25463 : loss : 0.162401, loss_ce: 0.031949, loss_dice: 0.292852
[11:53:56.426] TRAIN: iteration 25464 : loss : 0.143380, loss_ce: 0.001888, loss_dice: 0.284872
[11:53:56.639] TRAIN: iteration 25465 : loss : 0.081615, loss_ce: 0.001342, loss_dice: 0.161888
[11:53:56.851] TRAIN: iteration 25466 : loss : 0.230626, loss_ce: 0.002379, loss_dice: 0.458873
[11:53:57.060] TRAIN: iteration 25467 : loss : 0.074388, loss_ce: 0.009460, loss_dice: 0.139316
[11:53:57.275] TRAIN: iteration 25468 : loss : 0.058888, loss_ce: 0.002559, loss_dice: 0.115217
[11:53:58.963] TRAIN: iteration 25469 : loss : 0.071258, loss_ce: 0.000732, loss_dice: 0.141783
[11:53:59.172] TRAIN: iteration 25470 : loss : 0.047389, loss_ce: 0.006091, loss_dice: 0.088687
[11:53:59.380] TRAIN: iteration 25471 : loss : 0.040040, loss_ce: 0.001260, loss_dice: 0.078820
[11:53:59.590] TRAIN: iteration 25472 : loss : 0.043713, loss_ce: 0.001041, loss_dice: 0.086385
[11:53:59.797] TRAIN: iteration 25473 : loss : 0.143854, loss_ce: 0.004405, loss_dice: 0.283303
[11:54:00.006] TRAIN: iteration 25474 : loss : 0.044908, loss_ce: 0.002322, loss_dice: 0.087495
[11:54:00.722] TRAIN: iteration 25475 : loss : 0.254066, loss_ce: 0.009897, loss_dice: 0.498236
[11:54:00.930] TRAIN: iteration 25476 : loss : 0.078071, loss_ce: 0.003014, loss_dice: 0.153128
[11:54:02.068] TRAIN: iteration 25477 : loss : 0.078563, loss_ce: 0.006270, loss_dice: 0.150857
[11:54:02.276] TRAIN: iteration 25478 : loss : 0.016588, loss_ce: 0.001097, loss_dice: 0.032079
[11:54:02.488] TRAIN: iteration 25479 : loss : 0.039106, loss_ce: 0.003950, loss_dice: 0.074261
[11:54:02.699] TRAIN: iteration 25480 : loss : 0.095974, loss_ce: 0.006527, loss_dice: 0.185421
[11:54:02.936] TRAIN: iteration 25481 : loss : 0.115498, loss_ce: 0.009551, loss_dice: 0.221444
[11:54:03.149] TRAIN: iteration 25482 : loss : 0.056158, loss_ce: 0.003127, loss_dice: 0.109189
[11:54:04.197] TRAIN: iteration 25483 : loss : 0.250587, loss_ce: 0.001127, loss_dice: 0.500048
[11:54:04.407] TRAIN: iteration 25484 : loss : 0.076378, loss_ce: 0.005627, loss_dice: 0.147128
[11:54:05.472] TRAIN: iteration 25485 : loss : 0.072952, loss_ce: 0.006609, loss_dice: 0.139294
[11:54:05.680] TRAIN: iteration 25486 : loss : 0.023733, loss_ce: 0.001231, loss_dice: 0.046236
[11:54:05.890] TRAIN: iteration 25487 : loss : 0.074448, loss_ce: 0.001954, loss_dice: 0.146943
[11:54:06.099] TRAIN: iteration 25488 : loss : 0.194929, loss_ce: 0.003657, loss_dice: 0.386201
[11:54:06.315] TRAIN: iteration 25489 : loss : 0.053490, loss_ce: 0.002193, loss_dice: 0.104786
[11:54:06.529] TRAIN: iteration 25490 : loss : 0.182839, loss_ce: 0.007850, loss_dice: 0.357828
[11:54:06.737] TRAIN: iteration 25491 : loss : 0.176628, loss_ce: 0.004327, loss_dice: 0.348929
[11:54:06.945] TRAIN: iteration 25492 : loss : 0.077951, loss_ce: 0.004676, loss_dice: 0.151226
[11:54:07.154] TRAIN: iteration 25493 : loss : 0.251283, loss_ce: 0.002472, loss_dice: 0.500095
[11:54:07.364] TRAIN: iteration 25494 : loss : 0.140047, loss_ce: 0.004510, loss_dice: 0.275584
[11:54:07.571] TRAIN: iteration 25495 : loss : 0.042031, loss_ce: 0.010698, loss_dice: 0.073364
[11:54:07.779] TRAIN: iteration 25496 : loss : 0.100126, loss_ce: 0.002302, loss_dice: 0.197949
[11:54:07.989] TRAIN: iteration 25497 : loss : 0.106192, loss_ce: 0.009647, loss_dice: 0.202737
[11:54:08.197] TRAIN: iteration 25498 : loss : 0.083407, loss_ce: 0.002705, loss_dice: 0.164110
[11:54:08.406] TRAIN: iteration 25499 : loss : 0.246782, loss_ce: 0.001794, loss_dice: 0.491770
[11:54:08.617] TRAIN: iteration 25500 : loss : 0.084536, loss_ce: 0.005971, loss_dice: 0.163101
[11:54:08.861] TRAIN: iteration 25501 : loss : 0.126360, loss_ce: 0.006246, loss_dice: 0.246474
[11:54:09.068] TRAIN: iteration 25502 : loss : 0.126150, loss_ce: 0.003721, loss_dice: 0.248580
[11:54:09.278] TRAIN: iteration 25503 : loss : 0.122359, loss_ce: 0.000920, loss_dice: 0.243798
[11:54:09.491] TRAIN: iteration 25504 : loss : 0.100121, loss_ce: 0.003916, loss_dice: 0.196326
[11:54:09.699] TRAIN: iteration 25505 : loss : 0.102762, loss_ce: 0.009206, loss_dice: 0.196318
[11:54:09.917] TRAIN: iteration 25506 : loss : 0.068292, loss_ce: 0.001681, loss_dice: 0.134902
[11:54:10.945] TRAIN: iteration 25507 : loss : 0.248171, loss_ce: 0.001290, loss_dice: 0.495052
[11:54:11.432] TRAIN: iteration 25508 : loss : 0.082867, loss_ce: 0.000871, loss_dice: 0.164863
[11:54:11.640] TRAIN: iteration 25509 : loss : 0.039318, loss_ce: 0.001740, loss_dice: 0.076896
[11:54:11.848] TRAIN: iteration 25510 : loss : 0.088477, loss_ce: 0.003105, loss_dice: 0.173848
[11:54:12.059] TRAIN: iteration 25511 : loss : 0.206470, loss_ce: 0.001541, loss_dice: 0.411399
[11:54:12.267] TRAIN: iteration 25512 : loss : 0.106439, loss_ce: 0.001945, loss_dice: 0.210934
[11:54:12.477] TRAIN: iteration 25513 : loss : 0.250980, loss_ce: 0.001821, loss_dice: 0.500139
[11:54:12.684] TRAIN: iteration 25514 : loss : 0.030772, loss_ce: 0.003439, loss_dice: 0.058105
[11:54:13.804] TRAIN: iteration 25515 : loss : 0.077978, loss_ce: 0.004791, loss_dice: 0.151164
[11:54:14.012] TRAIN: iteration 25516 : loss : 0.250506, loss_ce: 0.000960, loss_dice: 0.500052
[11:54:14.220] TRAIN: iteration 25517 : loss : 0.048166, loss_ce: 0.005511, loss_dice: 0.090820
[11:54:14.428] TRAIN: iteration 25518 : loss : 0.060447, loss_ce: 0.001895, loss_dice: 0.118999
[11:54:14.639] TRAIN: iteration 25519 : loss : 0.178388, loss_ce: 0.001967, loss_dice: 0.354809
[11:54:15.338] TRAIN: iteration 25520 : loss : 0.102505, loss_ce: 0.003175, loss_dice: 0.201836
[11:54:16.673] TRAIN: iteration 25521 : loss : 0.074263, loss_ce: 0.001752, loss_dice: 0.146774
[11:54:16.886] TRAIN: iteration 25522 : loss : 0.054767, loss_ce: 0.002466, loss_dice: 0.107068
[11:54:17.094] TRAIN: iteration 25523 : loss : 0.105640, loss_ce: 0.003954, loss_dice: 0.207326
[11:54:17.302] TRAIN: iteration 25524 : loss : 0.251091, loss_ce: 0.002039, loss_dice: 0.500142
[11:54:17.515] TRAIN: iteration 25525 : loss : 0.073099, loss_ce: 0.000903, loss_dice: 0.145294
[11:54:17.722] TRAIN: iteration 25526 : loss : 0.250803, loss_ce: 0.001513, loss_dice: 0.500093
[11:54:17.930] TRAIN: iteration 25527 : loss : 0.127163, loss_ce: 0.004594, loss_dice: 0.249731
[11:54:20.254] TRAIN: iteration 25528 : loss : 0.062565, loss_ce: 0.003306, loss_dice: 0.121824
[11:54:20.467] TRAIN: iteration 25529 : loss : 0.125493, loss_ce: 0.012294, loss_dice: 0.238692
[11:54:20.674] TRAIN: iteration 25530 : loss : 0.021815, loss_ce: 0.000731, loss_dice: 0.042898
[11:54:20.883] TRAIN: iteration 25531 : loss : 0.237686, loss_ce: 0.000584, loss_dice: 0.474788
[11:54:21.092] TRAIN: iteration 25532 : loss : 0.087205, loss_ce: 0.003066, loss_dice: 0.171345
[11:54:21.299] TRAIN: iteration 25533 : loss : 0.105048, loss_ce: 0.007898, loss_dice: 0.202198
[11:54:21.507] TRAIN: iteration 25534 : loss : 0.250930, loss_ce: 0.001748, loss_dice: 0.500113
[11:54:21.716] TRAIN: iteration 25535 : loss : 0.033890, loss_ce: 0.002864, loss_dice: 0.064915
[11:54:22.552] TRAIN: iteration 25536 : loss : 0.075040, loss_ce: 0.011336, loss_dice: 0.138745
[11:54:22.788] TRAIN: iteration 25537 : loss : 0.062195, loss_ce: 0.001669, loss_dice: 0.122721
[11:54:22.996] TRAIN: iteration 25538 : loss : 0.099174, loss_ce: 0.004612, loss_dice: 0.193737
[11:54:23.210] TRAIN: iteration 25539 : loss : 0.063964, loss_ce: 0.001450, loss_dice: 0.126478
[11:54:23.418] TRAIN: iteration 25540 : loss : 0.244491, loss_ce: 0.005531, loss_dice: 0.483450
[11:54:23.655] TRAIN: iteration 25541 : loss : 0.077028, loss_ce: 0.003593, loss_dice: 0.150462
[11:54:23.863] TRAIN: iteration 25542 : loss : 0.071213, loss_ce: 0.001674, loss_dice: 0.140752
[11:54:24.071] TRAIN: iteration 25543 : loss : 0.213757, loss_ce: 0.009536, loss_dice: 0.417978
[11:54:26.004] TRAIN: iteration 25544 : loss : 0.076177, loss_ce: 0.004314, loss_dice: 0.148040
[11:54:26.219] TRAIN: iteration 25545 : loss : 0.049774, loss_ce: 0.002538, loss_dice: 0.097009
[11:54:26.429] TRAIN: iteration 25546 : loss : 0.086643, loss_ce: 0.000793, loss_dice: 0.172493
[11:54:26.639] TRAIN: iteration 25547 : loss : 0.051839, loss_ce: 0.004821, loss_dice: 0.098856
[11:54:26.848] TRAIN: iteration 25548 : loss : 0.250557, loss_ce: 0.001077, loss_dice: 0.500038
[11:54:27.057] TRAIN: iteration 25549 : loss : 0.058169, loss_ce: 0.002505, loss_dice: 0.113833
[11:54:27.265] TRAIN: iteration 25550 : loss : 0.034868, loss_ce: 0.000951, loss_dice: 0.068786
[11:54:27.476] TRAIN: iteration 25551 : loss : 0.117570, loss_ce: 0.003214, loss_dice: 0.231926
[11:54:29.483] TRAIN: iteration 25552 : loss : 0.249841, loss_ce: 0.002609, loss_dice: 0.497073
[11:54:29.696] TRAIN: iteration 25553 : loss : 0.076846, loss_ce: 0.002723, loss_dice: 0.150969
[11:54:29.905] TRAIN: iteration 25554 : loss : 0.099365, loss_ce: 0.001904, loss_dice: 0.196825
[11:54:30.114] TRAIN: iteration 25555 : loss : 0.099100, loss_ce: 0.004448, loss_dice: 0.193753
[11:54:30.322] TRAIN: iteration 25556 : loss : 0.043224, loss_ce: 0.001321, loss_dice: 0.085126
[11:54:30.538] TRAIN: iteration 25557 : loss : 0.246683, loss_ce: 0.002658, loss_dice: 0.490707
[11:54:30.754] TRAIN: iteration 25558 : loss : 0.060755, loss_ce: 0.004727, loss_dice: 0.116784
[11:54:30.961] TRAIN: iteration 25559 : loss : 0.101845, loss_ce: 0.009826, loss_dice: 0.193865
[11:54:32.276] TRAIN: iteration 25560 : loss : 0.032774, loss_ce: 0.002381, loss_dice: 0.063167
[11:54:32.518] TRAIN: iteration 25561 : loss : 0.250642, loss_ce: 0.001222, loss_dice: 0.500063
[11:54:32.726] TRAIN: iteration 25562 : loss : 0.251330, loss_ce: 0.003005, loss_dice: 0.499654
[11:54:32.934] TRAIN: iteration 25563 : loss : 0.193989, loss_ce: 0.011817, loss_dice: 0.376161
[11:54:33.143] TRAIN: iteration 25564 : loss : 0.029397, loss_ce: 0.000764, loss_dice: 0.058030
[11:54:33.351] TRAIN: iteration 25565 : loss : 0.030292, loss_ce: 0.001627, loss_dice: 0.058956
[11:54:33.816] TRAIN: iteration 25566 : loss : 0.152997, loss_ce: 0.003867, loss_dice: 0.302126
[11:54:34.029] TRAIN: iteration 25567 : loss : 0.094483, loss_ce: 0.003418, loss_dice: 0.185548
[11:54:34.755] TRAIN: iteration 25568 : loss : 0.047094, loss_ce: 0.007745, loss_dice: 0.086442
[11:54:38.299] TRAIN: iteration 25569 : loss : 0.202896, loss_ce: 0.005479, loss_dice: 0.400314
[11:54:38.512] TRAIN: iteration 25570 : loss : 0.077475, loss_ce: 0.002906, loss_dice: 0.152045
[11:54:38.720] TRAIN: iteration 25571 : loss : 0.250345, loss_ce: 0.004191, loss_dice: 0.496498
[11:54:38.929] TRAIN: iteration 25572 : loss : 0.113753, loss_ce: 0.005445, loss_dice: 0.222060
[11:54:40.903] TRAIN: iteration 25573 : loss : 0.147071, loss_ce: 0.002700, loss_dice: 0.291442
[11:54:41.114] TRAIN: iteration 25574 : loss : 0.014065, loss_ce: 0.000746, loss_dice: 0.027385
[11:54:41.324] TRAIN: iteration 25575 : loss : 0.066005, loss_ce: 0.005715, loss_dice: 0.126295
[11:54:41.533] TRAIN: iteration 25576 : loss : 0.251043, loss_ce: 0.001976, loss_dice: 0.500110
[11:54:41.747] TRAIN: iteration 25577 : loss : 0.051440, loss_ce: 0.004788, loss_dice: 0.098092
[11:54:41.956] TRAIN: iteration 25578 : loss : 0.133369, loss_ce: 0.006436, loss_dice: 0.260302
[11:54:42.170] TRAIN: iteration 25579 : loss : 0.052008, loss_ce: 0.002948, loss_dice: 0.101068
[11:54:42.378] TRAIN: iteration 25580 : loss : 0.250515, loss_ce: 0.000984, loss_dice: 0.500046
[11:54:42.613] TRAIN: iteration 25581 : loss : 0.094184, loss_ce: 0.001769, loss_dice: 0.186598
[11:54:42.822] TRAIN: iteration 25582 : loss : 0.252736, loss_ce: 0.005098, loss_dice: 0.500374
[11:54:43.034] TRAIN: iteration 25583 : loss : 0.141613, loss_ce: 0.002017, loss_dice: 0.281209
[11:54:43.245] TRAIN: iteration 25584 : loss : 0.222229, loss_ce: 0.003787, loss_dice: 0.440671
[11:54:43.509] TRAIN: iteration 25585 : loss : 0.047316, loss_ce: 0.005069, loss_dice: 0.089563
[11:54:43.716] TRAIN: iteration 25586 : loss : 0.251265, loss_ce: 0.002357, loss_dice: 0.500172
[11:54:43.924] TRAIN: iteration 25587 : loss : 0.078519, loss_ce: 0.012974, loss_dice: 0.144064
[11:54:44.133] TRAIN: iteration 25588 : loss : 0.064448, loss_ce: 0.003333, loss_dice: 0.125563
[11:54:44.343] TRAIN: iteration 25589 : loss : 0.038370, loss_ce: 0.001308, loss_dice: 0.075431
[11:54:44.676] TRAIN: iteration 25590 : loss : 0.035300, loss_ce: 0.003052, loss_dice: 0.067549
[11:54:44.888] TRAIN: iteration 25591 : loss : 0.066895, loss_ce: 0.002633, loss_dice: 0.131157
[11:54:47.451] TRAIN: iteration 25592 : loss : 0.056752, loss_ce: 0.004454, loss_dice: 0.109051
[11:54:47.658] TRAIN: iteration 25593 : loss : 0.054031, loss_ce: 0.005437, loss_dice: 0.102625
[11:54:47.866] TRAIN: iteration 25594 : loss : 0.093399, loss_ce: 0.014978, loss_dice: 0.171820
[11:54:48.081] TRAIN: iteration 25595 : loss : 0.034596, loss_ce: 0.003505, loss_dice: 0.065687
[11:54:48.292] TRAIN: iteration 25596 : loss : 0.023718, loss_ce: 0.001812, loss_dice: 0.045623
[11:54:48.507] TRAIN: iteration 25597 : loss : 0.058744, loss_ce: 0.002049, loss_dice: 0.115438
[11:54:48.715] TRAIN: iteration 25598 : loss : 0.039953, loss_ce: 0.001864, loss_dice: 0.078042
[11:54:48.924] TRAIN: iteration 25599 : loss : 0.060846, loss_ce: 0.005394, loss_dice: 0.116297
[11:54:49.783] TRAIN: iteration 25600 : loss : 0.104484, loss_ce: 0.003129, loss_dice: 0.205840
[11:54:50.055] TRAIN: iteration 25601 : loss : 0.027914, loss_ce: 0.001595, loss_dice: 0.054233
[11:54:50.263] TRAIN: iteration 25602 : loss : 0.251288, loss_ce: 0.002411, loss_dice: 0.500165
[11:54:50.473] TRAIN: iteration 25603 : loss : 0.045545, loss_ce: 0.009616, loss_dice: 0.081475
[11:54:50.683] TRAIN: iteration 25604 : loss : 0.028706, loss_ce: 0.001145, loss_dice: 0.056267
[11:54:50.895] TRAIN: iteration 25605 : loss : 0.127022, loss_ce: 0.001556, loss_dice: 0.252488
[11:54:51.106] TRAIN: iteration 25606 : loss : 0.252101, loss_ce: 0.003940, loss_dice: 0.500263
[11:54:51.318] TRAIN: iteration 25607 : loss : 0.188544, loss_ce: 0.006108, loss_dice: 0.370981
[11:54:54.192] TRAIN: iteration 25608 : loss : 0.064432, loss_ce: 0.002237, loss_dice: 0.126627
[11:54:54.402] TRAIN: iteration 25609 : loss : 0.035811, loss_ce: 0.001359, loss_dice: 0.070263
[11:54:54.609] TRAIN: iteration 25610 : loss : 0.027238, loss_ce: 0.002178, loss_dice: 0.052299
[11:54:54.961] TRAIN: iteration 25611 : loss : 0.250252, loss_ce: 0.000497, loss_dice: 0.500008
[11:54:55.171] TRAIN: iteration 25612 : loss : 0.077154, loss_ce: 0.003001, loss_dice: 0.151307
[11:54:55.379] TRAIN: iteration 25613 : loss : 0.119378, loss_ce: 0.002442, loss_dice: 0.236313
[11:54:57.623] TRAIN: iteration 25614 : loss : 0.204464, loss_ce: 0.023072, loss_dice: 0.385857
[11:54:57.831] TRAIN: iteration 25615 : loss : 0.034832, loss_ce: 0.005399, loss_dice: 0.064265
[11:54:58.104] TRAIN: iteration 25616 : loss : 0.073600, loss_ce: 0.001385, loss_dice: 0.145815
[11:54:58.314] TRAIN: iteration 25617 : loss : 0.251440, loss_ce: 0.003034, loss_dice: 0.499847
[11:54:58.524] TRAIN: iteration 25618 : loss : 0.251029, loss_ce: 0.001926, loss_dice: 0.500133
[11:54:58.835] TRAIN: iteration 25619 : loss : 0.250244, loss_ce: 0.000481, loss_dice: 0.500006
[11:54:59.046] TRAIN: iteration 25620 : loss : 0.082193, loss_ce: 0.007139, loss_dice: 0.157248
[11:54:59.285] TRAIN: iteration 25621 : loss : 0.042062, loss_ce: 0.001768, loss_dice: 0.082355
[11:55:01.507] TRAIN: iteration 25622 : loss : 0.250385, loss_ce: 0.000741, loss_dice: 0.500030
[11:55:01.720] TRAIN: iteration 25623 : loss : 0.009399, loss_ce: 0.000958, loss_dice: 0.017839
[11:55:01.928] TRAIN: iteration 25624 : loss : 0.230659, loss_ce: 0.000951, loss_dice: 0.460367
[11:55:02.140] TRAIN: iteration 25625 : loss : 0.097574, loss_ce: 0.003063, loss_dice: 0.192086
[11:55:02.348] TRAIN: iteration 25626 : loss : 0.229096, loss_ce: 0.005823, loss_dice: 0.452369
[11:55:02.558] TRAIN: iteration 25627 : loss : 0.075077, loss_ce: 0.002737, loss_dice: 0.147416
[11:55:02.768] TRAIN: iteration 25628 : loss : 0.080869, loss_ce: 0.006192, loss_dice: 0.155546
[11:55:02.980] TRAIN: iteration 25629 : loss : 0.094011, loss_ce: 0.011050, loss_dice: 0.176973
[11:55:05.439] TRAIN: iteration 25630 : loss : 0.064898, loss_ce: 0.001922, loss_dice: 0.127875
[11:55:05.647] TRAIN: iteration 25631 : loss : 0.063795, loss_ce: 0.002657, loss_dice: 0.124932
[11:55:06.532] TRAIN: iteration 25632 : loss : 0.098966, loss_ce: 0.002373, loss_dice: 0.195560
[11:55:06.740] TRAIN: iteration 25633 : loss : 0.050373, loss_ce: 0.001892, loss_dice: 0.098854
[11:55:06.947] TRAIN: iteration 25634 : loss : 0.155667, loss_ce: 0.003300, loss_dice: 0.308034
[11:55:07.156] TRAIN: iteration 25635 : loss : 0.112798, loss_ce: 0.003622, loss_dice: 0.221975
[11:55:07.365] TRAIN: iteration 25636 : loss : 0.068827, loss_ce: 0.003461, loss_dice: 0.134192
[11:55:07.573] TRAIN: iteration 25637 : loss : 0.119381, loss_ce: 0.024624, loss_dice: 0.214139
[11:55:07.782] TRAIN: iteration 25638 : loss : 0.055941, loss_ce: 0.003975, loss_dice: 0.107907
[11:55:07.990] TRAIN: iteration 25639 : loss : 0.240410, loss_ce: 0.007248, loss_dice: 0.473571
[11:55:08.421] TRAIN: iteration 25640 : loss : 0.053830, loss_ce: 0.002541, loss_dice: 0.105120
[11:55:08.660] TRAIN: iteration 25641 : loss : 0.233488, loss_ce: 0.003651, loss_dice: 0.463324
[11:55:08.868] TRAIN: iteration 25642 : loss : 0.238734, loss_ce: 0.003775, loss_dice: 0.473692
[11:55:09.075] TRAIN: iteration 25643 : loss : 0.122110, loss_ce: 0.002656, loss_dice: 0.241564
[11:55:09.283] TRAIN: iteration 25644 : loss : 0.063313, loss_ce: 0.004000, loss_dice: 0.122626
[11:55:09.492] TRAIN: iteration 25645 : loss : 0.092343, loss_ce: 0.003811, loss_dice: 0.180875
[11:55:10.321] TRAIN: iteration 25646 : loss : 0.246385, loss_ce: 0.001850, loss_dice: 0.490920
[11:55:10.530] TRAIN: iteration 25647 : loss : 0.251457, loss_ce: 0.002736, loss_dice: 0.500179
[11:55:13.727] TRAIN: iteration 25648 : loss : 0.076009, loss_ce: 0.004069, loss_dice: 0.147949
[11:55:13.936] TRAIN: iteration 25649 : loss : 0.030251, loss_ce: 0.002318, loss_dice: 0.058185
[11:55:14.145] TRAIN: iteration 25650 : loss : 0.018362, loss_ce: 0.001688, loss_dice: 0.035036
[11:55:14.359] TRAIN: iteration 25651 : loss : 0.086018, loss_ce: 0.002096, loss_dice: 0.169941
[11:55:14.566] TRAIN: iteration 25652 : loss : 0.050160, loss_ce: 0.001902, loss_dice: 0.098419
[11:55:14.774] TRAIN: iteration 25653 : loss : 0.099725, loss_ce: 0.006088, loss_dice: 0.193361
[11:55:14.983] TRAIN: iteration 25654 : loss : 0.183735, loss_ce: 0.001985, loss_dice: 0.365486
[11:55:15.193] TRAIN: iteration 25655 : loss : 0.050469, loss_ce: 0.004367, loss_dice: 0.096571
[11:55:18.116] TRAIN: iteration 25656 : loss : 0.251061, loss_ce: 0.002021, loss_dice: 0.500100
[11:55:18.325] TRAIN: iteration 25657 : loss : 0.234888, loss_ce: 0.006535, loss_dice: 0.463240
[11:55:18.534] TRAIN: iteration 25658 : loss : 0.036913, loss_ce: 0.001014, loss_dice: 0.072812
[11:55:18.742] TRAIN: iteration 25659 : loss : 0.097051, loss_ce: 0.005367, loss_dice: 0.188734
[11:55:18.950] TRAIN: iteration 25660 : loss : 0.175499, loss_ce: 0.002530, loss_dice: 0.348468
[11:55:19.190] TRAIN: iteration 25661 : loss : 0.054628, loss_ce: 0.006163, loss_dice: 0.103093
[11:55:19.398] TRAIN: iteration 25662 : loss : 0.027853, loss_ce: 0.002091, loss_dice: 0.053615
[11:55:19.605] TRAIN: iteration 25663 : loss : 0.030510, loss_ce: 0.001344, loss_dice: 0.059676
[11:55:23.234] TRAIN: iteration 25664 : loss : 0.069237, loss_ce: 0.005562, loss_dice: 0.132911
[11:55:23.448] TRAIN: iteration 25665 : loss : 0.119772, loss_ce: 0.001225, loss_dice: 0.238319
[11:55:23.658] TRAIN: iteration 25666 : loss : 0.101425, loss_ce: 0.005031, loss_dice: 0.197819
[11:55:23.866] TRAIN: iteration 25667 : loss : 0.028069, loss_ce: 0.002055, loss_dice: 0.054082
[11:55:24.086] TRAIN: iteration 25668 : loss : 0.082827, loss_ce: 0.005787, loss_dice: 0.159867
[11:55:24.295] TRAIN: iteration 25669 : loss : 0.230864, loss_ce: 0.001677, loss_dice: 0.460050
[11:55:24.503] TRAIN: iteration 25670 : loss : 0.137890, loss_ce: 0.002937, loss_dice: 0.272844
[11:55:24.713] TRAIN: iteration 25671 : loss : 0.034526, loss_ce: 0.005728, loss_dice: 0.063323
[11:55:26.890] TRAIN: iteration 25672 : loss : 0.087379, loss_ce: 0.002395, loss_dice: 0.172364
[11:55:27.099] TRAIN: iteration 25673 : loss : 0.087263, loss_ce: 0.002361, loss_dice: 0.172164
[11:55:27.307] TRAIN: iteration 25674 : loss : 0.043444, loss_ce: 0.001628, loss_dice: 0.085260
[11:55:27.515] TRAIN: iteration 25675 : loss : 0.049754, loss_ce: 0.001493, loss_dice: 0.098014
[11:55:27.726] TRAIN: iteration 25676 : loss : 0.130891, loss_ce: 0.003131, loss_dice: 0.258651
[11:55:27.934] TRAIN: iteration 25677 : loss : 0.160131, loss_ce: 0.001651, loss_dice: 0.318611
[11:55:28.144] TRAIN: iteration 25678 : loss : 0.186631, loss_ce: 0.003120, loss_dice: 0.370142
[11:55:28.352] TRAIN: iteration 25679 : loss : 0.065551, loss_ce: 0.001286, loss_dice: 0.129817
[11:55:31.340] TRAIN: iteration 25680 : loss : 0.037307, loss_ce: 0.001059, loss_dice: 0.073556
[11:55:31.573] TRAIN: iteration 25681 : loss : 0.042771, loss_ce: 0.005123, loss_dice: 0.080420
[11:55:31.782] TRAIN: iteration 25682 : loss : 0.153930, loss_ce: 0.001046, loss_dice: 0.306815
[11:55:31.989] TRAIN: iteration 25683 : loss : 0.250713, loss_ce: 0.001344, loss_dice: 0.500083
[11:55:32.197] TRAIN: iteration 25684 : loss : 0.158808, loss_ce: 0.003237, loss_dice: 0.314379
[11:55:32.406] TRAIN: iteration 25685 : loss : 0.243195, loss_ce: 0.011608, loss_dice: 0.474782
[11:55:32.614] TRAIN: iteration 25686 : loss : 0.169864, loss_ce: 0.003938, loss_dice: 0.335791
[11:55:32.821] TRAIN: iteration 25687 : loss : 0.105062, loss_ce: 0.001926, loss_dice: 0.208198
[11:55:36.026] TRAIN: iteration 25688 : loss : 0.117968, loss_ce: 0.001688, loss_dice: 0.234248
[11:55:36.235] TRAIN: iteration 25689 : loss : 0.030463, loss_ce: 0.001764, loss_dice: 0.059162
[11:55:36.445] TRAIN: iteration 25690 : loss : 0.110372, loss_ce: 0.003767, loss_dice: 0.216978
[11:55:36.653] TRAIN: iteration 25691 : loss : 0.039575, loss_ce: 0.000811, loss_dice: 0.078340
[11:55:36.892] TRAIN: iteration 25692 : loss : 0.094116, loss_ce: 0.002163, loss_dice: 0.186069
[11:55:37.104] TRAIN: iteration 25693 : loss : 0.067305, loss_ce: 0.003090, loss_dice: 0.131521
[11:55:37.315] TRAIN: iteration 25694 : loss : 0.199579, loss_ce: 0.002968, loss_dice: 0.396191
[11:55:37.523] TRAIN: iteration 25695 : loss : 0.104648, loss_ce: 0.002164, loss_dice: 0.207133
[11:55:39.634] TRAIN: iteration 25696 : loss : 0.191691, loss_ce: 0.003281, loss_dice: 0.380101
[11:55:39.842] TRAIN: iteration 25697 : loss : 0.076059, loss_ce: 0.002306, loss_dice: 0.149811
[11:55:40.051] TRAIN: iteration 25698 : loss : 0.053128, loss_ce: 0.005247, loss_dice: 0.101009
[11:55:40.260] TRAIN: iteration 25699 : loss : 0.207828, loss_ce: 0.001908, loss_dice: 0.413749
[11:55:40.468] TRAIN: iteration 25700 : loss : 0.037068, loss_ce: 0.001000, loss_dice: 0.073137
[11:55:40.709] TRAIN: iteration 25701 : loss : 0.129752, loss_ce: 0.002754, loss_dice: 0.256750
[11:55:40.917] TRAIN: iteration 25702 : loss : 0.091232, loss_ce: 0.004047, loss_dice: 0.178418
[11:55:41.129] TRAIN: iteration 25703 : loss : 0.251809, loss_ce: 0.011707, loss_dice: 0.491911
[11:55:43.611] TRAIN: iteration 25704 : loss : 0.242544, loss_ce: 0.004221, loss_dice: 0.480867
[11:55:43.820] TRAIN: iteration 25705 : loss : 0.250550, loss_ce: 0.001065, loss_dice: 0.500034
[11:55:44.029] TRAIN: iteration 25706 : loss : 0.152477, loss_ce: 0.001719, loss_dice: 0.303235
[11:55:44.297] TRAIN: iteration 25707 : loss : 0.056880, loss_ce: 0.001733, loss_dice: 0.112026
[11:55:44.506] TRAIN: iteration 25708 : loss : 0.119017, loss_ce: 0.004248, loss_dice: 0.233786
[11:55:44.714] TRAIN: iteration 25709 : loss : 0.118276, loss_ce: 0.002376, loss_dice: 0.234177
[11:55:44.922] TRAIN: iteration 25710 : loss : 0.135371, loss_ce: 0.001343, loss_dice: 0.269398
[11:55:45.135] TRAIN: iteration 25711 : loss : 0.065701, loss_ce: 0.001720, loss_dice: 0.129682
[11:55:48.942] TRAIN: iteration 25712 : loss : 0.090810, loss_ce: 0.005522, loss_dice: 0.176099
[11:55:49.151] TRAIN: iteration 25713 : loss : 0.132232, loss_ce: 0.001826, loss_dice: 0.262639
[11:55:49.360] TRAIN: iteration 25714 : loss : 0.053622, loss_ce: 0.001682, loss_dice: 0.105562
[11:55:49.569] TRAIN: iteration 25715 : loss : 0.069626, loss_ce: 0.002134, loss_dice: 0.137118
[11:55:49.778] TRAIN: iteration 25716 : loss : 0.055185, loss_ce: 0.000734, loss_dice: 0.109636
[11:55:49.988] TRAIN: iteration 25717 : loss : 0.168953, loss_ce: 0.002341, loss_dice: 0.335565
[11:55:50.199] TRAIN: iteration 25718 : loss : 0.120462, loss_ce: 0.008139, loss_dice: 0.232784
[11:55:50.407] TRAIN: iteration 25719 : loss : 0.122614, loss_ce: 0.001651, loss_dice: 0.243577
[11:55:54.374] TRAIN: iteration 25720 : loss : 0.079005, loss_ce: 0.001712, loss_dice: 0.156298
[11:55:54.611] TRAIN: iteration 25721 : loss : 0.038531, loss_ce: 0.001261, loss_dice: 0.075801
[11:55:54.819] TRAIN: iteration 25722 : loss : 0.168781, loss_ce: 0.008319, loss_dice: 0.329243
[11:55:55.027] TRAIN: iteration 25723 : loss : 0.154993, loss_ce: 0.003123, loss_dice: 0.306863
[11:55:55.237] TRAIN: iteration 25724 : loss : 0.046028, loss_ce: 0.005684, loss_dice: 0.086372
[11:55:55.445] TRAIN: iteration 25725 : loss : 0.117227, loss_ce: 0.006252, loss_dice: 0.228201
[11:55:55.661] TRAIN: iteration 25726 : loss : 0.131814, loss_ce: 0.001722, loss_dice: 0.261905
[11:55:55.868] TRAIN: iteration 25727 : loss : 0.163915, loss_ce: 0.002916, loss_dice: 0.324914
[11:55:57.354] TRAIN: iteration 25728 : loss : 0.034081, loss_ce: 0.003066, loss_dice: 0.065097
[11:55:57.568] TRAIN: iteration 25729 : loss : 0.102749, loss_ce: 0.003713, loss_dice: 0.201785
[11:55:57.775] TRAIN: iteration 25730 : loss : 0.089286, loss_ce: 0.003343, loss_dice: 0.175228
[11:55:57.983] TRAIN: iteration 25731 : loss : 0.066111, loss_ce: 0.001782, loss_dice: 0.130440
[11:55:58.192] TRAIN: iteration 25732 : loss : 0.060416, loss_ce: 0.003177, loss_dice: 0.117654
[11:55:58.400] TRAIN: iteration 25733 : loss : 0.073774, loss_ce: 0.001832, loss_dice: 0.145715
[11:55:58.607] TRAIN: iteration 25734 : loss : 0.070633, loss_ce: 0.004132, loss_dice: 0.137134
[11:55:58.817] TRAIN: iteration 25735 : loss : 0.250785, loss_ce: 0.001475, loss_dice: 0.500094
[11:56:03.748] TRAIN: iteration 25736 : loss : 0.033161, loss_ce: 0.001848, loss_dice: 0.064473
[11:56:03.956] TRAIN: iteration 25737 : loss : 0.250518, loss_ce: 0.000979, loss_dice: 0.500056
[11:56:04.163] TRAIN: iteration 25738 : loss : 0.029555, loss_ce: 0.002711, loss_dice: 0.056400
[11:56:04.372] TRAIN: iteration 25739 : loss : 0.060060, loss_ce: 0.002347, loss_dice: 0.117774
[11:56:04.580] TRAIN: iteration 25740 : loss : 0.250782, loss_ce: 0.001862, loss_dice: 0.499702
[11:56:04.827] TRAIN: iteration 25741 : loss : 0.169265, loss_ce: 0.016546, loss_dice: 0.321985
[11:56:05.037] TRAIN: iteration 25742 : loss : 0.047031, loss_ce: 0.006323, loss_dice: 0.087740
[11:56:05.252] TRAIN: iteration 25743 : loss : 0.086016, loss_ce: 0.007166, loss_dice: 0.164866
[11:56:08.606] TRAIN: iteration 25744 : loss : 0.225626, loss_ce: 0.002136, loss_dice: 0.449117
[11:56:08.812] TRAIN: iteration 25745 : loss : 0.039058, loss_ce: 0.001406, loss_dice: 0.076710
[11:56:09.023] TRAIN: iteration 25746 : loss : 0.250995, loss_ce: 0.001867, loss_dice: 0.500122
[11:56:09.232] TRAIN: iteration 25747 : loss : 0.120717, loss_ce: 0.003981, loss_dice: 0.237453
[11:56:09.439] TRAIN: iteration 25748 : loss : 0.249797, loss_ce: 0.003114, loss_dice: 0.496480
[11:56:09.648] TRAIN: iteration 25749 : loss : 0.157220, loss_ce: 0.009462, loss_dice: 0.304979
[11:56:09.855] TRAIN: iteration 25750 : loss : 0.220555, loss_ce: 0.002016, loss_dice: 0.439094
[11:56:10.065] TRAIN: iteration 25751 : loss : 0.063847, loss_ce: 0.002562, loss_dice: 0.125132
[11:56:11.122] TRAIN: iteration 25752 : loss : 0.114655, loss_ce: 0.004883, loss_dice: 0.224428
[11:56:13.423] TRAIN: iteration 25753 : loss : 0.074680, loss_ce: 0.005101, loss_dice: 0.144258
[11:56:13.635] TRAIN: iteration 25754 : loss : 0.112194, loss_ce: 0.006344, loss_dice: 0.218045
[11:56:13.847] TRAIN: iteration 25755 : loss : 0.241296, loss_ce: 0.002246, loss_dice: 0.480346
[11:56:14.055] TRAIN: iteration 25756 : loss : 0.196476, loss_ce: 0.022638, loss_dice: 0.370313
[11:56:14.263] TRAIN: iteration 25757 : loss : 0.074049, loss_ce: 0.002329, loss_dice: 0.145770
[11:56:14.471] TRAIN: iteration 25758 : loss : 0.052614, loss_ce: 0.000855, loss_dice: 0.104374
[11:56:14.679] TRAIN: iteration 25759 : loss : 0.251092, loss_ce: 0.002048, loss_dice: 0.500136
[11:56:15.607] TRAIN: iteration 25760 : loss : 0.092196, loss_ce: 0.002337, loss_dice: 0.182054
[11:56:16.945] TRAIN: iteration 25761 : loss : 0.099729, loss_ce: 0.006020, loss_dice: 0.193437
[11:56:17.154] TRAIN: iteration 25762 : loss : 0.139890, loss_ce: 0.001510, loss_dice: 0.278269
[11:56:17.364] TRAIN: iteration 25763 : loss : 0.022852, loss_ce: 0.001392, loss_dice: 0.044312
[11:56:17.572] TRAIN: iteration 25764 : loss : 0.045377, loss_ce: 0.002219, loss_dice: 0.088535
[11:56:17.780] TRAIN: iteration 25765 : loss : 0.189024, loss_ce: 0.014105, loss_dice: 0.363943
[11:56:17.990] TRAIN: iteration 25766 : loss : 0.192082, loss_ce: 0.004192, loss_dice: 0.379972
[11:56:18.198] TRAIN: iteration 25767 : loss : 0.063384, loss_ce: 0.004904, loss_dice: 0.121863
[11:56:20.923] TRAIN: iteration 25768 : loss : 0.156327, loss_ce: 0.017616, loss_dice: 0.295039
[11:56:21.134] TRAIN: iteration 25769 : loss : 0.069083, loss_ce: 0.002353, loss_dice: 0.135812
[11:56:21.742] TRAIN: iteration 25770 : loss : 0.250787, loss_ce: 0.001505, loss_dice: 0.500070
[11:56:21.957] TRAIN: iteration 25771 : loss : 0.041820, loss_ce: 0.005251, loss_dice: 0.078389
[11:56:22.168] TRAIN: iteration 25772 : loss : 0.226880, loss_ce: 0.006290, loss_dice: 0.447469
[11:56:22.377] TRAIN: iteration 25773 : loss : 0.051132, loss_ce: 0.005324, loss_dice: 0.096940
[11:56:22.585] TRAIN: iteration 25774 : loss : 0.051114, loss_ce: 0.001059, loss_dice: 0.101169
[11:56:22.795] TRAIN: iteration 25775 : loss : 0.138323, loss_ce: 0.005450, loss_dice: 0.271195
[11:56:24.857] TRAIN: iteration 25776 : loss : 0.251034, loss_ce: 0.001960, loss_dice: 0.500109
[11:56:25.745] TRAIN: iteration 25777 : loss : 0.032718, loss_ce: 0.001482, loss_dice: 0.063955
[11:56:26.145] TRAIN: iteration 25778 : loss : 0.054656, loss_ce: 0.006650, loss_dice: 0.102661
[11:56:26.352] TRAIN: iteration 25779 : loss : 0.251284, loss_ce: 0.002421, loss_dice: 0.500148
[11:56:26.560] TRAIN: iteration 25780 : loss : 0.228841, loss_ce: 0.003808, loss_dice: 0.453873
[11:56:26.796] TRAIN: iteration 25781 : loss : 0.251317, loss_ce: 0.002457, loss_dice: 0.500176
[11:56:28.825] TRAIN: iteration 25782 : loss : 0.089939, loss_ce: 0.004348, loss_dice: 0.175530
[11:56:29.039] TRAIN: iteration 25783 : loss : 0.045726, loss_ce: 0.001541, loss_dice: 0.089911
[11:56:31.763] TRAIN: iteration 25784 : loss : 0.172139, loss_ce: 0.003236, loss_dice: 0.341041
[11:56:31.971] TRAIN: iteration 25785 : loss : 0.026749, loss_ce: 0.005561, loss_dice: 0.047937
[11:56:32.179] TRAIN: iteration 25786 : loss : 0.065035, loss_ce: 0.001978, loss_dice: 0.128092
[11:56:32.388] TRAIN: iteration 25787 : loss : 0.252111, loss_ce: 0.003952, loss_dice: 0.500269
[11:56:32.596] TRAIN: iteration 25788 : loss : 0.040369, loss_ce: 0.003982, loss_dice: 0.076757
[11:56:33.889] TRAIN: iteration 25789 : loss : 0.036283, loss_ce: 0.003110, loss_dice: 0.069456
[11:56:34.097] TRAIN: iteration 25790 : loss : 0.252162, loss_ce: 0.007354, loss_dice: 0.496971
[11:56:34.311] TRAIN: iteration 25791 : loss : 0.236833, loss_ce: 0.005317, loss_dice: 0.468349
[11:56:34.520] TRAIN: iteration 25792 : loss : 0.116265, loss_ce: 0.003163, loss_dice: 0.229367
[11:56:34.729] TRAIN: iteration 25793 : loss : 0.250462, loss_ce: 0.000904, loss_dice: 0.500019
[11:56:36.491] TRAIN: iteration 25794 : loss : 0.079882, loss_ce: 0.004198, loss_dice: 0.155567
[11:56:37.389] TRAIN: iteration 25795 : loss : 0.043953, loss_ce: 0.007823, loss_dice: 0.080083
[11:56:37.596] TRAIN: iteration 25796 : loss : 0.058479, loss_ce: 0.003106, loss_dice: 0.113852
[11:56:39.592] TRAIN: iteration 25797 : loss : 0.124261, loss_ce: 0.022587, loss_dice: 0.225935
[11:56:39.800] TRAIN: iteration 25798 : loss : 0.042803, loss_ce: 0.002510, loss_dice: 0.083097
[11:56:40.008] TRAIN: iteration 25799 : loss : 0.049981, loss_ce: 0.001169, loss_dice: 0.098793
[11:56:41.316] TRAIN: iteration 25800 : loss : 0.151180, loss_ce: 0.004610, loss_dice: 0.297749
[11:56:41.550] TRAIN: iteration 25801 : loss : 0.049230, loss_ce: 0.003937, loss_dice: 0.094523
[11:56:43.345] TRAIN: iteration 25802 : loss : 0.250954, loss_ce: 0.001827, loss_dice: 0.500081
[11:56:43.552] TRAIN: iteration 25803 : loss : 0.069057, loss_ce: 0.001788, loss_dice: 0.136325
[11:56:43.762] TRAIN: iteration 25804 : loss : 0.038936, loss_ce: 0.005026, loss_dice: 0.072846
[11:56:44.481] TRAIN: iteration 25805 : loss : 0.206186, loss_ce: 0.011491, loss_dice: 0.400882
[11:56:44.732] TRAIN: iteration 25806 : loss : 0.098287, loss_ce: 0.002745, loss_dice: 0.193829
[11:56:44.940] TRAIN: iteration 25807 : loss : 0.247313, loss_ce: 0.006952, loss_dice: 0.487673
[11:56:47.481] TRAIN: iteration 25808 : loss : 0.063329, loss_ce: 0.002896, loss_dice: 0.123762
[11:56:47.690] TRAIN: iteration 25809 : loss : 0.026774, loss_ce: 0.002341, loss_dice: 0.051207
[11:56:50.576] TRAIN: iteration 25810 : loss : 0.106086, loss_ce: 0.001754, loss_dice: 0.210418
[11:56:50.789] TRAIN: iteration 25811 : loss : 0.250733, loss_ce: 0.001403, loss_dice: 0.500063
[11:56:50.997] TRAIN: iteration 25812 : loss : 0.163906, loss_ce: 0.005568, loss_dice: 0.322244
[11:56:51.206] TRAIN: iteration 25813 : loss : 0.060944, loss_ce: 0.002613, loss_dice: 0.119275
[11:56:51.415] TRAIN: iteration 25814 : loss : 0.122786, loss_ce: 0.003457, loss_dice: 0.242116
[11:56:51.624] TRAIN: iteration 25815 : loss : 0.154538, loss_ce: 0.003180, loss_dice: 0.305897
[11:56:53.552] TRAIN: iteration 25816 : loss : 0.079982, loss_ce: 0.002911, loss_dice: 0.157052
[11:56:53.761] TRAIN: iteration 25817 : loss : 0.244697, loss_ce: 0.003460, loss_dice: 0.485935
[11:56:55.986] TRAIN: iteration 25818 : loss : 0.182447, loss_ce: 0.002073, loss_dice: 0.362821
[11:56:56.200] TRAIN: iteration 25819 : loss : 0.038912, loss_ce: 0.003895, loss_dice: 0.073928
[11:56:56.409] TRAIN: iteration 25820 : loss : 0.250570, loss_ce: 0.001104, loss_dice: 0.500036
[11:56:56.646] TRAIN: iteration 25821 : loss : 0.089662, loss_ce: 0.005416, loss_dice: 0.173907
[11:56:57.060] TRAIN: iteration 25822 : loss : 0.062579, loss_ce: 0.005733, loss_dice: 0.119425
[11:56:57.273] TRAIN: iteration 25823 : loss : 0.161922, loss_ce: 0.005857, loss_dice: 0.317986
[11:57:00.004] TRAIN: iteration 25824 : loss : 0.041496, loss_ce: 0.005254, loss_dice: 0.077739
[11:57:00.218] TRAIN: iteration 25825 : loss : 0.141917, loss_ce: 0.008774, loss_dice: 0.275059
[11:57:01.254] TRAIN: iteration 25826 : loss : 0.251633, loss_ce: 0.003047, loss_dice: 0.500219
[11:57:01.462] TRAIN: iteration 25827 : loss : 0.168917, loss_ce: 0.009492, loss_dice: 0.328343
[11:57:01.670] TRAIN: iteration 25828 : loss : 0.251055, loss_ce: 0.002003, loss_dice: 0.500106
[11:57:01.878] TRAIN: iteration 25829 : loss : 0.169156, loss_ce: 0.005556, loss_dice: 0.332757
[11:57:03.661] TRAIN: iteration 25830 : loss : 0.220111, loss_ce: 0.003472, loss_dice: 0.436750
[11:57:03.869] TRAIN: iteration 25831 : loss : 0.135327, loss_ce: 0.007067, loss_dice: 0.263587
[11:57:08.880] TRAIN: iteration 25832 : loss : 0.169061, loss_ce: 0.002229, loss_dice: 0.335894
[11:57:09.095] TRAIN: iteration 25833 : loss : 0.125510, loss_ce: 0.001776, loss_dice: 0.249244
[11:57:09.302] TRAIN: iteration 25834 : loss : 0.030660, loss_ce: 0.001144, loss_dice: 0.060175
[11:57:09.510] TRAIN: iteration 25835 : loss : 0.061345, loss_ce: 0.008847, loss_dice: 0.113843
[11:57:09.718] TRAIN: iteration 25836 : loss : 0.144666, loss_ce: 0.009042, loss_dice: 0.280289
[11:57:09.925] TRAIN: iteration 25837 : loss : 0.051589, loss_ce: 0.003972, loss_dice: 0.099205
[11:57:10.133] TRAIN: iteration 25838 : loss : 0.072806, loss_ce: 0.005991, loss_dice: 0.139621
[11:57:10.340] TRAIN: iteration 25839 : loss : 0.113241, loss_ce: 0.002893, loss_dice: 0.223588
[11:57:14.998] TRAIN: iteration 25840 : loss : 0.251478, loss_ce: 0.002796, loss_dice: 0.500160
[11:57:15.234] TRAIN: iteration 25841 : loss : 0.088318, loss_ce: 0.000997, loss_dice: 0.175638
[11:57:16.249] TRAIN: iteration 25842 : loss : 0.081985, loss_ce: 0.003125, loss_dice: 0.160845
[11:57:16.457] TRAIN: iteration 25843 : loss : 0.050831, loss_ce: 0.003892, loss_dice: 0.097770
[11:57:16.665] TRAIN: iteration 25844 : loss : 0.068006, loss_ce: 0.002498, loss_dice: 0.133514
[11:57:16.874] TRAIN: iteration 25845 : loss : 0.179420, loss_ce: 0.002215, loss_dice: 0.356625
[11:57:17.084] TRAIN: iteration 25846 : loss : 0.049223, loss_ce: 0.005035, loss_dice: 0.093412
[11:57:17.293] TRAIN: iteration 25847 : loss : 0.252089, loss_ce: 0.003905, loss_dice: 0.500273
[11:57:21.963] TRAIN: iteration 25848 : loss : 0.057872, loss_ce: 0.003706, loss_dice: 0.112037
[11:57:22.176] TRAIN: iteration 25849 : loss : 0.018031, loss_ce: 0.001560, loss_dice: 0.034503
[11:57:22.385] TRAIN: iteration 25850 : loss : 0.047377, loss_ce: 0.001596, loss_dice: 0.093158
[11:57:22.594] TRAIN: iteration 25851 : loss : 0.042144, loss_ce: 0.001092, loss_dice: 0.083195
[11:57:22.803] TRAIN: iteration 25852 : loss : 0.195346, loss_ce: 0.001173, loss_dice: 0.389518
[11:57:23.015] TRAIN: iteration 25853 : loss : 0.106371, loss_ce: 0.006370, loss_dice: 0.206372
[11:57:23.226] TRAIN: iteration 25854 : loss : 0.013067, loss_ce: 0.001530, loss_dice: 0.024604
[11:57:23.435] TRAIN: iteration 25855 : loss : 0.057380, loss_ce: 0.009407, loss_dice: 0.105354
[11:57:27.404] TRAIN: iteration 25856 : loss : 0.116805, loss_ce: 0.009213, loss_dice: 0.224398
[11:57:27.614] TRAIN: iteration 25857 : loss : 0.046176, loss_ce: 0.001644, loss_dice: 0.090708
[11:57:29.984] TRAIN: iteration 25858 : loss : 0.187649, loss_ce: 0.001850, loss_dice: 0.373449
[11:57:30.202] TRAIN: iteration 25859 : loss : 0.017619, loss_ce: 0.001216, loss_dice: 0.034021
[11:57:30.411] TRAIN: iteration 25860 : loss : 0.083348, loss_ce: 0.002741, loss_dice: 0.163955
[11:57:30.662] TRAIN: iteration 25861 : loss : 0.083088, loss_ce: 0.002264, loss_dice: 0.163911
[11:57:30.869] TRAIN: iteration 25862 : loss : 0.250997, loss_ce: 0.001876, loss_dice: 0.500118
[11:57:31.082] TRAIN: iteration 25863 : loss : 0.032459, loss_ce: 0.006065, loss_dice: 0.058854
[11:57:35.117] TRAIN: iteration 25864 : loss : 0.250598, loss_ce: 0.001148, loss_dice: 0.500049
[11:57:35.325] TRAIN: iteration 25865 : loss : 0.088927, loss_ce: 0.003062, loss_dice: 0.174792
[11:57:35.533] TRAIN: iteration 25866 : loss : 0.087689, loss_ce: 0.003754, loss_dice: 0.171624
[11:57:35.741] TRAIN: iteration 25867 : loss : 0.250578, loss_ce: 0.001104, loss_dice: 0.500051
[11:57:35.951] TRAIN: iteration 25868 : loss : 0.226837, loss_ce: 0.010127, loss_dice: 0.443548
[11:57:36.159] TRAIN: iteration 25869 : loss : 0.120144, loss_ce: 0.002637, loss_dice: 0.237651
[11:57:36.366] TRAIN: iteration 25870 : loss : 0.077145, loss_ce: 0.001957, loss_dice: 0.152332
[11:57:36.581] TRAIN: iteration 25871 : loss : 0.041250, loss_ce: 0.001098, loss_dice: 0.081402
[11:57:43.873] TRAIN: iteration 25872 : loss : 0.083070, loss_ce: 0.002395, loss_dice: 0.163745
[11:57:44.088] TRAIN: iteration 25873 : loss : 0.249717, loss_ce: 0.000724, loss_dice: 0.498709
[11:57:44.295] TRAIN: iteration 25874 : loss : 0.250832, loss_ce: 0.001661, loss_dice: 0.500003
[11:57:44.502] TRAIN: iteration 25875 : loss : 0.047102, loss_ce: 0.000947, loss_dice: 0.093257
[11:57:44.710] TRAIN: iteration 25876 : loss : 0.198806, loss_ce: 0.006982, loss_dice: 0.390630
[11:57:44.918] TRAIN: iteration 25877 : loss : 0.089804, loss_ce: 0.002923, loss_dice: 0.176686
[11:57:45.125] TRAIN: iteration 25878 : loss : 0.073191, loss_ce: 0.001772, loss_dice: 0.144611
[11:57:45.333] TRAIN: iteration 25879 : loss : 0.110986, loss_ce: 0.002803, loss_dice: 0.219168
[11:57:51.553] TRAIN: iteration 25880 : loss : 0.111400, loss_ce: 0.000916, loss_dice: 0.221883
[11:57:51.790] TRAIN: iteration 25881 : loss : 0.061953, loss_ce: 0.005971, loss_dice: 0.117934
[11:57:51.997] TRAIN: iteration 25882 : loss : 0.059799, loss_ce: 0.004088, loss_dice: 0.115510
[11:57:52.206] TRAIN: iteration 25883 : loss : 0.063819, loss_ce: 0.004277, loss_dice: 0.123361
[11:57:52.413] TRAIN: iteration 25884 : loss : 0.113743, loss_ce: 0.007870, loss_dice: 0.219617
[11:57:52.623] TRAIN: iteration 25885 : loss : 0.136351, loss_ce: 0.002609, loss_dice: 0.270093
[11:57:52.830] TRAIN: iteration 25886 : loss : 0.217380, loss_ce: 0.002232, loss_dice: 0.432528
[11:57:53.045] TRAIN: iteration 25887 : loss : 0.081627, loss_ce: 0.004361, loss_dice: 0.158893
[11:57:58.587] TRAIN: iteration 25888 : loss : 0.178476, loss_ce: 0.002767, loss_dice: 0.354185
[11:57:58.796] TRAIN: iteration 25889 : loss : 0.025131, loss_ce: 0.002255, loss_dice: 0.048007
[11:57:59.005] TRAIN: iteration 25890 : loss : 0.194466, loss_ce: 0.003057, loss_dice: 0.385875
[11:57:59.215] TRAIN: iteration 25891 : loss : 0.022045, loss_ce: 0.000622, loss_dice: 0.043468
[11:57:59.424] TRAIN: iteration 25892 : loss : 0.111160, loss_ce: 0.003028, loss_dice: 0.219292
[11:57:59.639] TRAIN: iteration 25893 : loss : 0.030520, loss_ce: 0.002489, loss_dice: 0.058550
[11:57:59.847] TRAIN: iteration 25894 : loss : 0.060076, loss_ce: 0.008537, loss_dice: 0.111615
[11:58:00.056] TRAIN: iteration 25895 : loss : 0.204460, loss_ce: 0.003791, loss_dice: 0.405128
[11:58:07.274] TRAIN: iteration 25896 : loss : 0.047758, loss_ce: 0.003776, loss_dice: 0.091740
[11:58:07.485] TRAIN: iteration 25897 : loss : 0.051259, loss_ce: 0.004327, loss_dice: 0.098192
[11:58:07.694] TRAIN: iteration 25898 : loss : 0.101748, loss_ce: 0.010734, loss_dice: 0.192762
[11:58:07.902] TRAIN: iteration 25899 : loss : 0.251272, loss_ce: 0.003609, loss_dice: 0.498936
[11:58:08.110] TRAIN: iteration 25900 : loss : 0.073756, loss_ce: 0.005567, loss_dice: 0.141946
[11:58:08.350] TRAIN: iteration 25901 : loss : 0.213195, loss_ce: 0.003431, loss_dice: 0.422960
[11:58:08.557] TRAIN: iteration 25902 : loss : 0.251218, loss_ce: 0.002293, loss_dice: 0.500143
[11:58:08.766] TRAIN: iteration 25903 : loss : 0.023875, loss_ce: 0.001991, loss_dice: 0.045759
[11:58:15.499] TRAIN: iteration 25904 : loss : 0.251128, loss_ce: 0.002124, loss_dice: 0.500131
[11:58:15.707] TRAIN: iteration 25905 : loss : 0.115497, loss_ce: 0.001524, loss_dice: 0.229470
[11:58:15.916] TRAIN: iteration 25906 : loss : 0.145824, loss_ce: 0.003831, loss_dice: 0.287817
[11:58:16.123] TRAIN: iteration 25907 : loss : 0.078258, loss_ce: 0.004740, loss_dice: 0.151777
[11:58:16.338] TRAIN: iteration 25908 : loss : 0.060037, loss_ce: 0.002557, loss_dice: 0.117517
[11:58:16.549] TRAIN: iteration 25909 : loss : 0.144788, loss_ce: 0.004761, loss_dice: 0.284814
[11:58:16.758] TRAIN: iteration 25910 : loss : 0.086223, loss_ce: 0.001152, loss_dice: 0.171294
[11:58:16.967] TRAIN: iteration 25911 : loss : 0.124911, loss_ce: 0.010729, loss_dice: 0.239093
[11:58:21.628] TRAIN: iteration 25912 : loss : 0.150729, loss_ce: 0.004350, loss_dice: 0.297109
[11:58:21.836] TRAIN: iteration 25913 : loss : 0.059810, loss_ce: 0.002089, loss_dice: 0.117532
[11:58:22.043] TRAIN: iteration 25914 : loss : 0.250916, loss_ce: 0.001729, loss_dice: 0.500103
[11:58:22.251] TRAIN: iteration 25915 : loss : 0.051037, loss_ce: 0.001986, loss_dice: 0.100088
[11:58:22.460] TRAIN: iteration 25916 : loss : 0.061879, loss_ce: 0.002050, loss_dice: 0.121709
[11:58:22.668] TRAIN: iteration 25917 : loss : 0.073092, loss_ce: 0.002812, loss_dice: 0.143373
[11:58:22.876] TRAIN: iteration 25918 : loss : 0.250864, loss_ce: 0.001638, loss_dice: 0.500089
[11:58:23.086] TRAIN: iteration 25919 : loss : 0.061883, loss_ce: 0.002782, loss_dice: 0.120984
[11:58:29.807] TRAIN: iteration 25920 : loss : 0.104844, loss_ce: 0.001772, loss_dice: 0.207916
[11:58:30.043] TRAIN: iteration 25921 : loss : 0.070454, loss_ce: 0.004756, loss_dice: 0.136152
[11:58:30.857] TRAIN: iteration 25922 : loss : 0.251450, loss_ce: 0.002723, loss_dice: 0.500176
[11:58:31.065] TRAIN: iteration 25923 : loss : 0.160006, loss_ce: 0.005915, loss_dice: 0.314097
[11:58:31.274] TRAIN: iteration 25924 : loss : 0.059428, loss_ce: 0.005608, loss_dice: 0.113247
[11:58:31.487] TRAIN: iteration 25925 : loss : 0.128889, loss_ce: 0.005865, loss_dice: 0.251913
[11:58:31.697] TRAIN: iteration 25926 : loss : 0.160110, loss_ce: 0.020194, loss_dice: 0.300026
[11:58:31.906] TRAIN: iteration 25927 : loss : 0.021360, loss_ce: 0.001753, loss_dice: 0.040967
[11:58:36.852] TRAIN: iteration 25928 : loss : 0.125652, loss_ce: 0.006154, loss_dice: 0.245150
[11:58:37.060] TRAIN: iteration 25929 : loss : 0.046837, loss_ce: 0.002751, loss_dice: 0.090923
[11:58:38.207] TRAIN: iteration 25930 : loss : 0.045097, loss_ce: 0.002125, loss_dice: 0.088069
[11:58:38.415] TRAIN: iteration 25931 : loss : 0.084525, loss_ce: 0.003361, loss_dice: 0.165689
[11:58:38.623] TRAIN: iteration 25932 : loss : 0.217178, loss_ce: 0.001946, loss_dice: 0.432410
[11:58:38.831] TRAIN: iteration 25933 : loss : 0.107046, loss_ce: 0.004515, loss_dice: 0.209577
[11:58:39.039] TRAIN: iteration 25934 : loss : 0.078187, loss_ce: 0.004311, loss_dice: 0.152062
[11:58:39.246] TRAIN: iteration 25935 : loss : 0.059803, loss_ce: 0.005707, loss_dice: 0.113898
[11:58:43.448] TRAIN: iteration 25936 : loss : 0.115580, loss_ce: 0.008737, loss_dice: 0.222422
[11:58:43.658] TRAIN: iteration 25937 : loss : 0.182203, loss_ce: 0.003044, loss_dice: 0.361362
[11:58:45.903] TRAIN: iteration 25938 : loss : 0.251911, loss_ce: 0.003581, loss_dice: 0.500241
[11:58:46.110] TRAIN: iteration 25939 : loss : 0.138129, loss_ce: 0.002349, loss_dice: 0.273910
[11:58:46.329] TRAIN: iteration 25940 : loss : 0.093794, loss_ce: 0.004224, loss_dice: 0.183363
[11:58:46.569] TRAIN: iteration 25941 : loss : 0.250948, loss_ce: 0.001806, loss_dice: 0.500089
[11:58:46.778] TRAIN: iteration 25942 : loss : 0.251845, loss_ce: 0.003451, loss_dice: 0.500239
[11:58:46.986] TRAIN: iteration 25943 : loss : 0.040465, loss_ce: 0.007065, loss_dice: 0.073866
[11:58:51.812] TRAIN: iteration 25944 : loss : 0.193061, loss_ce: 0.002695, loss_dice: 0.383427
[11:58:52.023] TRAIN: iteration 25945 : loss : 0.251609, loss_ce: 0.003024, loss_dice: 0.500194
[11:58:53.795] TRAIN: iteration 25946 : loss : 0.039790, loss_ce: 0.006985, loss_dice: 0.072595
[11:58:54.004] TRAIN: iteration 25947 : loss : 0.122559, loss_ce: 0.002015, loss_dice: 0.243104
[11:58:54.212] TRAIN: iteration 25948 : loss : 0.037897, loss_ce: 0.002819, loss_dice: 0.072976
[11:58:54.420] TRAIN: iteration 25949 : loss : 0.059245, loss_ce: 0.004067, loss_dice: 0.114423
[11:58:54.633] TRAIN: iteration 25950 : loss : 0.251661, loss_ce: 0.003112, loss_dice: 0.500209
[11:58:54.841] TRAIN: iteration 25951 : loss : 0.021042, loss_ce: 0.000708, loss_dice: 0.041376
[11:59:01.321] TRAIN: iteration 25952 : loss : 0.106670, loss_ce: 0.002424, loss_dice: 0.210917
[11:59:01.531] TRAIN: iteration 25953 : loss : 0.134463, loss_ce: 0.002891, loss_dice: 0.266035
[11:59:01.741] TRAIN: iteration 25954 : loss : 0.121778, loss_ce: 0.002284, loss_dice: 0.241272
[11:59:01.949] TRAIN: iteration 25955 : loss : 0.250788, loss_ce: 0.001501, loss_dice: 0.500075
[11:59:02.157] TRAIN: iteration 25956 : loss : 0.250388, loss_ce: 0.000751, loss_dice: 0.500025
[11:59:02.365] TRAIN: iteration 25957 : loss : 0.195613, loss_ce: 0.021071, loss_dice: 0.370154
[11:59:02.575] TRAIN: iteration 25958 : loss : 0.046782, loss_ce: 0.001588, loss_dice: 0.091975
[11:59:02.783] TRAIN: iteration 25959 : loss : 0.050885, loss_ce: 0.002888, loss_dice: 0.098882
[11:59:11.224] TRAIN: iteration 25960 : loss : 0.020134, loss_ce: 0.001019, loss_dice: 0.039249
[11:59:11.463] TRAIN: iteration 25961 : loss : 0.252160, loss_ce: 0.008228, loss_dice: 0.496091
[11:59:11.670] TRAIN: iteration 25962 : loss : 0.071330, loss_ce: 0.004279, loss_dice: 0.138380
[11:59:11.880] TRAIN: iteration 25963 : loss : 0.041723, loss_ce: 0.003362, loss_dice: 0.080083
[11:59:12.087] TRAIN: iteration 25964 : loss : 0.128987, loss_ce: 0.004251, loss_dice: 0.253724
[11:59:12.295] TRAIN: iteration 25965 : loss : 0.250737, loss_ce: 0.001403, loss_dice: 0.500072
[11:59:12.503] TRAIN: iteration 25966 : loss : 0.069670, loss_ce: 0.002250, loss_dice: 0.137089
[11:59:12.711] TRAIN: iteration 25967 : loss : 0.102370, loss_ce: 0.001755, loss_dice: 0.202984
[11:59:20.037] TRAIN: iteration 25968 : loss : 0.213593, loss_ce: 0.003890, loss_dice: 0.423295
[11:59:20.248] TRAIN: iteration 25969 : loss : 0.076504, loss_ce: 0.003644, loss_dice: 0.149363
[11:59:20.456] TRAIN: iteration 25970 : loss : 0.042816, loss_ce: 0.000852, loss_dice: 0.084780
[11:59:20.670] TRAIN: iteration 25971 : loss : 0.136223, loss_ce: 0.008058, loss_dice: 0.264388
[11:59:20.879] TRAIN: iteration 25972 : loss : 0.024929, loss_ce: 0.003516, loss_dice: 0.046342
[11:59:21.086] TRAIN: iteration 25973 : loss : 0.251151, loss_ce: 0.002176, loss_dice: 0.500126
[11:59:21.294] TRAIN: iteration 25974 : loss : 0.237947, loss_ce: 0.002071, loss_dice: 0.473823
[11:59:21.501] TRAIN: iteration 25975 : loss : 0.033181, loss_ce: 0.001227, loss_dice: 0.065134
[11:59:28.322] TRAIN: iteration 25976 : loss : 0.138822, loss_ce: 0.002393, loss_dice: 0.275251
[11:59:28.530] TRAIN: iteration 25977 : loss : 0.104447, loss_ce: 0.013678, loss_dice: 0.195216
[11:59:28.738] TRAIN: iteration 25978 : loss : 0.024621, loss_ce: 0.002275, loss_dice: 0.046966
[11:59:28.946] TRAIN: iteration 25979 : loss : 0.070869, loss_ce: 0.005489, loss_dice: 0.136249
[11:59:29.154] TRAIN: iteration 25980 : loss : 0.142763, loss_ce: 0.008351, loss_dice: 0.277174
[11:59:29.394] TRAIN: iteration 25981 : loss : 0.075923, loss_ce: 0.004338, loss_dice: 0.147508
[11:59:29.686] TRAIN: iteration 25982 : loss : 0.167971, loss_ce: 0.009441, loss_dice: 0.326502
[11:59:29.897] TRAIN: iteration 25983 : loss : 0.086836, loss_ce: 0.005656, loss_dice: 0.168016
[11:59:37.946] TRAIN: iteration 25984 : loss : 0.087841, loss_ce: 0.004680, loss_dice: 0.171001
[11:59:38.153] TRAIN: iteration 25985 : loss : 0.211507, loss_ce: 0.003427, loss_dice: 0.419587
[11:59:38.364] TRAIN: iteration 25986 : loss : 0.091926, loss_ce: 0.004943, loss_dice: 0.178909
[11:59:38.571] TRAIN: iteration 25987 : loss : 0.127681, loss_ce: 0.001845, loss_dice: 0.253516
[11:59:38.779] TRAIN: iteration 25988 : loss : 0.081701, loss_ce: 0.004965, loss_dice: 0.158437
[11:59:38.988] TRAIN: iteration 25989 : loss : 0.139132, loss_ce: 0.006977, loss_dice: 0.271286
[11:59:39.198] TRAIN: iteration 25990 : loss : 0.076558, loss_ce: 0.005150, loss_dice: 0.147966
[11:59:39.407] TRAIN: iteration 25991 : loss : 0.058409, loss_ce: 0.003036, loss_dice: 0.113783
[11:59:47.404] TRAIN: iteration 25992 : loss : 0.143852, loss_ce: 0.002993, loss_dice: 0.284712
[11:59:47.613] TRAIN: iteration 25993 : loss : 0.014056, loss_ce: 0.000793, loss_dice: 0.027319
[11:59:47.821] TRAIN: iteration 25994 : loss : 0.250921, loss_ce: 0.001750, loss_dice: 0.500093
[11:59:48.030] TRAIN: iteration 25995 : loss : 0.059961, loss_ce: 0.002057, loss_dice: 0.117865
[11:59:48.240] TRAIN: iteration 25996 : loss : 0.054479, loss_ce: 0.002387, loss_dice: 0.106570
[11:59:48.449] TRAIN: iteration 25997 : loss : 0.251283, loss_ce: 0.002415, loss_dice: 0.500152
[11:59:48.657] TRAIN: iteration 25998 : loss : 0.097403, loss_ce: 0.005479, loss_dice: 0.189327
[11:59:48.866] TRAIN: iteration 25999 : loss : 0.072409, loss_ce: 0.003224, loss_dice: 0.141593
[11:59:55.670] TRAIN: iteration 26000 : loss : 0.062446, loss_ce: 0.001257, loss_dice: 0.123635
[11:59:55.908] TRAIN: iteration 26001 : loss : 0.167165, loss_ce: 0.010706, loss_dice: 0.323624
[11:59:56.117] TRAIN: iteration 26002 : loss : 0.124597, loss_ce: 0.002040, loss_dice: 0.247153
[11:59:56.535] TRAIN: iteration 26003 : loss : 0.145871, loss_ce: 0.004547, loss_dice: 0.287195
[11:59:56.743] TRAIN: iteration 26004 : loss : 0.186650, loss_ce: 0.020139, loss_dice: 0.353161
[11:59:56.950] TRAIN: iteration 26005 : loss : 0.103724, loss_ce: 0.002440, loss_dice: 0.205008
[11:59:57.159] TRAIN: iteration 26006 : loss : 0.108452, loss_ce: 0.006609, loss_dice: 0.210294
[11:59:57.370] TRAIN: iteration 26007 : loss : 0.078195, loss_ce: 0.004307, loss_dice: 0.152083
[12:00:05.996] TRAIN: iteration 26008 : loss : 0.249684, loss_ce: 0.002456, loss_dice: 0.496912
[12:00:06.208] TRAIN: iteration 26009 : loss : 0.061908, loss_ce: 0.007075, loss_dice: 0.116741
[12:00:06.416] TRAIN: iteration 26010 : loss : 0.152321, loss_ce: 0.001660, loss_dice: 0.302983
[12:00:06.624] TRAIN: iteration 26011 : loss : 0.052010, loss_ce: 0.004593, loss_dice: 0.099426
[12:00:06.832] TRAIN: iteration 26012 : loss : 0.057724, loss_ce: 0.003635, loss_dice: 0.111812
[12:00:07.041] TRAIN: iteration 26013 : loss : 0.051719, loss_ce: 0.001446, loss_dice: 0.101992
[12:00:07.252] TRAIN: iteration 26014 : loss : 0.066494, loss_ce: 0.002842, loss_dice: 0.130145
[12:00:07.462] TRAIN: iteration 26015 : loss : 0.041051, loss_ce: 0.007535, loss_dice: 0.074568
[12:00:15.552] TRAIN: iteration 26016 : loss : 0.135985, loss_ce: 0.001964, loss_dice: 0.270006
[12:00:15.762] TRAIN: iteration 26017 : loss : 0.250694, loss_ce: 0.001336, loss_dice: 0.500052
[12:00:15.969] TRAIN: iteration 26018 : loss : 0.251308, loss_ce: 0.002933, loss_dice: 0.499684
[12:00:16.178] TRAIN: iteration 26019 : loss : 0.047362, loss_ce: 0.006821, loss_dice: 0.087903
[12:00:16.385] TRAIN: iteration 26020 : loss : 0.086947, loss_ce: 0.016219, loss_dice: 0.157676
[12:00:16.386] NaN or Inf found in input tensor.
[12:00:16.600] TRAIN: iteration 26021 : loss : 0.070578, loss_ce: 0.007590, loss_dice: 0.133566
[12:00:16.808] TRAIN: iteration 26022 : loss : 0.045465, loss_ce: 0.004648, loss_dice: 0.086282
[12:00:17.019] TRAIN: iteration 26023 : loss : 0.054448, loss_ce: 0.001127, loss_dice: 0.107769
[12:00:23.230] TRAIN: iteration 26024 : loss : 0.111925, loss_ce: 0.001311, loss_dice: 0.222538
[12:00:23.326] TRAIN: iteration 26025 : loss : 0.125689, loss_ce: 0.005423, loss_dice: 0.245955
[12:05:44.629] VALIDATION: iteration 14 : loss : 0.116894, loss_ce: 0.004191, loss_dice: 0.229596
[12:05:46.216] TRAIN: iteration 26026 : loss : 0.132812, loss_ce: 0.001953, loss_dice: 0.263671
[12:05:46.424] TRAIN: iteration 26027 : loss : 0.095236, loss_ce: 0.001962, loss_dice: 0.188509
[12:05:46.729] TRAIN: iteration 26028 : loss : 0.043762, loss_ce: 0.000968, loss_dice: 0.086556
[12:05:46.941] TRAIN: iteration 26029 : loss : 0.035014, loss_ce: 0.005608, loss_dice: 0.064420
[12:05:47.154] TRAIN: iteration 26030 : loss : 0.170209, loss_ce: 0.003177, loss_dice: 0.337240
[12:05:47.367] TRAIN: iteration 26031 : loss : 0.067507, loss_ce: 0.004271, loss_dice: 0.130743
[12:05:47.596] TRAIN: iteration 26032 : loss : 0.044851, loss_ce: 0.001276, loss_dice: 0.088425
[12:05:47.811] TRAIN: iteration 26033 : loss : 0.043311, loss_ce: 0.002582, loss_dice: 0.084040
[12:05:48.028] TRAIN: iteration 26034 : loss : 0.242095, loss_ce: 0.001217, loss_dice: 0.482972
[12:05:48.242] TRAIN: iteration 26035 : loss : 0.146614, loss_ce: 0.001203, loss_dice: 0.292026
[12:05:48.453] TRAIN: iteration 26036 : loss : 0.047538, loss_ce: 0.003041, loss_dice: 0.092035
[12:05:49.065] TRAIN: iteration 26037 : loss : 0.095038, loss_ce: 0.004948, loss_dice: 0.185127
[12:05:49.274] TRAIN: iteration 26038 : loss : 0.044334, loss_ce: 0.004998, loss_dice: 0.083669
[12:05:49.491] TRAIN: iteration 26039 : loss : 0.154309, loss_ce: 0.002033, loss_dice: 0.306586
[12:05:49.705] TRAIN: iteration 26040 : loss : 0.250205, loss_ce: 0.000407, loss_dice: 0.500003
[12:05:49.951] TRAIN: iteration 26041 : loss : 0.088134, loss_ce: 0.001579, loss_dice: 0.174688
[12:05:50.160] TRAIN: iteration 26042 : loss : 0.046866, loss_ce: 0.007662, loss_dice: 0.086070
[12:05:50.368] TRAIN: iteration 26043 : loss : 0.042821, loss_ce: 0.003226, loss_dice: 0.082415
[12:05:50.577] TRAIN: iteration 26044 : loss : 0.142276, loss_ce: 0.008230, loss_dice: 0.276321
[12:05:50.785] TRAIN: iteration 26045 : loss : 0.022576, loss_ce: 0.001980, loss_dice: 0.043172
[12:05:50.993] TRAIN: iteration 26046 : loss : 0.044717, loss_ce: 0.001055, loss_dice: 0.088379
[12:05:51.209] TRAIN: iteration 26047 : loss : 0.248373, loss_ce: 0.002677, loss_dice: 0.494070
[12:05:51.422] TRAIN: iteration 26048 : loss : 0.153666, loss_ce: 0.013560, loss_dice: 0.293772
[12:05:51.632] TRAIN: iteration 26049 : loss : 0.050372, loss_ce: 0.000603, loss_dice: 0.100141
[12:05:52.507] TRAIN: iteration 26050 : loss : 0.075806, loss_ce: 0.002377, loss_dice: 0.149235
[12:05:52.716] TRAIN: iteration 26051 : loss : 0.076079, loss_ce: 0.003573, loss_dice: 0.148586
[12:05:52.932] TRAIN: iteration 26052 : loss : 0.146600, loss_ce: 0.005905, loss_dice: 0.287295
[12:05:53.144] TRAIN: iteration 26053 : loss : 0.020670, loss_ce: 0.000533, loss_dice: 0.040807
[12:05:53.352] TRAIN: iteration 26054 : loss : 0.100540, loss_ce: 0.002906, loss_dice: 0.198175
[12:05:53.567] TRAIN: iteration 26055 : loss : 0.166563, loss_ce: 0.003984, loss_dice: 0.329143
[12:05:53.774] TRAIN: iteration 26056 : loss : 0.239752, loss_ce: 0.001161, loss_dice: 0.478343
[12:05:53.982] TRAIN: iteration 26057 : loss : 0.227943, loss_ce: 0.001757, loss_dice: 0.454130
[12:05:54.190] TRAIN: iteration 26058 : loss : 0.147522, loss_ce: 0.005378, loss_dice: 0.289666
[12:05:54.397] TRAIN: iteration 26059 : loss : 0.092874, loss_ce: 0.001848, loss_dice: 0.183900
[12:05:54.605] TRAIN: iteration 26060 : loss : 0.046625, loss_ce: 0.002112, loss_dice: 0.091138
[12:05:54.849] TRAIN: iteration 26061 : loss : 0.085272, loss_ce: 0.003143, loss_dice: 0.167401
[12:05:55.059] TRAIN: iteration 26062 : loss : 0.073408, loss_ce: 0.001877, loss_dice: 0.144938
[12:05:55.273] TRAIN: iteration 26063 : loss : 0.251549, loss_ce: 0.002903, loss_dice: 0.500194
[12:05:55.482] TRAIN: iteration 26064 : loss : 0.212136, loss_ce: 0.001553, loss_dice: 0.422719
[12:05:55.692] TRAIN: iteration 26065 : loss : 0.025864, loss_ce: 0.001573, loss_dice: 0.050155
[12:05:56.041] TRAIN: iteration 26066 : loss : 0.016899, loss_ce: 0.001060, loss_dice: 0.032739
[12:05:56.252] TRAIN: iteration 26067 : loss : 0.023273, loss_ce: 0.000608, loss_dice: 0.045939
[12:05:56.466] TRAIN: iteration 26068 : loss : 0.048998, loss_ce: 0.001797, loss_dice: 0.096199
[12:05:56.674] TRAIN: iteration 26069 : loss : 0.225662, loss_ce: 0.001992, loss_dice: 0.449331
[12:05:56.883] TRAIN: iteration 26070 : loss : 0.037077, loss_ce: 0.004129, loss_dice: 0.070024
[12:05:57.092] TRAIN: iteration 26071 : loss : 0.249690, loss_ce: 0.001549, loss_dice: 0.497830
[12:05:57.301] TRAIN: iteration 26072 : loss : 0.162832, loss_ce: 0.022078, loss_dice: 0.303587
[12:05:57.511] TRAIN: iteration 26073 : loss : 0.225737, loss_ce: 0.003980, loss_dice: 0.447494
[12:05:57.719] TRAIN: iteration 26074 : loss : 0.081357, loss_ce: 0.001932, loss_dice: 0.160781
[12:05:57.927] TRAIN: iteration 26075 : loss : 0.054922, loss_ce: 0.002132, loss_dice: 0.107711
[12:05:58.155] TRAIN: iteration 26076 : loss : 0.113370, loss_ce: 0.005783, loss_dice: 0.220957
[12:05:58.366] TRAIN: iteration 26077 : loss : 0.113105, loss_ce: 0.003221, loss_dice: 0.222990
[12:05:58.581] TRAIN: iteration 26078 : loss : 0.114903, loss_ce: 0.002796, loss_dice: 0.227009
[12:05:58.792] TRAIN: iteration 26079 : loss : 0.049217, loss_ce: 0.001732, loss_dice: 0.096702
[12:05:59.003] TRAIN: iteration 26080 : loss : 0.106719, loss_ce: 0.020700, loss_dice: 0.192739
[12:05:59.242] TRAIN: iteration 26081 : loss : 0.125532, loss_ce: 0.003341, loss_dice: 0.247723
[12:05:59.451] TRAIN: iteration 26082 : loss : 0.251160, loss_ce: 0.002196, loss_dice: 0.500124
[12:05:59.659] TRAIN: iteration 26083 : loss : 0.165091, loss_ce: 0.006391, loss_dice: 0.323791
[12:05:59.868] TRAIN: iteration 26084 : loss : 0.207848, loss_ce: 0.015062, loss_dice: 0.400633
[12:06:00.078] TRAIN: iteration 26085 : loss : 0.236255, loss_ce: 0.001914, loss_dice: 0.470596
[12:06:00.289] TRAIN: iteration 26086 : loss : 0.101073, loss_ce: 0.005148, loss_dice: 0.196997
[12:06:00.500] TRAIN: iteration 26087 : loss : 0.046324, loss_ce: 0.002786, loss_dice: 0.089862
[12:06:00.712] TRAIN: iteration 26088 : loss : 0.251510, loss_ce: 0.002852, loss_dice: 0.500169
[12:06:00.921] TRAIN: iteration 26089 : loss : 0.062058, loss_ce: 0.002408, loss_dice: 0.121708
[12:06:01.131] TRAIN: iteration 26090 : loss : 0.251003, loss_ce: 0.001946, loss_dice: 0.500061
[12:06:01.339] TRAIN: iteration 26091 : loss : 0.116929, loss_ce: 0.003090, loss_dice: 0.230767
[12:06:01.550] TRAIN: iteration 26092 : loss : 0.082581, loss_ce: 0.014037, loss_dice: 0.151125
[12:06:01.760] TRAIN: iteration 26093 : loss : 0.056411, loss_ce: 0.002943, loss_dice: 0.109879
[12:06:01.973] TRAIN: iteration 26094 : loss : 0.251566, loss_ce: 0.002996, loss_dice: 0.500137
[12:06:02.181] TRAIN: iteration 26095 : loss : 0.031262, loss_ce: 0.001662, loss_dice: 0.060863
[12:06:02.390] TRAIN: iteration 26096 : loss : 0.058264, loss_ce: 0.003760, loss_dice: 0.112769
[12:06:02.599] TRAIN: iteration 26097 : loss : 0.063041, loss_ce: 0.002000, loss_dice: 0.124081
[12:06:02.809] TRAIN: iteration 26098 : loss : 0.066988, loss_ce: 0.005647, loss_dice: 0.128328
[12:06:03.017] TRAIN: iteration 26099 : loss : 0.251079, loss_ce: 0.002048, loss_dice: 0.500110
[12:06:03.227] TRAIN: iteration 26100 : loss : 0.078544, loss_ce: 0.003284, loss_dice: 0.153804
[12:06:03.464] TRAIN: iteration 26101 : loss : 0.111669, loss_ce: 0.002516, loss_dice: 0.220823
[12:06:03.672] TRAIN: iteration 26102 : loss : 0.206596, loss_ce: 0.002633, loss_dice: 0.410559
[12:06:03.879] TRAIN: iteration 26103 : loss : 0.033711, loss_ce: 0.001946, loss_dice: 0.065477
[12:06:04.093] TRAIN: iteration 26104 : loss : 0.071182, loss_ce: 0.006946, loss_dice: 0.135418
[12:06:04.308] TRAIN: iteration 26105 : loss : 0.250798, loss_ce: 0.001545, loss_dice: 0.500051
[12:06:04.522] TRAIN: iteration 26106 : loss : 0.185062, loss_ce: 0.002555, loss_dice: 0.367569
[12:06:04.735] TRAIN: iteration 26107 : loss : 0.075537, loss_ce: 0.002598, loss_dice: 0.148475
[12:06:04.952] TRAIN: iteration 26108 : loss : 0.080632, loss_ce: 0.003865, loss_dice: 0.157399
[12:06:05.166] TRAIN: iteration 26109 : loss : 0.030416, loss_ce: 0.002121, loss_dice: 0.058712
[12:06:05.374] TRAIN: iteration 26110 : loss : 0.020319, loss_ce: 0.001539, loss_dice: 0.039100
[12:06:05.588] TRAIN: iteration 26111 : loss : 0.251565, loss_ce: 0.002929, loss_dice: 0.500200
[12:06:05.800] TRAIN: iteration 26112 : loss : 0.032083, loss_ce: 0.001306, loss_dice: 0.062859
[12:06:06.009] TRAIN: iteration 26113 : loss : 0.250669, loss_ce: 0.001267, loss_dice: 0.500070
[12:06:06.222] TRAIN: iteration 26114 : loss : 0.030185, loss_ce: 0.001932, loss_dice: 0.058439
[12:06:06.430] TRAIN: iteration 26115 : loss : 0.031312, loss_ce: 0.003398, loss_dice: 0.059225
[12:06:06.644] TRAIN: iteration 26116 : loss : 0.070227, loss_ce: 0.001911, loss_dice: 0.138544
[12:06:06.852] TRAIN: iteration 26117 : loss : 0.027780, loss_ce: 0.000758, loss_dice: 0.054801
[12:06:07.062] TRAIN: iteration 26118 : loss : 0.250696, loss_ce: 0.001328, loss_dice: 0.500065
[12:06:07.275] TRAIN: iteration 26119 : loss : 0.056242, loss_ce: 0.005527, loss_dice: 0.106956
[12:06:07.483] TRAIN: iteration 26120 : loss : 0.119797, loss_ce: 0.025820, loss_dice: 0.213775
[12:06:07.724] TRAIN: iteration 26121 : loss : 0.079952, loss_ce: 0.003489, loss_dice: 0.156415
[12:06:07.932] TRAIN: iteration 26122 : loss : 0.027486, loss_ce: 0.000945, loss_dice: 0.054027
[12:06:08.141] TRAIN: iteration 26123 : loss : 0.110235, loss_ce: 0.008729, loss_dice: 0.211740
[12:06:08.353] TRAIN: iteration 26124 : loss : 0.155295, loss_ce: 0.006485, loss_dice: 0.304105
[12:06:08.561] TRAIN: iteration 26125 : loss : 0.107682, loss_ce: 0.003783, loss_dice: 0.211581
[12:06:08.769] TRAIN: iteration 26126 : loss : 0.250697, loss_ce: 0.001340, loss_dice: 0.500055
[12:06:08.980] TRAIN: iteration 26127 : loss : 0.101891, loss_ce: 0.009192, loss_dice: 0.194590
[12:06:09.189] TRAIN: iteration 26128 : loss : 0.250627, loss_ce: 0.001186, loss_dice: 0.500068
[12:06:09.575] TRAIN: iteration 26129 : loss : 0.051796, loss_ce: 0.001956, loss_dice: 0.101636
[12:06:09.784] TRAIN: iteration 26130 : loss : 0.072633, loss_ce: 0.008570, loss_dice: 0.136697
[12:06:09.999] TRAIN: iteration 26131 : loss : 0.051151, loss_ce: 0.002163, loss_dice: 0.100140
[12:06:10.206] TRAIN: iteration 26132 : loss : 0.130878, loss_ce: 0.001698, loss_dice: 0.260058
[12:06:10.413] TRAIN: iteration 26133 : loss : 0.068191, loss_ce: 0.002004, loss_dice: 0.134378
[12:06:10.621] TRAIN: iteration 26134 : loss : 0.045679, loss_ce: 0.003417, loss_dice: 0.087941
[12:06:10.832] TRAIN: iteration 26135 : loss : 0.046894, loss_ce: 0.001938, loss_dice: 0.091850
[12:06:11.072] TRAIN: iteration 26136 : loss : 0.084336, loss_ce: 0.001648, loss_dice: 0.167023
[12:06:11.279] TRAIN: iteration 26137 : loss : 0.155522, loss_ce: 0.001819, loss_dice: 0.309225
[12:06:11.488] TRAIN: iteration 26138 : loss : 0.100481, loss_ce: 0.001848, loss_dice: 0.199115
[12:06:11.696] TRAIN: iteration 26139 : loss : 0.052718, loss_ce: 0.001350, loss_dice: 0.104087
[12:06:11.906] TRAIN: iteration 26140 : loss : 0.047692, loss_ce: 0.000984, loss_dice: 0.094401
[12:06:12.168] TRAIN: iteration 26141 : loss : 0.082863, loss_ce: 0.002466, loss_dice: 0.163261
[12:06:12.376] TRAIN: iteration 26142 : loss : 0.037531, loss_ce: 0.000765, loss_dice: 0.074297
[12:06:12.584] TRAIN: iteration 26143 : loss : 0.098989, loss_ce: 0.007627, loss_dice: 0.190350
[12:06:12.793] TRAIN: iteration 26144 : loss : 0.045844, loss_ce: 0.002963, loss_dice: 0.088726
[12:06:13.001] TRAIN: iteration 26145 : loss : 0.116755, loss_ce: 0.009216, loss_dice: 0.224295
[12:06:13.216] TRAIN: iteration 26146 : loss : 0.205105, loss_ce: 0.001329, loss_dice: 0.408881
[12:06:13.429] TRAIN: iteration 26147 : loss : 0.084926, loss_ce: 0.003698, loss_dice: 0.166154
[12:06:13.641] TRAIN: iteration 26148 : loss : 0.061859, loss_ce: 0.001554, loss_dice: 0.122164
[12:06:13.854] TRAIN: iteration 26149 : loss : 0.068857, loss_ce: 0.001022, loss_dice: 0.136692
[12:06:14.064] TRAIN: iteration 26150 : loss : 0.096611, loss_ce: 0.007667, loss_dice: 0.185556
[12:06:14.277] TRAIN: iteration 26151 : loss : 0.250245, loss_ce: 0.000482, loss_dice: 0.500008
[12:06:14.489] TRAIN: iteration 26152 : loss : 0.151240, loss_ce: 0.002475, loss_dice: 0.300005
[12:06:14.702] TRAIN: iteration 26153 : loss : 0.076149, loss_ce: 0.003546, loss_dice: 0.148752
[12:06:14.911] TRAIN: iteration 26154 : loss : 0.199120, loss_ce: 0.006337, loss_dice: 0.391903
[12:06:15.120] TRAIN: iteration 26155 : loss : 0.117048, loss_ce: 0.001823, loss_dice: 0.232273
[12:06:15.337] TRAIN: iteration 26156 : loss : 0.033403, loss_ce: 0.002835, loss_dice: 0.063970
[12:06:15.545] TRAIN: iteration 26157 : loss : 0.058979, loss_ce: 0.005035, loss_dice: 0.112924
[12:06:15.754] TRAIN: iteration 26158 : loss : 0.099504, loss_ce: 0.002529, loss_dice: 0.196480
[12:06:15.967] TRAIN: iteration 26159 : loss : 0.234953, loss_ce: 0.001647, loss_dice: 0.468259
[12:06:16.175] TRAIN: iteration 26160 : loss : 0.128547, loss_ce: 0.001580, loss_dice: 0.255514
[12:06:16.423] TRAIN: iteration 26161 : loss : 0.090999, loss_ce: 0.005301, loss_dice: 0.176697
[12:06:16.632] TRAIN: iteration 26162 : loss : 0.073071, loss_ce: 0.002912, loss_dice: 0.143230
[12:06:16.845] TRAIN: iteration 26163 : loss : 0.047653, loss_ce: 0.003532, loss_dice: 0.091773
[12:06:17.058] TRAIN: iteration 26164 : loss : 0.215661, loss_ce: 0.005732, loss_dice: 0.425591
[12:06:17.269] TRAIN: iteration 26165 : loss : 0.043642, loss_ce: 0.000840, loss_dice: 0.086443
[12:06:17.481] TRAIN: iteration 26166 : loss : 0.057582, loss_ce: 0.001885, loss_dice: 0.113278
[12:06:17.695] TRAIN: iteration 26167 : loss : 0.232295, loss_ce: 0.001421, loss_dice: 0.463168
[12:06:17.903] TRAIN: iteration 26168 : loss : 0.046616, loss_ce: 0.001397, loss_dice: 0.091835
[12:06:18.114] TRAIN: iteration 26169 : loss : 0.077068, loss_ce: 0.001373, loss_dice: 0.152763
[12:06:18.322] TRAIN: iteration 26170 : loss : 0.041426, loss_ce: 0.007598, loss_dice: 0.075253
[12:06:18.529] TRAIN: iteration 26171 : loss : 0.082309, loss_ce: 0.004296, loss_dice: 0.160321
[12:06:18.737] TRAIN: iteration 26172 : loss : 0.063417, loss_ce: 0.001892, loss_dice: 0.124942
[12:06:18.946] TRAIN: iteration 26173 : loss : 0.096597, loss_ce: 0.004051, loss_dice: 0.189144
[12:06:19.153] TRAIN: iteration 26174 : loss : 0.062501, loss_ce: 0.005313, loss_dice: 0.119689
[12:06:19.361] TRAIN: iteration 26175 : loss : 0.222655, loss_ce: 0.002153, loss_dice: 0.443157
[12:06:19.568] TRAIN: iteration 26176 : loss : 0.038723, loss_ce: 0.000963, loss_dice: 0.076483
[12:06:19.778] TRAIN: iteration 26177 : loss : 0.250905, loss_ce: 0.001735, loss_dice: 0.500076
[12:06:19.988] TRAIN: iteration 26178 : loss : 0.075291, loss_ce: 0.011228, loss_dice: 0.139354
[12:06:20.195] TRAIN: iteration 26179 : loss : 0.222061, loss_ce: 0.000790, loss_dice: 0.443332
[12:06:20.403] TRAIN: iteration 26180 : loss : 0.036117, loss_ce: 0.002153, loss_dice: 0.070082
[12:06:20.636] TRAIN: iteration 26181 : loss : 0.240532, loss_ce: 0.002815, loss_dice: 0.478248
[12:06:20.844] TRAIN: iteration 26182 : loss : 0.224710, loss_ce: 0.001352, loss_dice: 0.448069
[12:06:21.051] TRAIN: iteration 26183 : loss : 0.114660, loss_ce: 0.002812, loss_dice: 0.226507
[12:06:21.259] TRAIN: iteration 26184 : loss : 0.113717, loss_ce: 0.004774, loss_dice: 0.222660
[12:06:21.468] TRAIN: iteration 26185 : loss : 0.033106, loss_ce: 0.002069, loss_dice: 0.064144
[12:06:21.680] TRAIN: iteration 26186 : loss : 0.053226, loss_ce: 0.008390, loss_dice: 0.098062
[12:06:21.887] TRAIN: iteration 26187 : loss : 0.076959, loss_ce: 0.014479, loss_dice: 0.139439
[12:06:22.095] TRAIN: iteration 26188 : loss : 0.045213, loss_ce: 0.007374, loss_dice: 0.083052
[12:06:22.311] TRAIN: iteration 26189 : loss : 0.119679, loss_ce: 0.001677, loss_dice: 0.237680
[12:06:22.521] TRAIN: iteration 26190 : loss : 0.107903, loss_ce: 0.001691, loss_dice: 0.214116
[12:06:22.732] TRAIN: iteration 26191 : loss : 0.072724, loss_ce: 0.005636, loss_dice: 0.139811
[12:06:22.947] TRAIN: iteration 26192 : loss : 0.079483, loss_ce: 0.001824, loss_dice: 0.157141
[12:06:23.155] TRAIN: iteration 26193 : loss : 0.222093, loss_ce: 0.001981, loss_dice: 0.442204
[12:06:23.369] TRAIN: iteration 26194 : loss : 0.244425, loss_ce: 0.001365, loss_dice: 0.487484
[12:06:23.577] TRAIN: iteration 26195 : loss : 0.040882, loss_ce: 0.003100, loss_dice: 0.078664
[12:06:23.788] TRAIN: iteration 26196 : loss : 0.042904, loss_ce: 0.005423, loss_dice: 0.080386
[12:06:24.003] TRAIN: iteration 26197 : loss : 0.113509, loss_ce: 0.003312, loss_dice: 0.223707
[12:06:24.211] TRAIN: iteration 26198 : loss : 0.240063, loss_ce: 0.007561, loss_dice: 0.472565
[12:06:24.420] TRAIN: iteration 26199 : loss : 0.045741, loss_ce: 0.002778, loss_dice: 0.088703
[12:06:24.632] TRAIN: iteration 26200 : loss : 0.128898, loss_ce: 0.004925, loss_dice: 0.252870
[12:06:24.868] TRAIN: iteration 26201 : loss : 0.054164, loss_ce: 0.008478, loss_dice: 0.099850
[12:06:25.080] TRAIN: iteration 26202 : loss : 0.060661, loss_ce: 0.000941, loss_dice: 0.120381
[12:06:25.295] TRAIN: iteration 26203 : loss : 0.209142, loss_ce: 0.002278, loss_dice: 0.416006
[12:06:25.502] TRAIN: iteration 26204 : loss : 0.029061, loss_ce: 0.002003, loss_dice: 0.056119
[12:06:25.708] TRAIN: iteration 26205 : loss : 0.195043, loss_ce: 0.001784, loss_dice: 0.388303
[12:06:25.918] TRAIN: iteration 26206 : loss : 0.221966, loss_ce: 0.001684, loss_dice: 0.442247
[12:06:26.131] TRAIN: iteration 26207 : loss : 0.057600, loss_ce: 0.001620, loss_dice: 0.113580
[12:06:26.339] TRAIN: iteration 26208 : loss : 0.038156, loss_ce: 0.003410, loss_dice: 0.072901
[12:06:26.586] TRAIN: iteration 26209 : loss : 0.102141, loss_ce: 0.012000, loss_dice: 0.192282
[12:06:26.793] TRAIN: iteration 26210 : loss : 0.075289, loss_ce: 0.004073, loss_dice: 0.146505
[12:06:27.000] TRAIN: iteration 26211 : loss : 0.050433, loss_ce: 0.005605, loss_dice: 0.095261
[12:06:27.209] TRAIN: iteration 26212 : loss : 0.094251, loss_ce: 0.004268, loss_dice: 0.184235
[12:06:27.415] TRAIN: iteration 26213 : loss : 0.057679, loss_ce: 0.003385, loss_dice: 0.111973
[12:06:27.623] TRAIN: iteration 26214 : loss : 0.028334, loss_ce: 0.002456, loss_dice: 0.054212
[12:06:27.831] TRAIN: iteration 26215 : loss : 0.203514, loss_ce: 0.002296, loss_dice: 0.404732
[12:06:28.039] TRAIN: iteration 26216 : loss : 0.016044, loss_ce: 0.002515, loss_dice: 0.029573
[12:06:28.247] TRAIN: iteration 26217 : loss : 0.073080, loss_ce: 0.002263, loss_dice: 0.143897
[12:06:28.457] TRAIN: iteration 26218 : loss : 0.250582, loss_ce: 0.001132, loss_dice: 0.500031
[12:06:28.666] TRAIN: iteration 26219 : loss : 0.078483, loss_ce: 0.002347, loss_dice: 0.154619
[12:06:28.874] TRAIN: iteration 26220 : loss : 0.240861, loss_ce: 0.003743, loss_dice: 0.477979
[12:06:29.115] TRAIN: iteration 26221 : loss : 0.146124, loss_ce: 0.001686, loss_dice: 0.290562
[12:06:29.324] TRAIN: iteration 26222 : loss : 0.040294, loss_ce: 0.002446, loss_dice: 0.078142
[12:06:29.532] TRAIN: iteration 26223 : loss : 0.186370, loss_ce: 0.001590, loss_dice: 0.371150
[12:06:29.744] TRAIN: iteration 26224 : loss : 0.031164, loss_ce: 0.004659, loss_dice: 0.057670
[12:06:29.959] TRAIN: iteration 26225 : loss : 0.136857, loss_ce: 0.005033, loss_dice: 0.268682
[12:06:30.166] TRAIN: iteration 26226 : loss : 0.167894, loss_ce: 0.003417, loss_dice: 0.332372
[12:06:30.372] TRAIN: iteration 26227 : loss : 0.021555, loss_ce: 0.002744, loss_dice: 0.040367
[12:06:30.579] TRAIN: iteration 26228 : loss : 0.043333, loss_ce: 0.004771, loss_dice: 0.081896
[12:06:30.787] TRAIN: iteration 26229 : loss : 0.139411, loss_ce: 0.006767, loss_dice: 0.272055
[12:06:30.994] TRAIN: iteration 26230 : loss : 0.232723, loss_ce: 0.002082, loss_dice: 0.463363
[12:06:31.201] TRAIN: iteration 26231 : loss : 0.167121, loss_ce: 0.003312, loss_dice: 0.330930
[12:06:31.409] TRAIN: iteration 26232 : loss : 0.059836, loss_ce: 0.008516, loss_dice: 0.111157
[12:06:31.617] TRAIN: iteration 26233 : loss : 0.030283, loss_ce: 0.001576, loss_dice: 0.058989
[12:06:31.826] TRAIN: iteration 26234 : loss : 0.108337, loss_ce: 0.004506, loss_dice: 0.212168
[12:06:32.034] TRAIN: iteration 26235 : loss : 0.081965, loss_ce: 0.001919, loss_dice: 0.162011
[12:06:32.242] TRAIN: iteration 26236 : loss : 0.040579, loss_ce: 0.001192, loss_dice: 0.079966
[12:06:32.457] TRAIN: iteration 26237 : loss : 0.090512, loss_ce: 0.002666, loss_dice: 0.178359
[12:06:32.665] TRAIN: iteration 26238 : loss : 0.148996, loss_ce: 0.003450, loss_dice: 0.294542
[12:06:32.874] TRAIN: iteration 26239 : loss : 0.148755, loss_ce: 0.003244, loss_dice: 0.294265
[12:06:33.082] TRAIN: iteration 26240 : loss : 0.244385, loss_ce: 0.001903, loss_dice: 0.486867
[12:06:33.328] TRAIN: iteration 26241 : loss : 0.074062, loss_ce: 0.006467, loss_dice: 0.141657
[12:06:33.536] TRAIN: iteration 26242 : loss : 0.199091, loss_ce: 0.001423, loss_dice: 0.396760
[12:06:33.744] TRAIN: iteration 26243 : loss : 0.073990, loss_ce: 0.005013, loss_dice: 0.142966
[12:06:33.953] TRAIN: iteration 26244 : loss : 0.055437, loss_ce: 0.006511, loss_dice: 0.104362
[12:06:34.161] TRAIN: iteration 26245 : loss : 0.117930, loss_ce: 0.002288, loss_dice: 0.233571
[12:06:34.376] TRAIN: iteration 26246 : loss : 0.148949, loss_ce: 0.005784, loss_dice: 0.292113
[12:06:34.584] TRAIN: iteration 26247 : loss : 0.047563, loss_ce: 0.006445, loss_dice: 0.088681
[12:06:34.791] TRAIN: iteration 26248 : loss : 0.125976, loss_ce: 0.002069, loss_dice: 0.249882
[12:06:34.999] TRAIN: iteration 26249 : loss : 0.061789, loss_ce: 0.003519, loss_dice: 0.120059
[12:06:35.207] TRAIN: iteration 26250 : loss : 0.038373, loss_ce: 0.004142, loss_dice: 0.072603
[12:06:35.416] TRAIN: iteration 26251 : loss : 0.193012, loss_ce: 0.002668, loss_dice: 0.383356
[12:06:35.625] TRAIN: iteration 26252 : loss : 0.100103, loss_ce: 0.001209, loss_dice: 0.198996
[12:06:35.833] TRAIN: iteration 26253 : loss : 0.162166, loss_ce: 0.001765, loss_dice: 0.322566
[12:06:36.043] TRAIN: iteration 26254 : loss : 0.032197, loss_ce: 0.004848, loss_dice: 0.059546
[12:06:36.252] TRAIN: iteration 26255 : loss : 0.114471, loss_ce: 0.005712, loss_dice: 0.223231
[12:06:36.465] TRAIN: iteration 26256 : loss : 0.022418, loss_ce: 0.002467, loss_dice: 0.042370
[12:06:36.675] TRAIN: iteration 26257 : loss : 0.040653, loss_ce: 0.001860, loss_dice: 0.079446
[12:06:36.982] TRAIN: iteration 26258 : loss : 0.073235, loss_ce: 0.004262, loss_dice: 0.142207
[12:06:37.194] TRAIN: iteration 26259 : loss : 0.100859, loss_ce: 0.001261, loss_dice: 0.200456
[12:06:37.405] TRAIN: iteration 26260 : loss : 0.062098, loss_ce: 0.001841, loss_dice: 0.122356
[12:06:37.639] TRAIN: iteration 26261 : loss : 0.075419, loss_ce: 0.002387, loss_dice: 0.148450
[12:06:37.847] TRAIN: iteration 26262 : loss : 0.250819, loss_ce: 0.001555, loss_dice: 0.500083
[12:06:38.061] TRAIN: iteration 26263 : loss : 0.082741, loss_ce: 0.006497, loss_dice: 0.158986
[12:06:38.269] TRAIN: iteration 26264 : loss : 0.087923, loss_ce: 0.005903, loss_dice: 0.169942
[12:06:38.479] TRAIN: iteration 26265 : loss : 0.092322, loss_ce: 0.002858, loss_dice: 0.181787
[12:06:38.690] TRAIN: iteration 26266 : loss : 0.134285, loss_ce: 0.002172, loss_dice: 0.266399
[12:06:38.898] TRAIN: iteration 26267 : loss : 0.121840, loss_ce: 0.005823, loss_dice: 0.237857
[12:06:39.107] TRAIN: iteration 26268 : loss : 0.090101, loss_ce: 0.000815, loss_dice: 0.179386
[12:06:39.316] TRAIN: iteration 26269 : loss : 0.153178, loss_ce: 0.004761, loss_dice: 0.301596
[12:06:39.523] TRAIN: iteration 26270 : loss : 0.079720, loss_ce: 0.003826, loss_dice: 0.155613
[12:06:39.733] TRAIN: iteration 26271 : loss : 0.073958, loss_ce: 0.003723, loss_dice: 0.144192
[12:06:39.946] TRAIN: iteration 26272 : loss : 0.086100, loss_ce: 0.002153, loss_dice: 0.170047
[12:06:40.170] TRAIN: iteration 26273 : loss : 0.082165, loss_ce: 0.004915, loss_dice: 0.159415
[12:06:40.378] TRAIN: iteration 26274 : loss : 0.061964, loss_ce: 0.002590, loss_dice: 0.121337
[12:06:40.586] TRAIN: iteration 26275 : loss : 0.049306, loss_ce: 0.006576, loss_dice: 0.092036
[12:06:40.798] TRAIN: iteration 26276 : loss : 0.098492, loss_ce: 0.001936, loss_dice: 0.195047
[12:06:41.006] TRAIN: iteration 26277 : loss : 0.169670, loss_ce: 0.020459, loss_dice: 0.318882
[12:06:41.215] TRAIN: iteration 26278 : loss : 0.121487, loss_ce: 0.004678, loss_dice: 0.238297
[12:06:41.431] TRAIN: iteration 26279 : loss : 0.250612, loss_ce: 0.001171, loss_dice: 0.500054
[12:06:41.640] TRAIN: iteration 26280 : loss : 0.066482, loss_ce: 0.004531, loss_dice: 0.128434
[12:06:41.883] TRAIN: iteration 26281 : loss : 0.165932, loss_ce: 0.005899, loss_dice: 0.325965
[12:06:42.090] TRAIN: iteration 26282 : loss : 0.115116, loss_ce: 0.003832, loss_dice: 0.226399
[12:06:42.296] TRAIN: iteration 26283 : loss : 0.140689, loss_ce: 0.006822, loss_dice: 0.274556
[12:06:42.521] TRAIN: iteration 26284 : loss : 0.029408, loss_ce: 0.001909, loss_dice: 0.056906
[12:06:42.731] TRAIN: iteration 26285 : loss : 0.116700, loss_ce: 0.005151, loss_dice: 0.228250
[12:06:42.947] TRAIN: iteration 26286 : loss : 0.250885, loss_ce: 0.001670, loss_dice: 0.500100
[12:06:43.156] TRAIN: iteration 26287 : loss : 0.172529, loss_ce: 0.002569, loss_dice: 0.342489
[12:06:43.365] TRAIN: iteration 26288 : loss : 0.252861, loss_ce: 0.005332, loss_dice: 0.500389
[12:06:43.573] TRAIN: iteration 26289 : loss : 0.048078, loss_ce: 0.004512, loss_dice: 0.091643
[12:06:43.781] TRAIN: iteration 26290 : loss : 0.251313, loss_ce: 0.002473, loss_dice: 0.500152
[12:06:43.989] TRAIN: iteration 26291 : loss : 0.075913, loss_ce: 0.001130, loss_dice: 0.150697
[12:06:44.198] TRAIN: iteration 26292 : loss : 0.109328, loss_ce: 0.002270, loss_dice: 0.216386
[12:06:44.405] TRAIN: iteration 26293 : loss : 0.107794, loss_ce: 0.001623, loss_dice: 0.213964
[12:06:44.612] TRAIN: iteration 26294 : loss : 0.144593, loss_ce: 0.007922, loss_dice: 0.281264
[12:06:44.819] TRAIN: iteration 26295 : loss : 0.065985, loss_ce: 0.001503, loss_dice: 0.130468
[12:06:45.026] TRAIN: iteration 26296 : loss : 0.246491, loss_ce: 0.005606, loss_dice: 0.487377
[12:06:45.234] TRAIN: iteration 26297 : loss : 0.064259, loss_ce: 0.002452, loss_dice: 0.126067
[12:06:45.443] TRAIN: iteration 26298 : loss : 0.040633, loss_ce: 0.004153, loss_dice: 0.077113
[12:06:45.651] TRAIN: iteration 26299 : loss : 0.093293, loss_ce: 0.002181, loss_dice: 0.184405
[12:06:45.860] TRAIN: iteration 26300 : loss : 0.026209, loss_ce: 0.000677, loss_dice: 0.051741
[12:06:46.090] TRAIN: iteration 26301 : loss : 0.115146, loss_ce: 0.005469, loss_dice: 0.224823
[12:06:46.297] TRAIN: iteration 26302 : loss : 0.020182, loss_ce: 0.001209, loss_dice: 0.039156
[12:06:46.509] TRAIN: iteration 26303 : loss : 0.048953, loss_ce: 0.000871, loss_dice: 0.097036
[12:06:46.716] TRAIN: iteration 26304 : loss : 0.088865, loss_ce: 0.002731, loss_dice: 0.174999
[12:06:46.923] TRAIN: iteration 26305 : loss : 0.150102, loss_ce: 0.001242, loss_dice: 0.298962
[12:06:47.131] TRAIN: iteration 26306 : loss : 0.092169, loss_ce: 0.003888, loss_dice: 0.180450
[12:06:47.338] TRAIN: iteration 26307 : loss : 0.095364, loss_ce: 0.014947, loss_dice: 0.175780
[12:06:47.547] TRAIN: iteration 26308 : loss : 0.098564, loss_ce: 0.001509, loss_dice: 0.195620
[12:06:47.754] TRAIN: iteration 26309 : loss : 0.051862, loss_ce: 0.009743, loss_dice: 0.093982
[12:06:47.962] TRAIN: iteration 26310 : loss : 0.050944, loss_ce: 0.003219, loss_dice: 0.098668
[12:06:48.171] TRAIN: iteration 26311 : loss : 0.033674, loss_ce: 0.000518, loss_dice: 0.066831
[12:06:48.402] TRAIN: iteration 26312 : loss : 0.064199, loss_ce: 0.001095, loss_dice: 0.127303
[12:06:48.609] TRAIN: iteration 26313 : loss : 0.250252, loss_ce: 0.000496, loss_dice: 0.500009
[12:06:48.817] TRAIN: iteration 26314 : loss : 0.026445, loss_ce: 0.000446, loss_dice: 0.052445
[12:06:49.024] TRAIN: iteration 26315 : loss : 0.055810, loss_ce: 0.002978, loss_dice: 0.108643
[12:06:49.231] TRAIN: iteration 26316 : loss : 0.037163, loss_ce: 0.006430, loss_dice: 0.067897
[12:06:49.440] TRAIN: iteration 26317 : loss : 0.250811, loss_ce: 0.001515, loss_dice: 0.500107
[12:06:49.652] TRAIN: iteration 26318 : loss : 0.044073, loss_ce: 0.009489, loss_dice: 0.078657
[12:06:49.861] TRAIN: iteration 26319 : loss : 0.125133, loss_ce: 0.002761, loss_dice: 0.247504
[12:06:50.069] TRAIN: iteration 26320 : loss : 0.092387, loss_ce: 0.002572, loss_dice: 0.182202
[12:06:50.308] TRAIN: iteration 26321 : loss : 0.056536, loss_ce: 0.003349, loss_dice: 0.109722
[12:06:50.523] TRAIN: iteration 26322 : loss : 0.051386, loss_ce: 0.004924, loss_dice: 0.097847
[12:06:50.734] TRAIN: iteration 26323 : loss : 0.250557, loss_ce: 0.001059, loss_dice: 0.500054
[12:06:50.942] TRAIN: iteration 26324 : loss : 0.250563, loss_ce: 0.001061, loss_dice: 0.500066
[12:06:51.152] TRAIN: iteration 26325 : loss : 0.022187, loss_ce: 0.000449, loss_dice: 0.043926
[12:06:51.359] TRAIN: iteration 26326 : loss : 0.063012, loss_ce: 0.002347, loss_dice: 0.123677
[12:06:51.567] TRAIN: iteration 26327 : loss : 0.026853, loss_ce: 0.002792, loss_dice: 0.050914
[12:06:51.776] TRAIN: iteration 26328 : loss : 0.063760, loss_ce: 0.001739, loss_dice: 0.125781
[12:06:51.989] TRAIN: iteration 26329 : loss : 0.251453, loss_ce: 0.002719, loss_dice: 0.500187
[12:06:52.198] TRAIN: iteration 26330 : loss : 0.140423, loss_ce: 0.002685, loss_dice: 0.278161
[12:06:52.409] TRAIN: iteration 26331 : loss : 0.251119, loss_ce: 0.002086, loss_dice: 0.500151
[12:06:52.617] TRAIN: iteration 26332 : loss : 0.142910, loss_ce: 0.003606, loss_dice: 0.282214
[12:06:52.826] TRAIN: iteration 26333 : loss : 0.144876, loss_ce: 0.002529, loss_dice: 0.287223
[12:06:53.039] TRAIN: iteration 26334 : loss : 0.250914, loss_ce: 0.001724, loss_dice: 0.500103
[12:06:53.251] TRAIN: iteration 26335 : loss : 0.041760, loss_ce: 0.002083, loss_dice: 0.081438
[12:06:53.459] TRAIN: iteration 26336 : loss : 0.082491, loss_ce: 0.015155, loss_dice: 0.149827
[12:06:53.667] TRAIN: iteration 26337 : loss : 0.250677, loss_ce: 0.001275, loss_dice: 0.500078
[12:06:53.877] TRAIN: iteration 26338 : loss : 0.252105, loss_ce: 0.003921, loss_dice: 0.500290
[12:06:54.092] TRAIN: iteration 26339 : loss : 0.097370, loss_ce: 0.001628, loss_dice: 0.193112
[12:06:54.299] TRAIN: iteration 26340 : loss : 0.249941, loss_ce: 0.001129, loss_dice: 0.498753
[12:06:54.537] TRAIN: iteration 26341 : loss : 0.108496, loss_ce: 0.003657, loss_dice: 0.213334
[12:06:54.746] TRAIN: iteration 26342 : loss : 0.250817, loss_ce: 0.001535, loss_dice: 0.500099
[12:06:54.953] TRAIN: iteration 26343 : loss : 0.171759, loss_ce: 0.003821, loss_dice: 0.339698
[12:06:55.160] TRAIN: iteration 26344 : loss : 0.248864, loss_ce: 0.004149, loss_dice: 0.493579
[12:06:55.373] TRAIN: iteration 26345 : loss : 0.151392, loss_ce: 0.006662, loss_dice: 0.296122
[12:06:55.589] TRAIN: iteration 26346 : loss : 0.250840, loss_ce: 0.001577, loss_dice: 0.500102
[12:06:55.800] TRAIN: iteration 26347 : loss : 0.251375, loss_ce: 0.002587, loss_dice: 0.500163
[12:06:56.012] TRAIN: iteration 26348 : loss : 0.088533, loss_ce: 0.005353, loss_dice: 0.171713
[12:06:56.235] TRAIN: iteration 26349 : loss : 0.149853, loss_ce: 0.002091, loss_dice: 0.297615
[12:06:56.450] TRAIN: iteration 26350 : loss : 0.141315, loss_ce: 0.008198, loss_dice: 0.274431
[12:06:56.658] TRAIN: iteration 26351 : loss : 0.237859, loss_ce: 0.002213, loss_dice: 0.473505
[12:06:56.866] TRAIN: iteration 26352 : loss : 0.095543, loss_ce: 0.003163, loss_dice: 0.187923
[12:06:57.075] TRAIN: iteration 26353 : loss : 0.199754, loss_ce: 0.001771, loss_dice: 0.397738
[12:06:57.291] TRAIN: iteration 26354 : loss : 0.009126, loss_ce: 0.001140, loss_dice: 0.017112
[12:06:57.508] TRAIN: iteration 26355 : loss : 0.241949, loss_ce: 0.001664, loss_dice: 0.482233
[12:06:57.717] TRAIN: iteration 26356 : loss : 0.049763, loss_ce: 0.002845, loss_dice: 0.096682
[12:06:57.925] TRAIN: iteration 26357 : loss : 0.050581, loss_ce: 0.001800, loss_dice: 0.099363
[12:06:58.132] TRAIN: iteration 26358 : loss : 0.047076, loss_ce: 0.001221, loss_dice: 0.092931
[12:06:58.340] TRAIN: iteration 26359 : loss : 0.028770, loss_ce: 0.003998, loss_dice: 0.053542
[12:06:58.547] TRAIN: iteration 26360 : loss : 0.113744, loss_ce: 0.005423, loss_dice: 0.222065
[12:06:58.785] TRAIN: iteration 26361 : loss : 0.051660, loss_ce: 0.000971, loss_dice: 0.102349
[12:06:58.994] TRAIN: iteration 26362 : loss : 0.109515, loss_ce: 0.004005, loss_dice: 0.215025
[12:06:59.201] TRAIN: iteration 26363 : loss : 0.023711, loss_ce: 0.000982, loss_dice: 0.046441
[12:06:59.408] TRAIN: iteration 26364 : loss : 0.250712, loss_ce: 0.001361, loss_dice: 0.500063
[12:06:59.616] TRAIN: iteration 26365 : loss : 0.063958, loss_ce: 0.004496, loss_dice: 0.123421
[12:06:59.830] TRAIN: iteration 26366 : loss : 0.057595, loss_ce: 0.001943, loss_dice: 0.113247
[12:07:00.037] TRAIN: iteration 26367 : loss : 0.093147, loss_ce: 0.004667, loss_dice: 0.181627
[12:07:00.986] TRAIN: iteration 26368 : loss : 0.250632, loss_ce: 0.001206, loss_dice: 0.500058
[12:07:01.195] TRAIN: iteration 26369 : loss : 0.077499, loss_ce: 0.002468, loss_dice: 0.152530
[12:07:01.410] TRAIN: iteration 26370 : loss : 0.118683, loss_ce: 0.007093, loss_dice: 0.230272
[12:07:01.618] TRAIN: iteration 26371 : loss : 0.047255, loss_ce: 0.003073, loss_dice: 0.091437
[12:07:01.826] TRAIN: iteration 26372 : loss : 0.112893, loss_ce: 0.003015, loss_dice: 0.222772
[12:07:02.047] TRAIN: iteration 26373 : loss : 0.223461, loss_ce: 0.002068, loss_dice: 0.444853
[12:07:02.261] TRAIN: iteration 26374 : loss : 0.047238, loss_ce: 0.001395, loss_dice: 0.093082
[12:07:02.478] TRAIN: iteration 26375 : loss : 0.069778, loss_ce: 0.006816, loss_dice: 0.132740
[12:07:02.690] TRAIN: iteration 26376 : loss : 0.054739, loss_ce: 0.004360, loss_dice: 0.105118
[12:07:02.902] TRAIN: iteration 26377 : loss : 0.234100, loss_ce: 0.002603, loss_dice: 0.465597
[12:07:03.115] TRAIN: iteration 26378 : loss : 0.060168, loss_ce: 0.002600, loss_dice: 0.117736
[12:07:03.322] TRAIN: iteration 26379 : loss : 0.250878, loss_ce: 0.001667, loss_dice: 0.500090
[12:07:03.539] TRAIN: iteration 26380 : loss : 0.112920, loss_ce: 0.003455, loss_dice: 0.222385
[12:07:03.787] TRAIN: iteration 26381 : loss : 0.187791, loss_ce: 0.003041, loss_dice: 0.372541
[12:07:03.995] TRAIN: iteration 26382 : loss : 0.250580, loss_ce: 0.001119, loss_dice: 0.500040
[12:07:04.203] TRAIN: iteration 26383 : loss : 0.052217, loss_ce: 0.002181, loss_dice: 0.102254
[12:07:04.417] TRAIN: iteration 26384 : loss : 0.083152, loss_ce: 0.001223, loss_dice: 0.165081
[12:07:04.625] TRAIN: iteration 26385 : loss : 0.055522, loss_ce: 0.006362, loss_dice: 0.104682
[12:07:04.834] TRAIN: iteration 26386 : loss : 0.041774, loss_ce: 0.003404, loss_dice: 0.080145
[12:07:05.042] TRAIN: iteration 26387 : loss : 0.250735, loss_ce: 0.001410, loss_dice: 0.500060
[12:07:05.255] TRAIN: iteration 26388 : loss : 0.251011, loss_ce: 0.001900, loss_dice: 0.500121
[12:07:05.462] TRAIN: iteration 26389 : loss : 0.097813, loss_ce: 0.006632, loss_dice: 0.188994
[12:07:05.672] TRAIN: iteration 26390 : loss : 0.084970, loss_ce: 0.009088, loss_dice: 0.160852
[12:07:05.879] TRAIN: iteration 26391 : loss : 0.205337, loss_ce: 0.002815, loss_dice: 0.407859
[12:07:06.087] TRAIN: iteration 26392 : loss : 0.061202, loss_ce: 0.003891, loss_dice: 0.118512
[12:07:06.297] TRAIN: iteration 26393 : loss : 0.250933, loss_ce: 0.001757, loss_dice: 0.500108
[12:07:06.514] TRAIN: iteration 26394 : loss : 0.119563, loss_ce: 0.003064, loss_dice: 0.236061
[12:07:06.721] TRAIN: iteration 26395 : loss : 0.139226, loss_ce: 0.001925, loss_dice: 0.276527
[12:07:06.928] TRAIN: iteration 26396 : loss : 0.142701, loss_ce: 0.001984, loss_dice: 0.283418
[12:07:07.139] TRAIN: iteration 26397 : loss : 0.132838, loss_ce: 0.005660, loss_dice: 0.260016
[12:07:07.346] TRAIN: iteration 26398 : loss : 0.068317, loss_ce: 0.003422, loss_dice: 0.133213
[12:07:07.557] TRAIN: iteration 26399 : loss : 0.031445, loss_ce: 0.001499, loss_dice: 0.061392
[12:07:07.773] TRAIN: iteration 26400 : loss : 0.127982, loss_ce: 0.005597, loss_dice: 0.250368
[12:07:08.014] TRAIN: iteration 26401 : loss : 0.059458, loss_ce: 0.002904, loss_dice: 0.116013
[12:07:08.227] TRAIN: iteration 26402 : loss : 0.078378, loss_ce: 0.001837, loss_dice: 0.154918
[12:07:08.628] TRAIN: iteration 26403 : loss : 0.120707, loss_ce: 0.002794, loss_dice: 0.238619
[12:07:08.836] TRAIN: iteration 26404 : loss : 0.122660, loss_ce: 0.017202, loss_dice: 0.228117
[12:07:09.044] TRAIN: iteration 26405 : loss : 0.061527, loss_ce: 0.002142, loss_dice: 0.120912
[12:07:09.258] TRAIN: iteration 26406 : loss : 0.178995, loss_ce: 0.002662, loss_dice: 0.355327
[12:07:09.468] TRAIN: iteration 26407 : loss : 0.097548, loss_ce: 0.001214, loss_dice: 0.193883
[12:07:09.682] TRAIN: iteration 26408 : loss : 0.029504, loss_ce: 0.001480, loss_dice: 0.057528
[12:07:09.890] TRAIN: iteration 26409 : loss : 0.041275, loss_ce: 0.002342, loss_dice: 0.080207
[12:07:10.104] TRAIN: iteration 26410 : loss : 0.188230, loss_ce: 0.024114, loss_dice: 0.352347
[12:07:10.322] TRAIN: iteration 26411 : loss : 0.132735, loss_ce: 0.002585, loss_dice: 0.262885
[12:07:10.542] TRAIN: iteration 26412 : loss : 0.239949, loss_ce: 0.009767, loss_dice: 0.470132
[12:07:10.750] TRAIN: iteration 26413 : loss : 0.249896, loss_ce: 0.002261, loss_dice: 0.497531
[12:07:10.957] TRAIN: iteration 26414 : loss : 0.048948, loss_ce: 0.003312, loss_dice: 0.094584
[12:07:11.798] TRAIN: iteration 26415 : loss : 0.033186, loss_ce: 0.001607, loss_dice: 0.064765
[12:07:12.006] TRAIN: iteration 26416 : loss : 0.019647, loss_ce: 0.001068, loss_dice: 0.038226
[12:07:12.213] TRAIN: iteration 26417 : loss : 0.096924, loss_ce: 0.001535, loss_dice: 0.192313
[12:07:12.422] TRAIN: iteration 26418 : loss : 0.016432, loss_ce: 0.000669, loss_dice: 0.032196
[12:07:12.633] TRAIN: iteration 26419 : loss : 0.132707, loss_ce: 0.003903, loss_dice: 0.261512
[12:07:12.846] TRAIN: iteration 26420 : loss : 0.123493, loss_ce: 0.003722, loss_dice: 0.243264
[12:07:13.086] TRAIN: iteration 26421 : loss : 0.082802, loss_ce: 0.009208, loss_dice: 0.156396
[12:07:13.297] TRAIN: iteration 26422 : loss : 0.043573, loss_ce: 0.001818, loss_dice: 0.085328
[12:07:13.514] TRAIN: iteration 26423 : loss : 0.068477, loss_ce: 0.001296, loss_dice: 0.135658
[12:07:13.721] TRAIN: iteration 26424 : loss : 0.031154, loss_ce: 0.000810, loss_dice: 0.061498
[12:07:13.952] TRAIN: iteration 26425 : loss : 0.141895, loss_ce: 0.002770, loss_dice: 0.281020
[12:07:14.159] TRAIN: iteration 26426 : loss : 0.046914, loss_ce: 0.002446, loss_dice: 0.091381
[12:07:14.370] TRAIN: iteration 26427 : loss : 0.069648, loss_ce: 0.003421, loss_dice: 0.135875
[12:07:14.578] TRAIN: iteration 26428 : loss : 0.026811, loss_ce: 0.006683, loss_dice: 0.046939
[12:07:14.790] TRAIN: iteration 26429 : loss : 0.124323, loss_ce: 0.001313, loss_dice: 0.247332
[12:07:14.999] TRAIN: iteration 26430 : loss : 0.249551, loss_ce: 0.001199, loss_dice: 0.497903
[12:07:15.207] TRAIN: iteration 26431 : loss : 0.068610, loss_ce: 0.004724, loss_dice: 0.132496
[12:07:16.181] TRAIN: iteration 26432 : loss : 0.117163, loss_ce: 0.001015, loss_dice: 0.233312
[12:07:16.397] TRAIN: iteration 26433 : loss : 0.114937, loss_ce: 0.004588, loss_dice: 0.225286
[12:07:16.604] TRAIN: iteration 26434 : loss : 0.037543, loss_ce: 0.005337, loss_dice: 0.069748
[12:07:16.812] TRAIN: iteration 26435 : loss : 0.251130, loss_ce: 0.002116, loss_dice: 0.500145
[12:07:17.020] TRAIN: iteration 26436 : loss : 0.251055, loss_ce: 0.001967, loss_dice: 0.500143
[12:07:17.231] TRAIN: iteration 26437 : loss : 0.021758, loss_ce: 0.001445, loss_dice: 0.042070
[12:07:17.439] TRAIN: iteration 26438 : loss : 0.146690, loss_ce: 0.001865, loss_dice: 0.291514
[12:07:17.646] TRAIN: iteration 26439 : loss : 0.127327, loss_ce: 0.024415, loss_dice: 0.230240
[12:07:17.855] TRAIN: iteration 26440 : loss : 0.069114, loss_ce: 0.002301, loss_dice: 0.135927
[12:07:18.093] TRAIN: iteration 26441 : loss : 0.113450, loss_ce: 0.003181, loss_dice: 0.223719
[12:07:18.304] TRAIN: iteration 26442 : loss : 0.020321, loss_ce: 0.001181, loss_dice: 0.039461
[12:07:18.512] TRAIN: iteration 26443 : loss : 0.085775, loss_ce: 0.003313, loss_dice: 0.168238
[12:07:18.719] TRAIN: iteration 26444 : loss : 0.040762, loss_ce: 0.005506, loss_dice: 0.076019
[12:07:18.927] TRAIN: iteration 26445 : loss : 0.159495, loss_ce: 0.006235, loss_dice: 0.312755
[12:07:19.135] TRAIN: iteration 26446 : loss : 0.064109, loss_ce: 0.004355, loss_dice: 0.123863
[12:07:19.348] TRAIN: iteration 26447 : loss : 0.098254, loss_ce: 0.002798, loss_dice: 0.193710
[12:07:19.557] TRAIN: iteration 26448 : loss : 0.117565, loss_ce: 0.003994, loss_dice: 0.231137
[12:07:19.766] TRAIN: iteration 26449 : loss : 0.128624, loss_ce: 0.009265, loss_dice: 0.247983
[12:07:19.974] TRAIN: iteration 26450 : loss : 0.243035, loss_ce: 0.001824, loss_dice: 0.484245
[12:07:20.183] TRAIN: iteration 26451 : loss : 0.057137, loss_ce: 0.003356, loss_dice: 0.110918
[12:07:20.392] TRAIN: iteration 26452 : loss : 0.174124, loss_ce: 0.001566, loss_dice: 0.346682
[12:07:20.830] TRAIN: iteration 26453 : loss : 0.031448, loss_ce: 0.002257, loss_dice: 0.060639
[12:07:21.038] TRAIN: iteration 26454 : loss : 0.150960, loss_ce: 0.010939, loss_dice: 0.290981
[12:07:21.246] TRAIN: iteration 26455 : loss : 0.057349, loss_ce: 0.004866, loss_dice: 0.109832
[12:07:21.453] TRAIN: iteration 26456 : loss : 0.106905, loss_ce: 0.013464, loss_dice: 0.200347
[12:07:21.663] TRAIN: iteration 26457 : loss : 0.102472, loss_ce: 0.003729, loss_dice: 0.201215
[12:07:21.881] TRAIN: iteration 26458 : loss : 0.250874, loss_ce: 0.001656, loss_dice: 0.500092
[12:07:22.095] TRAIN: iteration 26459 : loss : 0.084538, loss_ce: 0.003690, loss_dice: 0.165385
[12:07:22.302] TRAIN: iteration 26460 : loss : 0.059070, loss_ce: 0.002229, loss_dice: 0.115911
[12:07:22.543] TRAIN: iteration 26461 : loss : 0.246870, loss_ce: 0.001958, loss_dice: 0.491782
[12:07:22.756] TRAIN: iteration 26462 : loss : 0.085984, loss_ce: 0.013357, loss_dice: 0.158611
[12:07:22.966] TRAIN: iteration 26463 : loss : 0.028996, loss_ce: 0.000881, loss_dice: 0.057110
[12:07:23.176] TRAIN: iteration 26464 : loss : 0.101001, loss_ce: 0.001714, loss_dice: 0.200288
[12:07:23.433] TRAIN: iteration 26465 : loss : 0.188168, loss_ce: 0.007571, loss_dice: 0.368765
[12:07:23.645] TRAIN: iteration 26466 : loss : 0.105677, loss_ce: 0.002756, loss_dice: 0.208597
[12:07:23.853] TRAIN: iteration 26467 : loss : 0.242885, loss_ce: 0.001269, loss_dice: 0.484501
[12:07:24.061] TRAIN: iteration 26468 : loss : 0.248384, loss_ce: 0.007458, loss_dice: 0.489310
[12:07:24.269] TRAIN: iteration 26469 : loss : 0.052082, loss_ce: 0.004082, loss_dice: 0.100081
[12:07:24.480] TRAIN: iteration 26470 : loss : 0.077194, loss_ce: 0.001928, loss_dice: 0.152459
[12:07:24.693] TRAIN: iteration 26471 : loss : 0.050343, loss_ce: 0.002632, loss_dice: 0.098054
[12:07:24.904] TRAIN: iteration 26472 : loss : 0.108029, loss_ce: 0.002805, loss_dice: 0.213253
[12:07:25.112] TRAIN: iteration 26473 : loss : 0.056523, loss_ce: 0.003515, loss_dice: 0.109530
[12:07:25.325] TRAIN: iteration 26474 : loss : 0.191814, loss_ce: 0.001442, loss_dice: 0.382187
[12:07:25.538] TRAIN: iteration 26475 : loss : 0.250389, loss_ce: 0.000758, loss_dice: 0.500020
[12:07:25.746] TRAIN: iteration 26476 : loss : 0.059753, loss_ce: 0.001338, loss_dice: 0.118169
[12:07:25.954] TRAIN: iteration 26477 : loss : 0.088072, loss_ce: 0.004629, loss_dice: 0.171515
[12:07:26.162] TRAIN: iteration 26478 : loss : 0.250631, loss_ce: 0.001908, loss_dice: 0.499354
[12:07:26.380] TRAIN: iteration 26479 : loss : 0.142127, loss_ce: 0.008815, loss_dice: 0.275439
[12:07:26.588] TRAIN: iteration 26480 : loss : 0.121222, loss_ce: 0.003895, loss_dice: 0.238549
[12:07:26.831] TRAIN: iteration 26481 : loss : 0.207751, loss_ce: 0.003585, loss_dice: 0.411918
[12:07:27.040] TRAIN: iteration 26482 : loss : 0.070869, loss_ce: 0.005542, loss_dice: 0.136196
[12:07:27.255] TRAIN: iteration 26483 : loss : 0.035115, loss_ce: 0.001571, loss_dice: 0.068660
[12:07:27.467] TRAIN: iteration 26484 : loss : 0.108631, loss_ce: 0.007937, loss_dice: 0.209326
[12:07:27.683] TRAIN: iteration 26485 : loss : 0.146514, loss_ce: 0.002335, loss_dice: 0.290693
[12:07:27.891] TRAIN: iteration 26486 : loss : 0.251347, loss_ce: 0.002841, loss_dice: 0.499852
[12:07:28.099] TRAIN: iteration 26487 : loss : 0.070341, loss_ce: 0.003138, loss_dice: 0.137544
[12:07:28.310] TRAIN: iteration 26488 : loss : 0.080436, loss_ce: 0.003501, loss_dice: 0.157371
[12:07:28.518] TRAIN: iteration 26489 : loss : 0.055188, loss_ce: 0.002365, loss_dice: 0.108011
[12:07:28.729] TRAIN: iteration 26490 : loss : 0.011738, loss_ce: 0.001173, loss_dice: 0.022304
[12:07:28.937] TRAIN: iteration 26491 : loss : 0.048742, loss_ce: 0.005034, loss_dice: 0.092450
[12:07:29.154] TRAIN: iteration 26492 : loss : 0.178581, loss_ce: 0.002675, loss_dice: 0.354488
[12:07:29.360] TRAIN: iteration 26493 : loss : 0.115261, loss_ce: 0.002241, loss_dice: 0.228281
[12:07:30.302] TRAIN: iteration 26494 : loss : 0.084430, loss_ce: 0.004355, loss_dice: 0.164505
[12:07:30.509] TRAIN: iteration 26495 : loss : 0.035474, loss_ce: 0.005071, loss_dice: 0.065878
[12:07:30.717] TRAIN: iteration 26496 : loss : 0.041793, loss_ce: 0.004274, loss_dice: 0.079312
[12:07:30.933] TRAIN: iteration 26497 : loss : 0.055746, loss_ce: 0.010617, loss_dice: 0.100874
[12:07:31.141] TRAIN: iteration 26498 : loss : 0.133857, loss_ce: 0.004541, loss_dice: 0.263173
[12:07:31.352] TRAIN: iteration 26499 : loss : 0.040810, loss_ce: 0.001944, loss_dice: 0.079676
[12:07:31.559] TRAIN: iteration 26500 : loss : 0.065398, loss_ce: 0.005661, loss_dice: 0.125136
[12:07:31.798] TRAIN: iteration 26501 : loss : 0.086207, loss_ce: 0.003940, loss_dice: 0.168475
[12:07:32.005] TRAIN: iteration 26502 : loss : 0.214754, loss_ce: 0.004849, loss_dice: 0.424659
[12:07:32.215] TRAIN: iteration 26503 : loss : 0.234130, loss_ce: 0.008035, loss_dice: 0.460225
[12:07:32.443] TRAIN: iteration 26504 : loss : 0.061103, loss_ce: 0.003282, loss_dice: 0.118925
[12:07:32.651] TRAIN: iteration 26505 : loss : 0.037305, loss_ce: 0.003692, loss_dice: 0.070918
[12:07:32.859] TRAIN: iteration 26506 : loss : 0.251215, loss_ce: 0.002296, loss_dice: 0.500133
[12:07:33.068] TRAIN: iteration 26507 : loss : 0.026299, loss_ce: 0.002370, loss_dice: 0.050228
[12:07:33.275] TRAIN: iteration 26508 : loss : 0.118977, loss_ce: 0.002025, loss_dice: 0.235928
[12:07:33.483] TRAIN: iteration 26509 : loss : 0.046071, loss_ce: 0.005569, loss_dice: 0.086574
[12:07:33.744] TRAIN: iteration 26510 : loss : 0.250812, loss_ce: 0.001539, loss_dice: 0.500084
[12:07:33.952] TRAIN: iteration 26511 : loss : 0.089990, loss_ce: 0.001352, loss_dice: 0.178629
[12:07:34.161] TRAIN: iteration 26512 : loss : 0.091733, loss_ce: 0.013581, loss_dice: 0.169885
[12:07:34.369] TRAIN: iteration 26513 : loss : 0.250792, loss_ce: 0.001506, loss_dice: 0.500078
[12:07:34.583] TRAIN: iteration 26514 : loss : 0.052227, loss_ce: 0.002384, loss_dice: 0.102070
[12:07:34.790] TRAIN: iteration 26515 : loss : 0.219258, loss_ce: 0.002084, loss_dice: 0.436432
[12:07:35.001] TRAIN: iteration 26516 : loss : 0.181321, loss_ce: 0.007159, loss_dice: 0.355482
[12:07:35.210] TRAIN: iteration 26517 : loss : 0.047661, loss_ce: 0.004072, loss_dice: 0.091250
[12:07:35.420] TRAIN: iteration 26518 : loss : 0.065963, loss_ce: 0.004250, loss_dice: 0.127676
[12:07:35.629] TRAIN: iteration 26519 : loss : 0.071936, loss_ce: 0.002547, loss_dice: 0.141325
[12:07:35.837] TRAIN: iteration 26520 : loss : 0.124711, loss_ce: 0.011165, loss_dice: 0.238258
[12:07:36.080] TRAIN: iteration 26521 : loss : 0.061769, loss_ce: 0.001767, loss_dice: 0.121772
[12:07:36.302] TRAIN: iteration 26522 : loss : 0.247551, loss_ce: 0.003210, loss_dice: 0.491891
[12:07:36.509] TRAIN: iteration 26523 : loss : 0.042013, loss_ce: 0.004685, loss_dice: 0.079341
[12:07:36.726] TRAIN: iteration 26524 : loss : 0.075044, loss_ce: 0.003204, loss_dice: 0.146883
[12:07:36.935] TRAIN: iteration 26525 : loss : 0.137027, loss_ce: 0.002491, loss_dice: 0.271563
[12:07:37.144] TRAIN: iteration 26526 : loss : 0.073945, loss_ce: 0.000973, loss_dice: 0.146916
[12:07:37.351] TRAIN: iteration 26527 : loss : 0.243404, loss_ce: 0.002993, loss_dice: 0.483815
[12:07:37.560] TRAIN: iteration 26528 : loss : 0.242961, loss_ce: 0.001183, loss_dice: 0.484740
[12:07:37.773] TRAIN: iteration 26529 : loss : 0.250639, loss_ce: 0.001215, loss_dice: 0.500063
[12:07:37.980] TRAIN: iteration 26530 : loss : 0.175409, loss_ce: 0.003511, loss_dice: 0.347308
[12:07:38.188] TRAIN: iteration 26531 : loss : 0.043797, loss_ce: 0.003075, loss_dice: 0.084518
[12:07:38.397] TRAIN: iteration 26532 : loss : 0.086229, loss_ce: 0.000985, loss_dice: 0.171474
[12:07:38.681] TRAIN: iteration 26533 : loss : 0.193908, loss_ce: 0.005430, loss_dice: 0.382387
[12:07:38.890] TRAIN: iteration 26534 : loss : 0.161706, loss_ce: 0.005838, loss_dice: 0.317575
[12:07:39.099] TRAIN: iteration 26535 : loss : 0.056228, loss_ce: 0.001437, loss_dice: 0.111020
[12:07:39.309] TRAIN: iteration 26536 : loss : 0.054982, loss_ce: 0.004263, loss_dice: 0.105702
[12:07:39.517] TRAIN: iteration 26537 : loss : 0.252292, loss_ce: 0.007002, loss_dice: 0.497583
[12:07:39.732] TRAIN: iteration 26538 : loss : 0.248365, loss_ce: 0.001676, loss_dice: 0.495054
[12:07:39.941] TRAIN: iteration 26539 : loss : 0.027806, loss_ce: 0.005483, loss_dice: 0.050128
[12:07:40.149] TRAIN: iteration 26540 : loss : 0.254968, loss_ce: 0.010247, loss_dice: 0.499690
[12:07:40.385] TRAIN: iteration 26541 : loss : 0.048441, loss_ce: 0.001196, loss_dice: 0.095685
[12:07:40.592] TRAIN: iteration 26542 : loss : 0.124698, loss_ce: 0.003111, loss_dice: 0.246284
[12:07:41.392] TRAIN: iteration 26543 : loss : 0.134417, loss_ce: 0.002420, loss_dice: 0.266414
[12:07:41.599] TRAIN: iteration 26544 : loss : 0.042648, loss_ce: 0.004780, loss_dice: 0.080516
[12:07:41.807] TRAIN: iteration 26545 : loss : 0.108679, loss_ce: 0.002983, loss_dice: 0.214375
[12:07:42.017] TRAIN: iteration 26546 : loss : 0.250760, loss_ce: 0.001462, loss_dice: 0.500059
[12:07:42.225] TRAIN: iteration 26547 : loss : 0.144167, loss_ce: 0.011498, loss_dice: 0.276836
[12:07:42.432] TRAIN: iteration 26548 : loss : 0.164291, loss_ce: 0.006515, loss_dice: 0.322066
[12:08:04.384] TRAIN: iteration 26549 : loss : 0.108510, loss_ce: 0.005633, loss_dice: 0.211387
[12:08:04.591] TRAIN: iteration 26550 : loss : 0.101859, loss_ce: 0.011380, loss_dice: 0.192338
[12:08:04.798] TRAIN: iteration 26551 : loss : 0.042723, loss_ce: 0.001475, loss_dice: 0.083971
[12:08:05.005] TRAIN: iteration 26552 : loss : 0.058650, loss_ce: 0.005893, loss_dice: 0.111408
[12:08:05.212] TRAIN: iteration 26553 : loss : 0.213275, loss_ce: 0.002368, loss_dice: 0.424182
[12:08:05.419] TRAIN: iteration 26554 : loss : 0.224329, loss_ce: 0.004775, loss_dice: 0.443884
[12:08:05.627] TRAIN: iteration 26555 : loss : 0.044732, loss_ce: 0.001161, loss_dice: 0.088303
[12:08:05.835] TRAIN: iteration 26556 : loss : 0.209989, loss_ce: 0.004633, loss_dice: 0.415344
[12:08:06.044] TRAIN: iteration 26557 : loss : 0.053212, loss_ce: 0.001655, loss_dice: 0.104769
[12:08:06.251] TRAIN: iteration 26558 : loss : 0.098937, loss_ce: 0.001891, loss_dice: 0.195982
[12:08:06.461] TRAIN: iteration 26559 : loss : 0.243158, loss_ce: 0.003502, loss_dice: 0.482814
[12:08:06.700] TRAIN: iteration 26560 : loss : 0.105059, loss_ce: 0.002185, loss_dice: 0.207932
[12:08:06.937] TRAIN: iteration 26561 : loss : 0.201824, loss_ce: 0.002932, loss_dice: 0.400715
[12:08:07.147] TRAIN: iteration 26562 : loss : 0.150949, loss_ce: 0.001948, loss_dice: 0.299950
[12:08:07.357] TRAIN: iteration 26563 : loss : 0.224517, loss_ce: 0.005356, loss_dice: 0.443678
[12:08:07.596] TRAIN: iteration 26564 : loss : 0.053201, loss_ce: 0.004099, loss_dice: 0.102303
[12:08:07.805] TRAIN: iteration 26565 : loss : 0.250832, loss_ce: 0.001594, loss_dice: 0.500070
[12:08:08.013] TRAIN: iteration 26566 : loss : 0.178440, loss_ce: 0.002378, loss_dice: 0.354503
[12:08:08.222] TRAIN: iteration 26567 : loss : 0.063303, loss_ce: 0.001342, loss_dice: 0.125264
[12:08:08.430] TRAIN: iteration 26568 : loss : 0.220657, loss_ce: 0.001452, loss_dice: 0.439862
[12:08:08.637] TRAIN: iteration 26569 : loss : 0.107427, loss_ce: 0.013428, loss_dice: 0.201426
[12:08:08.846] TRAIN: iteration 26570 : loss : 0.022639, loss_ce: 0.001643, loss_dice: 0.043636
[12:08:09.053] TRAIN: iteration 26571 : loss : 0.043019, loss_ce: 0.001217, loss_dice: 0.084821
[12:08:09.260] TRAIN: iteration 26572 : loss : 0.143745, loss_ce: 0.001138, loss_dice: 0.286353
[12:08:09.472] TRAIN: iteration 26573 : loss : 0.076093, loss_ce: 0.005898, loss_dice: 0.146288
[12:08:09.681] TRAIN: iteration 26574 : loss : 0.106208, loss_ce: 0.002336, loss_dice: 0.210079
[12:08:09.888] TRAIN: iteration 26575 : loss : 0.053745, loss_ce: 0.006590, loss_dice: 0.100900
[12:08:10.096] TRAIN: iteration 26576 : loss : 0.045459, loss_ce: 0.003756, loss_dice: 0.087162
[12:08:10.302] TRAIN: iteration 26577 : loss : 0.048251, loss_ce: 0.006634, loss_dice: 0.089869
[12:08:10.511] TRAIN: iteration 26578 : loss : 0.150592, loss_ce: 0.006695, loss_dice: 0.294488
[12:08:10.717] TRAIN: iteration 26579 : loss : 0.099969, loss_ce: 0.007244, loss_dice: 0.192694
[12:08:10.924] TRAIN: iteration 26580 : loss : 0.105121, loss_ce: 0.023544, loss_dice: 0.186698
[12:08:11.164] TRAIN: iteration 26581 : loss : 0.155216, loss_ce: 0.002036, loss_dice: 0.308396
[12:08:11.371] TRAIN: iteration 26582 : loss : 0.213428, loss_ce: 0.001240, loss_dice: 0.425616
[12:08:11.578] TRAIN: iteration 26583 : loss : 0.105249, loss_ce: 0.001827, loss_dice: 0.208670
[12:08:11.786] TRAIN: iteration 26584 : loss : 0.053589, loss_ce: 0.001415, loss_dice: 0.105762
[12:08:11.995] TRAIN: iteration 26585 : loss : 0.119162, loss_ce: 0.001030, loss_dice: 0.237294
[12:08:12.209] TRAIN: iteration 26586 : loss : 0.250370, loss_ce: 0.000734, loss_dice: 0.500006
[12:08:12.416] TRAIN: iteration 26587 : loss : 0.164693, loss_ce: 0.004733, loss_dice: 0.324654
[12:08:12.625] TRAIN: iteration 26588 : loss : 0.152886, loss_ce: 0.002811, loss_dice: 0.302962
[12:08:12.835] TRAIN: iteration 26589 : loss : 0.138911, loss_ce: 0.005799, loss_dice: 0.272023
[12:08:13.049] TRAIN: iteration 26590 : loss : 0.120524, loss_ce: 0.001791, loss_dice: 0.239257
[12:08:13.258] TRAIN: iteration 26591 : loss : 0.158474, loss_ce: 0.001296, loss_dice: 0.315651
[12:08:13.468] TRAIN: iteration 26592 : loss : 0.086088, loss_ce: 0.001378, loss_dice: 0.170799
[12:08:13.679] TRAIN: iteration 26593 : loss : 0.201592, loss_ce: 0.000776, loss_dice: 0.402408
[12:08:13.893] TRAIN: iteration 26594 : loss : 0.075087, loss_ce: 0.001006, loss_dice: 0.149168
[12:08:14.104] TRAIN: iteration 26595 : loss : 0.030155, loss_ce: 0.002581, loss_dice: 0.057729
[12:08:14.316] TRAIN: iteration 26596 : loss : 0.086917, loss_ce: 0.001885, loss_dice: 0.171948
[12:08:14.527] TRAIN: iteration 26597 : loss : 0.250279, loss_ce: 0.000552, loss_dice: 0.500006
[12:08:14.766] TRAIN: iteration 26598 : loss : 0.104545, loss_ce: 0.005366, loss_dice: 0.203724
[12:08:14.980] TRAIN: iteration 26599 : loss : 0.189259, loss_ce: 0.008705, loss_dice: 0.369814
[12:08:15.187] TRAIN: iteration 26600 : loss : 0.024539, loss_ce: 0.001184, loss_dice: 0.047893
[12:08:15.430] TRAIN: iteration 26601 : loss : 0.045050, loss_ce: 0.002099, loss_dice: 0.088001
[12:08:15.648] TRAIN: iteration 26602 : loss : 0.056735, loss_ce: 0.003125, loss_dice: 0.110345
[12:08:15.862] TRAIN: iteration 26603 : loss : 0.019481, loss_ce: 0.001049, loss_dice: 0.037912
[12:08:16.079] TRAIN: iteration 26604 : loss : 0.111997, loss_ce: 0.001132, loss_dice: 0.222862
[12:08:16.294] TRAIN: iteration 26605 : loss : 0.055765, loss_ce: 0.003769, loss_dice: 0.107762
[12:08:16.503] TRAIN: iteration 26606 : loss : 0.090230, loss_ce: 0.002895, loss_dice: 0.177566
[12:08:16.711] TRAIN: iteration 26607 : loss : 0.049570, loss_ce: 0.002876, loss_dice: 0.096265
[12:08:16.918] TRAIN: iteration 26608 : loss : 0.250523, loss_ce: 0.000991, loss_dice: 0.500056
[12:08:17.126] TRAIN: iteration 26609 : loss : 0.076446, loss_ce: 0.003032, loss_dice: 0.149860
[12:08:17.333] TRAIN: iteration 26610 : loss : 0.057364, loss_ce: 0.001235, loss_dice: 0.113494
[12:08:17.540] TRAIN: iteration 26611 : loss : 0.071451, loss_ce: 0.003045, loss_dice: 0.139858
[12:08:17.748] TRAIN: iteration 26612 : loss : 0.094839, loss_ce: 0.003048, loss_dice: 0.186629
[12:08:17.956] TRAIN: iteration 26613 : loss : 0.163759, loss_ce: 0.003931, loss_dice: 0.323587
[12:08:18.169] TRAIN: iteration 26614 : loss : 0.066860, loss_ce: 0.002106, loss_dice: 0.131613
[12:08:18.380] TRAIN: iteration 26615 : loss : 0.049065, loss_ce: 0.000772, loss_dice: 0.097357
[12:08:18.589] TRAIN: iteration 26616 : loss : 0.058306, loss_ce: 0.001595, loss_dice: 0.115016
[12:08:18.798] TRAIN: iteration 26617 : loss : 0.063800, loss_ce: 0.001480, loss_dice: 0.126120
[12:08:19.007] TRAIN: iteration 26618 : loss : 0.181823, loss_ce: 0.001330, loss_dice: 0.362316
[12:08:19.216] TRAIN: iteration 26619 : loss : 0.082925, loss_ce: 0.002222, loss_dice: 0.163628
[12:08:19.425] TRAIN: iteration 26620 : loss : 0.250784, loss_ce: 0.001499, loss_dice: 0.500069
[12:08:19.664] TRAIN: iteration 26621 : loss : 0.125992, loss_ce: 0.003966, loss_dice: 0.248018
[12:08:19.876] TRAIN: iteration 26622 : loss : 0.098027, loss_ce: 0.004668, loss_dice: 0.191385
[12:08:20.085] TRAIN: iteration 26623 : loss : 0.069704, loss_ce: 0.002116, loss_dice: 0.137291
[12:08:20.292] TRAIN: iteration 26624 : loss : 0.043847, loss_ce: 0.009669, loss_dice: 0.078026
[12:08:20.506] TRAIN: iteration 26625 : loss : 0.033074, loss_ce: 0.001907, loss_dice: 0.064241
[12:08:20.714] TRAIN: iteration 26626 : loss : 0.067694, loss_ce: 0.001643, loss_dice: 0.133745
[12:08:20.937] TRAIN: iteration 26627 : loss : 0.133943, loss_ce: 0.003895, loss_dice: 0.263991
[12:08:21.148] TRAIN: iteration 26628 : loss : 0.116595, loss_ce: 0.004365, loss_dice: 0.228824
[12:08:21.355] TRAIN: iteration 26629 : loss : 0.039521, loss_ce: 0.004390, loss_dice: 0.074653
[12:08:21.715] TRAIN: iteration 26630 : loss : 0.250145, loss_ce: 0.000288, loss_dice: 0.500001
[12:08:21.926] TRAIN: iteration 26631 : loss : 0.148057, loss_ce: 0.001783, loss_dice: 0.294331
[12:08:22.134] TRAIN: iteration 26632 : loss : 0.246435, loss_ce: 0.002962, loss_dice: 0.489907
[12:08:22.344] TRAIN: iteration 26633 : loss : 0.019809, loss_ce: 0.001881, loss_dice: 0.037737
[12:08:22.553] TRAIN: iteration 26634 : loss : 0.107669, loss_ce: 0.004770, loss_dice: 0.210567
[12:08:22.762] TRAIN: iteration 26635 : loss : 0.037234, loss_ce: 0.001852, loss_dice: 0.072617
[12:08:22.973] TRAIN: iteration 26636 : loss : 0.225427, loss_ce: 0.001756, loss_dice: 0.449097
[12:08:23.184] TRAIN: iteration 26637 : loss : 0.093781, loss_ce: 0.001825, loss_dice: 0.185737
[12:08:23.602] TRAIN: iteration 26638 : loss : 0.046889, loss_ce: 0.003941, loss_dice: 0.089837
[12:08:23.811] TRAIN: iteration 26639 : loss : 0.049286, loss_ce: 0.002188, loss_dice: 0.096384
[12:08:24.019] TRAIN: iteration 26640 : loss : 0.072577, loss_ce: 0.005479, loss_dice: 0.139675
[12:08:24.260] TRAIN: iteration 26641 : loss : 0.072897, loss_ce: 0.002256, loss_dice: 0.143539
[12:08:24.482] TRAIN: iteration 26642 : loss : 0.170843, loss_ce: 0.002985, loss_dice: 0.338702
[12:08:24.698] TRAIN: iteration 26643 : loss : 0.252133, loss_ce: 0.003977, loss_dice: 0.500288
[12:08:24.915] TRAIN: iteration 26644 : loss : 0.057531, loss_ce: 0.000789, loss_dice: 0.114274
[12:08:25.130] TRAIN: iteration 26645 : loss : 0.078980, loss_ce: 0.001710, loss_dice: 0.156249
[12:08:25.741] TRAIN: iteration 26646 : loss : 0.091519, loss_ce: 0.004170, loss_dice: 0.178867
[12:08:25.950] TRAIN: iteration 26647 : loss : 0.080351, loss_ce: 0.003420, loss_dice: 0.157281
[12:08:26.160] TRAIN: iteration 26648 : loss : 0.098630, loss_ce: 0.002767, loss_dice: 0.194494
[12:08:26.369] TRAIN: iteration 26649 : loss : 0.136575, loss_ce: 0.001810, loss_dice: 0.271339
[12:08:26.577] TRAIN: iteration 26650 : loss : 0.021791, loss_ce: 0.000976, loss_dice: 0.042606
[12:08:26.784] TRAIN: iteration 26651 : loss : 0.145164, loss_ce: 0.004190, loss_dice: 0.286138
[12:08:26.992] TRAIN: iteration 26652 : loss : 0.062248, loss_ce: 0.001272, loss_dice: 0.123224
[12:08:27.204] TRAIN: iteration 26653 : loss : 0.178272, loss_ce: 0.001412, loss_dice: 0.355132
[12:08:28.919] TRAIN: iteration 26654 : loss : 0.248355, loss_ce: 0.004660, loss_dice: 0.492050
[12:08:29.128] TRAIN: iteration 26655 : loss : 0.166561, loss_ce: 0.001138, loss_dice: 0.331983
[12:08:29.337] TRAIN: iteration 26656 : loss : 0.017477, loss_ce: 0.000968, loss_dice: 0.033987
[12:08:29.546] TRAIN: iteration 26657 : loss : 0.211198, loss_ce: 0.002367, loss_dice: 0.420028
[12:08:29.754] TRAIN: iteration 26658 : loss : 0.196394, loss_ce: 0.002106, loss_dice: 0.390681
[12:08:29.968] TRAIN: iteration 26659 : loss : 0.122141, loss_ce: 0.002206, loss_dice: 0.242075
[12:08:30.177] TRAIN: iteration 26660 : loss : 0.250988, loss_ce: 0.001917, loss_dice: 0.500059
[12:08:30.415] TRAIN: iteration 26661 : loss : 0.028213, loss_ce: 0.002223, loss_dice: 0.054203
[12:08:30.623] TRAIN: iteration 26662 : loss : 0.079028, loss_ce: 0.001281, loss_dice: 0.156775
[12:08:30.831] TRAIN: iteration 26663 : loss : 0.030544, loss_ce: 0.001112, loss_dice: 0.059976
[12:08:31.038] TRAIN: iteration 26664 : loss : 0.046605, loss_ce: 0.003971, loss_dice: 0.089240
[12:08:31.249] TRAIN: iteration 26665 : loss : 0.035199, loss_ce: 0.004089, loss_dice: 0.066309
[12:08:31.459] TRAIN: iteration 26666 : loss : 0.149511, loss_ce: 0.009230, loss_dice: 0.289792
[12:08:31.666] TRAIN: iteration 26667 : loss : 0.073049, loss_ce: 0.002815, loss_dice: 0.143282
[12:08:31.879] TRAIN: iteration 26668 : loss : 0.250597, loss_ce: 0.001141, loss_dice: 0.500054
[12:08:32.087] TRAIN: iteration 26669 : loss : 0.052307, loss_ce: 0.002843, loss_dice: 0.101772
[12:08:32.296] TRAIN: iteration 26670 : loss : 0.044354, loss_ce: 0.003491, loss_dice: 0.085216
[12:08:32.504] TRAIN: iteration 26671 : loss : 0.045538, loss_ce: 0.002817, loss_dice: 0.088258
[12:08:33.748] TRAIN: iteration 26672 : loss : 0.067599, loss_ce: 0.005748, loss_dice: 0.129451
[12:08:33.979] TRAIN: iteration 26673 : loss : 0.031994, loss_ce: 0.001970, loss_dice: 0.062019
[12:08:34.188] TRAIN: iteration 26674 : loss : 0.127415, loss_ce: 0.002219, loss_dice: 0.252611
[12:08:34.397] TRAIN: iteration 26675 : loss : 0.126269, loss_ce: 0.008425, loss_dice: 0.244114
[12:08:34.606] TRAIN: iteration 26676 : loss : 0.156395, loss_ce: 0.001913, loss_dice: 0.310877
[12:08:34.817] TRAIN: iteration 26677 : loss : 0.250742, loss_ce: 0.001407, loss_dice: 0.500078
[12:08:35.026] TRAIN: iteration 26678 : loss : 0.053274, loss_ce: 0.001826, loss_dice: 0.104723
[12:08:35.235] TRAIN: iteration 26679 : loss : 0.250396, loss_ce: 0.000769, loss_dice: 0.500022
[12:08:35.443] TRAIN: iteration 26680 : loss : 0.248678, loss_ce: 0.002433, loss_dice: 0.494922
[12:08:35.681] TRAIN: iteration 26681 : loss : 0.070627, loss_ce: 0.006557, loss_dice: 0.134697
[12:08:35.889] TRAIN: iteration 26682 : loss : 0.033167, loss_ce: 0.001676, loss_dice: 0.064659
[12:08:36.097] TRAIN: iteration 26683 : loss : 0.066052, loss_ce: 0.002174, loss_dice: 0.129929
[12:08:36.305] TRAIN: iteration 26684 : loss : 0.206291, loss_ce: 0.002449, loss_dice: 0.410132
[12:08:36.513] TRAIN: iteration 26685 : loss : 0.257292, loss_ce: 0.017492, loss_dice: 0.497092
[12:08:36.964] TRAIN: iteration 26686 : loss : 0.069240, loss_ce: 0.004869, loss_dice: 0.133611
[12:08:37.172] TRAIN: iteration 26687 : loss : 0.251685, loss_ce: 0.003149, loss_dice: 0.500222
[12:08:37.381] TRAIN: iteration 26688 : loss : 0.069802, loss_ce: 0.002495, loss_dice: 0.137110
[12:08:37.590] TRAIN: iteration 26689 : loss : 0.152281, loss_ce: 0.020629, loss_dice: 0.283933
[12:08:37.800] TRAIN: iteration 26690 : loss : 0.106453, loss_ce: 0.002305, loss_dice: 0.210602
[12:08:38.008] TRAIN: iteration 26691 : loss : 0.025538, loss_ce: 0.001510, loss_dice: 0.049566
[12:08:38.217] TRAIN: iteration 26692 : loss : 0.122030, loss_ce: 0.002291, loss_dice: 0.241770
[12:08:38.432] TRAIN: iteration 26693 : loss : 0.065928, loss_ce: 0.001759, loss_dice: 0.130098
[12:08:39.060] TRAIN: iteration 26694 : loss : 0.249087, loss_ce: 0.001376, loss_dice: 0.496798
[12:08:39.268] TRAIN: iteration 26695 : loss : 0.236375, loss_ce: 0.005659, loss_dice: 0.467091
[12:08:39.476] TRAIN: iteration 26696 : loss : 0.039970, loss_ce: 0.001711, loss_dice: 0.078230
[12:08:39.686] TRAIN: iteration 26697 : loss : 0.027675, loss_ce: 0.001757, loss_dice: 0.053592
[12:08:39.897] TRAIN: iteration 26698 : loss : 0.133314, loss_ce: 0.002486, loss_dice: 0.264142
[12:08:40.108] TRAIN: iteration 26699 : loss : 0.060957, loss_ce: 0.001056, loss_dice: 0.120858
[12:08:40.328] TRAIN: iteration 26700 : loss : 0.139602, loss_ce: 0.000903, loss_dice: 0.278300
[12:08:40.573] TRAIN: iteration 26701 : loss : 0.055485, loss_ce: 0.002989, loss_dice: 0.107980
[12:08:40.779] TRAIN: iteration 26702 : loss : 0.247859, loss_ce: 0.002558, loss_dice: 0.493160
[12:08:40.994] TRAIN: iteration 26703 : loss : 0.129107, loss_ce: 0.003738, loss_dice: 0.254477
[12:08:41.203] TRAIN: iteration 26704 : loss : 0.057640, loss_ce: 0.001505, loss_dice: 0.113775
[12:08:41.417] TRAIN: iteration 26705 : loss : 0.015336, loss_ce: 0.000470, loss_dice: 0.030202
[12:08:41.625] TRAIN: iteration 26706 : loss : 0.227303, loss_ce: 0.007711, loss_dice: 0.446896
[12:08:41.833] TRAIN: iteration 26707 : loss : 0.250365, loss_ce: 0.000718, loss_dice: 0.500013
[12:08:42.042] TRAIN: iteration 26708 : loss : 0.251238, loss_ce: 0.010121, loss_dice: 0.492355
[12:08:42.251] TRAIN: iteration 26709 : loss : 0.055945, loss_ce: 0.001278, loss_dice: 0.110611
[12:08:42.460] TRAIN: iteration 26710 : loss : 0.092919, loss_ce: 0.004256, loss_dice: 0.181582
[12:08:42.670] TRAIN: iteration 26711 : loss : 0.149034, loss_ce: 0.005474, loss_dice: 0.292595
[12:08:42.887] TRAIN: iteration 26712 : loss : 0.057561, loss_ce: 0.004719, loss_dice: 0.110403
[12:08:43.096] TRAIN: iteration 26713 : loss : 0.069313, loss_ce: 0.000928, loss_dice: 0.137699
[12:08:43.305] TRAIN: iteration 26714 : loss : 0.053569, loss_ce: 0.004114, loss_dice: 0.103024
[12:08:43.516] TRAIN: iteration 26715 : loss : 0.053959, loss_ce: 0.001036, loss_dice: 0.106883
[12:08:43.725] TRAIN: iteration 26716 : loss : 0.231608, loss_ce: 0.005295, loss_dice: 0.457920
[12:08:43.950] TRAIN: iteration 26717 : loss : 0.042106, loss_ce: 0.001039, loss_dice: 0.083173
[12:08:44.163] TRAIN: iteration 26718 : loss : 0.043102, loss_ce: 0.002622, loss_dice: 0.083582
[12:08:44.374] TRAIN: iteration 26719 : loss : 0.112973, loss_ce: 0.003468, loss_dice: 0.222478
[12:08:44.583] TRAIN: iteration 26720 : loss : 0.071224, loss_ce: 0.004208, loss_dice: 0.138240
[12:08:44.817] TRAIN: iteration 26721 : loss : 0.032480, loss_ce: 0.006202, loss_dice: 0.058758
[12:08:45.092] TRAIN: iteration 26722 : loss : 0.244068, loss_ce: 0.001261, loss_dice: 0.486876
[12:08:45.310] TRAIN: iteration 26723 : loss : 0.030100, loss_ce: 0.001361, loss_dice: 0.058839
[12:08:45.519] TRAIN: iteration 26724 : loss : 0.181127, loss_ce: 0.019203, loss_dice: 0.343051
[12:08:45.726] TRAIN: iteration 26725 : loss : 0.244044, loss_ce: 0.003093, loss_dice: 0.484995
[12:08:45.940] TRAIN: iteration 26726 : loss : 0.057250, loss_ce: 0.002340, loss_dice: 0.112159
[12:08:46.148] TRAIN: iteration 26727 : loss : 0.066511, loss_ce: 0.001106, loss_dice: 0.131915
[12:08:46.355] TRAIN: iteration 26728 : loss : 0.250334, loss_ce: 0.000660, loss_dice: 0.500008
[12:08:46.563] TRAIN: iteration 26729 : loss : 0.225801, loss_ce: 0.002479, loss_dice: 0.449123
[12:08:46.771] TRAIN: iteration 26730 : loss : 0.114326, loss_ce: 0.001536, loss_dice: 0.227117
[12:08:46.983] TRAIN: iteration 26731 : loss : 0.050871, loss_ce: 0.007119, loss_dice: 0.094623
[12:08:47.195] TRAIN: iteration 26732 : loss : 0.083222, loss_ce: 0.007293, loss_dice: 0.159150
[12:08:47.404] TRAIN: iteration 26733 : loss : 0.252345, loss_ce: 0.004385, loss_dice: 0.500304
[12:08:47.613] TRAIN: iteration 26734 : loss : 0.059524, loss_ce: 0.002151, loss_dice: 0.116896
[12:08:47.828] TRAIN: iteration 26735 : loss : 0.251659, loss_ce: 0.003113, loss_dice: 0.500205
[12:08:48.036] TRAIN: iteration 26736 : loss : 0.052046, loss_ce: 0.002355, loss_dice: 0.101736
[12:08:48.245] TRAIN: iteration 26737 : loss : 0.031709, loss_ce: 0.004759, loss_dice: 0.058660
[12:08:48.460] TRAIN: iteration 26738 : loss : 0.251259, loss_ce: 0.002355, loss_dice: 0.500162
[12:08:48.675] TRAIN: iteration 26739 : loss : 0.106166, loss_ce: 0.003674, loss_dice: 0.208658
[12:08:48.890] TRAIN: iteration 26740 : loss : 0.035434, loss_ce: 0.001158, loss_dice: 0.069709
[12:08:49.128] TRAIN: iteration 26741 : loss : 0.086189, loss_ce: 0.002424, loss_dice: 0.169954
[12:08:49.335] TRAIN: iteration 26742 : loss : 0.095673, loss_ce: 0.002349, loss_dice: 0.188996
[12:08:49.542] TRAIN: iteration 26743 : loss : 0.104047, loss_ce: 0.002508, loss_dice: 0.205586
[12:08:49.750] TRAIN: iteration 26744 : loss : 0.043409, loss_ce: 0.004438, loss_dice: 0.082380
[12:08:49.957] TRAIN: iteration 26745 : loss : 0.109557, loss_ce: 0.005186, loss_dice: 0.213929
[12:08:50.462] TRAIN: iteration 26746 : loss : 0.061621, loss_ce: 0.006582, loss_dice: 0.116659
[12:08:50.669] TRAIN: iteration 26747 : loss : 0.096840, loss_ce: 0.003672, loss_dice: 0.190008
[12:08:50.879] TRAIN: iteration 26748 : loss : 0.046652, loss_ce: 0.001936, loss_dice: 0.091369
[12:08:51.088] TRAIN: iteration 26749 : loss : 0.078716, loss_ce: 0.002103, loss_dice: 0.155329
[12:08:51.295] TRAIN: iteration 26750 : loss : 0.075836, loss_ce: 0.002473, loss_dice: 0.149199
[12:08:51.507] TRAIN: iteration 26751 : loss : 0.086658, loss_ce: 0.002293, loss_dice: 0.171022
[12:08:51.716] TRAIN: iteration 26752 : loss : 0.085553, loss_ce: 0.002920, loss_dice: 0.168186
[12:08:51.926] TRAIN: iteration 26753 : loss : 0.127074, loss_ce: 0.009362, loss_dice: 0.244787
[12:08:52.141] TRAIN: iteration 26754 : loss : 0.072078, loss_ce: 0.002516, loss_dice: 0.141641
[12:08:52.351] TRAIN: iteration 26755 : loss : 0.091901, loss_ce: 0.012515, loss_dice: 0.171287
[12:08:52.562] TRAIN: iteration 26756 : loss : 0.251182, loss_ce: 0.002221, loss_dice: 0.500143
[12:08:52.779] TRAIN: iteration 26757 : loss : 0.025712, loss_ce: 0.000752, loss_dice: 0.050672
[12:08:52.991] TRAIN: iteration 26758 : loss : 0.226014, loss_ce: 0.001163, loss_dice: 0.450865
[12:08:53.200] TRAIN: iteration 26759 : loss : 0.126092, loss_ce: 0.003510, loss_dice: 0.248674
[12:08:53.410] TRAIN: iteration 26760 : loss : 0.058865, loss_ce: 0.004644, loss_dice: 0.113087
[12:08:53.647] TRAIN: iteration 26761 : loss : 0.102500, loss_ce: 0.001861, loss_dice: 0.203138
[12:08:53.855] TRAIN: iteration 26762 : loss : 0.047093, loss_ce: 0.001442, loss_dice: 0.092745
[12:08:54.063] TRAIN: iteration 26763 : loss : 0.158988, loss_ce: 0.005828, loss_dice: 0.312148
[12:08:54.272] TRAIN: iteration 26764 : loss : 0.096480, loss_ce: 0.006332, loss_dice: 0.186627
[12:08:54.480] TRAIN: iteration 26765 : loss : 0.052094, loss_ce: 0.001827, loss_dice: 0.102361
[12:08:54.987] TRAIN: iteration 26766 : loss : 0.187486, loss_ce: 0.005304, loss_dice: 0.369667
[12:08:55.196] TRAIN: iteration 26767 : loss : 0.026519, loss_ce: 0.003852, loss_dice: 0.049185
[12:08:55.403] TRAIN: iteration 26768 : loss : 0.202071, loss_ce: 0.006753, loss_dice: 0.397389
[12:08:55.610] TRAIN: iteration 26769 : loss : 0.068522, loss_ce: 0.002468, loss_dice: 0.134576
[12:08:55.819] TRAIN: iteration 26770 : loss : 0.250444, loss_ce: 0.000847, loss_dice: 0.500041
[12:08:56.147] TRAIN: iteration 26771 : loss : 0.032456, loss_ce: 0.001115, loss_dice: 0.063797
[12:08:56.357] TRAIN: iteration 26772 : loss : 0.101792, loss_ce: 0.001747, loss_dice: 0.201837
[12:08:56.564] TRAIN: iteration 26773 : loss : 0.250291, loss_ce: 0.000568, loss_dice: 0.500014
[12:08:56.915] TRAIN: iteration 26774 : loss : 0.086377, loss_ce: 0.002045, loss_dice: 0.170708
[12:08:57.124] TRAIN: iteration 26775 : loss : 0.073140, loss_ce: 0.005825, loss_dice: 0.140456
[12:08:57.339] TRAIN: iteration 26776 : loss : 0.250926, loss_ce: 0.001748, loss_dice: 0.500105
[12:08:57.549] TRAIN: iteration 26777 : loss : 0.089555, loss_ce: 0.005477, loss_dice: 0.173633
[12:08:57.765] TRAIN: iteration 26778 : loss : 0.045372, loss_ce: 0.004063, loss_dice: 0.086681
[12:08:57.972] TRAIN: iteration 26779 : loss : 0.071836, loss_ce: 0.002268, loss_dice: 0.141403
[12:08:58.184] TRAIN: iteration 26780 : loss : 0.065614, loss_ce: 0.002197, loss_dice: 0.129031
[12:08:58.429] TRAIN: iteration 26781 : loss : 0.052355, loss_ce: 0.001846, loss_dice: 0.102864
[12:08:58.636] TRAIN: iteration 26782 : loss : 0.055837, loss_ce: 0.002759, loss_dice: 0.108915
[12:08:58.842] TRAIN: iteration 26783 : loss : 0.175096, loss_ce: 0.001165, loss_dice: 0.349027
[12:08:59.053] TRAIN: iteration 26784 : loss : 0.122056, loss_ce: 0.001148, loss_dice: 0.242963
[12:08:59.267] TRAIN: iteration 26785 : loss : 0.084065, loss_ce: 0.004375, loss_dice: 0.163754
[12:08:59.478] TRAIN: iteration 26786 : loss : 0.142648, loss_ce: 0.011435, loss_dice: 0.273861
[12:08:59.687] TRAIN: iteration 26787 : loss : 0.057955, loss_ce: 0.003967, loss_dice: 0.111943
[12:08:59.895] TRAIN: iteration 26788 : loss : 0.085043, loss_ce: 0.005452, loss_dice: 0.164633
[12:09:00.106] TRAIN: iteration 26789 : loss : 0.250683, loss_ce: 0.001304, loss_dice: 0.500063
[12:09:00.313] TRAIN: iteration 26790 : loss : 0.250556, loss_ce: 0.001073, loss_dice: 0.500039
[12:09:00.523] TRAIN: iteration 26791 : loss : 0.135555, loss_ce: 0.002106, loss_dice: 0.269003
[12:09:00.733] TRAIN: iteration 26792 : loss : 0.053761, loss_ce: 0.010941, loss_dice: 0.096581
[12:09:00.942] TRAIN: iteration 26793 : loss : 0.105622, loss_ce: 0.004409, loss_dice: 0.206836
[12:09:01.152] TRAIN: iteration 26794 : loss : 0.049906, loss_ce: 0.002462, loss_dice: 0.097349
[12:09:01.365] TRAIN: iteration 26795 : loss : 0.214501, loss_ce: 0.002966, loss_dice: 0.426036
[12:09:01.575] TRAIN: iteration 26796 : loss : 0.038141, loss_ce: 0.001744, loss_dice: 0.074539
[12:09:01.784] TRAIN: iteration 26797 : loss : 0.223568, loss_ce: 0.002064, loss_dice: 0.445072
[12:09:01.992] TRAIN: iteration 26798 : loss : 0.224588, loss_ce: 0.004555, loss_dice: 0.444621
[12:09:02.200] TRAIN: iteration 26799 : loss : 0.234405, loss_ce: 0.002890, loss_dice: 0.465921
[12:09:02.407] TRAIN: iteration 26800 : loss : 0.052219, loss_ce: 0.002759, loss_dice: 0.101680
[12:09:02.647] TRAIN: iteration 26801 : loss : 0.037837, loss_ce: 0.003594, loss_dice: 0.072080
[12:09:02.855] TRAIN: iteration 26802 : loss : 0.054894, loss_ce: 0.005370, loss_dice: 0.104418
[12:09:03.064] TRAIN: iteration 26803 : loss : 0.252064, loss_ce: 0.003847, loss_dice: 0.500281
[12:09:03.274] TRAIN: iteration 26804 : loss : 0.251256, loss_ce: 0.005296, loss_dice: 0.497216
[12:09:03.483] TRAIN: iteration 26805 : loss : 0.248764, loss_ce: 0.002261, loss_dice: 0.495267
[12:09:03.693] TRAIN: iteration 26806 : loss : 0.101416, loss_ce: 0.004144, loss_dice: 0.198688
[12:09:03.904] TRAIN: iteration 26807 : loss : 0.048481, loss_ce: 0.001998, loss_dice: 0.094965
[12:09:04.113] TRAIN: iteration 26808 : loss : 0.250995, loss_ce: 0.001889, loss_dice: 0.500100
[12:09:04.324] TRAIN: iteration 26809 : loss : 0.060357, loss_ce: 0.003767, loss_dice: 0.116946
[12:09:04.533] TRAIN: iteration 26810 : loss : 0.057054, loss_ce: 0.004131, loss_dice: 0.109977
[12:09:04.744] TRAIN: iteration 26811 : loss : 0.040756, loss_ce: 0.002323, loss_dice: 0.079189
[12:09:04.959] TRAIN: iteration 26812 : loss : 0.249569, loss_ce: 0.004278, loss_dice: 0.494860
[12:09:05.169] TRAIN: iteration 26813 : loss : 0.061011, loss_ce: 0.011590, loss_dice: 0.110433
[12:09:05.379] TRAIN: iteration 26814 : loss : 0.034203, loss_ce: 0.001736, loss_dice: 0.066670
[12:09:05.589] TRAIN: iteration 26815 : loss : 0.093031, loss_ce: 0.005248, loss_dice: 0.180814
[12:09:05.799] TRAIN: iteration 26816 : loss : 0.092200, loss_ce: 0.001770, loss_dice: 0.182630
[12:09:06.011] TRAIN: iteration 26817 : loss : 0.154443, loss_ce: 0.001971, loss_dice: 0.306915
[12:09:06.226] TRAIN: iteration 26818 : loss : 0.099623, loss_ce: 0.002372, loss_dice: 0.196875
[12:09:06.444] TRAIN: iteration 26819 : loss : 0.080471, loss_ce: 0.004688, loss_dice: 0.156254
[12:09:06.652] TRAIN: iteration 26820 : loss : 0.087328, loss_ce: 0.004214, loss_dice: 0.170443
[12:09:06.886] TRAIN: iteration 26821 : loss : 0.250688, loss_ce: 0.001319, loss_dice: 0.500058
[12:09:07.094] TRAIN: iteration 26822 : loss : 0.250637, loss_ce: 0.001213, loss_dice: 0.500061
[12:09:07.306] TRAIN: iteration 26823 : loss : 0.068537, loss_ce: 0.004039, loss_dice: 0.133035
[12:09:07.518] TRAIN: iteration 26824 : loss : 0.077111, loss_ce: 0.003791, loss_dice: 0.150430
[12:09:07.726] TRAIN: iteration 26825 : loss : 0.218529, loss_ce: 0.023263, loss_dice: 0.413795
[12:09:07.934] TRAIN: iteration 26826 : loss : 0.024098, loss_ce: 0.001905, loss_dice: 0.046291
[12:09:08.144] TRAIN: iteration 26827 : loss : 0.248865, loss_ce: 0.008327, loss_dice: 0.489404
[12:09:08.352] TRAIN: iteration 26828 : loss : 0.047547, loss_ce: 0.001170, loss_dice: 0.093924
[12:09:08.564] TRAIN: iteration 26829 : loss : 0.217617, loss_ce: 0.002377, loss_dice: 0.432858
[12:09:10.166] TRAIN: iteration 26830 : loss : 0.036007, loss_ce: 0.002418, loss_dice: 0.069595
[12:09:10.375] TRAIN: iteration 26831 : loss : 0.169502, loss_ce: 0.009848, loss_dice: 0.329156
[12:09:10.585] TRAIN: iteration 26832 : loss : 0.021260, loss_ce: 0.003089, loss_dice: 0.039431
[12:09:10.794] TRAIN: iteration 26833 : loss : 0.092844, loss_ce: 0.002439, loss_dice: 0.183248
[12:09:11.006] TRAIN: iteration 26834 : loss : 0.037082, loss_ce: 0.003986, loss_dice: 0.070177
[12:09:11.220] TRAIN: iteration 26835 : loss : 0.235472, loss_ce: 0.007718, loss_dice: 0.463226
[12:09:11.429] TRAIN: iteration 26836 : loss : 0.047241, loss_ce: 0.005558, loss_dice: 0.088925
[12:09:11.639] TRAIN: iteration 26837 : loss : 0.038421, loss_ce: 0.004025, loss_dice: 0.072817
[12:09:13.377] TRAIN: iteration 26838 : loss : 0.147191, loss_ce: 0.004382, loss_dice: 0.290000
[12:09:13.587] TRAIN: iteration 26839 : loss : 0.173772, loss_ce: 0.001579, loss_dice: 0.345965
[12:09:13.796] TRAIN: iteration 26840 : loss : 0.116795, loss_ce: 0.001672, loss_dice: 0.231918
[12:09:14.047] TRAIN: iteration 26841 : loss : 0.079327, loss_ce: 0.007950, loss_dice: 0.150705
[12:09:14.257] TRAIN: iteration 26842 : loss : 0.240786, loss_ce: 0.003663, loss_dice: 0.477910
[12:09:14.465] TRAIN: iteration 26843 : loss : 0.049175, loss_ce: 0.006271, loss_dice: 0.092080
[12:09:14.674] TRAIN: iteration 26844 : loss : 0.102274, loss_ce: 0.002885, loss_dice: 0.201663
[12:09:14.883] TRAIN: iteration 26845 : loss : 0.086712, loss_ce: 0.002448, loss_dice: 0.170976
[12:09:15.511] TRAIN: iteration 26846 : loss : 0.250810, loss_ce: 0.001543, loss_dice: 0.500076
[12:09:15.723] TRAIN: iteration 26847 : loss : 0.038813, loss_ce: 0.002817, loss_dice: 0.074808
[12:09:15.931] TRAIN: iteration 26848 : loss : 0.158863, loss_ce: 0.003062, loss_dice: 0.314665
[12:09:16.141] TRAIN: iteration 26849 : loss : 0.150761, loss_ce: 0.003279, loss_dice: 0.298243
[12:09:16.350] TRAIN: iteration 26850 : loss : 0.197407, loss_ce: 0.024224, loss_dice: 0.370590
[12:09:16.558] TRAIN: iteration 26851 : loss : 0.086739, loss_ce: 0.002336, loss_dice: 0.171143
[12:09:17.123] TRAIN: iteration 26852 : loss : 0.227654, loss_ce: 0.001153, loss_dice: 0.454155
[12:09:17.334] TRAIN: iteration 26853 : loss : 0.066464, loss_ce: 0.001262, loss_dice: 0.131666
[12:09:17.570] TRAIN: iteration 26854 : loss : 0.182114, loss_ce: 0.001608, loss_dice: 0.362620
[12:09:17.780] TRAIN: iteration 26855 : loss : 0.149315, loss_ce: 0.001240, loss_dice: 0.297389
[12:09:17.988] TRAIN: iteration 26856 : loss : 0.102919, loss_ce: 0.005627, loss_dice: 0.200210
[12:09:18.197] TRAIN: iteration 26857 : loss : 0.125976, loss_ce: 0.003532, loss_dice: 0.248419
[12:09:18.407] TRAIN: iteration 26858 : loss : 0.035109, loss_ce: 0.002460, loss_dice: 0.067757
[12:09:18.635] TRAIN: iteration 26859 : loss : 0.048734, loss_ce: 0.001998, loss_dice: 0.095471
[12:09:18.842] TRAIN: iteration 26860 : loss : 0.016524, loss_ce: 0.001223, loss_dice: 0.031825
[12:09:18.843] NaN or Inf found in input tensor.
[12:09:19.060] TRAIN: iteration 26861 : loss : 0.152854, loss_ce: 0.001642, loss_dice: 0.304065
[12:09:19.272] TRAIN: iteration 26862 : loss : 0.075293, loss_ce: 0.002025, loss_dice: 0.148561
[12:09:19.480] TRAIN: iteration 26863 : loss : 0.250202, loss_ce: 0.000402, loss_dice: 0.500002
[12:09:19.689] TRAIN: iteration 26864 : loss : 0.087972, loss_ce: 0.005702, loss_dice: 0.170243
[12:09:20.718] TRAIN: iteration 26865 : loss : 0.086199, loss_ce: 0.012725, loss_dice: 0.159674
[12:09:20.932] TRAIN: iteration 26866 : loss : 0.162121, loss_ce: 0.000355, loss_dice: 0.323887
[12:09:21.140] TRAIN: iteration 26867 : loss : 0.127465, loss_ce: 0.004283, loss_dice: 0.250647
[12:09:22.137] TRAIN: iteration 26868 : loss : 0.164610, loss_ce: 0.001241, loss_dice: 0.327980
[12:09:22.346] TRAIN: iteration 26869 : loss : 0.250342, loss_ce: 0.000669, loss_dice: 0.500014
[12:09:22.555] TRAIN: iteration 26870 : loss : 0.074558, loss_ce: 0.002787, loss_dice: 0.146328
[12:09:22.762] TRAIN: iteration 26871 : loss : 0.077719, loss_ce: 0.004801, loss_dice: 0.150638
[12:09:22.969] TRAIN: iteration 26872 : loss : 0.196789, loss_ce: 0.020282, loss_dice: 0.373297
[12:09:23.180] TRAIN: iteration 26873 : loss : 0.063402, loss_ce: 0.006696, loss_dice: 0.120108
[12:09:23.395] TRAIN: iteration 26874 : loss : 0.133545, loss_ce: 0.001867, loss_dice: 0.265223
[12:09:23.610] TRAIN: iteration 26875 : loss : 0.131033, loss_ce: 0.002680, loss_dice: 0.259385
[12:09:23.823] TRAIN: iteration 26876 : loss : 0.088577, loss_ce: 0.001106, loss_dice: 0.176047
[12:09:24.031] TRAIN: iteration 26877 : loss : 0.063014, loss_ce: 0.001039, loss_dice: 0.124989
[12:09:24.240] TRAIN: iteration 26878 : loss : 0.031775, loss_ce: 0.000773, loss_dice: 0.062776
[12:09:24.449] TRAIN: iteration 26879 : loss : 0.070807, loss_ce: 0.003552, loss_dice: 0.138063
[12:09:24.657] TRAIN: iteration 26880 : loss : 0.250294, loss_ce: 0.000574, loss_dice: 0.500015
[12:09:24.899] TRAIN: iteration 26881 : loss : 0.099558, loss_ce: 0.004026, loss_dice: 0.195090
[12:09:25.109] TRAIN: iteration 26882 : loss : 0.053840, loss_ce: 0.001752, loss_dice: 0.105927
[12:09:25.322] TRAIN: iteration 26883 : loss : 0.124227, loss_ce: 0.007090, loss_dice: 0.241364
[12:09:27.006] TRAIN: iteration 26884 : loss : 0.103151, loss_ce: 0.002838, loss_dice: 0.203464
[12:09:27.221] TRAIN: iteration 26885 : loss : 0.065560, loss_ce: 0.004563, loss_dice: 0.126557
[12:09:27.429] TRAIN: iteration 26886 : loss : 0.009502, loss_ce: 0.000859, loss_dice: 0.018144
[12:09:27.645] TRAIN: iteration 26887 : loss : 0.065973, loss_ce: 0.004176, loss_dice: 0.127770
[12:09:27.861] TRAIN: iteration 26888 : loss : 0.023508, loss_ce: 0.000900, loss_dice: 0.046115
[12:09:28.069] TRAIN: iteration 26889 : loss : 0.046195, loss_ce: 0.004473, loss_dice: 0.087918
[12:09:28.278] TRAIN: iteration 26890 : loss : 0.057980, loss_ce: 0.001347, loss_dice: 0.114613
[12:09:28.490] TRAIN: iteration 26891 : loss : 0.051239, loss_ce: 0.003426, loss_dice: 0.099052
[12:09:28.704] TRAIN: iteration 26892 : loss : 0.067992, loss_ce: 0.006495, loss_dice: 0.129490
[12:09:28.921] TRAIN: iteration 26893 : loss : 0.045205, loss_ce: 0.001479, loss_dice: 0.088932
[12:09:29.131] TRAIN: iteration 26894 : loss : 0.022420, loss_ce: 0.001257, loss_dice: 0.043583
[12:09:29.340] TRAIN: iteration 26895 : loss : 0.041854, loss_ce: 0.004393, loss_dice: 0.079315
[12:09:29.549] TRAIN: iteration 26896 : loss : 0.050548, loss_ce: 0.007544, loss_dice: 0.093553
[12:09:29.759] TRAIN: iteration 26897 : loss : 0.250611, loss_ce: 0.001158, loss_dice: 0.500064
[12:09:29.968] TRAIN: iteration 26898 : loss : 0.088752, loss_ce: 0.004513, loss_dice: 0.172990
[12:09:30.178] TRAIN: iteration 26899 : loss : 0.065460, loss_ce: 0.001560, loss_dice: 0.129359
[12:09:30.387] TRAIN: iteration 26900 : loss : 0.062065, loss_ce: 0.005227, loss_dice: 0.118904
[12:09:30.625] TRAIN: iteration 26901 : loss : 0.065835, loss_ce: 0.008097, loss_dice: 0.123573
[12:09:30.833] TRAIN: iteration 26902 : loss : 0.250683, loss_ce: 0.001302, loss_dice: 0.500065
[12:09:31.041] TRAIN: iteration 26903 : loss : 0.030846, loss_ce: 0.000618, loss_dice: 0.061074
[12:09:31.250] TRAIN: iteration 26904 : loss : 0.066285, loss_ce: 0.006158, loss_dice: 0.126412
[12:09:31.458] TRAIN: iteration 26905 : loss : 0.031606, loss_ce: 0.001421, loss_dice: 0.061791
[12:09:31.666] TRAIN: iteration 26906 : loss : 0.053731, loss_ce: 0.002345, loss_dice: 0.105118
[12:09:31.881] TRAIN: iteration 26907 : loss : 0.048143, loss_ce: 0.001171, loss_dice: 0.095116
[12:09:32.239] TRAIN: iteration 26908 : loss : 0.250410, loss_ce: 0.000782, loss_dice: 0.500038
[12:09:32.451] TRAIN: iteration 26909 : loss : 0.237913, loss_ce: 0.000505, loss_dice: 0.475320
[12:09:32.666] TRAIN: iteration 26910 : loss : 0.100273, loss_ce: 0.001744, loss_dice: 0.198803
[12:09:32.874] TRAIN: iteration 26911 : loss : 0.144008, loss_ce: 0.003312, loss_dice: 0.284705
[12:09:33.084] TRAIN: iteration 26912 : loss : 0.080143, loss_ce: 0.001837, loss_dice: 0.158448
[12:09:33.291] TRAIN: iteration 26913 : loss : 0.128286, loss_ce: 0.001671, loss_dice: 0.254901
[12:09:33.499] TRAIN: iteration 26914 : loss : 0.089136, loss_ce: 0.004012, loss_dice: 0.174261
[12:09:33.707] TRAIN: iteration 26915 : loss : 0.250342, loss_ce: 0.000661, loss_dice: 0.500023
[12:09:33.917] TRAIN: iteration 26916 : loss : 0.069747, loss_ce: 0.011410, loss_dice: 0.128083
[12:09:34.127] TRAIN: iteration 26917 : loss : 0.010744, loss_ce: 0.000953, loss_dice: 0.020535
[12:09:34.335] TRAIN: iteration 26918 : loss : 0.014250, loss_ce: 0.000848, loss_dice: 0.027651
[12:09:34.544] TRAIN: iteration 26919 : loss : 0.149728, loss_ce: 0.003204, loss_dice: 0.296251
[12:09:34.752] TRAIN: iteration 26920 : loss : 0.017490, loss_ce: 0.001618, loss_dice: 0.033362
[12:09:34.993] TRAIN: iteration 26921 : loss : 0.040542, loss_ce: 0.001699, loss_dice: 0.079386
[12:09:35.210] TRAIN: iteration 26922 : loss : 0.194086, loss_ce: 0.011744, loss_dice: 0.376427
[12:09:35.418] TRAIN: iteration 26923 : loss : 0.076163, loss_ce: 0.004743, loss_dice: 0.147582
[12:09:35.773] TRAIN: iteration 26924 : loss : 0.244070, loss_ce: 0.001643, loss_dice: 0.486496
[12:09:35.985] TRAIN: iteration 26925 : loss : 0.066157, loss_ce: 0.001102, loss_dice: 0.131213
[12:09:36.194] TRAIN: iteration 26926 : loss : 0.063181, loss_ce: 0.007642, loss_dice: 0.118720
[12:09:36.402] TRAIN: iteration 26927 : loss : 0.084446, loss_ce: 0.003030, loss_dice: 0.165862
[12:09:36.610] TRAIN: iteration 26928 : loss : 0.113496, loss_ce: 0.001575, loss_dice: 0.225417
[12:09:36.818] TRAIN: iteration 26929 : loss : 0.028049, loss_ce: 0.004685, loss_dice: 0.051413
[12:09:37.027] TRAIN: iteration 26930 : loss : 0.058856, loss_ce: 0.002114, loss_dice: 0.115598
[12:09:37.240] TRAIN: iteration 26931 : loss : 0.250214, loss_ce: 0.000421, loss_dice: 0.500006
[12:09:37.448] TRAIN: iteration 26932 : loss : 0.100901, loss_ce: 0.005224, loss_dice: 0.196577
[12:09:37.656] TRAIN: iteration 26933 : loss : 0.161465, loss_ce: 0.001252, loss_dice: 0.321678
[12:09:37.864] TRAIN: iteration 26934 : loss : 0.059696, loss_ce: 0.002144, loss_dice: 0.117247
[12:09:38.074] TRAIN: iteration 26935 : loss : 0.250258, loss_ce: 0.000505, loss_dice: 0.500010
[12:09:38.282] TRAIN: iteration 26936 : loss : 0.026413, loss_ce: 0.001456, loss_dice: 0.051370
[12:09:38.489] TRAIN: iteration 26937 : loss : 0.251215, loss_ce: 0.002273, loss_dice: 0.500157
[12:09:38.698] TRAIN: iteration 26938 : loss : 0.220150, loss_ce: 0.001648, loss_dice: 0.438653
[12:09:38.911] TRAIN: iteration 26939 : loss : 0.070758, loss_ce: 0.007416, loss_dice: 0.134100
[12:09:39.120] TRAIN: iteration 26940 : loss : 0.036202, loss_ce: 0.003077, loss_dice: 0.069327
[12:09:40.797] TRAIN: iteration 26941 : loss : 0.194572, loss_ce: 0.003324, loss_dice: 0.385821
[12:09:41.006] TRAIN: iteration 26942 : loss : 0.235585, loss_ce: 0.005427, loss_dice: 0.465743
[12:09:41.215] TRAIN: iteration 26943 : loss : 0.062923, loss_ce: 0.008235, loss_dice: 0.117610
[12:09:41.423] TRAIN: iteration 26944 : loss : 0.250468, loss_ce: 0.000893, loss_dice: 0.500043
[12:09:41.639] TRAIN: iteration 26945 : loss : 0.112451, loss_ce: 0.000746, loss_dice: 0.224157
[12:09:41.853] TRAIN: iteration 26946 : loss : 0.047453, loss_ce: 0.011531, loss_dice: 0.083374
[12:09:42.060] TRAIN: iteration 26947 : loss : 0.160769, loss_ce: 0.005503, loss_dice: 0.316035
[12:09:42.273] TRAIN: iteration 26948 : loss : 0.080931, loss_ce: 0.006791, loss_dice: 0.155071
[12:09:42.843] TRAIN: iteration 26949 : loss : 0.032409, loss_ce: 0.002629, loss_dice: 0.062189
[12:09:43.051] TRAIN: iteration 26950 : loss : 0.189913, loss_ce: 0.015139, loss_dice: 0.364687
[12:09:43.258] TRAIN: iteration 26951 : loss : 0.156088, loss_ce: 0.001725, loss_dice: 0.310452
[12:09:43.466] TRAIN: iteration 26952 : loss : 0.042854, loss_ce: 0.001549, loss_dice: 0.084159
[12:09:43.675] TRAIN: iteration 26953 : loss : 0.065420, loss_ce: 0.002884, loss_dice: 0.127956
[12:09:43.883] TRAIN: iteration 26954 : loss : 0.250627, loss_ce: 0.001189, loss_dice: 0.500065
[12:09:44.091] TRAIN: iteration 26955 : loss : 0.073309, loss_ce: 0.001727, loss_dice: 0.144891
[12:09:44.298] TRAIN: iteration 26956 : loss : 0.076286, loss_ce: 0.005805, loss_dice: 0.146766
[12:09:45.843] TRAIN: iteration 26957 : loss : 0.140076, loss_ce: 0.006022, loss_dice: 0.274129
[12:09:46.053] TRAIN: iteration 26958 : loss : 0.223571, loss_ce: 0.006458, loss_dice: 0.440685
[12:09:46.262] TRAIN: iteration 26959 : loss : 0.241542, loss_ce: 0.000985, loss_dice: 0.482098
[12:09:46.470] TRAIN: iteration 26960 : loss : 0.139698, loss_ce: 0.001844, loss_dice: 0.277552
[12:09:46.704] TRAIN: iteration 26961 : loss : 0.083018, loss_ce: 0.007145, loss_dice: 0.158891
[12:09:46.913] TRAIN: iteration 26962 : loss : 0.033680, loss_ce: 0.003411, loss_dice: 0.063949
[12:09:47.122] TRAIN: iteration 26963 : loss : 0.069499, loss_ce: 0.003419, loss_dice: 0.135579
[12:09:47.339] TRAIN: iteration 26964 : loss : 0.038946, loss_ce: 0.001343, loss_dice: 0.076548
[12:09:47.547] TRAIN: iteration 26965 : loss : 0.060071, loss_ce: 0.002226, loss_dice: 0.117916
[12:09:47.754] TRAIN: iteration 26966 : loss : 0.157542, loss_ce: 0.005587, loss_dice: 0.309496
[12:09:47.962] TRAIN: iteration 26967 : loss : 0.197641, loss_ce: 0.003595, loss_dice: 0.391687
[12:09:48.173] TRAIN: iteration 26968 : loss : 0.043400, loss_ce: 0.002035, loss_dice: 0.084764
[12:09:48.380] TRAIN: iteration 26969 : loss : 0.250519, loss_ce: 0.000990, loss_dice: 0.500047
[12:09:48.587] TRAIN: iteration 26970 : loss : 0.250373, loss_ce: 0.000735, loss_dice: 0.500012
[12:09:49.377] TRAIN: iteration 26971 : loss : 0.250931, loss_ce: 0.002149, loss_dice: 0.499714
[12:09:49.585] TRAIN: iteration 26972 : loss : 0.034488, loss_ce: 0.002867, loss_dice: 0.066110
[12:09:49.792] TRAIN: iteration 26973 : loss : 0.254863, loss_ce: 0.010460, loss_dice: 0.499267
[12:09:50.000] TRAIN: iteration 26974 : loss : 0.082970, loss_ce: 0.004484, loss_dice: 0.161455
[12:09:50.208] TRAIN: iteration 26975 : loss : 0.086958, loss_ce: 0.002257, loss_dice: 0.171660
[12:09:50.416] TRAIN: iteration 26976 : loss : 0.030946, loss_ce: 0.002186, loss_dice: 0.059706
[12:09:50.626] TRAIN: iteration 26977 : loss : 0.035219, loss_ce: 0.001722, loss_dice: 0.068715
[12:09:50.834] TRAIN: iteration 26978 : loss : 0.093833, loss_ce: 0.005193, loss_dice: 0.182473
[12:09:51.408] TRAIN: iteration 26979 : loss : 0.110238, loss_ce: 0.002212, loss_dice: 0.218264
[12:09:51.616] TRAIN: iteration 26980 : loss : 0.250982, loss_ce: 0.001854, loss_dice: 0.500110
[12:09:51.858] TRAIN: iteration 26981 : loss : 0.122276, loss_ce: 0.010826, loss_dice: 0.233725
[12:09:52.073] TRAIN: iteration 26982 : loss : 0.065749, loss_ce: 0.004014, loss_dice: 0.127484
[12:09:52.280] TRAIN: iteration 26983 : loss : 0.109265, loss_ce: 0.013318, loss_dice: 0.205211
[12:09:52.488] TRAIN: iteration 26984 : loss : 0.197839, loss_ce: 0.003038, loss_dice: 0.392640
[12:09:52.698] TRAIN: iteration 26985 : loss : 0.135513, loss_ce: 0.003614, loss_dice: 0.267412
[12:09:52.908] TRAIN: iteration 26986 : loss : 0.250862, loss_ce: 0.001638, loss_dice: 0.500085
[12:09:53.667] TRAIN: iteration 26987 : loss : 0.192084, loss_ce: 0.005199, loss_dice: 0.378968
[12:09:53.875] TRAIN: iteration 26988 : loss : 0.054481, loss_ce: 0.002510, loss_dice: 0.106453
[12:09:54.084] TRAIN: iteration 26989 : loss : 0.251570, loss_ce: 0.002942, loss_dice: 0.500198
[12:09:54.294] TRAIN: iteration 26990 : loss : 0.045490, loss_ce: 0.003052, loss_dice: 0.087927
[12:09:55.847] TRAIN: iteration 26991 : loss : 0.023881, loss_ce: 0.003851, loss_dice: 0.043912
[12:09:56.053] TRAIN: iteration 26992 : loss : 0.069842, loss_ce: 0.004114, loss_dice: 0.135569
[12:09:56.265] TRAIN: iteration 26993 : loss : 0.250860, loss_ce: 0.001619, loss_dice: 0.500100
[12:09:56.479] TRAIN: iteration 26994 : loss : 0.021479, loss_ce: 0.001927, loss_dice: 0.041031
[12:09:56.689] TRAIN: iteration 26995 : loss : 0.180475, loss_ce: 0.003196, loss_dice: 0.357755
[12:09:56.897] TRAIN: iteration 26996 : loss : 0.069291, loss_ce: 0.003681, loss_dice: 0.134901
[12:09:57.109] TRAIN: iteration 26997 : loss : 0.179824, loss_ce: 0.001680, loss_dice: 0.357969
[12:09:57.317] TRAIN: iteration 26998 : loss : 0.251496, loss_ce: 0.002805, loss_dice: 0.500186
[12:09:57.524] TRAIN: iteration 26999 : loss : 0.100750, loss_ce: 0.005360, loss_dice: 0.196141
[12:09:57.738] TRAIN: iteration 27000 : loss : 0.151644, loss_ce: 0.009194, loss_dice: 0.294095
[12:09:57.981] TRAIN: iteration 27001 : loss : 0.165134, loss_ce: 0.001449, loss_dice: 0.328819
[12:09:58.192] TRAIN: iteration 27002 : loss : 0.013576, loss_ce: 0.001075, loss_dice: 0.026078
[12:09:58.400] TRAIN: iteration 27003 : loss : 0.056861, loss_ce: 0.009446, loss_dice: 0.104276
[12:09:58.607] TRAIN: iteration 27004 : loss : 0.073871, loss_ce: 0.002345, loss_dice: 0.145397
[12:09:58.827] TRAIN: iteration 27005 : loss : 0.065143, loss_ce: 0.002719, loss_dice: 0.127566
[12:09:59.034] TRAIN: iteration 27006 : loss : 0.107433, loss_ce: 0.004382, loss_dice: 0.210484
[12:09:59.241] TRAIN: iteration 27007 : loss : 0.250529, loss_ce: 0.001018, loss_dice: 0.500041
[12:09:59.450] TRAIN: iteration 27008 : loss : 0.250516, loss_ce: 0.005670, loss_dice: 0.495363
[12:09:59.663] TRAIN: iteration 27009 : loss : 0.027429, loss_ce: 0.001611, loss_dice: 0.053247
[12:09:59.871] TRAIN: iteration 27010 : loss : 0.103799, loss_ce: 0.006212, loss_dice: 0.201387
[12:10:00.078] TRAIN: iteration 27011 : loss : 0.111292, loss_ce: 0.005605, loss_dice: 0.216978
[12:10:00.295] TRAIN: iteration 27012 : loss : 0.055090, loss_ce: 0.002165, loss_dice: 0.108015
[12:10:00.510] TRAIN: iteration 27013 : loss : 0.143830, loss_ce: 0.006055, loss_dice: 0.281606
[12:10:00.724] TRAIN: iteration 27014 : loss : 0.149444, loss_ce: 0.001509, loss_dice: 0.297380
[12:10:01.216] TRAIN: iteration 27015 : loss : 0.221535, loss_ce: 0.001124, loss_dice: 0.441947
[12:10:01.424] TRAIN: iteration 27016 : loss : 0.228343, loss_ce: 0.001628, loss_dice: 0.455057
[12:10:01.631] TRAIN: iteration 27017 : loss : 0.069222, loss_ce: 0.003214, loss_dice: 0.135230
[12:10:01.838] TRAIN: iteration 27018 : loss : 0.211701, loss_ce: 0.004022, loss_dice: 0.419381
[12:10:02.122] TRAIN: iteration 27019 : loss : 0.247760, loss_ce: 0.002011, loss_dice: 0.493510
[12:10:02.346] TRAIN: iteration 27020 : loss : 0.038560, loss_ce: 0.005021, loss_dice: 0.072099
[12:10:02.575] TRAIN: iteration 27021 : loss : 0.070972, loss_ce: 0.002650, loss_dice: 0.139295
[12:10:02.781] TRAIN: iteration 27022 : loss : 0.043035, loss_ce: 0.005038, loss_dice: 0.081032
[12:10:02.989] TRAIN: iteration 27023 : loss : 0.192589, loss_ce: 0.002752, loss_dice: 0.382425
[12:10:03.196] TRAIN: iteration 27024 : loss : 0.062657, loss_ce: 0.004425, loss_dice: 0.120890
[12:10:03.409] TRAIN: iteration 27025 : loss : 0.046898, loss_ce: 0.000954, loss_dice: 0.092841
[12:10:03.620] TRAIN: iteration 27026 : loss : 0.173455, loss_ce: 0.001615, loss_dice: 0.345295
[12:10:03.829] TRAIN: iteration 27027 : loss : 0.154927, loss_ce: 0.020442, loss_dice: 0.289412
[12:10:04.039] TRAIN: iteration 27028 : loss : 0.250850, loss_ce: 0.001613, loss_dice: 0.500086
[12:10:04.249] TRAIN: iteration 27029 : loss : 0.087401, loss_ce: 0.005862, loss_dice: 0.168940
[12:10:05.051] TRAIN: iteration 27030 : loss : 0.250701, loss_ce: 0.001345, loss_dice: 0.500058
[12:10:05.259] TRAIN: iteration 27031 : loss : 0.038548, loss_ce: 0.006096, loss_dice: 0.071000
[12:10:05.467] TRAIN: iteration 27032 : loss : 0.095544, loss_ce: 0.008266, loss_dice: 0.182821
[12:10:05.683] TRAIN: iteration 27033 : loss : 0.251019, loss_ce: 0.001944, loss_dice: 0.500095
[12:10:05.896] TRAIN: iteration 27034 : loss : 0.038613, loss_ce: 0.003745, loss_dice: 0.073482
[12:10:06.105] TRAIN: iteration 27035 : loss : 0.151121, loss_ce: 0.004144, loss_dice: 0.298098
[12:10:06.319] TRAIN: iteration 27036 : loss : 0.068804, loss_ce: 0.004610, loss_dice: 0.132998
[12:10:07.838] TRAIN: iteration 27037 : loss : 0.059449, loss_ce: 0.007814, loss_dice: 0.111084
[12:10:08.047] TRAIN: iteration 27038 : loss : 0.025780, loss_ce: 0.001593, loss_dice: 0.049968
[12:10:08.257] TRAIN: iteration 27039 : loss : 0.066439, loss_ce: 0.001884, loss_dice: 0.130994
[12:10:08.473] TRAIN: iteration 27040 : loss : 0.037495, loss_ce: 0.002440, loss_dice: 0.072549
[12:10:08.714] TRAIN: iteration 27041 : loss : 0.115030, loss_ce: 0.002704, loss_dice: 0.227357
[12:10:08.921] TRAIN: iteration 27042 : loss : 0.250879, loss_ce: 0.001672, loss_dice: 0.500087
[12:10:09.129] TRAIN: iteration 27043 : loss : 0.065522, loss_ce: 0.006674, loss_dice: 0.124369
[12:10:09.338] TRAIN: iteration 27044 : loss : 0.165571, loss_ce: 0.003457, loss_dice: 0.327685
[12:10:09.546] TRAIN: iteration 27045 : loss : 0.149075, loss_ce: 0.003492, loss_dice: 0.294658
[12:10:09.755] TRAIN: iteration 27046 : loss : 0.041482, loss_ce: 0.001038, loss_dice: 0.081925
[12:10:09.962] TRAIN: iteration 27047 : loss : 0.044293, loss_ce: 0.001563, loss_dice: 0.087023
[12:10:10.171] TRAIN: iteration 27048 : loss : 0.055689, loss_ce: 0.001106, loss_dice: 0.110271
[12:10:10.380] TRAIN: iteration 27049 : loss : 0.028422, loss_ce: 0.002297, loss_dice: 0.054547
[12:10:10.587] TRAIN: iteration 27050 : loss : 0.236328, loss_ce: 0.002341, loss_dice: 0.470315
[12:10:10.794] TRAIN: iteration 27051 : loss : 0.071119, loss_ce: 0.002320, loss_dice: 0.139918
[12:10:11.003] TRAIN: iteration 27052 : loss : 0.222195, loss_ce: 0.002680, loss_dice: 0.441711
[12:10:11.772] TRAIN: iteration 27053 : loss : 0.050032, loss_ce: 0.003324, loss_dice: 0.096739
[12:10:11.979] TRAIN: iteration 27054 : loss : 0.250394, loss_ce: 0.000769, loss_dice: 0.500020
[12:10:12.380] TRAIN: iteration 27055 : loss : 0.079999, loss_ce: 0.003158, loss_dice: 0.156841
[12:10:12.602] TRAIN: iteration 27056 : loss : 0.140534, loss_ce: 0.005368, loss_dice: 0.275699
[12:10:12.817] TRAIN: iteration 27057 : loss : 0.121782, loss_ce: 0.007231, loss_dice: 0.236334
[12:10:13.024] TRAIN: iteration 27058 : loss : 0.028700, loss_ce: 0.001213, loss_dice: 0.056187
[12:10:13.237] TRAIN: iteration 27059 : loss : 0.074510, loss_ce: 0.001545, loss_dice: 0.147475
[12:10:13.446] TRAIN: iteration 27060 : loss : 0.230885, loss_ce: 0.001039, loss_dice: 0.460731
[12:10:13.685] TRAIN: iteration 27061 : loss : 0.108510, loss_ce: 0.003251, loss_dice: 0.213769
[12:10:13.893] TRAIN: iteration 27062 : loss : 0.041227, loss_ce: 0.001516, loss_dice: 0.080938
[12:10:15.242] TRAIN: iteration 27063 : loss : 0.173194, loss_ce: 0.003472, loss_dice: 0.342916
[12:10:15.450] TRAIN: iteration 27064 : loss : 0.066430, loss_ce: 0.002664, loss_dice: 0.130196
[12:10:15.661] TRAIN: iteration 27065 : loss : 0.095812, loss_ce: 0.001207, loss_dice: 0.190417
[12:10:15.869] TRAIN: iteration 27066 : loss : 0.079057, loss_ce: 0.005353, loss_dice: 0.152760
[12:10:16.083] TRAIN: iteration 27067 : loss : 0.087041, loss_ce: 0.001520, loss_dice: 0.172562
[12:10:16.293] TRAIN: iteration 27068 : loss : 0.024442, loss_ce: 0.000653, loss_dice: 0.048231
[12:10:16.501] TRAIN: iteration 27069 : loss : 0.139856, loss_ce: 0.001406, loss_dice: 0.278306
[12:10:16.716] TRAIN: iteration 27070 : loss : 0.044701, loss_ce: 0.001978, loss_dice: 0.087423
[12:10:16.929] TRAIN: iteration 27071 : loss : 0.250746, loss_ce: 0.001421, loss_dice: 0.500072
[12:10:17.141] TRAIN: iteration 27072 : loss : 0.225966, loss_ce: 0.001863, loss_dice: 0.450068
[12:10:17.352] TRAIN: iteration 27073 : loss : 0.100010, loss_ce: 0.007446, loss_dice: 0.192573
[12:10:17.560] TRAIN: iteration 27074 : loss : 0.068319, loss_ce: 0.002289, loss_dice: 0.134349
[12:10:17.768] TRAIN: iteration 27075 : loss : 0.075194, loss_ce: 0.009845, loss_dice: 0.140542
[12:10:17.976] TRAIN: iteration 27076 : loss : 0.093520, loss_ce: 0.001098, loss_dice: 0.185942
[12:10:18.740] TRAIN: iteration 27077 : loss : 0.036287, loss_ce: 0.002060, loss_dice: 0.070515
[12:10:18.957] TRAIN: iteration 27078 : loss : 0.087481, loss_ce: 0.002070, loss_dice: 0.172892
[12:10:19.165] TRAIN: iteration 27079 : loss : 0.250458, loss_ce: 0.000879, loss_dice: 0.500036
[12:10:19.373] TRAIN: iteration 27080 : loss : 0.053894, loss_ce: 0.004802, loss_dice: 0.102987
[12:10:19.611] TRAIN: iteration 27081 : loss : 0.214198, loss_ce: 0.007181, loss_dice: 0.421216
[12:10:19.818] TRAIN: iteration 27082 : loss : 0.061497, loss_ce: 0.004630, loss_dice: 0.118364
[12:10:20.026] TRAIN: iteration 27083 : loss : 0.089855, loss_ce: 0.008089, loss_dice: 0.171621
[12:10:20.239] TRAIN: iteration 27084 : loss : 0.043969, loss_ce: 0.001813, loss_dice: 0.086124
[12:10:20.806] TRAIN: iteration 27085 : loss : 0.044919, loss_ce: 0.002377, loss_dice: 0.087461
[12:10:21.014] TRAIN: iteration 27086 : loss : 0.045399, loss_ce: 0.001487, loss_dice: 0.089312
[12:10:21.224] TRAIN: iteration 27087 : loss : 0.051199, loss_ce: 0.002949, loss_dice: 0.099449
[12:10:21.433] TRAIN: iteration 27088 : loss : 0.141882, loss_ce: 0.006567, loss_dice: 0.277196
[12:10:21.643] TRAIN: iteration 27089 : loss : 0.085908, loss_ce: 0.003847, loss_dice: 0.167968
[12:10:21.851] TRAIN: iteration 27090 : loss : 0.076869, loss_ce: 0.005446, loss_dice: 0.148293
[12:10:22.060] TRAIN: iteration 27091 : loss : 0.047573, loss_ce: 0.008409, loss_dice: 0.086736
[12:10:22.267] TRAIN: iteration 27092 : loss : 0.031503, loss_ce: 0.002785, loss_dice: 0.060221
[12:10:23.943] TRAIN: iteration 27093 : loss : 0.109697, loss_ce: 0.003296, loss_dice: 0.216098
[12:10:24.152] TRAIN: iteration 27094 : loss : 0.036712, loss_ce: 0.001222, loss_dice: 0.072202
[12:10:24.359] TRAIN: iteration 27095 : loss : 0.083311, loss_ce: 0.004544, loss_dice: 0.162079
[12:10:24.568] TRAIN: iteration 27096 : loss : 0.126157, loss_ce: 0.004082, loss_dice: 0.248232
[12:10:24.775] TRAIN: iteration 27097 : loss : 0.046315, loss_ce: 0.003407, loss_dice: 0.089222
[12:10:24.982] TRAIN: iteration 27098 : loss : 0.027018, loss_ce: 0.002856, loss_dice: 0.051180
[12:10:25.189] TRAIN: iteration 27099 : loss : 0.251140, loss_ce: 0.002159, loss_dice: 0.500121
[12:10:25.398] TRAIN: iteration 27100 : loss : 0.107328, loss_ce: 0.005296, loss_dice: 0.209360
[12:10:26.320] TRAIN: iteration 27101 : loss : 0.106655, loss_ce: 0.001451, loss_dice: 0.211859
[12:10:26.555] TRAIN: iteration 27102 : loss : 0.062669, loss_ce: 0.002564, loss_dice: 0.122775
[12:10:26.762] TRAIN: iteration 27103 : loss : 0.045681, loss_ce: 0.003576, loss_dice: 0.087786
[12:10:26.970] TRAIN: iteration 27104 : loss : 0.067402, loss_ce: 0.002282, loss_dice: 0.132521
[12:10:27.177] TRAIN: iteration 27105 : loss : 0.029971, loss_ce: 0.001560, loss_dice: 0.058381
[12:10:27.385] TRAIN: iteration 27106 : loss : 0.048409, loss_ce: 0.003197, loss_dice: 0.093621
[12:10:27.592] TRAIN: iteration 27107 : loss : 0.126453, loss_ce: 0.006528, loss_dice: 0.246379
[12:10:27.800] TRAIN: iteration 27108 : loss : 0.228197, loss_ce: 0.001984, loss_dice: 0.454410
[12:10:28.010] TRAIN: iteration 27109 : loss : 0.047477, loss_ce: 0.004029, loss_dice: 0.090925
[12:10:28.227] TRAIN: iteration 27110 : loss : 0.162547, loss_ce: 0.002180, loss_dice: 0.322913
[12:10:28.435] TRAIN: iteration 27111 : loss : 0.044150, loss_ce: 0.001865, loss_dice: 0.086434
[12:10:28.653] TRAIN: iteration 27112 : loss : 0.075616, loss_ce: 0.002668, loss_dice: 0.148564
[12:10:28.861] TRAIN: iteration 27113 : loss : 0.209109, loss_ce: 0.006769, loss_dice: 0.411449
[12:10:29.073] TRAIN: iteration 27114 : loss : 0.041795, loss_ce: 0.006001, loss_dice: 0.077588
[12:10:29.284] TRAIN: iteration 27115 : loss : 0.045665, loss_ce: 0.000989, loss_dice: 0.090341
[12:10:29.492] TRAIN: iteration 27116 : loss : 0.250765, loss_ce: 0.001469, loss_dice: 0.500061
[12:10:29.700] TRAIN: iteration 27117 : loss : 0.128038, loss_ce: 0.004909, loss_dice: 0.251167
[12:10:29.915] TRAIN: iteration 27118 : loss : 0.134825, loss_ce: 0.009300, loss_dice: 0.260351
[12:10:30.128] TRAIN: iteration 27119 : loss : 0.248529, loss_ce: 0.001187, loss_dice: 0.495870
[12:10:30.341] TRAIN: iteration 27120 : loss : 0.250820, loss_ce: 0.002348, loss_dice: 0.499292
[12:10:30.580] TRAIN: iteration 27121 : loss : 0.230212, loss_ce: 0.003803, loss_dice: 0.456620
[12:10:30.795] TRAIN: iteration 27122 : loss : 0.245837, loss_ce: 0.002513, loss_dice: 0.489161
[12:10:31.003] TRAIN: iteration 27123 : loss : 0.064437, loss_ce: 0.003720, loss_dice: 0.125154
[12:10:31.210] TRAIN: iteration 27124 : loss : 0.250812, loss_ce: 0.001578, loss_dice: 0.500045
[12:10:32.428] TRAIN: iteration 27125 : loss : 0.251222, loss_ce: 0.002414, loss_dice: 0.500030
[12:10:32.646] TRAIN: iteration 27126 : loss : 0.250868, loss_ce: 0.001686, loss_dice: 0.500050
[12:10:32.861] TRAIN: iteration 27127 : loss : 0.128173, loss_ce: 0.005449, loss_dice: 0.250897
[12:10:33.072] TRAIN: iteration 27128 : loss : 0.071340, loss_ce: 0.003421, loss_dice: 0.139260
[12:10:33.281] TRAIN: iteration 27129 : loss : 0.075528, loss_ce: 0.001834, loss_dice: 0.149222
[12:10:33.490] TRAIN: iteration 27130 : loss : 0.118823, loss_ce: 0.002965, loss_dice: 0.234681
[12:10:35.211] TRAIN: iteration 27131 : loss : 0.039320, loss_ce: 0.002787, loss_dice: 0.075852
[12:10:35.418] TRAIN: iteration 27132 : loss : 0.113462, loss_ce: 0.015058, loss_dice: 0.211866
[12:10:35.625] TRAIN: iteration 27133 : loss : 0.187585, loss_ce: 0.033830, loss_dice: 0.341340
[12:10:35.839] TRAIN: iteration 27134 : loss : 0.028817, loss_ce: 0.001859, loss_dice: 0.055775
[12:10:36.047] TRAIN: iteration 27135 : loss : 0.058590, loss_ce: 0.001592, loss_dice: 0.115589
[12:10:36.256] TRAIN: iteration 27136 : loss : 0.030500, loss_ce: 0.001183, loss_dice: 0.059816
[12:10:36.463] TRAIN: iteration 27137 : loss : 0.209772, loss_ce: 0.008673, loss_dice: 0.410872
[12:10:36.676] TRAIN: iteration 27138 : loss : 0.032515, loss_ce: 0.001435, loss_dice: 0.063595
[12:10:38.655] TRAIN: iteration 27139 : loss : 0.038811, loss_ce: 0.001823, loss_dice: 0.075798
[12:10:38.863] TRAIN: iteration 27140 : loss : 0.037412, loss_ce: 0.002950, loss_dice: 0.071875
[12:10:39.107] TRAIN: iteration 27141 : loss : 0.045086, loss_ce: 0.003477, loss_dice: 0.086696
[12:10:39.314] TRAIN: iteration 27142 : loss : 0.236291, loss_ce: 0.003285, loss_dice: 0.469298
[12:10:39.523] TRAIN: iteration 27143 : loss : 0.096445, loss_ce: 0.003595, loss_dice: 0.189294
[12:10:39.730] TRAIN: iteration 27144 : loss : 0.085307, loss_ce: 0.008733, loss_dice: 0.161880
[12:10:39.937] TRAIN: iteration 27145 : loss : 0.051636, loss_ce: 0.002646, loss_dice: 0.100627
[12:10:40.144] TRAIN: iteration 27146 : loss : 0.091565, loss_ce: 0.010892, loss_dice: 0.172238
[12:10:42.262] TRAIN: iteration 27147 : loss : 0.169658, loss_ce: 0.002919, loss_dice: 0.336397
[12:10:42.470] TRAIN: iteration 27148 : loss : 0.036225, loss_ce: 0.003157, loss_dice: 0.069293
[12:10:42.678] TRAIN: iteration 27149 : loss : 0.064790, loss_ce: 0.003688, loss_dice: 0.125892
[12:10:42.893] TRAIN: iteration 27150 : loss : 0.025610, loss_ce: 0.004688, loss_dice: 0.046533
[12:10:43.134] TRAIN: iteration 27151 : loss : 0.121307, loss_ce: 0.002127, loss_dice: 0.240488
[12:10:43.343] TRAIN: iteration 27152 : loss : 0.088369, loss_ce: 0.007119, loss_dice: 0.169619
[12:10:43.550] TRAIN: iteration 27153 : loss : 0.033544, loss_ce: 0.003671, loss_dice: 0.063418
[12:10:43.764] TRAIN: iteration 27154 : loss : 0.037129, loss_ce: 0.006913, loss_dice: 0.067344
[12:10:43.971] TRAIN: iteration 27155 : loss : 0.031719, loss_ce: 0.001977, loss_dice: 0.061461
[12:10:44.178] TRAIN: iteration 27156 : loss : 0.250819, loss_ce: 0.001557, loss_dice: 0.500081
[12:10:44.386] TRAIN: iteration 27157 : loss : 0.100656, loss_ce: 0.004022, loss_dice: 0.197290
[12:10:44.594] TRAIN: iteration 27158 : loss : 0.247079, loss_ce: 0.001965, loss_dice: 0.492193
[12:10:44.807] TRAIN: iteration 27159 : loss : 0.233818, loss_ce: 0.003063, loss_dice: 0.464574
[12:10:45.015] TRAIN: iteration 27160 : loss : 0.099632, loss_ce: 0.001850, loss_dice: 0.197413
[12:10:45.261] TRAIN: iteration 27161 : loss : 0.061341, loss_ce: 0.003140, loss_dice: 0.119542
[12:10:45.468] TRAIN: iteration 27162 : loss : 0.093728, loss_ce: 0.003116, loss_dice: 0.184340
[12:10:45.747] TRAIN: iteration 27163 : loss : 0.099383, loss_ce: 0.008980, loss_dice: 0.189786
[12:10:45.959] TRAIN: iteration 27164 : loss : 0.169764, loss_ce: 0.003406, loss_dice: 0.336123
[12:10:46.168] TRAIN: iteration 27165 : loss : 0.105235, loss_ce: 0.005545, loss_dice: 0.204924
[12:10:46.376] TRAIN: iteration 27166 : loss : 0.045764, loss_ce: 0.003086, loss_dice: 0.088443
[12:10:46.584] TRAIN: iteration 27167 : loss : 0.182891, loss_ce: 0.002965, loss_dice: 0.362817
[12:10:47.140] TRAIN: iteration 27168 : loss : 0.068516, loss_ce: 0.003717, loss_dice: 0.133316
[12:10:47.350] TRAIN: iteration 27169 : loss : 0.068196, loss_ce: 0.005572, loss_dice: 0.130820
[12:10:47.558] TRAIN: iteration 27170 : loss : 0.083745, loss_ce: 0.003498, loss_dice: 0.163992
[12:10:48.175] TRAIN: iteration 27171 : loss : 0.195261, loss_ce: 0.006415, loss_dice: 0.384107
[12:10:49.085] TRAIN: iteration 27172 : loss : 0.042295, loss_ce: 0.005811, loss_dice: 0.078779
[12:10:49.293] TRAIN: iteration 27173 : loss : 0.037592, loss_ce: 0.003026, loss_dice: 0.072158
[12:10:49.500] TRAIN: iteration 27174 : loss : 0.050990, loss_ce: 0.001019, loss_dice: 0.100961
[12:10:49.709] TRAIN: iteration 27175 : loss : 0.161023, loss_ce: 0.002826, loss_dice: 0.319221
[12:10:50.177] TRAIN: iteration 27176 : loss : 0.061625, loss_ce: 0.006551, loss_dice: 0.116700
[12:10:50.384] TRAIN: iteration 27177 : loss : 0.250956, loss_ce: 0.001802, loss_dice: 0.500110
[12:10:50.592] TRAIN: iteration 27178 : loss : 0.052058, loss_ce: 0.001627, loss_dice: 0.102488
[12:10:52.532] TRAIN: iteration 27179 : loss : 0.050783, loss_ce: 0.000821, loss_dice: 0.100745
[12:10:52.750] TRAIN: iteration 27180 : loss : 0.206001, loss_ce: 0.006526, loss_dice: 0.405476
[12:10:52.990] TRAIN: iteration 27181 : loss : 0.250661, loss_ce: 0.001265, loss_dice: 0.500057
[12:10:53.203] TRAIN: iteration 27182 : loss : 0.050069, loss_ce: 0.001275, loss_dice: 0.098863
[12:10:53.410] TRAIN: iteration 27183 : loss : 0.261198, loss_ce: 0.025379, loss_dice: 0.497017
[12:10:53.619] TRAIN: iteration 27184 : loss : 0.189493, loss_ce: 0.004359, loss_dice: 0.374628
[12:10:54.830] TRAIN: iteration 27185 : loss : 0.122088, loss_ce: 0.001751, loss_dice: 0.242425
[12:10:55.039] TRAIN: iteration 27186 : loss : 0.032083, loss_ce: 0.005362, loss_dice: 0.058804
[12:10:56.854] TRAIN: iteration 27187 : loss : 0.203548, loss_ce: 0.001398, loss_dice: 0.405698
[12:10:57.061] TRAIN: iteration 27188 : loss : 0.101017, loss_ce: 0.004655, loss_dice: 0.197378
[12:10:57.269] TRAIN: iteration 27189 : loss : 0.141123, loss_ce: 0.007299, loss_dice: 0.274947
[12:10:57.477] TRAIN: iteration 27190 : loss : 0.167151, loss_ce: 0.004496, loss_dice: 0.329807
[12:10:57.685] TRAIN: iteration 27191 : loss : 0.216479, loss_ce: 0.001664, loss_dice: 0.431294
[12:10:57.900] TRAIN: iteration 27192 : loss : 0.061200, loss_ce: 0.007297, loss_dice: 0.115104
[12:10:59.196] TRAIN: iteration 27193 : loss : 0.120427, loss_ce: 0.001665, loss_dice: 0.239190
[12:10:59.404] TRAIN: iteration 27194 : loss : 0.250632, loss_ce: 0.001776, loss_dice: 0.499488
[12:10:59.652] TRAIN: iteration 27195 : loss : 0.129410, loss_ce: 0.004012, loss_dice: 0.254807
[12:10:59.860] TRAIN: iteration 27196 : loss : 0.102280, loss_ce: 0.007784, loss_dice: 0.196777
[12:11:00.072] TRAIN: iteration 27197 : loss : 0.249484, loss_ce: 0.001795, loss_dice: 0.497172
[12:11:00.284] TRAIN: iteration 27198 : loss : 0.039213, loss_ce: 0.000893, loss_dice: 0.077532
[12:11:00.493] TRAIN: iteration 27199 : loss : 0.149155, loss_ce: 0.001822, loss_dice: 0.296489
[12:11:00.703] TRAIN: iteration 27200 : loss : 0.134544, loss_ce: 0.001804, loss_dice: 0.267284
[12:11:01.615] TRAIN: iteration 27201 : loss : 0.021581, loss_ce: 0.001183, loss_dice: 0.041979
[12:11:01.824] TRAIN: iteration 27202 : loss : 0.056645, loss_ce: 0.003362, loss_dice: 0.109928
[12:11:02.033] TRAIN: iteration 27203 : loss : 0.089909, loss_ce: 0.003849, loss_dice: 0.175969
[12:11:02.245] TRAIN: iteration 27204 : loss : 0.061745, loss_ce: 0.002713, loss_dice: 0.120776
[12:11:02.455] TRAIN: iteration 27205 : loss : 0.244581, loss_ce: 0.001697, loss_dice: 0.487464
[12:11:02.663] TRAIN: iteration 27206 : loss : 0.208537, loss_ce: 0.006094, loss_dice: 0.410981
[12:11:02.872] TRAIN: iteration 27207 : loss : 0.204620, loss_ce: 0.002637, loss_dice: 0.406602
[12:11:03.092] TRAIN: iteration 27208 : loss : 0.072765, loss_ce: 0.006026, loss_dice: 0.139504
[12:11:03.751] TRAIN: iteration 27209 : loss : 0.229919, loss_ce: 0.004120, loss_dice: 0.455718
[12:11:03.959] TRAIN: iteration 27210 : loss : 0.043633, loss_ce: 0.003321, loss_dice: 0.083945
[12:11:04.167] TRAIN: iteration 27211 : loss : 0.127207, loss_ce: 0.004122, loss_dice: 0.250291
[12:11:04.375] TRAIN: iteration 27212 : loss : 0.092938, loss_ce: 0.001847, loss_dice: 0.184030
[12:11:04.584] TRAIN: iteration 27213 : loss : 0.079430, loss_ce: 0.003018, loss_dice: 0.155843
[12:11:04.794] TRAIN: iteration 27214 : loss : 0.042639, loss_ce: 0.004918, loss_dice: 0.080360
[12:11:05.003] TRAIN: iteration 27215 : loss : 0.086388, loss_ce: 0.002486, loss_dice: 0.170291
[12:11:05.214] TRAIN: iteration 27216 : loss : 0.160567, loss_ce: 0.003939, loss_dice: 0.317195
[12:11:05.423] TRAIN: iteration 27217 : loss : 0.018947, loss_ce: 0.001138, loss_dice: 0.036755
[12:11:05.630] TRAIN: iteration 27218 : loss : 0.095544, loss_ce: 0.003720, loss_dice: 0.187368
[12:11:05.839] TRAIN: iteration 27219 : loss : 0.077199, loss_ce: 0.005826, loss_dice: 0.148573
[12:11:07.263] TRAIN: iteration 27220 : loss : 0.043269, loss_ce: 0.001444, loss_dice: 0.085094
[12:11:07.608] TRAIN: iteration 27221 : loss : 0.131922, loss_ce: 0.004883, loss_dice: 0.258962
[12:11:07.823] TRAIN: iteration 27222 : loss : 0.111265, loss_ce: 0.002407, loss_dice: 0.220122
[12:11:08.033] TRAIN: iteration 27223 : loss : 0.251146, loss_ce: 0.003095, loss_dice: 0.499197
[12:11:08.239] TRAIN: iteration 27224 : loss : 0.248768, loss_ce: 0.001539, loss_dice: 0.495996
[12:11:09.217] TRAIN: iteration 27225 : loss : 0.200296, loss_ce: 0.001392, loss_dice: 0.399201
[12:11:09.425] TRAIN: iteration 27226 : loss : 0.044350, loss_ce: 0.001629, loss_dice: 0.087071
[12:11:09.633] TRAIN: iteration 27227 : loss : 0.191401, loss_ce: 0.001750, loss_dice: 0.381053
[12:11:09.841] TRAIN: iteration 27228 : loss : 0.032259, loss_ce: 0.003109, loss_dice: 0.061408
[12:11:10.390] TRAIN: iteration 27229 : loss : 0.045876, loss_ce: 0.003466, loss_dice: 0.088285
[12:11:10.597] TRAIN: iteration 27230 : loss : 0.250369, loss_ce: 0.000715, loss_dice: 0.500023
[12:11:10.805] TRAIN: iteration 27231 : loss : 0.031357, loss_ce: 0.001513, loss_dice: 0.061201
[12:11:11.013] TRAIN: iteration 27232 : loss : 0.075097, loss_ce: 0.002792, loss_dice: 0.147402
[12:11:11.220] TRAIN: iteration 27233 : loss : 0.038393, loss_ce: 0.001287, loss_dice: 0.075500
[12:11:11.429] TRAIN: iteration 27234 : loss : 0.054105, loss_ce: 0.002901, loss_dice: 0.105310
[12:11:11.639] TRAIN: iteration 27235 : loss : 0.131819, loss_ce: 0.002322, loss_dice: 0.261317
[12:11:11.853] TRAIN: iteration 27236 : loss : 0.094950, loss_ce: 0.007390, loss_dice: 0.182511
[12:11:13.952] TRAIN: iteration 27237 : loss : 0.158928, loss_ce: 0.002391, loss_dice: 0.315466
[12:11:14.254] TRAIN: iteration 27238 : loss : 0.053264, loss_ce: 0.002217, loss_dice: 0.104312
[12:11:14.464] TRAIN: iteration 27239 : loss : 0.250370, loss_ce: 0.000715, loss_dice: 0.500026
[12:11:14.676] TRAIN: iteration 27240 : loss : 0.220695, loss_ce: 0.002183, loss_dice: 0.439208
[12:11:14.916] TRAIN: iteration 27241 : loss : 0.109085, loss_ce: 0.003013, loss_dice: 0.215157
[12:11:15.125] TRAIN: iteration 27242 : loss : 0.083825, loss_ce: 0.007364, loss_dice: 0.160286
[12:11:15.338] TRAIN: iteration 27243 : loss : 0.084153, loss_ce: 0.005532, loss_dice: 0.162774
[12:11:15.545] TRAIN: iteration 27244 : loss : 0.046974, loss_ce: 0.001400, loss_dice: 0.092548
[12:11:17.388] TRAIN: iteration 27245 : loss : 0.050885, loss_ce: 0.001992, loss_dice: 0.099778
[12:11:17.597] TRAIN: iteration 27246 : loss : 0.190675, loss_ce: 0.003659, loss_dice: 0.377691
[12:11:17.805] TRAIN: iteration 27247 : loss : 0.041864, loss_ce: 0.001129, loss_dice: 0.082600
[12:11:18.013] TRAIN: iteration 27248 : loss : 0.080297, loss_ce: 0.002364, loss_dice: 0.158230
[12:11:18.221] TRAIN: iteration 27249 : loss : 0.039260, loss_ce: 0.004831, loss_dice: 0.073689
[12:11:18.431] TRAIN: iteration 27250 : loss : 0.241683, loss_ce: 0.001809, loss_dice: 0.481557
[12:11:18.643] TRAIN: iteration 27251 : loss : 0.043994, loss_ce: 0.002051, loss_dice: 0.085938
[12:11:19.453] TRAIN: iteration 27252 : loss : 0.250432, loss_ce: 0.000833, loss_dice: 0.500031
[12:11:19.660] TRAIN: iteration 27253 : loss : 0.066061, loss_ce: 0.003935, loss_dice: 0.128187
[12:11:19.869] TRAIN: iteration 27254 : loss : 0.065080, loss_ce: 0.006275, loss_dice: 0.123886
[12:11:20.080] TRAIN: iteration 27255 : loss : 0.215337, loss_ce: 0.003388, loss_dice: 0.427287
[12:11:20.288] TRAIN: iteration 27256 : loss : 0.094228, loss_ce: 0.008470, loss_dice: 0.179985
[12:11:20.495] TRAIN: iteration 27257 : loss : 0.248478, loss_ce: 0.002582, loss_dice: 0.494373
[12:11:20.729] TRAIN: iteration 27258 : loss : 0.054279, loss_ce: 0.001405, loss_dice: 0.107153
[12:11:20.938] TRAIN: iteration 27259 : loss : 0.245330, loss_ce: 0.001135, loss_dice: 0.489525
[12:11:21.502] TRAIN: iteration 27260 : loss : 0.106496, loss_ce: 0.001116, loss_dice: 0.211876
[12:11:21.738] TRAIN: iteration 27261 : loss : 0.134427, loss_ce: 0.005624, loss_dice: 0.263230
[12:11:21.947] TRAIN: iteration 27262 : loss : 0.071726, loss_ce: 0.014509, loss_dice: 0.128943
[12:11:22.158] TRAIN: iteration 27263 : loss : 0.047445, loss_ce: 0.002003, loss_dice: 0.092888
[12:11:22.366] TRAIN: iteration 27264 : loss : 0.049286, loss_ce: 0.002334, loss_dice: 0.096238
[12:11:22.574] TRAIN: iteration 27265 : loss : 0.106392, loss_ce: 0.006438, loss_dice: 0.206346
[12:11:22.785] TRAIN: iteration 27266 : loss : 0.043843, loss_ce: 0.003265, loss_dice: 0.084420
[12:11:22.993] TRAIN: iteration 27267 : loss : 0.054799, loss_ce: 0.008597, loss_dice: 0.101002
[12:11:24.475] TRAIN: iteration 27268 : loss : 0.044659, loss_ce: 0.004428, loss_dice: 0.084891
[12:11:24.682] TRAIN: iteration 27269 : loss : 0.086537, loss_ce: 0.005113, loss_dice: 0.167960
[12:11:26.908] TRAIN: iteration 27270 : loss : 0.104772, loss_ce: 0.001681, loss_dice: 0.207862
[12:11:27.124] TRAIN: iteration 27271 : loss : 0.092971, loss_ce: 0.004727, loss_dice: 0.181214
[12:11:27.332] TRAIN: iteration 27272 : loss : 0.080301, loss_ce: 0.003149, loss_dice: 0.157452
[12:11:27.541] TRAIN: iteration 27273 : loss : 0.056890, loss_ce: 0.005529, loss_dice: 0.108251
[12:11:27.755] TRAIN: iteration 27274 : loss : 0.068413, loss_ce: 0.003817, loss_dice: 0.133010
[12:11:27.962] TRAIN: iteration 27275 : loss : 0.104083, loss_ce: 0.001201, loss_dice: 0.206965
[12:11:28.177] TRAIN: iteration 27276 : loss : 0.039798, loss_ce: 0.004872, loss_dice: 0.074724
[12:11:28.386] TRAIN: iteration 27277 : loss : 0.250694, loss_ce: 0.001317, loss_dice: 0.500071
[12:11:29.614] TRAIN: iteration 27278 : loss : 0.111512, loss_ce: 0.003788, loss_dice: 0.219237
[12:11:29.822] TRAIN: iteration 27279 : loss : 0.124832, loss_ce: 0.001475, loss_dice: 0.248189
[12:11:30.032] TRAIN: iteration 27280 : loss : 0.220792, loss_ce: 0.001567, loss_dice: 0.440016
[12:11:30.285] TRAIN: iteration 27281 : loss : 0.051039, loss_ce: 0.001419, loss_dice: 0.100660
[12:11:30.495] TRAIN: iteration 27282 : loss : 0.129496, loss_ce: 0.002971, loss_dice: 0.256021
[12:11:30.703] TRAIN: iteration 27283 : loss : 0.070718, loss_ce: 0.003699, loss_dice: 0.137737
[12:11:32.269] TRAIN: iteration 27284 : loss : 0.251594, loss_ce: 0.002955, loss_dice: 0.500233
[12:11:32.483] TRAIN: iteration 27285 : loss : 0.104136, loss_ce: 0.011959, loss_dice: 0.196313
[12:11:32.691] TRAIN: iteration 27286 : loss : 0.232605, loss_ce: 0.002705, loss_dice: 0.462504
[12:11:32.900] TRAIN: iteration 27287 : loss : 0.057111, loss_ce: 0.003978, loss_dice: 0.110245
[12:11:33.108] TRAIN: iteration 27288 : loss : 0.068056, loss_ce: 0.003170, loss_dice: 0.132943
[12:11:33.315] TRAIN: iteration 27289 : loss : 0.248866, loss_ce: 0.007544, loss_dice: 0.490187
[12:11:33.524] TRAIN: iteration 27290 : loss : 0.053168, loss_ce: 0.003092, loss_dice: 0.103244
[12:11:33.732] TRAIN: iteration 27291 : loss : 0.182873, loss_ce: 0.001675, loss_dice: 0.364071
[12:11:36.949] TRAIN: iteration 27292 : loss : 0.135037, loss_ce: 0.005432, loss_dice: 0.264642
[12:11:37.157] TRAIN: iteration 27293 : loss : 0.034998, loss_ce: 0.001740, loss_dice: 0.068256
[12:11:37.365] TRAIN: iteration 27294 : loss : 0.093505, loss_ce: 0.002246, loss_dice: 0.184764
[12:11:37.573] TRAIN: iteration 27295 : loss : 0.105941, loss_ce: 0.002043, loss_dice: 0.209839
[12:11:37.780] TRAIN: iteration 27296 : loss : 0.055809, loss_ce: 0.004211, loss_dice: 0.107408
[12:11:37.987] TRAIN: iteration 27297 : loss : 0.250679, loss_ce: 0.001314, loss_dice: 0.500044
[12:11:38.194] TRAIN: iteration 27298 : loss : 0.164591, loss_ce: 0.002419, loss_dice: 0.326763
[12:11:38.402] TRAIN: iteration 27299 : loss : 0.065860, loss_ce: 0.002150, loss_dice: 0.129570
[12:11:42.220] TRAIN: iteration 27300 : loss : 0.040615, loss_ce: 0.003305, loss_dice: 0.077925
[12:11:42.458] TRAIN: iteration 27301 : loss : 0.038102, loss_ce: 0.002871, loss_dice: 0.073333
[12:11:42.667] TRAIN: iteration 27302 : loss : 0.250618, loss_ce: 0.001201, loss_dice: 0.500036
[12:11:42.876] TRAIN: iteration 27303 : loss : 0.042129, loss_ce: 0.002712, loss_dice: 0.081546
[12:11:43.085] TRAIN: iteration 27304 : loss : 0.099405, loss_ce: 0.003253, loss_dice: 0.195557
[12:11:43.294] TRAIN: iteration 27305 : loss : 0.220785, loss_ce: 0.002439, loss_dice: 0.439132
[12:11:43.507] TRAIN: iteration 27306 : loss : 0.083627, loss_ce: 0.008968, loss_dice: 0.158285
[12:11:43.717] TRAIN: iteration 27307 : loss : 0.250918, loss_ce: 0.001730, loss_dice: 0.500106
[12:11:44.971] TRAIN: iteration 27308 : loss : 0.219025, loss_ce: 0.001088, loss_dice: 0.436961
[12:11:45.179] TRAIN: iteration 27309 : loss : 0.059639, loss_ce: 0.004423, loss_dice: 0.114855
[12:11:45.388] TRAIN: iteration 27310 : loss : 0.091760, loss_ce: 0.006166, loss_dice: 0.177354
[12:11:45.598] TRAIN: iteration 27311 : loss : 0.129990, loss_ce: 0.007991, loss_dice: 0.251989
[12:11:45.808] TRAIN: iteration 27312 : loss : 0.044874, loss_ce: 0.002800, loss_dice: 0.086948
[12:11:46.016] TRAIN: iteration 27313 : loss : 0.250628, loss_ce: 0.001193, loss_dice: 0.500063
[12:11:46.227] TRAIN: iteration 27314 : loss : 0.187841, loss_ce: 0.014267, loss_dice: 0.361415
[12:11:46.435] TRAIN: iteration 27315 : loss : 0.065451, loss_ce: 0.001926, loss_dice: 0.128976
[12:11:49.250] TRAIN: iteration 27316 : loss : 0.159564, loss_ce: 0.001719, loss_dice: 0.317410
[12:11:49.463] TRAIN: iteration 27317 : loss : 0.228026, loss_ce: 0.004025, loss_dice: 0.452027
[12:11:49.670] TRAIN: iteration 27318 : loss : 0.184298, loss_ce: 0.001186, loss_dice: 0.367411
[12:11:49.876] TRAIN: iteration 27319 : loss : 0.235554, loss_ce: 0.004383, loss_dice: 0.466725
[12:11:50.083] TRAIN: iteration 27320 : loss : 0.145566, loss_ce: 0.002340, loss_dice: 0.288792
[12:11:50.320] TRAIN: iteration 27321 : loss : 0.082677, loss_ce: 0.005606, loss_dice: 0.159749
[12:11:50.527] TRAIN: iteration 27322 : loss : 0.210115, loss_ce: 0.001726, loss_dice: 0.418503
[12:11:50.735] TRAIN: iteration 27323 : loss : 0.234938, loss_ce: 0.001731, loss_dice: 0.468146
[12:11:51.243] TRAIN: iteration 27324 : loss : 0.072817, loss_ce: 0.004183, loss_dice: 0.141452
[12:11:53.099] TRAIN: iteration 27325 : loss : 0.050058, loss_ce: 0.001572, loss_dice: 0.098545
[12:11:53.306] TRAIN: iteration 27326 : loss : 0.035782, loss_ce: 0.001405, loss_dice: 0.070159
[12:11:53.519] TRAIN: iteration 27327 : loss : 0.043180, loss_ce: 0.003135, loss_dice: 0.083226
[12:11:53.726] TRAIN: iteration 27328 : loss : 0.149872, loss_ce: 0.005511, loss_dice: 0.294233
[12:11:53.937] TRAIN: iteration 27329 : loss : 0.114037, loss_ce: 0.000820, loss_dice: 0.227255
[12:11:54.145] TRAIN: iteration 27330 : loss : 0.099449, loss_ce: 0.005959, loss_dice: 0.192938
[12:11:54.355] TRAIN: iteration 27331 : loss : 0.060292, loss_ce: 0.004736, loss_dice: 0.115848
[12:11:55.359] TRAIN: iteration 27332 : loss : 0.094843, loss_ce: 0.011692, loss_dice: 0.177993
[12:11:56.019] TRAIN: iteration 27333 : loss : 0.027046, loss_ce: 0.002951, loss_dice: 0.051141
[12:11:56.227] TRAIN: iteration 27334 : loss : 0.208590, loss_ce: 0.003020, loss_dice: 0.414159
[12:11:56.440] TRAIN: iteration 27335 : loss : 0.038831, loss_ce: 0.004666, loss_dice: 0.072995
[12:11:56.651] TRAIN: iteration 27336 : loss : 0.151366, loss_ce: 0.006411, loss_dice: 0.296321
[12:11:56.858] TRAIN: iteration 27337 : loss : 0.099203, loss_ce: 0.002455, loss_dice: 0.195950
[12:11:57.755] TRAIN: iteration 27338 : loss : 0.066683, loss_ce: 0.002509, loss_dice: 0.130857
[12:11:57.962] TRAIN: iteration 27339 : loss : 0.044300, loss_ce: 0.002161, loss_dice: 0.086439
[12:11:58.442] TRAIN: iteration 27340 : loss : 0.253501, loss_ce: 0.007701, loss_dice: 0.499300
[12:11:58.442] NaN or Inf found in input tensor.
[12:11:58.657] TRAIN: iteration 27341 : loss : 0.008957, loss_ce: 0.000932, loss_dice: 0.016981
[12:11:59.658] TRAIN: iteration 27342 : loss : 0.083861, loss_ce: 0.002140, loss_dice: 0.165582
[12:11:59.865] TRAIN: iteration 27343 : loss : 0.179576, loss_ce: 0.006944, loss_dice: 0.352207
[12:12:00.072] TRAIN: iteration 27344 : loss : 0.053154, loss_ce: 0.004462, loss_dice: 0.101847
[12:12:00.280] TRAIN: iteration 27345 : loss : 0.250159, loss_ce: 0.000319, loss_dice: 0.499999
[12:12:01.646] TRAIN: iteration 27346 : loss : 0.083889, loss_ce: 0.002541, loss_dice: 0.165238
[12:12:01.857] TRAIN: iteration 27347 : loss : 0.054105, loss_ce: 0.001954, loss_dice: 0.106257
[12:12:03.185] TRAIN: iteration 27348 : loss : 0.252326, loss_ce: 0.007631, loss_dice: 0.497021
[12:12:03.393] TRAIN: iteration 27349 : loss : 0.113251, loss_ce: 0.010751, loss_dice: 0.215752
[12:12:03.601] TRAIN: iteration 27350 : loss : 0.097095, loss_ce: 0.005082, loss_dice: 0.189108
[12:12:03.808] TRAIN: iteration 27351 : loss : 0.050015, loss_ce: 0.007698, loss_dice: 0.092333
[12:12:04.017] TRAIN: iteration 27352 : loss : 0.047047, loss_ce: 0.005418, loss_dice: 0.088675
[12:12:04.224] TRAIN: iteration 27353 : loss : 0.155631, loss_ce: 0.011632, loss_dice: 0.299631
[12:12:04.490] TRAIN: iteration 27354 : loss : 0.147978, loss_ce: 0.005225, loss_dice: 0.290731
[12:12:04.700] TRAIN: iteration 27355 : loss : 0.012913, loss_ce: 0.000606, loss_dice: 0.025221
[12:12:07.156] TRAIN: iteration 27356 : loss : 0.206359, loss_ce: 0.004402, loss_dice: 0.408316
[12:12:07.363] TRAIN: iteration 27357 : loss : 0.230767, loss_ce: 0.001303, loss_dice: 0.460230
[12:12:07.573] TRAIN: iteration 27358 : loss : 0.250401, loss_ce: 0.000778, loss_dice: 0.500023
[12:12:07.783] TRAIN: iteration 27359 : loss : 0.051556, loss_ce: 0.006819, loss_dice: 0.096292
[12:12:07.992] TRAIN: iteration 27360 : loss : 0.148586, loss_ce: 0.002640, loss_dice: 0.294531
[12:12:08.239] TRAIN: iteration 27361 : loss : 0.025332, loss_ce: 0.001718, loss_dice: 0.048945
[12:12:08.447] TRAIN: iteration 27362 : loss : 0.050608, loss_ce: 0.002587, loss_dice: 0.098630
[12:12:08.659] TRAIN: iteration 27363 : loss : 0.056043, loss_ce: 0.001135, loss_dice: 0.110951
[12:12:09.522] TRAIN: iteration 27364 : loss : 0.121944, loss_ce: 0.008056, loss_dice: 0.235832
[12:12:09.730] TRAIN: iteration 27365 : loss : 0.055926, loss_ce: 0.002242, loss_dice: 0.109609
[12:12:09.937] TRAIN: iteration 27366 : loss : 0.046468, loss_ce: 0.001054, loss_dice: 0.091883
[12:12:10.144] TRAIN: iteration 27367 : loss : 0.060524, loss_ce: 0.006282, loss_dice: 0.114766
[12:12:10.351] TRAIN: iteration 27368 : loss : 0.250677, loss_ce: 0.001285, loss_dice: 0.500068
[12:12:10.561] TRAIN: iteration 27369 : loss : 0.095788, loss_ce: 0.008152, loss_dice: 0.183424
[12:12:10.768] TRAIN: iteration 27370 : loss : 0.090190, loss_ce: 0.002295, loss_dice: 0.178084
[12:12:10.975] TRAIN: iteration 27371 : loss : 0.104347, loss_ce: 0.002160, loss_dice: 0.206533
[12:12:11.184] TRAIN: iteration 27372 : loss : 0.061890, loss_ce: 0.002183, loss_dice: 0.121596
[12:12:12.610] TRAIN: iteration 27373 : loss : 0.052449, loss_ce: 0.003820, loss_dice: 0.101077
[12:12:12.820] TRAIN: iteration 27374 : loss : 0.065046, loss_ce: 0.005101, loss_dice: 0.124991
[12:12:13.027] TRAIN: iteration 27375 : loss : 0.069856, loss_ce: 0.007397, loss_dice: 0.132316
[12:12:13.238] TRAIN: iteration 27376 : loss : 0.043379, loss_ce: 0.007701, loss_dice: 0.079057
[12:12:13.445] TRAIN: iteration 27377 : loss : 0.057944, loss_ce: 0.001874, loss_dice: 0.114014
[12:12:15.883] TRAIN: iteration 27378 : loss : 0.188307, loss_ce: 0.005982, loss_dice: 0.370631
[12:12:16.090] TRAIN: iteration 27379 : loss : 0.051456, loss_ce: 0.002260, loss_dice: 0.100652
[12:12:16.304] TRAIN: iteration 27380 : loss : 0.250545, loss_ce: 0.001055, loss_dice: 0.500034
[12:12:16.547] TRAIN: iteration 27381 : loss : 0.081670, loss_ce: 0.001720, loss_dice: 0.161621
[12:12:16.754] TRAIN: iteration 27382 : loss : 0.154167, loss_ce: 0.003277, loss_dice: 0.305058
[12:12:16.962] TRAIN: iteration 27383 : loss : 0.250328, loss_ce: 0.000650, loss_dice: 0.500007
[12:12:17.173] TRAIN: iteration 27384 : loss : 0.065142, loss_ce: 0.012038, loss_dice: 0.118247
[12:12:17.382] TRAIN: iteration 27385 : loss : 0.029773, loss_ce: 0.003263, loss_dice: 0.056283
[12:12:20.268] TRAIN: iteration 27386 : loss : 0.250791, loss_ce: 0.001496, loss_dice: 0.500087
[12:12:20.478] TRAIN: iteration 27387 : loss : 0.169029, loss_ce: 0.002528, loss_dice: 0.335530
[12:12:20.685] TRAIN: iteration 27388 : loss : 0.101866, loss_ce: 0.001683, loss_dice: 0.202049
[12:12:20.894] TRAIN: iteration 27389 : loss : 0.086766, loss_ce: 0.001697, loss_dice: 0.171836
[12:12:21.101] TRAIN: iteration 27390 : loss : 0.136290, loss_ce: 0.009206, loss_dice: 0.263374
[12:12:21.309] TRAIN: iteration 27391 : loss : 0.053326, loss_ce: 0.001885, loss_dice: 0.104767
[12:12:21.517] TRAIN: iteration 27392 : loss : 0.015142, loss_ce: 0.000629, loss_dice: 0.029655
[12:12:21.726] TRAIN: iteration 27393 : loss : 0.107615, loss_ce: 0.003356, loss_dice: 0.211875
[12:12:25.067] TRAIN: iteration 27394 : loss : 0.131638, loss_ce: 0.005629, loss_dice: 0.257647
[12:12:25.277] TRAIN: iteration 27395 : loss : 0.212897, loss_ce: 0.001196, loss_dice: 0.424598
[12:12:25.485] TRAIN: iteration 27396 : loss : 0.130104, loss_ce: 0.000772, loss_dice: 0.259436
[12:12:25.693] TRAIN: iteration 27397 : loss : 0.237673, loss_ce: 0.001138, loss_dice: 0.474208
[12:12:25.900] TRAIN: iteration 27398 : loss : 0.086686, loss_ce: 0.003061, loss_dice: 0.170311
[12:12:26.107] TRAIN: iteration 27399 : loss : 0.094182, loss_ce: 0.016643, loss_dice: 0.171722
[12:12:26.318] TRAIN: iteration 27400 : loss : 0.146850, loss_ce: 0.003013, loss_dice: 0.290686
[12:12:26.549] TRAIN: iteration 27401 : loss : 0.098402, loss_ce: 0.002349, loss_dice: 0.194455
[12:12:29.165] TRAIN: iteration 27402 : loss : 0.134534, loss_ce: 0.004801, loss_dice: 0.264267
[12:12:29.372] TRAIN: iteration 27403 : loss : 0.243631, loss_ce: 0.000853, loss_dice: 0.486409
[12:12:29.582] TRAIN: iteration 27404 : loss : 0.218133, loss_ce: 0.001313, loss_dice: 0.434953
[12:12:29.792] TRAIN: iteration 27405 : loss : 0.250520, loss_ce: 0.000995, loss_dice: 0.500044
[12:12:30.043] TRAIN: iteration 27406 : loss : 0.147352, loss_ce: 0.013480, loss_dice: 0.281224
[12:12:30.251] TRAIN: iteration 27407 : loss : 0.092179, loss_ce: 0.004538, loss_dice: 0.179821
[12:12:30.459] TRAIN: iteration 27408 : loss : 0.062269, loss_ce: 0.002725, loss_dice: 0.121812
[12:12:30.674] TRAIN: iteration 27409 : loss : 0.064411, loss_ce: 0.006068, loss_dice: 0.122754
[12:12:34.029] TRAIN: iteration 27410 : loss : 0.034332, loss_ce: 0.006294, loss_dice: 0.062370
[12:12:34.242] TRAIN: iteration 27411 : loss : 0.059492, loss_ce: 0.012841, loss_dice: 0.106143
[12:12:34.449] TRAIN: iteration 27412 : loss : 0.227630, loss_ce: 0.003654, loss_dice: 0.451605
[12:12:34.657] TRAIN: iteration 27413 : loss : 0.064400, loss_ce: 0.008536, loss_dice: 0.120265
[12:12:34.867] TRAIN: iteration 27414 : loss : 0.080080, loss_ce: 0.003770, loss_dice: 0.156390
[12:12:35.075] TRAIN: iteration 27415 : loss : 0.051317, loss_ce: 0.001863, loss_dice: 0.100772
[12:12:35.281] TRAIN: iteration 27416 : loss : 0.022427, loss_ce: 0.001117, loss_dice: 0.043736
[12:12:35.488] TRAIN: iteration 27417 : loss : 0.114460, loss_ce: 0.008641, loss_dice: 0.220279
[12:12:37.006] TRAIN: iteration 27418 : loss : 0.244607, loss_ce: 0.001408, loss_dice: 0.487807
[12:12:37.213] TRAIN: iteration 27419 : loss : 0.033143, loss_ce: 0.004873, loss_dice: 0.061414
[12:12:37.423] TRAIN: iteration 27420 : loss : 0.061123, loss_ce: 0.001613, loss_dice: 0.120632
[12:12:37.658] TRAIN: iteration 27421 : loss : 0.078273, loss_ce: 0.002776, loss_dice: 0.153770
[12:12:37.871] TRAIN: iteration 27422 : loss : 0.250924, loss_ce: 0.001757, loss_dice: 0.500091
[12:12:38.079] TRAIN: iteration 27423 : loss : 0.043846, loss_ce: 0.000857, loss_dice: 0.086836
[12:12:38.291] TRAIN: iteration 27424 : loss : 0.148403, loss_ce: 0.000920, loss_dice: 0.295887
[12:12:38.498] TRAIN: iteration 27425 : loss : 0.117366, loss_ce: 0.002925, loss_dice: 0.231808
[12:12:39.782] TRAIN: iteration 27426 : loss : 0.106785, loss_ce: 0.007219, loss_dice: 0.206352
[12:12:39.990] TRAIN: iteration 27427 : loss : 0.052408, loss_ce: 0.003550, loss_dice: 0.101266
[12:12:40.198] TRAIN: iteration 27428 : loss : 0.125397, loss_ce: 0.001133, loss_dice: 0.249661
[12:12:40.476] TRAIN: iteration 27429 : loss : 0.042096, loss_ce: 0.009646, loss_dice: 0.074546
[12:12:40.683] TRAIN: iteration 27430 : loss : 0.012554, loss_ce: 0.001035, loss_dice: 0.024074
[12:12:40.891] TRAIN: iteration 27431 : loss : 0.052337, loss_ce: 0.004592, loss_dice: 0.100081
[12:12:41.100] TRAIN: iteration 27432 : loss : 0.063946, loss_ce: 0.002728, loss_dice: 0.125164
[12:12:41.307] TRAIN: iteration 27433 : loss : 0.059267, loss_ce: 0.004214, loss_dice: 0.114320
[12:12:43.995] TRAIN: iteration 27434 : loss : 0.118722, loss_ce: 0.007396, loss_dice: 0.230048
[12:12:44.204] TRAIN: iteration 27435 : loss : 0.168102, loss_ce: 0.001325, loss_dice: 0.334879
[12:12:44.412] TRAIN: iteration 27436 : loss : 0.250261, loss_ce: 0.000511, loss_dice: 0.500010
[12:12:44.620] TRAIN: iteration 27437 : loss : 0.171761, loss_ce: 0.001354, loss_dice: 0.342168
[12:12:44.830] TRAIN: iteration 27438 : loss : 0.153177, loss_ce: 0.001884, loss_dice: 0.304469
[12:12:45.587] TRAIN: iteration 27439 : loss : 0.250422, loss_ce: 0.000823, loss_dice: 0.500020
[12:12:45.794] TRAIN: iteration 27440 : loss : 0.020103, loss_ce: 0.001139, loss_dice: 0.039067
[12:12:46.035] TRAIN: iteration 27441 : loss : 0.089952, loss_ce: 0.008697, loss_dice: 0.171208
[12:12:47.400] TRAIN: iteration 27442 : loss : 0.050644, loss_ce: 0.007353, loss_dice: 0.093935
[12:12:47.609] TRAIN: iteration 27443 : loss : 0.035421, loss_ce: 0.003792, loss_dice: 0.067050
[12:12:47.816] TRAIN: iteration 27444 : loss : 0.102031, loss_ce: 0.007696, loss_dice: 0.196366
[12:12:48.286] TRAIN: iteration 27445 : loss : 0.088867, loss_ce: 0.012976, loss_dice: 0.164757
[12:12:49.333] TRAIN: iteration 27446 : loss : 0.032870, loss_ce: 0.003700, loss_dice: 0.062041
[12:12:49.540] TRAIN: iteration 27447 : loss : 0.060913, loss_ce: 0.004314, loss_dice: 0.117512
[12:12:49.750] TRAIN: iteration 27448 : loss : 0.060950, loss_ce: 0.002558, loss_dice: 0.119341
[12:12:49.959] TRAIN: iteration 27449 : loss : 0.049275, loss_ce: 0.006777, loss_dice: 0.091773
[12:12:50.997] TRAIN: iteration 27450 : loss : 0.093208, loss_ce: 0.003456, loss_dice: 0.182959
[12:12:52.570] TRAIN: iteration 27451 : loss : 0.091259, loss_ce: 0.004314, loss_dice: 0.178204
[12:12:52.778] TRAIN: iteration 27452 : loss : 0.243887, loss_ce: 0.001686, loss_dice: 0.486089
[12:12:52.986] TRAIN: iteration 27453 : loss : 0.089311, loss_ce: 0.003446, loss_dice: 0.175175
[12:12:53.193] TRAIN: iteration 27454 : loss : 0.039078, loss_ce: 0.001353, loss_dice: 0.076804
[12:12:54.486] TRAIN: iteration 27455 : loss : 0.031246, loss_ce: 0.001736, loss_dice: 0.060755
[12:12:54.693] TRAIN: iteration 27456 : loss : 0.074583, loss_ce: 0.003203, loss_dice: 0.145962
[12:12:54.901] TRAIN: iteration 27457 : loss : 0.053641, loss_ce: 0.005313, loss_dice: 0.101969
[12:12:56.950] TRAIN: iteration 27458 : loss : 0.250627, loss_ce: 0.001208, loss_dice: 0.500045
[12:12:57.158] TRAIN: iteration 27459 : loss : 0.078113, loss_ce: 0.003354, loss_dice: 0.152871
[12:12:57.366] TRAIN: iteration 27460 : loss : 0.169414, loss_ce: 0.001447, loss_dice: 0.337381
[12:12:57.604] TRAIN: iteration 27461 : loss : 0.072148, loss_ce: 0.002037, loss_dice: 0.142260
[12:12:57.813] TRAIN: iteration 27462 : loss : 0.093426, loss_ce: 0.001616, loss_dice: 0.185236
[12:12:58.023] TRAIN: iteration 27463 : loss : 0.159527, loss_ce: 0.001425, loss_dice: 0.317630
[12:12:58.232] TRAIN: iteration 27464 : loss : 0.146253, loss_ce: 0.002479, loss_dice: 0.290027
[12:12:58.441] TRAIN: iteration 27465 : loss : 0.036449, loss_ce: 0.001207, loss_dice: 0.071691
[12:13:00.307] TRAIN: iteration 27466 : loss : 0.250627, loss_ce: 0.001183, loss_dice: 0.500070
[12:13:01.629] TRAIN: iteration 27467 : loss : 0.021796, loss_ce: 0.000997, loss_dice: 0.042595
[12:13:01.836] TRAIN: iteration 27468 : loss : 0.208222, loss_ce: 0.001964, loss_dice: 0.414481
[12:13:02.044] TRAIN: iteration 27469 : loss : 0.102341, loss_ce: 0.001320, loss_dice: 0.203361
[12:13:02.251] TRAIN: iteration 27470 : loss : 0.250392, loss_ce: 0.000756, loss_dice: 0.500029
[12:13:02.459] TRAIN: iteration 27471 : loss : 0.081157, loss_ce: 0.001887, loss_dice: 0.160426
[12:13:02.667] TRAIN: iteration 27472 : loss : 0.160744, loss_ce: 0.003311, loss_dice: 0.318178
[12:13:02.873] TRAIN: iteration 27473 : loss : 0.038753, loss_ce: 0.003944, loss_dice: 0.073563
[12:13:03.081] TRAIN: iteration 27474 : loss : 0.074835, loss_ce: 0.019341, loss_dice: 0.130329
[12:13:06.146] TRAIN: iteration 27475 : loss : 0.127682, loss_ce: 0.002971, loss_dice: 0.252394
[12:13:07.227] TRAIN: iteration 27476 : loss : 0.115266, loss_ce: 0.001569, loss_dice: 0.228963
[12:13:07.436] TRAIN: iteration 27477 : loss : 0.058300, loss_ce: 0.002437, loss_dice: 0.114162
[12:13:07.645] TRAIN: iteration 27478 : loss : 0.049553, loss_ce: 0.004008, loss_dice: 0.095098
[12:13:07.853] TRAIN: iteration 27479 : loss : 0.066838, loss_ce: 0.002245, loss_dice: 0.131432
[12:13:08.060] TRAIN: iteration 27480 : loss : 0.145377, loss_ce: 0.005072, loss_dice: 0.285682
[12:13:08.294] TRAIN: iteration 27481 : loss : 0.088614, loss_ce: 0.006481, loss_dice: 0.170747
[12:13:08.862] TRAIN: iteration 27482 : loss : 0.060791, loss_ce: 0.002024, loss_dice: 0.119557
[12:13:10.677] TRAIN: iteration 27483 : loss : 0.236109, loss_ce: 0.001121, loss_dice: 0.471096
[12:13:14.244] TRAIN: iteration 27484 : loss : 0.061989, loss_ce: 0.003120, loss_dice: 0.120857
[12:13:14.452] TRAIN: iteration 27485 : loss : 0.036677, loss_ce: 0.001715, loss_dice: 0.071640
[12:13:14.659] TRAIN: iteration 27486 : loss : 0.088763, loss_ce: 0.005387, loss_dice: 0.172139
[12:13:14.868] TRAIN: iteration 27487 : loss : 0.141699, loss_ce: 0.010300, loss_dice: 0.273098
[12:13:15.076] TRAIN: iteration 27488 : loss : 0.229784, loss_ce: 0.001524, loss_dice: 0.458043
[12:13:15.285] TRAIN: iteration 27489 : loss : 0.251077, loss_ce: 0.002019, loss_dice: 0.500134
[12:13:15.492] TRAIN: iteration 27490 : loss : 0.234288, loss_ce: 0.001714, loss_dice: 0.466861
[12:13:16.104] TRAIN: iteration 27491 : loss : 0.105662, loss_ce: 0.006333, loss_dice: 0.204992
[12:13:20.333] TRAIN: iteration 27492 : loss : 0.069106, loss_ce: 0.003527, loss_dice: 0.134686
[12:13:20.544] TRAIN: iteration 27493 : loss : 0.229589, loss_ce: 0.002256, loss_dice: 0.456922
[12:13:20.754] TRAIN: iteration 27494 : loss : 0.113672, loss_ce: 0.003628, loss_dice: 0.223717
[12:13:20.963] TRAIN: iteration 27495 : loss : 0.136606, loss_ce: 0.003980, loss_dice: 0.269232
[12:13:21.175] TRAIN: iteration 27496 : loss : 0.028431, loss_ce: 0.004212, loss_dice: 0.052650
[12:13:21.387] TRAIN: iteration 27497 : loss : 0.182719, loss_ce: 0.005826, loss_dice: 0.359613
[12:13:21.595] TRAIN: iteration 27498 : loss : 0.244907, loss_ce: 0.003352, loss_dice: 0.486463
[12:13:22.139] TRAIN: iteration 27499 : loss : 0.145086, loss_ce: 0.003411, loss_dice: 0.286762
[12:13:25.109] TRAIN: iteration 27500 : loss : 0.094068, loss_ce: 0.004102, loss_dice: 0.184035
[12:13:25.346] TRAIN: iteration 27501 : loss : 0.140718, loss_ce: 0.003499, loss_dice: 0.277938
[12:13:25.553] TRAIN: iteration 27502 : loss : 0.252037, loss_ce: 0.003801, loss_dice: 0.500273
[12:13:25.768] TRAIN: iteration 27503 : loss : 0.251697, loss_ce: 0.003174, loss_dice: 0.500219
[12:13:25.976] TRAIN: iteration 27504 : loss : 0.251281, loss_ce: 0.002406, loss_dice: 0.500157
[12:13:26.183] TRAIN: iteration 27505 : loss : 0.080009, loss_ce: 0.008661, loss_dice: 0.151357
[12:13:26.389] TRAIN: iteration 27506 : loss : 0.121277, loss_ce: 0.005159, loss_dice: 0.237395
[12:13:27.970] TRAIN: iteration 27507 : loss : 0.103152, loss_ce: 0.004149, loss_dice: 0.202156
[12:13:31.182] TRAIN: iteration 27508 : loss : 0.184320, loss_ce: 0.002303, loss_dice: 0.366336
[12:13:31.390] TRAIN: iteration 27509 : loss : 0.113877, loss_ce: 0.006361, loss_dice: 0.221392
[12:13:31.598] TRAIN: iteration 27510 : loss : 0.053597, loss_ce: 0.001883, loss_dice: 0.105310
[12:13:31.807] TRAIN: iteration 27511 : loss : 0.036689, loss_ce: 0.003044, loss_dice: 0.070335
[12:13:32.016] TRAIN: iteration 27512 : loss : 0.120184, loss_ce: 0.002462, loss_dice: 0.237906
[12:13:32.224] TRAIN: iteration 27513 : loss : 0.074729, loss_ce: 0.002972, loss_dice: 0.146487
[12:13:32.435] TRAIN: iteration 27514 : loss : 0.252543, loss_ce: 0.004774, loss_dice: 0.500312
[12:13:32.735] TRAIN: iteration 27515 : loss : 0.075797, loss_ce: 0.004057, loss_dice: 0.147537
[12:13:39.092] TRAIN: iteration 27516 : loss : 0.247255, loss_ce: 0.002135, loss_dice: 0.492375
[12:13:39.301] TRAIN: iteration 27517 : loss : 0.234657, loss_ce: 0.003114, loss_dice: 0.466200
[12:13:39.509] TRAIN: iteration 27518 : loss : 0.129202, loss_ce: 0.005809, loss_dice: 0.252595
[12:13:39.717] TRAIN: iteration 27519 : loss : 0.112832, loss_ce: 0.001438, loss_dice: 0.224227
[12:13:39.925] TRAIN: iteration 27520 : loss : 0.195757, loss_ce: 0.001620, loss_dice: 0.389894
[12:13:40.163] TRAIN: iteration 27521 : loss : 0.250628, loss_ce: 0.001208, loss_dice: 0.500047
[12:13:40.370] TRAIN: iteration 27522 : loss : 0.189235, loss_ce: 0.002287, loss_dice: 0.376183
[12:13:40.579] TRAIN: iteration 27523 : loss : 0.093111, loss_ce: 0.002171, loss_dice: 0.184051
[12:13:43.195] TRAIN: iteration 27524 : loss : 0.147273, loss_ce: 0.004456, loss_dice: 0.290089
[12:13:43.403] TRAIN: iteration 27525 : loss : 0.158633, loss_ce: 0.002439, loss_dice: 0.314826
[12:13:43.613] TRAIN: iteration 27526 : loss : 0.078599, loss_ce: 0.001481, loss_dice: 0.155717
[12:13:43.821] TRAIN: iteration 27527 : loss : 0.053167, loss_ce: 0.003059, loss_dice: 0.103275
[12:13:44.029] TRAIN: iteration 27528 : loss : 0.250525, loss_ce: 0.001008, loss_dice: 0.500042
[12:13:44.241] TRAIN: iteration 27529 : loss : 0.139041, loss_ce: 0.002903, loss_dice: 0.275179
[12:13:44.449] TRAIN: iteration 27530 : loss : 0.068018, loss_ce: 0.005047, loss_dice: 0.130988
[12:13:44.799] TRAIN: iteration 27531 : loss : 0.133357, loss_ce: 0.002856, loss_dice: 0.263859
[12:13:50.986] TRAIN: iteration 27532 : loss : 0.049328, loss_ce: 0.003525, loss_dice: 0.095130
[12:13:51.195] TRAIN: iteration 27533 : loss : 0.083123, loss_ce: 0.001837, loss_dice: 0.164409
[12:13:51.402] TRAIN: iteration 27534 : loss : 0.101585, loss_ce: 0.004842, loss_dice: 0.198328
[12:13:51.615] TRAIN: iteration 27535 : loss : 0.062260, loss_ce: 0.001960, loss_dice: 0.122561
[12:13:51.825] TRAIN: iteration 27536 : loss : 0.032119, loss_ce: 0.001116, loss_dice: 0.063123
[12:13:52.032] TRAIN: iteration 27537 : loss : 0.155807, loss_ce: 0.003230, loss_dice: 0.308384
[12:13:52.240] TRAIN: iteration 27538 : loss : 0.238569, loss_ce: 0.002296, loss_dice: 0.474842
[12:13:52.448] TRAIN: iteration 27539 : loss : 0.055204, loss_ce: 0.003844, loss_dice: 0.106564
[12:13:56.947] TRAIN: iteration 27540 : loss : 0.105168, loss_ce: 0.003663, loss_dice: 0.206673
[12:13:57.185] TRAIN: iteration 27541 : loss : 0.081844, loss_ce: 0.002224, loss_dice: 0.161463
[12:13:57.392] TRAIN: iteration 27542 : loss : 0.115934, loss_ce: 0.003436, loss_dice: 0.228431
[12:13:57.605] TRAIN: iteration 27543 : loss : 0.041697, loss_ce: 0.002018, loss_dice: 0.081376
[12:13:57.816] TRAIN: iteration 27544 : loss : 0.251355, loss_ce: 0.002545, loss_dice: 0.500164
[12:13:58.024] TRAIN: iteration 27545 : loss : 0.250898, loss_ce: 0.002142, loss_dice: 0.499653
[12:13:58.231] TRAIN: iteration 27546 : loss : 0.251301, loss_ce: 0.002448, loss_dice: 0.500154
[12:13:58.439] TRAIN: iteration 27547 : loss : 0.134922, loss_ce: 0.002822, loss_dice: 0.267022
[12:14:02.870] TRAIN: iteration 27548 : loss : 0.071800, loss_ce: 0.004940, loss_dice: 0.138660
[12:14:03.078] TRAIN: iteration 27549 : loss : 0.031423, loss_ce: 0.001500, loss_dice: 0.061347
[12:14:03.286] TRAIN: iteration 27550 : loss : 0.082146, loss_ce: 0.006474, loss_dice: 0.157819
[12:14:03.494] TRAIN: iteration 27551 : loss : 0.023831, loss_ce: 0.000742, loss_dice: 0.046920
[12:14:03.702] TRAIN: iteration 27552 : loss : 0.041740, loss_ce: 0.005916, loss_dice: 0.077564
[12:14:03.910] TRAIN: iteration 27553 : loss : 0.177120, loss_ce: 0.001355, loss_dice: 0.352886
[12:14:04.118] TRAIN: iteration 27554 : loss : 0.150675, loss_ce: 0.001661, loss_dice: 0.299688
[12:14:04.325] TRAIN: iteration 27555 : loss : 0.080973, loss_ce: 0.001602, loss_dice: 0.160344
[12:14:08.485] TRAIN: iteration 27556 : loss : 0.022331, loss_ce: 0.001790, loss_dice: 0.042871
[12:14:08.697] TRAIN: iteration 27557 : loss : 0.031875, loss_ce: 0.000940, loss_dice: 0.062810
[12:14:08.905] TRAIN: iteration 27558 : loss : 0.014827, loss_ce: 0.000416, loss_dice: 0.029238
[12:14:09.115] TRAIN: iteration 27559 : loss : 0.057103, loss_ce: 0.002007, loss_dice: 0.112200
[12:14:09.324] TRAIN: iteration 27560 : loss : 0.071425, loss_ce: 0.000615, loss_dice: 0.142234
[12:14:09.568] TRAIN: iteration 27561 : loss : 0.174630, loss_ce: 0.001076, loss_dice: 0.348184
[12:14:09.775] TRAIN: iteration 27562 : loss : 0.108989, loss_ce: 0.001003, loss_dice: 0.216975
[12:14:09.983] TRAIN: iteration 27563 : loss : 0.104818, loss_ce: 0.002595, loss_dice: 0.207041
[12:14:14.884] TRAIN: iteration 27564 : loss : 0.118405, loss_ce: 0.012344, loss_dice: 0.224466
[12:14:15.092] TRAIN: iteration 27565 : loss : 0.033186, loss_ce: 0.001250, loss_dice: 0.065122
[12:14:15.306] TRAIN: iteration 27566 : loss : 0.131600, loss_ce: 0.006317, loss_dice: 0.256883
[12:14:15.513] TRAIN: iteration 27567 : loss : 0.121390, loss_ce: 0.003921, loss_dice: 0.238859
[12:14:15.722] TRAIN: iteration 27568 : loss : 0.073316, loss_ce: 0.003732, loss_dice: 0.142901
[12:14:15.930] TRAIN: iteration 27569 : loss : 0.243878, loss_ce: 0.004175, loss_dice: 0.483580
[12:14:16.139] TRAIN: iteration 27570 : loss : 0.039218, loss_ce: 0.006612, loss_dice: 0.071823
[12:14:16.346] TRAIN: iteration 27571 : loss : 0.128571, loss_ce: 0.006871, loss_dice: 0.250271
[12:14:22.095] TRAIN: iteration 27572 : loss : 0.043294, loss_ce: 0.001526, loss_dice: 0.085063
[12:14:22.311] TRAIN: iteration 27573 : loss : 0.233073, loss_ce: 0.003196, loss_dice: 0.462949
[12:14:22.519] TRAIN: iteration 27574 : loss : 0.044582, loss_ce: 0.002219, loss_dice: 0.086945
[12:14:22.795] TRAIN: iteration 27575 : loss : 0.111171, loss_ce: 0.002391, loss_dice: 0.219951
[12:14:23.004] TRAIN: iteration 27576 : loss : 0.041381, loss_ce: 0.002833, loss_dice: 0.079929
[12:14:23.212] TRAIN: iteration 27577 : loss : 0.048611, loss_ce: 0.002804, loss_dice: 0.094417
[12:14:23.420] TRAIN: iteration 27578 : loss : 0.249576, loss_ce: 0.001824, loss_dice: 0.497327
[12:14:23.627] TRAIN: iteration 27579 : loss : 0.250898, loss_ce: 0.002622, loss_dice: 0.499173
[12:14:28.547] TRAIN: iteration 27580 : loss : 0.029807, loss_ce: 0.003223, loss_dice: 0.056392
[12:14:28.788] TRAIN: iteration 27581 : loss : 0.143508, loss_ce: 0.001588, loss_dice: 0.285429
[12:14:28.995] TRAIN: iteration 27582 : loss : 0.039352, loss_ce: 0.005440, loss_dice: 0.073263
[12:14:29.203] TRAIN: iteration 27583 : loss : 0.065542, loss_ce: 0.002143, loss_dice: 0.128941
[12:14:29.422] TRAIN: iteration 27584 : loss : 0.199267, loss_ce: 0.003571, loss_dice: 0.394963
[12:14:29.632] TRAIN: iteration 27585 : loss : 0.216160, loss_ce: 0.002398, loss_dice: 0.429922
[12:14:29.839] TRAIN: iteration 27586 : loss : 0.099416, loss_ce: 0.008794, loss_dice: 0.190039
[12:14:30.046] TRAIN: iteration 27587 : loss : 0.250960, loss_ce: 0.001815, loss_dice: 0.500105
[12:14:37.310] TRAIN: iteration 27588 : loss : 0.104584, loss_ce: 0.003096, loss_dice: 0.206072
[12:14:37.519] TRAIN: iteration 27589 : loss : 0.044368, loss_ce: 0.003623, loss_dice: 0.085114
[12:14:37.729] TRAIN: iteration 27590 : loss : 0.123628, loss_ce: 0.005876, loss_dice: 0.241380
[12:14:37.938] TRAIN: iteration 27591 : loss : 0.057607, loss_ce: 0.003597, loss_dice: 0.111616
[12:14:38.148] TRAIN: iteration 27592 : loss : 0.057518, loss_ce: 0.003592, loss_dice: 0.111445
[12:14:38.358] TRAIN: iteration 27593 : loss : 0.228395, loss_ce: 0.001631, loss_dice: 0.455158
[12:14:38.566] TRAIN: iteration 27594 : loss : 0.075393, loss_ce: 0.004213, loss_dice: 0.146572
[12:14:38.774] TRAIN: iteration 27595 : loss : 0.058650, loss_ce: 0.007335, loss_dice: 0.109965
[12:14:44.500] TRAIN: iteration 27596 : loss : 0.039422, loss_ce: 0.007468, loss_dice: 0.071377
[12:14:44.710] TRAIN: iteration 27597 : loss : 0.205724, loss_ce: 0.003770, loss_dice: 0.407679
[12:14:44.925] TRAIN: iteration 27598 : loss : 0.252332, loss_ce: 0.004360, loss_dice: 0.500304
[12:14:45.133] TRAIN: iteration 27599 : loss : 0.100949, loss_ce: 0.002423, loss_dice: 0.199475
[12:14:45.341] TRAIN: iteration 27600 : loss : 0.088242, loss_ce: 0.004439, loss_dice: 0.172045
[12:14:45.583] TRAIN: iteration 27601 : loss : 0.166999, loss_ce: 0.009531, loss_dice: 0.324468
[12:14:45.882] TRAIN: iteration 27602 : loss : 0.250844, loss_ce: 0.001605, loss_dice: 0.500084
[12:14:46.095] TRAIN: iteration 27603 : loss : 0.251746, loss_ce: 0.003272, loss_dice: 0.500220
[12:14:55.250] TRAIN: iteration 27604 : loss : 0.091351, loss_ce: 0.009722, loss_dice: 0.172980
[12:14:55.463] TRAIN: iteration 27605 : loss : 0.241558, loss_ce: 0.004750, loss_dice: 0.478366
[12:14:55.671] TRAIN: iteration 27606 : loss : 0.036112, loss_ce: 0.005797, loss_dice: 0.066428
[12:14:55.885] TRAIN: iteration 27607 : loss : 0.071685, loss_ce: 0.001810, loss_dice: 0.141560
[12:14:56.095] TRAIN: iteration 27608 : loss : 0.031442, loss_ce: 0.002406, loss_dice: 0.060478
[12:14:56.302] TRAIN: iteration 27609 : loss : 0.025213, loss_ce: 0.003354, loss_dice: 0.047072
[12:14:56.512] TRAIN: iteration 27610 : loss : 0.196073, loss_ce: 0.006291, loss_dice: 0.385855
[12:14:56.721] TRAIN: iteration 27611 : loss : 0.106229, loss_ce: 0.011320, loss_dice: 0.201137
[12:14:59.073] TRAIN: iteration 27612 : loss : 0.250962, loss_ce: 0.001822, loss_dice: 0.500101
[12:14:59.286] TRAIN: iteration 27613 : loss : 0.194715, loss_ce: 0.002129, loss_dice: 0.387301
[12:14:59.494] TRAIN: iteration 27614 : loss : 0.116916, loss_ce: 0.006278, loss_dice: 0.227554
[12:14:59.701] TRAIN: iteration 27615 : loss : 0.251155, loss_ce: 0.002174, loss_dice: 0.500136
[12:14:59.910] TRAIN: iteration 27616 : loss : 0.062340, loss_ce: 0.001678, loss_dice: 0.123001
[12:15:00.194] TRAIN: iteration 27617 : loss : 0.077087, loss_ce: 0.005164, loss_dice: 0.149010
[12:15:00.402] TRAIN: iteration 27618 : loss : 0.053894, loss_ce: 0.001447, loss_dice: 0.106340
[12:15:00.610] TRAIN: iteration 27619 : loss : 0.251214, loss_ce: 0.002284, loss_dice: 0.500144
[12:15:05.061] TRAIN: iteration 27620 : loss : 0.237799, loss_ce: 0.002365, loss_dice: 0.473232
[12:15:05.301] TRAIN: iteration 27621 : loss : 0.248086, loss_ce: 0.003201, loss_dice: 0.492971
[12:15:05.508] TRAIN: iteration 27622 : loss : 0.100117, loss_ce: 0.001926, loss_dice: 0.198309
[12:15:05.716] TRAIN: iteration 27623 : loss : 0.058267, loss_ce: 0.004580, loss_dice: 0.111955
[12:15:05.925] TRAIN: iteration 27624 : loss : 0.146360, loss_ce: 0.002443, loss_dice: 0.290276
[12:15:06.133] TRAIN: iteration 27625 : loss : 0.082316, loss_ce: 0.002154, loss_dice: 0.162478
[12:15:06.342] TRAIN: iteration 27626 : loss : 0.064891, loss_ce: 0.001661, loss_dice: 0.128121
[12:15:06.550] TRAIN: iteration 27627 : loss : 0.198703, loss_ce: 0.007020, loss_dice: 0.390386
[12:15:15.153] TRAIN: iteration 27628 : loss : 0.094549, loss_ce: 0.002684, loss_dice: 0.186413
[12:15:15.362] TRAIN: iteration 27629 : loss : 0.084371, loss_ce: 0.002939, loss_dice: 0.165802
[12:15:15.577] TRAIN: iteration 27630 : loss : 0.057520, loss_ce: 0.001766, loss_dice: 0.113274
[12:15:15.784] TRAIN: iteration 27631 : loss : 0.054946, loss_ce: 0.001863, loss_dice: 0.108028
[12:15:15.992] TRAIN: iteration 27632 : loss : 0.145802, loss_ce: 0.002078, loss_dice: 0.289527
[12:15:16.201] TRAIN: iteration 27633 : loss : 0.076540, loss_ce: 0.002796, loss_dice: 0.150283
[12:15:16.408] TRAIN: iteration 27634 : loss : 0.055781, loss_ce: 0.010375, loss_dice: 0.101187
[12:15:16.615] TRAIN: iteration 27635 : loss : 0.098130, loss_ce: 0.007826, loss_dice: 0.188433
[12:15:22.356] TRAIN: iteration 27636 : loss : 0.044582, loss_ce: 0.001119, loss_dice: 0.088045
[12:15:22.566] TRAIN: iteration 27637 : loss : 0.114401, loss_ce: 0.003159, loss_dice: 0.225644
[12:15:22.776] TRAIN: iteration 27638 : loss : 0.081158, loss_ce: 0.006572, loss_dice: 0.155743
[12:15:22.984] TRAIN: iteration 27639 : loss : 0.071387, loss_ce: 0.001545, loss_dice: 0.141228
[12:15:23.191] TRAIN: iteration 27640 : loss : 0.146390, loss_ce: 0.002241, loss_dice: 0.290539
[12:15:23.434] TRAIN: iteration 27641 : loss : 0.015209, loss_ce: 0.000830, loss_dice: 0.029588
[12:15:23.641] TRAIN: iteration 27642 : loss : 0.109140, loss_ce: 0.007717, loss_dice: 0.210564
[12:15:23.848] TRAIN: iteration 27643 : loss : 0.045297, loss_ce: 0.002921, loss_dice: 0.087673
[12:15:30.964] TRAIN: iteration 27644 : loss : 0.240604, loss_ce: 0.008439, loss_dice: 0.472769
[12:15:31.171] TRAIN: iteration 27645 : loss : 0.077231, loss_ce: 0.003804, loss_dice: 0.150657
[12:15:31.380] TRAIN: iteration 27646 : loss : 0.178086, loss_ce: 0.001606, loss_dice: 0.354565
[12:15:31.670] TRAIN: iteration 27647 : loss : 0.024877, loss_ce: 0.001370, loss_dice: 0.048384
[12:15:31.877] TRAIN: iteration 27648 : loss : 0.050969, loss_ce: 0.002020, loss_dice: 0.099918
[12:15:32.084] TRAIN: iteration 27649 : loss : 0.119425, loss_ce: 0.014744, loss_dice: 0.224105
[12:15:32.292] TRAIN: iteration 27650 : loss : 0.055734, loss_ce: 0.005154, loss_dice: 0.106315
[12:15:32.499] TRAIN: iteration 27651 : loss : 0.228713, loss_ce: 0.001991, loss_dice: 0.455435
[12:15:39.532] TRAIN: iteration 27652 : loss : 0.145895, loss_ce: 0.003631, loss_dice: 0.288158
[12:15:39.744] TRAIN: iteration 27653 : loss : 0.041735, loss_ce: 0.002982, loss_dice: 0.080489
[12:15:39.951] TRAIN: iteration 27654 : loss : 0.096303, loss_ce: 0.003056, loss_dice: 0.189550
[12:15:40.160] TRAIN: iteration 27655 : loss : 0.079360, loss_ce: 0.001787, loss_dice: 0.156933
[12:15:40.368] TRAIN: iteration 27656 : loss : 0.045909, loss_ce: 0.001568, loss_dice: 0.090250
[12:15:40.576] TRAIN: iteration 27657 : loss : 0.250702, loss_ce: 0.001343, loss_dice: 0.500062
[12:15:40.786] TRAIN: iteration 27658 : loss : 0.023704, loss_ce: 0.002185, loss_dice: 0.045223
[12:15:40.994] TRAIN: iteration 27659 : loss : 0.127205, loss_ce: 0.002809, loss_dice: 0.251601
[12:15:46.999] TRAIN: iteration 27660 : loss : 0.054814, loss_ce: 0.002897, loss_dice: 0.106731
[12:15:47.240] TRAIN: iteration 27661 : loss : 0.007125, loss_ce: 0.000676, loss_dice: 0.013573
[12:15:47.446] TRAIN: iteration 27662 : loss : 0.051177, loss_ce: 0.001505, loss_dice: 0.100848
[12:15:47.653] TRAIN: iteration 27663 : loss : 0.235908, loss_ce: 0.005109, loss_dice: 0.466708
[12:15:47.860] TRAIN: iteration 27664 : loss : 0.029930, loss_ce: 0.003379, loss_dice: 0.056482
[12:15:48.067] TRAIN: iteration 27665 : loss : 0.051569, loss_ce: 0.002536, loss_dice: 0.100601
[12:15:48.278] TRAIN: iteration 27666 : loss : 0.059967, loss_ce: 0.002342, loss_dice: 0.117592
[12:15:48.486] TRAIN: iteration 27667 : loss : 0.022631, loss_ce: 0.002469, loss_dice: 0.042794
[12:15:54.729] TRAIN: iteration 27668 : loss : 0.108659, loss_ce: 0.002651, loss_dice: 0.214666
[12:15:54.937] TRAIN: iteration 27669 : loss : 0.161159, loss_ce: 0.004660, loss_dice: 0.317659
[12:15:55.292] TRAIN: iteration 27670 : loss : 0.091283, loss_ce: 0.002822, loss_dice: 0.179743
[12:15:55.507] TRAIN: iteration 27671 : loss : 0.120050, loss_ce: 0.002674, loss_dice: 0.237427
[12:15:55.714] TRAIN: iteration 27672 : loss : 0.065476, loss_ce: 0.001392, loss_dice: 0.129559
[12:15:55.921] TRAIN: iteration 27673 : loss : 0.036020, loss_ce: 0.002963, loss_dice: 0.069077
[12:15:56.131] TRAIN: iteration 27674 : loss : 0.096854, loss_ce: 0.004110, loss_dice: 0.189598
[12:15:56.338] TRAIN: iteration 27675 : loss : 0.241853, loss_ce: 0.001524, loss_dice: 0.482182
[12:16:02.363] TRAIN: iteration 27676 : loss : 0.042638, loss_ce: 0.004757, loss_dice: 0.080518
[12:16:02.570] TRAIN: iteration 27677 : loss : 0.116625, loss_ce: 0.006076, loss_dice: 0.227173
[12:16:02.779] TRAIN: iteration 27678 : loss : 0.025927, loss_ce: 0.002188, loss_dice: 0.049665
[12:16:02.989] TRAIN: iteration 27679 : loss : 0.250374, loss_ce: 0.002522, loss_dice: 0.498225
[12:16:03.196] TRAIN: iteration 27680 : loss : 0.029749, loss_ce: 0.001521, loss_dice: 0.057977
[12:16:03.431] TRAIN: iteration 27681 : loss : 0.058642, loss_ce: 0.004139, loss_dice: 0.113146
[12:16:03.638] TRAIN: iteration 27682 : loss : 0.063809, loss_ce: 0.004143, loss_dice: 0.123474
[12:16:04.740] TRAIN: iteration 27683 : loss : 0.028385, loss_ce: 0.003051, loss_dice: 0.053718
[12:16:12.124] TRAIN: iteration 27684 : loss : 0.067736, loss_ce: 0.002178, loss_dice: 0.133295
[12:16:12.333] TRAIN: iteration 27685 : loss : 0.151055, loss_ce: 0.009217, loss_dice: 0.292894
[12:16:12.541] TRAIN: iteration 27686 : loss : 0.109002, loss_ce: 0.005639, loss_dice: 0.212365
[12:16:12.748] TRAIN: iteration 27687 : loss : 0.178139, loss_ce: 0.004732, loss_dice: 0.351546
[12:16:12.955] TRAIN: iteration 27688 : loss : 0.046333, loss_ce: 0.005851, loss_dice: 0.086815
[12:16:13.163] TRAIN: iteration 27689 : loss : 0.044478, loss_ce: 0.003031, loss_dice: 0.085926
[12:16:13.370] TRAIN: iteration 27690 : loss : 0.026831, loss_ce: 0.003270, loss_dice: 0.050393
[12:16:13.577] TRAIN: iteration 27691 : loss : 0.113316, loss_ce: 0.003276, loss_dice: 0.223356
[12:16:21.280] TRAIN: iteration 27692 : loss : 0.035714, loss_ce: 0.001873, loss_dice: 0.069554
[12:16:21.487] TRAIN: iteration 27693 : loss : 0.113464, loss_ce: 0.003859, loss_dice: 0.223069
[12:16:21.700] TRAIN: iteration 27694 : loss : 0.247886, loss_ce: 0.001752, loss_dice: 0.494020
[12:16:21.911] TRAIN: iteration 27695 : loss : 0.097157, loss_ce: 0.003006, loss_dice: 0.191308
[12:16:22.124] TRAIN: iteration 27696 : loss : 0.068778, loss_ce: 0.004913, loss_dice: 0.132643
[12:16:22.335] TRAIN: iteration 27697 : loss : 0.132423, loss_ce: 0.002583, loss_dice: 0.262263
[12:16:22.543] TRAIN: iteration 27698 : loss : 0.250609, loss_ce: 0.001168, loss_dice: 0.500049
[12:16:22.753] TRAIN: iteration 27699 : loss : 0.156068, loss_ce: 0.002129, loss_dice: 0.310007
[12:16:28.833] TRAIN: iteration 27700 : loss : 0.029412, loss_ce: 0.001929, loss_dice: 0.056894
[12:16:29.068] TRAIN: iteration 27701 : loss : 0.039241, loss_ce: 0.002365, loss_dice: 0.076116
[12:16:29.277] TRAIN: iteration 27702 : loss : 0.110141, loss_ce: 0.001848, loss_dice: 0.218435
[12:16:29.484] TRAIN: iteration 27703 : loss : 0.054688, loss_ce: 0.005518, loss_dice: 0.103858
[12:16:29.691] TRAIN: iteration 27704 : loss : 0.058487, loss_ce: 0.005180, loss_dice: 0.111794
[12:16:29.898] TRAIN: iteration 27705 : loss : 0.138086, loss_ce: 0.002106, loss_dice: 0.274067
[12:16:30.108] TRAIN: iteration 27706 : loss : 0.035422, loss_ce: 0.001387, loss_dice: 0.069457
[12:16:32.463] TRAIN: iteration 27707 : loss : 0.250531, loss_ce: 0.001029, loss_dice: 0.500033
[12:16:37.611] TRAIN: iteration 27708 : loss : 0.256133, loss_ce: 0.014810, loss_dice: 0.497456
[12:16:37.824] TRAIN: iteration 27709 : loss : 0.055000, loss_ce: 0.001177, loss_dice: 0.108822
[12:16:38.032] TRAIN: iteration 27710 : loss : 0.089470, loss_ce: 0.003242, loss_dice: 0.175699
[12:16:38.239] TRAIN: iteration 27711 : loss : 0.076497, loss_ce: 0.004258, loss_dice: 0.148736
[12:16:38.447] TRAIN: iteration 27712 : loss : 0.250449, loss_ce: 0.000873, loss_dice: 0.500025
[12:16:38.657] TRAIN: iteration 27713 : loss : 0.017269, loss_ce: 0.002028, loss_dice: 0.032509
[12:16:38.864] TRAIN: iteration 27714 : loss : 0.053099, loss_ce: 0.003516, loss_dice: 0.102682
[12:16:41.185] TRAIN: iteration 27715 : loss : 0.075417, loss_ce: 0.001884, loss_dice: 0.148950
[12:16:46.925] TRAIN: iteration 27716 : loss : 0.149184, loss_ce: 0.001468, loss_dice: 0.296899
[12:16:47.134] TRAIN: iteration 27717 : loss : 0.059469, loss_ce: 0.003046, loss_dice: 0.115893
[12:16:47.343] TRAIN: iteration 27718 : loss : 0.079213, loss_ce: 0.003377, loss_dice: 0.155049
[12:16:47.551] TRAIN: iteration 27719 : loss : 0.049310, loss_ce: 0.000823, loss_dice: 0.097797
[12:16:47.759] TRAIN: iteration 27720 : loss : 0.039882, loss_ce: 0.000598, loss_dice: 0.079166
[12:16:48.002] TRAIN: iteration 27721 : loss : 0.080486, loss_ce: 0.002653, loss_dice: 0.158318
[12:16:48.209] TRAIN: iteration 27722 : loss : 0.209012, loss_ce: 0.002027, loss_dice: 0.415997
[12:16:48.416] TRAIN: iteration 27723 : loss : 0.050082, loss_ce: 0.002053, loss_dice: 0.098112
[12:16:55.075] TRAIN: iteration 27724 : loss : 0.250348, loss_ce: 0.000680, loss_dice: 0.500016
[12:16:55.282] TRAIN: iteration 27725 : loss : 0.040851, loss_ce: 0.002284, loss_dice: 0.079418
[12:16:55.489] TRAIN: iteration 27726 : loss : 0.035562, loss_ce: 0.001758, loss_dice: 0.069366
[12:16:55.696] TRAIN: iteration 27727 : loss : 0.023557, loss_ce: 0.001017, loss_dice: 0.046097
[12:16:55.904] TRAIN: iteration 27728 : loss : 0.038217, loss_ce: 0.000709, loss_dice: 0.075726
[12:16:56.112] TRAIN: iteration 27729 : loss : 0.237688, loss_ce: 0.003430, loss_dice: 0.471946
[12:16:56.319] TRAIN: iteration 27730 : loss : 0.250342, loss_ce: 0.000665, loss_dice: 0.500018
[12:16:58.815] TRAIN: iteration 27731 : loss : 0.074685, loss_ce: 0.002314, loss_dice: 0.147056
[12:17:03.646] TRAIN: iteration 27732 : loss : 0.069158, loss_ce: 0.001246, loss_dice: 0.137070
[12:17:03.853] TRAIN: iteration 27733 : loss : 0.081654, loss_ce: 0.001560, loss_dice: 0.161749
[12:17:04.063] TRAIN: iteration 27734 : loss : 0.130127, loss_ce: 0.006029, loss_dice: 0.254226
[12:17:04.272] TRAIN: iteration 27735 : loss : 0.250431, loss_ce: 0.000825, loss_dice: 0.500037
[12:17:04.480] TRAIN: iteration 27736 : loss : 0.167398, loss_ce: 0.001564, loss_dice: 0.333231
[12:17:04.687] TRAIN: iteration 27737 : loss : 0.216549, loss_ce: 0.000603, loss_dice: 0.432495
[12:17:04.893] TRAIN: iteration 27738 : loss : 0.095584, loss_ce: 0.002341, loss_dice: 0.188827
[12:17:07.477] TRAIN: iteration 27739 : loss : 0.080775, loss_ce: 0.003387, loss_dice: 0.158164
[12:17:10.406] TRAIN: iteration 27740 : loss : 0.077072, loss_ce: 0.004484, loss_dice: 0.149660
[12:17:10.644] TRAIN: iteration 27741 : loss : 0.078112, loss_ce: 0.005752, loss_dice: 0.150471
[12:17:10.853] TRAIN: iteration 27742 : loss : 0.048435, loss_ce: 0.001297, loss_dice: 0.095574
[12:17:12.065] TRAIN: iteration 27743 : loss : 0.250793, loss_ce: 0.001493, loss_dice: 0.500093
[12:17:12.275] TRAIN: iteration 27744 : loss : 0.057114, loss_ce: 0.008469, loss_dice: 0.105759
[12:17:12.483] TRAIN: iteration 27745 : loss : 0.040859, loss_ce: 0.001792, loss_dice: 0.079926
[12:17:12.689] TRAIN: iteration 27746 : loss : 0.055016, loss_ce: 0.005422, loss_dice: 0.104611
[12:17:16.357] TRAIN: iteration 27747 : loss : 0.078554, loss_ce: 0.003539, loss_dice: 0.153569
[12:17:22.172] TRAIN: iteration 27748 : loss : 0.250792, loss_ce: 0.001505, loss_dice: 0.500079
[12:17:22.381] TRAIN: iteration 27749 : loss : 0.035409, loss_ce: 0.004349, loss_dice: 0.066470
[12:17:22.589] TRAIN: iteration 27750 : loss : 0.035384, loss_ce: 0.000680, loss_dice: 0.070088
[12:17:22.797] TRAIN: iteration 27751 : loss : 0.030512, loss_ce: 0.001253, loss_dice: 0.059771
[12:17:23.319] TRAIN: iteration 27752 : loss : 0.210046, loss_ce: 0.001453, loss_dice: 0.418639
[12:17:23.529] TRAIN: iteration 27753 : loss : 0.028775, loss_ce: 0.006380, loss_dice: 0.051170
[12:17:23.737] TRAIN: iteration 27754 : loss : 0.250539, loss_ce: 0.001040, loss_dice: 0.500039
[12:17:23.946] TRAIN: iteration 27755 : loss : 0.043743, loss_ce: 0.004511, loss_dice: 0.082975
[12:17:31.619] TRAIN: iteration 27756 : loss : 0.219502, loss_ce: 0.001819, loss_dice: 0.437185
[12:17:31.827] TRAIN: iteration 27757 : loss : 0.218402, loss_ce: 0.001658, loss_dice: 0.435146
[12:17:32.034] TRAIN: iteration 27758 : loss : 0.068989, loss_ce: 0.006325, loss_dice: 0.131652
[12:17:32.245] TRAIN: iteration 27759 : loss : 0.213012, loss_ce: 0.001129, loss_dice: 0.424896
[12:17:32.341] TRAIN: iteration 27760 : loss : 0.250491, loss_ce: 0.000965, loss_dice: 0.500017
[12:22:54.911] VALIDATION: iteration 15 : loss : 0.115705, loss_ce: 0.003835, loss_dice: 0.227576
[12:22:56.640] TRAIN: iteration 27761 : loss : 0.251036, loss_ce: 0.001942, loss_dice: 0.500130
[12:22:56.869] TRAIN: iteration 27762 : loss : 0.199796, loss_ce: 0.002962, loss_dice: 0.396631
[12:22:57.079] TRAIN: iteration 27763 : loss : 0.106800, loss_ce: 0.005890, loss_dice: 0.207710
[12:22:57.288] TRAIN: iteration 27764 : loss : 0.039210, loss_ce: 0.001801, loss_dice: 0.076619
[12:22:57.497] TRAIN: iteration 27765 : loss : 0.092649, loss_ce: 0.003794, loss_dice: 0.181504
[12:22:57.712] TRAIN: iteration 27766 : loss : 0.150234, loss_ce: 0.001450, loss_dice: 0.299017
[12:22:57.920] TRAIN: iteration 27767 : loss : 0.062652, loss_ce: 0.003782, loss_dice: 0.121522
[12:22:58.131] TRAIN: iteration 27768 : loss : 0.040780, loss_ce: 0.010517, loss_dice: 0.071043
[12:22:58.339] TRAIN: iteration 27769 : loss : 0.023820, loss_ce: 0.000856, loss_dice: 0.046784
[12:22:58.563] TRAIN: iteration 27770 : loss : 0.080376, loss_ce: 0.004497, loss_dice: 0.156255
[12:22:58.779] TRAIN: iteration 27771 : loss : 0.062789, loss_ce: 0.001838, loss_dice: 0.123741
[12:22:58.990] TRAIN: iteration 27772 : loss : 0.080834, loss_ce: 0.002843, loss_dice: 0.158824
[12:22:59.200] TRAIN: iteration 27773 : loss : 0.120833, loss_ce: 0.004399, loss_dice: 0.237268
[12:22:59.409] TRAIN: iteration 27774 : loss : 0.204732, loss_ce: 0.006570, loss_dice: 0.402893
[12:22:59.619] TRAIN: iteration 27775 : loss : 0.018017, loss_ce: 0.001295, loss_dice: 0.034738
[12:22:59.827] TRAIN: iteration 27776 : loss : 0.048197, loss_ce: 0.000958, loss_dice: 0.095436
[12:23:00.038] TRAIN: iteration 27777 : loss : 0.027547, loss_ce: 0.005341, loss_dice: 0.049753
[12:23:00.246] TRAIN: iteration 27778 : loss : 0.164939, loss_ce: 0.001028, loss_dice: 0.328850
[12:23:00.517] TRAIN: iteration 27779 : loss : 0.250807, loss_ce: 0.001518, loss_dice: 0.500096
[12:23:00.725] TRAIN: iteration 27780 : loss : 0.205916, loss_ce: 0.001262, loss_dice: 0.410570
[12:23:00.726] NaN or Inf found in input tensor.
[12:23:00.944] TRAIN: iteration 27781 : loss : 0.058248, loss_ce: 0.001039, loss_dice: 0.115456
[12:23:01.154] TRAIN: iteration 27782 : loss : 0.218549, loss_ce: 0.002587, loss_dice: 0.434510
[12:23:01.362] TRAIN: iteration 27783 : loss : 0.250554, loss_ce: 0.001064, loss_dice: 0.500044
[12:23:01.573] TRAIN: iteration 27784 : loss : 0.028463, loss_ce: 0.004851, loss_dice: 0.052075
[12:23:01.787] TRAIN: iteration 27785 : loss : 0.027062, loss_ce: 0.000540, loss_dice: 0.053584
[12:23:01.997] TRAIN: iteration 27786 : loss : 0.033197, loss_ce: 0.002185, loss_dice: 0.064208
[12:23:02.204] TRAIN: iteration 27787 : loss : 0.042416, loss_ce: 0.002487, loss_dice: 0.082345
[12:23:02.412] TRAIN: iteration 27788 : loss : 0.047601, loss_ce: 0.000777, loss_dice: 0.094425
[12:23:02.621] TRAIN: iteration 27789 : loss : 0.143651, loss_ce: 0.002454, loss_dice: 0.284848
[12:23:02.831] TRAIN: iteration 27790 : loss : 0.030964, loss_ce: 0.001888, loss_dice: 0.060040
[12:23:03.038] TRAIN: iteration 27791 : loss : 0.028802, loss_ce: 0.001072, loss_dice: 0.056533
[12:23:03.247] TRAIN: iteration 27792 : loss : 0.248334, loss_ce: 0.002608, loss_dice: 0.494060
[12:23:03.456] TRAIN: iteration 27793 : loss : 0.045974, loss_ce: 0.003278, loss_dice: 0.088670
[12:23:03.664] TRAIN: iteration 27794 : loss : 0.118418, loss_ce: 0.024971, loss_dice: 0.211864
[12:23:03.873] TRAIN: iteration 27795 : loss : 0.035119, loss_ce: 0.001108, loss_dice: 0.069129
[12:23:04.083] TRAIN: iteration 27796 : loss : 0.093572, loss_ce: 0.003254, loss_dice: 0.183889
[12:23:04.300] TRAIN: iteration 27797 : loss : 0.087905, loss_ce: 0.003693, loss_dice: 0.172117
[12:23:04.513] TRAIN: iteration 27798 : loss : 0.076850, loss_ce: 0.004301, loss_dice: 0.149398
[12:23:04.722] TRAIN: iteration 27799 : loss : 0.065405, loss_ce: 0.001391, loss_dice: 0.129419
[12:23:04.931] TRAIN: iteration 27800 : loss : 0.132440, loss_ce: 0.001497, loss_dice: 0.263383
[12:23:05.172] TRAIN: iteration 27801 : loss : 0.037233, loss_ce: 0.002857, loss_dice: 0.071609
[12:23:05.544] TRAIN: iteration 27802 : loss : 0.074343, loss_ce: 0.004597, loss_dice: 0.144088
[12:23:05.752] TRAIN: iteration 27803 : loss : 0.058890, loss_ce: 0.002920, loss_dice: 0.114859
[12:23:05.959] TRAIN: iteration 27804 : loss : 0.025460, loss_ce: 0.001549, loss_dice: 0.049370
[12:23:06.169] TRAIN: iteration 27805 : loss : 0.238955, loss_ce: 0.003104, loss_dice: 0.474806
[12:23:06.388] TRAIN: iteration 27806 : loss : 0.149158, loss_ce: 0.017251, loss_dice: 0.281065
[12:23:06.604] TRAIN: iteration 27807 : loss : 0.149390, loss_ce: 0.001432, loss_dice: 0.297348
[12:23:06.842] TRAIN: iteration 27808 : loss : 0.065212, loss_ce: 0.002019, loss_dice: 0.128405
[12:23:07.050] TRAIN: iteration 27809 : loss : 0.038690, loss_ce: 0.008034, loss_dice: 0.069347
[12:23:07.256] TRAIN: iteration 27810 : loss : 0.065969, loss_ce: 0.003376, loss_dice: 0.128563
[12:23:07.464] TRAIN: iteration 27811 : loss : 0.049799, loss_ce: 0.003554, loss_dice: 0.096044
[12:23:07.676] TRAIN: iteration 27812 : loss : 0.033661, loss_ce: 0.006713, loss_dice: 0.060609
[12:23:07.885] TRAIN: iteration 27813 : loss : 0.036941, loss_ce: 0.002102, loss_dice: 0.071779
[12:23:08.104] TRAIN: iteration 27814 : loss : 0.034040, loss_ce: 0.006704, loss_dice: 0.061376
[12:23:08.311] TRAIN: iteration 27815 : loss : 0.086035, loss_ce: 0.001127, loss_dice: 0.170942
[12:23:08.523] TRAIN: iteration 27816 : loss : 0.071705, loss_ce: 0.003988, loss_dice: 0.139423
[12:23:08.732] TRAIN: iteration 27817 : loss : 0.208321, loss_ce: 0.006556, loss_dice: 0.410087
[12:23:08.940] TRAIN: iteration 27818 : loss : 0.250580, loss_ce: 0.001102, loss_dice: 0.500058
[12:23:09.153] TRAIN: iteration 27819 : loss : 0.091646, loss_ce: 0.003207, loss_dice: 0.180085
[12:23:09.364] TRAIN: iteration 27820 : loss : 0.048436, loss_ce: 0.001222, loss_dice: 0.095651
[12:23:09.602] TRAIN: iteration 27821 : loss : 0.120004, loss_ce: 0.003472, loss_dice: 0.236537
[12:23:09.810] TRAIN: iteration 27822 : loss : 0.250441, loss_ce: 0.000842, loss_dice: 0.500041
[12:23:10.019] TRAIN: iteration 27823 : loss : 0.129545, loss_ce: 0.000482, loss_dice: 0.258608
[12:23:10.227] TRAIN: iteration 27824 : loss : 0.206914, loss_ce: 0.002288, loss_dice: 0.411539
[12:23:10.435] TRAIN: iteration 27825 : loss : 0.154515, loss_ce: 0.005853, loss_dice: 0.303176
[12:23:10.643] TRAIN: iteration 27826 : loss : 0.058502, loss_ce: 0.000881, loss_dice: 0.116123
[12:23:10.856] TRAIN: iteration 27827 : loss : 0.039581, loss_ce: 0.005128, loss_dice: 0.074034
[12:23:11.063] TRAIN: iteration 27828 : loss : 0.250856, loss_ce: 0.001613, loss_dice: 0.500098
[12:23:11.270] TRAIN: iteration 27829 : loss : 0.115362, loss_ce: 0.001176, loss_dice: 0.229548
[12:23:11.479] TRAIN: iteration 27830 : loss : 0.032211, loss_ce: 0.000413, loss_dice: 0.064009
[12:23:11.687] TRAIN: iteration 27831 : loss : 0.028932, loss_ce: 0.000771, loss_dice: 0.057093
[12:23:11.893] TRAIN: iteration 27832 : loss : 0.158952, loss_ce: 0.001730, loss_dice: 0.316174
[12:23:12.100] TRAIN: iteration 27833 : loss : 0.088005, loss_ce: 0.002583, loss_dice: 0.173427
[12:23:12.317] TRAIN: iteration 27834 : loss : 0.246411, loss_ce: 0.002278, loss_dice: 0.490545
[12:23:12.524] TRAIN: iteration 27835 : loss : 0.105138, loss_ce: 0.008419, loss_dice: 0.201857
[12:23:12.732] TRAIN: iteration 27836 : loss : 0.051247, loss_ce: 0.005564, loss_dice: 0.096929
[12:23:12.939] TRAIN: iteration 27837 : loss : 0.085995, loss_ce: 0.000602, loss_dice: 0.171388
[12:23:13.148] TRAIN: iteration 27838 : loss : 0.148479, loss_ce: 0.005753, loss_dice: 0.291204
[12:23:13.355] TRAIN: iteration 27839 : loss : 0.042683, loss_ce: 0.004692, loss_dice: 0.080673
[12:23:13.561] TRAIN: iteration 27840 : loss : 0.056771, loss_ce: 0.001237, loss_dice: 0.112305
[12:23:13.803] TRAIN: iteration 27841 : loss : 0.250564, loss_ce: 0.001064, loss_dice: 0.500064
[12:23:14.015] TRAIN: iteration 27842 : loss : 0.054460, loss_ce: 0.003831, loss_dice: 0.105089
[12:23:14.229] TRAIN: iteration 27843 : loss : 0.042275, loss_ce: 0.000643, loss_dice: 0.083907
[12:23:14.437] TRAIN: iteration 27844 : loss : 0.046100, loss_ce: 0.000743, loss_dice: 0.091456
[12:23:14.647] TRAIN: iteration 27845 : loss : 0.250349, loss_ce: 0.000666, loss_dice: 0.500032
[12:23:14.855] TRAIN: iteration 27846 : loss : 0.053203, loss_ce: 0.005333, loss_dice: 0.101072
[12:23:15.065] TRAIN: iteration 27847 : loss : 0.101284, loss_ce: 0.002307, loss_dice: 0.200261
[12:23:15.273] TRAIN: iteration 27848 : loss : 0.226493, loss_ce: 0.002629, loss_dice: 0.450356
[12:23:15.487] TRAIN: iteration 27849 : loss : 0.011602, loss_ce: 0.000792, loss_dice: 0.022411
[12:23:15.699] TRAIN: iteration 27850 : loss : 0.032931, loss_ce: 0.000701, loss_dice: 0.065160
[12:23:15.906] TRAIN: iteration 27851 : loss : 0.056928, loss_ce: 0.001478, loss_dice: 0.112379
[12:23:16.116] TRAIN: iteration 27852 : loss : 0.115954, loss_ce: 0.001893, loss_dice: 0.230015
[12:23:16.326] TRAIN: iteration 27853 : loss : 0.248507, loss_ce: 0.000511, loss_dice: 0.496504
[12:23:16.534] TRAIN: iteration 27854 : loss : 0.250454, loss_ce: 0.000863, loss_dice: 0.500045
[12:23:16.744] TRAIN: iteration 27855 : loss : 0.173598, loss_ce: 0.005806, loss_dice: 0.341391
[12:23:16.960] TRAIN: iteration 27856 : loss : 0.211620, loss_ce: 0.001447, loss_dice: 0.421793
[12:23:17.177] TRAIN: iteration 27857 : loss : 0.102412, loss_ce: 0.005418, loss_dice: 0.199406
[12:23:17.390] TRAIN: iteration 27858 : loss : 0.047073, loss_ce: 0.001689, loss_dice: 0.092457
[12:23:17.600] TRAIN: iteration 27859 : loss : 0.155704, loss_ce: 0.005894, loss_dice: 0.305514
[12:23:17.809] TRAIN: iteration 27860 : loss : 0.037147, loss_ce: 0.003516, loss_dice: 0.070778
[12:23:18.055] TRAIN: iteration 27861 : loss : 0.072548, loss_ce: 0.006068, loss_dice: 0.139028
[12:23:18.267] TRAIN: iteration 27862 : loss : 0.253776, loss_ce: 0.009191, loss_dice: 0.498361
[12:23:18.477] TRAIN: iteration 27863 : loss : 0.101763, loss_ce: 0.004376, loss_dice: 0.199151
[12:23:18.685] TRAIN: iteration 27864 : loss : 0.103271, loss_ce: 0.002903, loss_dice: 0.203638
[12:23:18.897] TRAIN: iteration 27865 : loss : 0.062482, loss_ce: 0.002227, loss_dice: 0.122737
[12:23:19.106] TRAIN: iteration 27866 : loss : 0.099153, loss_ce: 0.007548, loss_dice: 0.190758
[12:23:19.314] TRAIN: iteration 27867 : loss : 0.065213, loss_ce: 0.006770, loss_dice: 0.123655
[12:23:19.522] TRAIN: iteration 27868 : loss : 0.063827, loss_ce: 0.003038, loss_dice: 0.124617
[12:23:19.733] TRAIN: iteration 27869 : loss : 0.136335, loss_ce: 0.006136, loss_dice: 0.266534
[12:23:19.940] TRAIN: iteration 27870 : loss : 0.120631, loss_ce: 0.005098, loss_dice: 0.236163
[12:23:20.149] TRAIN: iteration 27871 : loss : 0.232812, loss_ce: 0.007770, loss_dice: 0.457854
[12:23:20.364] TRAIN: iteration 27872 : loss : 0.250964, loss_ce: 0.001812, loss_dice: 0.500115
[12:23:20.571] TRAIN: iteration 27873 : loss : 0.249709, loss_ce: 0.006481, loss_dice: 0.492937
[12:23:20.778] TRAIN: iteration 27874 : loss : 0.009271, loss_ce: 0.001093, loss_dice: 0.017449
[12:23:20.986] TRAIN: iteration 27875 : loss : 0.103543, loss_ce: 0.003255, loss_dice: 0.203832
[12:23:21.193] TRAIN: iteration 27876 : loss : 0.041726, loss_ce: 0.002466, loss_dice: 0.080987
[12:23:21.400] TRAIN: iteration 27877 : loss : 0.022471, loss_ce: 0.001258, loss_dice: 0.043685
[12:23:21.607] TRAIN: iteration 27878 : loss : 0.066353, loss_ce: 0.001455, loss_dice: 0.131251
[12:23:21.814] TRAIN: iteration 27879 : loss : 0.039669, loss_ce: 0.001560, loss_dice: 0.077778
[12:23:22.024] TRAIN: iteration 27880 : loss : 0.016432, loss_ce: 0.001108, loss_dice: 0.031756
[12:23:22.266] TRAIN: iteration 27881 : loss : 0.039904, loss_ce: 0.002065, loss_dice: 0.077744
[12:23:22.473] TRAIN: iteration 27882 : loss : 0.046336, loss_ce: 0.004175, loss_dice: 0.088498
[12:23:22.679] TRAIN: iteration 27883 : loss : 0.025751, loss_ce: 0.001674, loss_dice: 0.049829
[12:23:22.893] TRAIN: iteration 27884 : loss : 0.245332, loss_ce: 0.002667, loss_dice: 0.487996
[12:23:23.101] TRAIN: iteration 27885 : loss : 0.081113, loss_ce: 0.006609, loss_dice: 0.155618
[12:23:23.310] TRAIN: iteration 27886 : loss : 0.050470, loss_ce: 0.002274, loss_dice: 0.098666
[12:23:23.521] TRAIN: iteration 27887 : loss : 0.128552, loss_ce: 0.003252, loss_dice: 0.253852
[12:23:23.737] TRAIN: iteration 27888 : loss : 0.079878, loss_ce: 0.002300, loss_dice: 0.157456
[12:23:23.945] TRAIN: iteration 27889 : loss : 0.027579, loss_ce: 0.001113, loss_dice: 0.054045
[12:23:24.155] TRAIN: iteration 27890 : loss : 0.071575, loss_ce: 0.006289, loss_dice: 0.136861
[12:23:24.369] TRAIN: iteration 27891 : loss : 0.250810, loss_ce: 0.001528, loss_dice: 0.500093
[12:23:24.576] TRAIN: iteration 27892 : loss : 0.054028, loss_ce: 0.003314, loss_dice: 0.104741
[12:23:24.785] TRAIN: iteration 27893 : loss : 0.190366, loss_ce: 0.003910, loss_dice: 0.376822
[12:23:24.994] TRAIN: iteration 27894 : loss : 0.064676, loss_ce: 0.002923, loss_dice: 0.126429
[12:23:25.203] TRAIN: iteration 27895 : loss : 0.144967, loss_ce: 0.002874, loss_dice: 0.287059
[12:23:25.412] TRAIN: iteration 27896 : loss : 0.101764, loss_ce: 0.001256, loss_dice: 0.202272
[12:23:25.859] TRAIN: iteration 27897 : loss : 0.076635, loss_ce: 0.002876, loss_dice: 0.150393
[12:23:26.068] TRAIN: iteration 27898 : loss : 0.014162, loss_ce: 0.000748, loss_dice: 0.027577
[12:23:26.278] TRAIN: iteration 27899 : loss : 0.033171, loss_ce: 0.007225, loss_dice: 0.059116
[12:23:26.495] TRAIN: iteration 27900 : loss : 0.053915, loss_ce: 0.006628, loss_dice: 0.101202
[12:23:26.496] NaN or Inf found in input tensor.
[12:23:26.719] TRAIN: iteration 27901 : loss : 0.091224, loss_ce: 0.006462, loss_dice: 0.175986
[12:23:26.930] TRAIN: iteration 27902 : loss : 0.055056, loss_ce: 0.000656, loss_dice: 0.109455
[12:23:27.141] TRAIN: iteration 27903 : loss : 0.234292, loss_ce: 0.002434, loss_dice: 0.466151
[12:23:27.350] TRAIN: iteration 27904 : loss : 0.174546, loss_ce: 0.021587, loss_dice: 0.327505
[12:23:27.561] TRAIN: iteration 27905 : loss : 0.123311, loss_ce: 0.002601, loss_dice: 0.244021
[12:23:27.770] TRAIN: iteration 27906 : loss : 0.166085, loss_ce: 0.005386, loss_dice: 0.326783
[12:23:27.981] TRAIN: iteration 27907 : loss : 0.064620, loss_ce: 0.014155, loss_dice: 0.115085
[12:23:28.195] TRAIN: iteration 27908 : loss : 0.087727, loss_ce: 0.007544, loss_dice: 0.167910
[12:23:28.404] TRAIN: iteration 27909 : loss : 0.065805, loss_ce: 0.006875, loss_dice: 0.124734
[12:23:28.614] TRAIN: iteration 27910 : loss : 0.250649, loss_ce: 0.001235, loss_dice: 0.500064
[12:23:28.831] TRAIN: iteration 27911 : loss : 0.250775, loss_ce: 0.001474, loss_dice: 0.500076
[12:23:29.046] TRAIN: iteration 27912 : loss : 0.049488, loss_ce: 0.004664, loss_dice: 0.094311
[12:23:29.254] TRAIN: iteration 27913 : loss : 0.084511, loss_ce: 0.008539, loss_dice: 0.160484
[12:23:29.462] TRAIN: iteration 27914 : loss : 0.250551, loss_ce: 0.001063, loss_dice: 0.500039
[12:23:29.673] TRAIN: iteration 27915 : loss : 0.075175, loss_ce: 0.002056, loss_dice: 0.148295
[12:23:29.886] TRAIN: iteration 27916 : loss : 0.250967, loss_ce: 0.001840, loss_dice: 0.500095
[12:23:30.097] TRAIN: iteration 27917 : loss : 0.049725, loss_ce: 0.002479, loss_dice: 0.096972
[12:23:30.306] TRAIN: iteration 27918 : loss : 0.250624, loss_ce: 0.001913, loss_dice: 0.499336
[12:23:30.513] TRAIN: iteration 27919 : loss : 0.031492, loss_ce: 0.007428, loss_dice: 0.055556
[12:23:30.721] TRAIN: iteration 27920 : loss : 0.052753, loss_ce: 0.004465, loss_dice: 0.101040
[12:23:30.722] NaN or Inf found in input tensor.
[12:23:30.937] TRAIN: iteration 27921 : loss : 0.071275, loss_ce: 0.001640, loss_dice: 0.140910
[12:23:31.145] TRAIN: iteration 27922 : loss : 0.102232, loss_ce: 0.001758, loss_dice: 0.202706
[12:23:31.354] TRAIN: iteration 27923 : loss : 0.054191, loss_ce: 0.001914, loss_dice: 0.106468
[12:23:31.568] TRAIN: iteration 27924 : loss : 0.141032, loss_ce: 0.003229, loss_dice: 0.278835
[12:23:31.777] TRAIN: iteration 27925 : loss : 0.195225, loss_ce: 0.002803, loss_dice: 0.387648
[12:23:31.985] TRAIN: iteration 27926 : loss : 0.128726, loss_ce: 0.002724, loss_dice: 0.254729
[12:23:32.198] TRAIN: iteration 27927 : loss : 0.039182, loss_ce: 0.004166, loss_dice: 0.074198
[12:23:32.776] TRAIN: iteration 27928 : loss : 0.087376, loss_ce: 0.003782, loss_dice: 0.170969
[12:23:32.984] TRAIN: iteration 27929 : loss : 0.028460, loss_ce: 0.000658, loss_dice: 0.056262
[12:23:33.199] TRAIN: iteration 27930 : loss : 0.143547, loss_ce: 0.010430, loss_dice: 0.276663
[12:23:33.409] TRAIN: iteration 27931 : loss : 0.058298, loss_ce: 0.003025, loss_dice: 0.113571
[12:23:33.616] TRAIN: iteration 27932 : loss : 0.065001, loss_ce: 0.002836, loss_dice: 0.127165
[12:23:33.823] TRAIN: iteration 27933 : loss : 0.206619, loss_ce: 0.001855, loss_dice: 0.411383
[12:23:34.036] TRAIN: iteration 27934 : loss : 0.150136, loss_ce: 0.002884, loss_dice: 0.297388
[12:23:34.252] TRAIN: iteration 27935 : loss : 0.051852, loss_ce: 0.001915, loss_dice: 0.101789
[12:23:35.852] TRAIN: iteration 27936 : loss : 0.154169, loss_ce: 0.006905, loss_dice: 0.301432
[12:23:36.060] TRAIN: iteration 27937 : loss : 0.086935, loss_ce: 0.004099, loss_dice: 0.169770
[12:23:36.269] TRAIN: iteration 27938 : loss : 0.123706, loss_ce: 0.003068, loss_dice: 0.244344
[12:23:36.477] TRAIN: iteration 27939 : loss : 0.140840, loss_ce: 0.003183, loss_dice: 0.278497
[12:23:36.691] TRAIN: iteration 27940 : loss : 0.227773, loss_ce: 0.002509, loss_dice: 0.453037
[12:23:36.692] NaN or Inf found in input tensor.
[12:23:36.906] TRAIN: iteration 27941 : loss : 0.250868, loss_ce: 0.001631, loss_dice: 0.500105
[12:23:37.115] TRAIN: iteration 27942 : loss : 0.034848, loss_ce: 0.001608, loss_dice: 0.068089
[12:23:37.323] TRAIN: iteration 27943 : loss : 0.137367, loss_ce: 0.002232, loss_dice: 0.272501
[12:23:37.547] TRAIN: iteration 27944 : loss : 0.127751, loss_ce: 0.004326, loss_dice: 0.251176
[12:23:37.759] TRAIN: iteration 27945 : loss : 0.074264, loss_ce: 0.001975, loss_dice: 0.146554
[12:23:37.974] TRAIN: iteration 27946 : loss : 0.250584, loss_ce: 0.001117, loss_dice: 0.500050
[12:23:38.187] TRAIN: iteration 27947 : loss : 0.245831, loss_ce: 0.001139, loss_dice: 0.490522
[12:23:38.401] TRAIN: iteration 27948 : loss : 0.036321, loss_ce: 0.001578, loss_dice: 0.071063
[12:23:38.608] TRAIN: iteration 27949 : loss : 0.100965, loss_ce: 0.008671, loss_dice: 0.193260
[12:23:38.815] TRAIN: iteration 27950 : loss : 0.090570, loss_ce: 0.004350, loss_dice: 0.176789
[12:23:39.024] TRAIN: iteration 27951 : loss : 0.151123, loss_ce: 0.011085, loss_dice: 0.291160
[12:23:39.233] TRAIN: iteration 27952 : loss : 0.101328, loss_ce: 0.006112, loss_dice: 0.196544
[12:23:39.446] TRAIN: iteration 27953 : loss : 0.102314, loss_ce: 0.004136, loss_dice: 0.200493
[12:23:39.657] TRAIN: iteration 27954 : loss : 0.247672, loss_ce: 0.001570, loss_dice: 0.493773
[12:23:39.872] TRAIN: iteration 27955 : loss : 0.250910, loss_ce: 0.001715, loss_dice: 0.500106
[12:23:40.081] TRAIN: iteration 27956 : loss : 0.080420, loss_ce: 0.001254, loss_dice: 0.159587
[12:23:40.290] TRAIN: iteration 27957 : loss : 0.032849, loss_ce: 0.006949, loss_dice: 0.058749
[12:23:40.504] TRAIN: iteration 27958 : loss : 0.180783, loss_ce: 0.004392, loss_dice: 0.357174
[12:23:40.717] TRAIN: iteration 27959 : loss : 0.038164, loss_ce: 0.001080, loss_dice: 0.075248
[12:23:40.924] TRAIN: iteration 27960 : loss : 0.250941, loss_ce: 0.002290, loss_dice: 0.499592
[12:23:41.165] TRAIN: iteration 27961 : loss : 0.085242, loss_ce: 0.002733, loss_dice: 0.167751
[12:23:41.373] TRAIN: iteration 27962 : loss : 0.250601, loss_ce: 0.001146, loss_dice: 0.500055
[12:23:41.584] TRAIN: iteration 27963 : loss : 0.089041, loss_ce: 0.004726, loss_dice: 0.173357
[12:23:41.792] TRAIN: iteration 27964 : loss : 0.150644, loss_ce: 0.006562, loss_dice: 0.294725
[12:23:42.002] TRAIN: iteration 27965 : loss : 0.211060, loss_ce: 0.009731, loss_dice: 0.412389
[12:23:42.214] TRAIN: iteration 27966 : loss : 0.065554, loss_ce: 0.001570, loss_dice: 0.129538
[12:23:42.422] TRAIN: iteration 27967 : loss : 0.069856, loss_ce: 0.003693, loss_dice: 0.136018
[12:23:42.629] TRAIN: iteration 27968 : loss : 0.057822, loss_ce: 0.003462, loss_dice: 0.112181
[12:23:42.838] TRAIN: iteration 27969 : loss : 0.027876, loss_ce: 0.003237, loss_dice: 0.052515
[12:23:43.046] TRAIN: iteration 27970 : loss : 0.043970, loss_ce: 0.003332, loss_dice: 0.084609
[12:23:43.254] TRAIN: iteration 27971 : loss : 0.160235, loss_ce: 0.005243, loss_dice: 0.315227
[12:23:43.463] TRAIN: iteration 27972 : loss : 0.104131, loss_ce: 0.002564, loss_dice: 0.205698
[12:23:43.671] TRAIN: iteration 27973 : loss : 0.042518, loss_ce: 0.000945, loss_dice: 0.084091
[12:23:43.878] TRAIN: iteration 27974 : loss : 0.113147, loss_ce: 0.003104, loss_dice: 0.223189
[12:23:44.087] TRAIN: iteration 27975 : loss : 0.103635, loss_ce: 0.004257, loss_dice: 0.203012
[12:23:44.295] TRAIN: iteration 27976 : loss : 0.112685, loss_ce: 0.001830, loss_dice: 0.223541
[12:23:44.504] TRAIN: iteration 27977 : loss : 0.074411, loss_ce: 0.002438, loss_dice: 0.146383
[12:23:44.713] TRAIN: iteration 27978 : loss : 0.172539, loss_ce: 0.009059, loss_dice: 0.336019
[12:23:44.923] TRAIN: iteration 27979 : loss : 0.048574, loss_ce: 0.001220, loss_dice: 0.095928
[12:23:45.133] TRAIN: iteration 27980 : loss : 0.178798, loss_ce: 0.001293, loss_dice: 0.356303
[12:23:45.370] TRAIN: iteration 27981 : loss : 0.064971, loss_ce: 0.002190, loss_dice: 0.127753
[12:23:45.579] TRAIN: iteration 27982 : loss : 0.110206, loss_ce: 0.007251, loss_dice: 0.213160
[12:23:45.788] TRAIN: iteration 27983 : loss : 0.086101, loss_ce: 0.008336, loss_dice: 0.163867
[12:23:46.004] TRAIN: iteration 27984 : loss : 0.018679, loss_ce: 0.000759, loss_dice: 0.036598
[12:23:46.212] TRAIN: iteration 27985 : loss : 0.095810, loss_ce: 0.001600, loss_dice: 0.190020
[12:23:46.427] TRAIN: iteration 27986 : loss : 0.163999, loss_ce: 0.003039, loss_dice: 0.324960
[12:23:46.639] TRAIN: iteration 27987 : loss : 0.068806, loss_ce: 0.005595, loss_dice: 0.132016
[12:23:46.848] TRAIN: iteration 27988 : loss : 0.064529, loss_ce: 0.002785, loss_dice: 0.126272
[12:23:47.059] TRAIN: iteration 27989 : loss : 0.115346, loss_ce: 0.002006, loss_dice: 0.228686
[12:23:47.273] TRAIN: iteration 27990 : loss : 0.090030, loss_ce: 0.002984, loss_dice: 0.177075
[12:23:47.485] TRAIN: iteration 27991 : loss : 0.033692, loss_ce: 0.002362, loss_dice: 0.065021
[12:23:47.692] TRAIN: iteration 27992 : loss : 0.243751, loss_ce: 0.001346, loss_dice: 0.486155
[12:23:47.900] TRAIN: iteration 27993 : loss : 0.052701, loss_ce: 0.002343, loss_dice: 0.103060
[12:23:48.108] TRAIN: iteration 27994 : loss : 0.227907, loss_ce: 0.006524, loss_dice: 0.449291
[12:23:48.317] TRAIN: iteration 27995 : loss : 0.035511, loss_ce: 0.003831, loss_dice: 0.067190
[12:23:48.530] TRAIN: iteration 27996 : loss : 0.061613, loss_ce: 0.001254, loss_dice: 0.121973
[12:23:48.737] TRAIN: iteration 27997 : loss : 0.030087, loss_ce: 0.002197, loss_dice: 0.057976
[12:23:48.947] TRAIN: iteration 27998 : loss : 0.059901, loss_ce: 0.004439, loss_dice: 0.115364
[12:23:49.155] TRAIN: iteration 27999 : loss : 0.035963, loss_ce: 0.006143, loss_dice: 0.065782
[12:23:49.363] TRAIN: iteration 28000 : loss : 0.251333, loss_ce: 0.002497, loss_dice: 0.500169
[12:23:49.603] TRAIN: iteration 28001 : loss : 0.141212, loss_ce: 0.004261, loss_dice: 0.278164
[12:23:49.819] TRAIN: iteration 28002 : loss : 0.061115, loss_ce: 0.002324, loss_dice: 0.119905
[12:23:50.027] TRAIN: iteration 28003 : loss : 0.250696, loss_ce: 0.001326, loss_dice: 0.500066
[12:23:50.235] TRAIN: iteration 28004 : loss : 0.042316, loss_ce: 0.002994, loss_dice: 0.081638
[12:23:50.442] TRAIN: iteration 28005 : loss : 0.250501, loss_ce: 0.000975, loss_dice: 0.500026
[12:23:50.652] TRAIN: iteration 28006 : loss : 0.147992, loss_ce: 0.008438, loss_dice: 0.287546
[12:23:50.861] TRAIN: iteration 28007 : loss : 0.077086, loss_ce: 0.004774, loss_dice: 0.149397
[12:23:51.070] TRAIN: iteration 28008 : loss : 0.069218, loss_ce: 0.001284, loss_dice: 0.137152
[12:23:51.279] TRAIN: iteration 28009 : loss : 0.059292, loss_ce: 0.006400, loss_dice: 0.112184
[12:23:51.552] TRAIN: iteration 28010 : loss : 0.078473, loss_ce: 0.002336, loss_dice: 0.154610
[12:23:51.761] TRAIN: iteration 28011 : loss : 0.026580, loss_ce: 0.001534, loss_dice: 0.051626
[12:23:51.969] TRAIN: iteration 28012 : loss : 0.244357, loss_ce: 0.003560, loss_dice: 0.485154
[12:23:52.179] TRAIN: iteration 28013 : loss : 0.039988, loss_ce: 0.000987, loss_dice: 0.078989
[12:23:52.390] TRAIN: iteration 28014 : loss : 0.250480, loss_ce: 0.000935, loss_dice: 0.500025
[12:23:52.603] TRAIN: iteration 28015 : loss : 0.068668, loss_ce: 0.007974, loss_dice: 0.129363
[12:23:52.814] TRAIN: iteration 28016 : loss : 0.192433, loss_ce: 0.002897, loss_dice: 0.381968
[12:23:53.026] TRAIN: iteration 28017 : loss : 0.064156, loss_ce: 0.003912, loss_dice: 0.124399
[12:23:53.242] TRAIN: iteration 28018 : loss : 0.094994, loss_ce: 0.007179, loss_dice: 0.182810
[12:23:53.453] TRAIN: iteration 28019 : loss : 0.067626, loss_ce: 0.001014, loss_dice: 0.134238
[12:23:54.167] TRAIN: iteration 28020 : loss : 0.043807, loss_ce: 0.004343, loss_dice: 0.083270
[12:23:54.168] NaN or Inf found in input tensor.
[12:23:54.383] TRAIN: iteration 28021 : loss : 0.205404, loss_ce: 0.001340, loss_dice: 0.409467
[12:23:54.590] TRAIN: iteration 28022 : loss : 0.050694, loss_ce: 0.002183, loss_dice: 0.099205
[12:23:54.798] TRAIN: iteration 28023 : loss : 0.097499, loss_ce: 0.002351, loss_dice: 0.192647
[12:23:55.007] TRAIN: iteration 28024 : loss : 0.057965, loss_ce: 0.001740, loss_dice: 0.114190
[12:23:55.222] TRAIN: iteration 28025 : loss : 0.250370, loss_ce: 0.000716, loss_dice: 0.500024
[12:23:55.430] TRAIN: iteration 28026 : loss : 0.097121, loss_ce: 0.013131, loss_dice: 0.181111
[12:23:55.639] TRAIN: iteration 28027 : loss : 0.253803, loss_ce: 0.007069, loss_dice: 0.500537
[12:23:55.847] TRAIN: iteration 28028 : loss : 0.067036, loss_ce: 0.003227, loss_dice: 0.130845
[12:23:56.062] TRAIN: iteration 28029 : loss : 0.250535, loss_ce: 0.001027, loss_dice: 0.500042
[12:23:56.272] TRAIN: iteration 28030 : loss : 0.038504, loss_ce: 0.007785, loss_dice: 0.069223
[12:23:56.480] TRAIN: iteration 28031 : loss : 0.251850, loss_ce: 0.003460, loss_dice: 0.500241
[12:23:56.688] TRAIN: iteration 28032 : loss : 0.178818, loss_ce: 0.004440, loss_dice: 0.353197
[12:23:56.895] TRAIN: iteration 28033 : loss : 0.250317, loss_ce: 0.000621, loss_dice: 0.500013
[12:23:57.103] TRAIN: iteration 28034 : loss : 0.062439, loss_ce: 0.006338, loss_dice: 0.118540
[12:23:57.318] TRAIN: iteration 28035 : loss : 0.069808, loss_ce: 0.006038, loss_dice: 0.133578
[12:23:57.525] TRAIN: iteration 28036 : loss : 0.065375, loss_ce: 0.001729, loss_dice: 0.129022
[12:23:57.732] TRAIN: iteration 28037 : loss : 0.103420, loss_ce: 0.004178, loss_dice: 0.202662
[12:23:57.944] TRAIN: iteration 28038 : loss : 0.055959, loss_ce: 0.003524, loss_dice: 0.108394
[12:23:58.173] TRAIN: iteration 28039 : loss : 0.077444, loss_ce: 0.002346, loss_dice: 0.152542
[12:23:58.387] TRAIN: iteration 28040 : loss : 0.158182, loss_ce: 0.002875, loss_dice: 0.313488
[12:23:58.624] TRAIN: iteration 28041 : loss : 0.043857, loss_ce: 0.002488, loss_dice: 0.085226
[12:23:58.831] TRAIN: iteration 28042 : loss : 0.008462, loss_ce: 0.001196, loss_dice: 0.015729
[12:23:59.039] TRAIN: iteration 28043 : loss : 0.032397, loss_ce: 0.001347, loss_dice: 0.063447
[12:23:59.246] TRAIN: iteration 28044 : loss : 0.250427, loss_ce: 0.000836, loss_dice: 0.500018
[12:23:59.457] TRAIN: iteration 28045 : loss : 0.040836, loss_ce: 0.003016, loss_dice: 0.078655
[12:23:59.666] TRAIN: iteration 28046 : loss : 0.216038, loss_ce: 0.004503, loss_dice: 0.427573
[12:23:59.873] TRAIN: iteration 28047 : loss : 0.251020, loss_ce: 0.001914, loss_dice: 0.500127
[12:24:00.081] TRAIN: iteration 28048 : loss : 0.053946, loss_ce: 0.002240, loss_dice: 0.105652
[12:24:00.288] TRAIN: iteration 28049 : loss : 0.029294, loss_ce: 0.005518, loss_dice: 0.053069
[12:24:00.497] TRAIN: iteration 28050 : loss : 0.076400, loss_ce: 0.001168, loss_dice: 0.151631
[12:24:00.705] TRAIN: iteration 28051 : loss : 0.240258, loss_ce: 0.001500, loss_dice: 0.479016
[12:24:00.915] TRAIN: iteration 28052 : loss : 0.029526, loss_ce: 0.006166, loss_dice: 0.052886
[12:24:01.125] TRAIN: iteration 28053 : loss : 0.145451, loss_ce: 0.000692, loss_dice: 0.290210
[12:24:01.332] TRAIN: iteration 28054 : loss : 0.063810, loss_ce: 0.002889, loss_dice: 0.124732
[12:24:01.541] TRAIN: iteration 28055 : loss : 0.250197, loss_ce: 0.000391, loss_dice: 0.500004
[12:24:01.757] TRAIN: iteration 28056 : loss : 0.196946, loss_ce: 0.001397, loss_dice: 0.392495
[12:24:01.964] TRAIN: iteration 28057 : loss : 0.065757, loss_ce: 0.003506, loss_dice: 0.128008
[12:24:02.172] TRAIN: iteration 28058 : loss : 0.200015, loss_ce: 0.001232, loss_dice: 0.398799
[12:24:02.380] TRAIN: iteration 28059 : loss : 0.096200, loss_ce: 0.005353, loss_dice: 0.187048
[12:24:02.588] TRAIN: iteration 28060 : loss : 0.250936, loss_ce: 0.001748, loss_dice: 0.500124
[12:24:02.827] TRAIN: iteration 28061 : loss : 0.139649, loss_ce: 0.004303, loss_dice: 0.274994
[12:24:03.039] TRAIN: iteration 28062 : loss : 0.189397, loss_ce: 0.021492, loss_dice: 0.357303
[12:24:03.248] TRAIN: iteration 28063 : loss : 0.249932, loss_ce: 0.000518, loss_dice: 0.499345
[12:24:03.464] TRAIN: iteration 28064 : loss : 0.026450, loss_ce: 0.000814, loss_dice: 0.052086
[12:24:03.679] TRAIN: iteration 28065 : loss : 0.058386, loss_ce: 0.001976, loss_dice: 0.114796
[12:24:03.894] TRAIN: iteration 28066 : loss : 0.119109, loss_ce: 0.008083, loss_dice: 0.230136
[12:24:04.104] TRAIN: iteration 28067 : loss : 0.024173, loss_ce: 0.000549, loss_dice: 0.047798
[12:24:04.312] TRAIN: iteration 28068 : loss : 0.250449, loss_ce: 0.000853, loss_dice: 0.500045
[12:24:04.526] TRAIN: iteration 28069 : loss : 0.085107, loss_ce: 0.002635, loss_dice: 0.167580
[12:24:04.735] TRAIN: iteration 28070 : loss : 0.250888, loss_ce: 0.001681, loss_dice: 0.500095
[12:24:04.950] TRAIN: iteration 28071 : loss : 0.047403, loss_ce: 0.005338, loss_dice: 0.089467
[12:24:05.163] TRAIN: iteration 28072 : loss : 0.252506, loss_ce: 0.004648, loss_dice: 0.500364
[12:24:05.377] TRAIN: iteration 28073 : loss : 0.050091, loss_ce: 0.013068, loss_dice: 0.087115
[12:24:05.588] TRAIN: iteration 28074 : loss : 0.029957, loss_ce: 0.000758, loss_dice: 0.059157
[12:24:05.798] TRAIN: iteration 28075 : loss : 0.250341, loss_ce: 0.000654, loss_dice: 0.500029
[12:24:06.014] TRAIN: iteration 28076 : loss : 0.019462, loss_ce: 0.001691, loss_dice: 0.037232
[12:24:06.225] TRAIN: iteration 28077 : loss : 0.096557, loss_ce: 0.001464, loss_dice: 0.191650
[12:24:06.434] TRAIN: iteration 28078 : loss : 0.040504, loss_ce: 0.001312, loss_dice: 0.079695
[12:24:06.644] TRAIN: iteration 28079 : loss : 0.053706, loss_ce: 0.001094, loss_dice: 0.106318
[12:24:06.857] TRAIN: iteration 28080 : loss : 0.042558, loss_ce: 0.001058, loss_dice: 0.084058
[12:24:07.097] TRAIN: iteration 28081 : loss : 0.188115, loss_ce: 0.001184, loss_dice: 0.375047
[12:24:07.304] TRAIN: iteration 28082 : loss : 0.157983, loss_ce: 0.001615, loss_dice: 0.314350
[12:24:07.512] TRAIN: iteration 28083 : loss : 0.043003, loss_ce: 0.000994, loss_dice: 0.085013
[12:24:07.720] TRAIN: iteration 28084 : loss : 0.097170, loss_ce: 0.019942, loss_dice: 0.174398
[12:24:07.932] TRAIN: iteration 28085 : loss : 0.067654, loss_ce: 0.005410, loss_dice: 0.129899
[12:24:08.141] TRAIN: iteration 28086 : loss : 0.114299, loss_ce: 0.001545, loss_dice: 0.227053
[12:24:08.351] TRAIN: iteration 28087 : loss : 0.113718, loss_ce: 0.004417, loss_dice: 0.223019
[12:24:08.559] TRAIN: iteration 28088 : loss : 0.189093, loss_ce: 0.001686, loss_dice: 0.376499
[12:24:08.766] TRAIN: iteration 28089 : loss : 0.031595, loss_ce: 0.003739, loss_dice: 0.059451
[12:24:08.979] TRAIN: iteration 28090 : loss : 0.121271, loss_ce: 0.018476, loss_dice: 0.224066
[12:24:09.188] TRAIN: iteration 28091 : loss : 0.206120, loss_ce: 0.011471, loss_dice: 0.400770
[12:24:09.396] TRAIN: iteration 28092 : loss : 0.048448, loss_ce: 0.001882, loss_dice: 0.095014
[12:24:09.605] TRAIN: iteration 28093 : loss : 0.163192, loss_ce: 0.007308, loss_dice: 0.319077
[12:24:09.816] TRAIN: iteration 28094 : loss : 0.021360, loss_ce: 0.003181, loss_dice: 0.039540
[12:24:10.023] TRAIN: iteration 28095 : loss : 0.126133, loss_ce: 0.002653, loss_dice: 0.249613
[12:24:10.232] TRAIN: iteration 28096 : loss : 0.021072, loss_ce: 0.001047, loss_dice: 0.041097
[12:24:10.443] TRAIN: iteration 28097 : loss : 0.084514, loss_ce: 0.005124, loss_dice: 0.163905
[12:24:10.653] TRAIN: iteration 28098 : loss : 0.083522, loss_ce: 0.006444, loss_dice: 0.160600
[12:24:11.294] TRAIN: iteration 28099 : loss : 0.214937, loss_ce: 0.006379, loss_dice: 0.423496
[12:24:11.502] TRAIN: iteration 28100 : loss : 0.070874, loss_ce: 0.006469, loss_dice: 0.135278
[12:24:11.739] TRAIN: iteration 28101 : loss : 0.080730, loss_ce: 0.006028, loss_dice: 0.155432
[12:24:11.946] TRAIN: iteration 28102 : loss : 0.039468, loss_ce: 0.002106, loss_dice: 0.076830
[12:24:12.154] TRAIN: iteration 28103 : loss : 0.128565, loss_ce: 0.005504, loss_dice: 0.251625
[12:24:12.364] TRAIN: iteration 28104 : loss : 0.242652, loss_ce: 0.001816, loss_dice: 0.483489
[12:24:12.574] TRAIN: iteration 28105 : loss : 0.053869, loss_ce: 0.001690, loss_dice: 0.106048
[12:24:12.783] TRAIN: iteration 28106 : loss : 0.110339, loss_ce: 0.001994, loss_dice: 0.218685
[12:24:13.691] TRAIN: iteration 28107 : loss : 0.086100, loss_ce: 0.008548, loss_dice: 0.163653
[12:24:13.900] TRAIN: iteration 28108 : loss : 0.175780, loss_ce: 0.008691, loss_dice: 0.342869
[12:24:14.108] TRAIN: iteration 28109 : loss : 0.216754, loss_ce: 0.001534, loss_dice: 0.431973
[12:24:14.318] TRAIN: iteration 28110 : loss : 0.213810, loss_ce: 0.007859, loss_dice: 0.419761
[12:24:14.531] TRAIN: iteration 28111 : loss : 0.119080, loss_ce: 0.002245, loss_dice: 0.235916
[12:24:14.739] TRAIN: iteration 28112 : loss : 0.127785, loss_ce: 0.011522, loss_dice: 0.244048
[12:24:14.947] TRAIN: iteration 28113 : loss : 0.250983, loss_ce: 0.001854, loss_dice: 0.500112
[12:24:15.159] TRAIN: iteration 28114 : loss : 0.066177, loss_ce: 0.001167, loss_dice: 0.131187
[12:24:15.366] TRAIN: iteration 28115 : loss : 0.100653, loss_ce: 0.001676, loss_dice: 0.199629
[12:24:15.573] TRAIN: iteration 28116 : loss : 0.061615, loss_ce: 0.005391, loss_dice: 0.117839
[12:24:15.780] TRAIN: iteration 28117 : loss : 0.103154, loss_ce: 0.001539, loss_dice: 0.204769
[12:24:15.996] TRAIN: iteration 28118 : loss : 0.247360, loss_ce: 0.001583, loss_dice: 0.493138
[12:24:16.204] TRAIN: iteration 28119 : loss : 0.200465, loss_ce: 0.001284, loss_dice: 0.399646
[12:24:16.412] TRAIN: iteration 28120 : loss : 0.238679, loss_ce: 0.009171, loss_dice: 0.468186
[12:24:16.647] TRAIN: iteration 28121 : loss : 0.074186, loss_ce: 0.003897, loss_dice: 0.144475
[12:24:16.856] TRAIN: iteration 28122 : loss : 0.048894, loss_ce: 0.001161, loss_dice: 0.096626
[12:24:17.072] TRAIN: iteration 28123 : loss : 0.067917, loss_ce: 0.003562, loss_dice: 0.132272
[12:24:17.280] TRAIN: iteration 28124 : loss : 0.060174, loss_ce: 0.001849, loss_dice: 0.118499
[12:24:17.491] TRAIN: iteration 28125 : loss : 0.220223, loss_ce: 0.003366, loss_dice: 0.437079
[12:24:17.699] TRAIN: iteration 28126 : loss : 0.165339, loss_ce: 0.003021, loss_dice: 0.327657
[12:24:17.913] TRAIN: iteration 28127 : loss : 0.079945, loss_ce: 0.002470, loss_dice: 0.157421
[12:24:18.123] TRAIN: iteration 28128 : loss : 0.122383, loss_ce: 0.002576, loss_dice: 0.242190
[12:24:18.332] TRAIN: iteration 28129 : loss : 0.161031, loss_ce: 0.002444, loss_dice: 0.319617
[12:24:18.547] TRAIN: iteration 28130 : loss : 0.060437, loss_ce: 0.003883, loss_dice: 0.116992
[12:24:18.755] TRAIN: iteration 28131 : loss : 0.087390, loss_ce: 0.004180, loss_dice: 0.170600
[12:24:18.967] TRAIN: iteration 28132 : loss : 0.032513, loss_ce: 0.002729, loss_dice: 0.062296
[12:24:19.178] TRAIN: iteration 28133 : loss : 0.074164, loss_ce: 0.005617, loss_dice: 0.142710
[12:24:19.391] TRAIN: iteration 28134 : loss : 0.049202, loss_ce: 0.003580, loss_dice: 0.094824
[12:24:19.605] TRAIN: iteration 28135 : loss : 0.109456, loss_ce: 0.003687, loss_dice: 0.215224
[12:24:19.813] TRAIN: iteration 28136 : loss : 0.023651, loss_ce: 0.001403, loss_dice: 0.045899
[12:24:20.022] TRAIN: iteration 28137 : loss : 0.058393, loss_ce: 0.001025, loss_dice: 0.115761
[12:24:20.233] TRAIN: iteration 28138 : loss : 0.108387, loss_ce: 0.002762, loss_dice: 0.214012
[12:24:20.441] TRAIN: iteration 28139 : loss : 0.067960, loss_ce: 0.005104, loss_dice: 0.130816
[12:24:20.849] TRAIN: iteration 28140 : loss : 0.034136, loss_ce: 0.000831, loss_dice: 0.067442
[12:24:21.093] TRAIN: iteration 28141 : loss : 0.053229, loss_ce: 0.007458, loss_dice: 0.099000
[12:24:21.308] TRAIN: iteration 28142 : loss : 0.107904, loss_ce: 0.003834, loss_dice: 0.211975
[12:24:21.516] TRAIN: iteration 28143 : loss : 0.034029, loss_ce: 0.004726, loss_dice: 0.063332
[12:24:21.725] TRAIN: iteration 28144 : loss : 0.060314, loss_ce: 0.003221, loss_dice: 0.117406
[12:24:21.935] TRAIN: iteration 28145 : loss : 0.038639, loss_ce: 0.003887, loss_dice: 0.073391
[12:24:22.144] TRAIN: iteration 28146 : loss : 0.246501, loss_ce: 0.004062, loss_dice: 0.488940
[12:24:22.358] TRAIN: iteration 28147 : loss : 0.250891, loss_ce: 0.001699, loss_dice: 0.500083
[12:24:24.474] TRAIN: iteration 28148 : loss : 0.054251, loss_ce: 0.005705, loss_dice: 0.102797
[12:24:24.682] TRAIN: iteration 28149 : loss : 0.046625, loss_ce: 0.004884, loss_dice: 0.088365
[12:24:24.892] TRAIN: iteration 28150 : loss : 0.039166, loss_ce: 0.002384, loss_dice: 0.075948
[12:24:25.104] TRAIN: iteration 28151 : loss : 0.064659, loss_ce: 0.006942, loss_dice: 0.122376
[12:24:25.312] TRAIN: iteration 28152 : loss : 0.029687, loss_ce: 0.000962, loss_dice: 0.058412
[12:24:25.521] TRAIN: iteration 28153 : loss : 0.250855, loss_ce: 0.001627, loss_dice: 0.500083
[12:24:25.731] TRAIN: iteration 28154 : loss : 0.048986, loss_ce: 0.004235, loss_dice: 0.093737
[12:24:25.941] TRAIN: iteration 28155 : loss : 0.250927, loss_ce: 0.001743, loss_dice: 0.500111
[12:24:26.570] TRAIN: iteration 28156 : loss : 0.095082, loss_ce: 0.001492, loss_dice: 0.188671
[12:24:26.779] TRAIN: iteration 28157 : loss : 0.096076, loss_ce: 0.003587, loss_dice: 0.188565
[12:24:26.987] TRAIN: iteration 28158 : loss : 0.131660, loss_ce: 0.001436, loss_dice: 0.261884
[12:24:27.195] TRAIN: iteration 28159 : loss : 0.116100, loss_ce: 0.001417, loss_dice: 0.230782
[12:24:27.403] TRAIN: iteration 28160 : loss : 0.126984, loss_ce: 0.003488, loss_dice: 0.250479
[12:24:27.404] NaN or Inf found in input tensor.
[12:24:27.620] TRAIN: iteration 28161 : loss : 0.117981, loss_ce: 0.012559, loss_dice: 0.223404
[12:24:27.828] TRAIN: iteration 28162 : loss : 0.047542, loss_ce: 0.000945, loss_dice: 0.094138
[12:24:28.038] TRAIN: iteration 28163 : loss : 0.246653, loss_ce: 0.001707, loss_dice: 0.491599
[12:24:29.763] TRAIN: iteration 28164 : loss : 0.085370, loss_ce: 0.002645, loss_dice: 0.168095
[12:24:29.971] TRAIN: iteration 28165 : loss : 0.028951, loss_ce: 0.001026, loss_dice: 0.056877
[12:24:30.183] TRAIN: iteration 28166 : loss : 0.112528, loss_ce: 0.002412, loss_dice: 0.222644
[12:24:30.396] TRAIN: iteration 28167 : loss : 0.252671, loss_ce: 0.005885, loss_dice: 0.499456
[12:24:30.605] TRAIN: iteration 28168 : loss : 0.107308, loss_ce: 0.009389, loss_dice: 0.205226
[12:24:30.824] TRAIN: iteration 28169 : loss : 0.051123, loss_ce: 0.002775, loss_dice: 0.099471
[12:24:31.031] TRAIN: iteration 28170 : loss : 0.250595, loss_ce: 0.001131, loss_dice: 0.500059
[12:24:31.238] TRAIN: iteration 28171 : loss : 0.117253, loss_ce: 0.002060, loss_dice: 0.232445
[12:24:31.962] TRAIN: iteration 28172 : loss : 0.086238, loss_ce: 0.004038, loss_dice: 0.168437
[12:24:32.169] TRAIN: iteration 28173 : loss : 0.116570, loss_ce: 0.007511, loss_dice: 0.225628
[12:24:32.378] TRAIN: iteration 28174 : loss : 0.039058, loss_ce: 0.000648, loss_dice: 0.077469
[12:24:32.588] TRAIN: iteration 28175 : loss : 0.199879, loss_ce: 0.002805, loss_dice: 0.396954
[12:24:32.795] TRAIN: iteration 28176 : loss : 0.086512, loss_ce: 0.010879, loss_dice: 0.162146
[12:24:33.007] TRAIN: iteration 28177 : loss : 0.239577, loss_ce: 0.008065, loss_dice: 0.471089
[12:24:33.217] TRAIN: iteration 28178 : loss : 0.098685, loss_ce: 0.001373, loss_dice: 0.195997
[12:24:33.427] TRAIN: iteration 28179 : loss : 0.251908, loss_ce: 0.004959, loss_dice: 0.498856
[12:24:33.643] TRAIN: iteration 28180 : loss : 0.063885, loss_ce: 0.002991, loss_dice: 0.124779
[12:24:33.883] TRAIN: iteration 28181 : loss : 0.062891, loss_ce: 0.003084, loss_dice: 0.122699
[12:24:34.091] TRAIN: iteration 28182 : loss : 0.089944, loss_ce: 0.001384, loss_dice: 0.178504
[12:24:34.307] TRAIN: iteration 28183 : loss : 0.135531, loss_ce: 0.011175, loss_dice: 0.259887
[12:24:34.520] TRAIN: iteration 28184 : loss : 0.250480, loss_ce: 0.000924, loss_dice: 0.500036
[12:24:34.728] TRAIN: iteration 28185 : loss : 0.208037, loss_ce: 0.002708, loss_dice: 0.413366
[12:24:34.938] TRAIN: iteration 28186 : loss : 0.059139, loss_ce: 0.007437, loss_dice: 0.110840
[12:24:35.146] TRAIN: iteration 28187 : loss : 0.030040, loss_ce: 0.001453, loss_dice: 0.058628
[12:24:35.362] TRAIN: iteration 28188 : loss : 0.108248, loss_ce: 0.001124, loss_dice: 0.215371
[12:24:35.570] TRAIN: iteration 28189 : loss : 0.076199, loss_ce: 0.003207, loss_dice: 0.149192
[12:24:35.781] TRAIN: iteration 28190 : loss : 0.216903, loss_ce: 0.001939, loss_dice: 0.431868
[12:24:35.995] TRAIN: iteration 28191 : loss : 0.065449, loss_ce: 0.001869, loss_dice: 0.129029
[12:24:36.204] TRAIN: iteration 28192 : loss : 0.098503, loss_ce: 0.013048, loss_dice: 0.183958
[12:24:36.413] TRAIN: iteration 28193 : loss : 0.161742, loss_ce: 0.002633, loss_dice: 0.320851
[12:24:36.629] TRAIN: iteration 28194 : loss : 0.034856, loss_ce: 0.001067, loss_dice: 0.068644
[12:24:36.845] TRAIN: iteration 28195 : loss : 0.250977, loss_ce: 0.001991, loss_dice: 0.499963
[12:24:37.052] TRAIN: iteration 28196 : loss : 0.027113, loss_ce: 0.000724, loss_dice: 0.053503
[12:24:37.261] TRAIN: iteration 28197 : loss : 0.170586, loss_ce: 0.006050, loss_dice: 0.335121
[12:24:37.476] TRAIN: iteration 28198 : loss : 0.251189, loss_ce: 0.002931, loss_dice: 0.499447
[12:24:37.684] TRAIN: iteration 28199 : loss : 0.225056, loss_ce: 0.001013, loss_dice: 0.449099
[12:24:37.891] TRAIN: iteration 28200 : loss : 0.239051, loss_ce: 0.001795, loss_dice: 0.476307
[12:24:38.138] TRAIN: iteration 28201 : loss : 0.145620, loss_ce: 0.008193, loss_dice: 0.283048
[12:24:38.354] TRAIN: iteration 28202 : loss : 0.098893, loss_ce: 0.003123, loss_dice: 0.194663
[12:24:38.563] TRAIN: iteration 28203 : loss : 0.068158, loss_ce: 0.007080, loss_dice: 0.129235
[12:24:38.773] TRAIN: iteration 28204 : loss : 0.034726, loss_ce: 0.004975, loss_dice: 0.064476
[12:24:38.981] TRAIN: iteration 28205 : loss : 0.227789, loss_ce: 0.005193, loss_dice: 0.450385
[12:24:39.189] TRAIN: iteration 28206 : loss : 0.044885, loss_ce: 0.001696, loss_dice: 0.088073
[12:24:40.224] TRAIN: iteration 28207 : loss : 0.066183, loss_ce: 0.005730, loss_dice: 0.126636
[12:24:40.438] TRAIN: iteration 28208 : loss : 0.048145, loss_ce: 0.001487, loss_dice: 0.094804
[12:24:40.647] TRAIN: iteration 28209 : loss : 0.028236, loss_ce: 0.001153, loss_dice: 0.055319
[12:24:40.862] TRAIN: iteration 28210 : loss : 0.041023, loss_ce: 0.000639, loss_dice: 0.081407
[12:24:41.071] TRAIN: iteration 28211 : loss : 0.060620, loss_ce: 0.002357, loss_dice: 0.118883
[12:24:41.281] TRAIN: iteration 28212 : loss : 0.041412, loss_ce: 0.004924, loss_dice: 0.077900
[12:24:41.493] TRAIN: iteration 28213 : loss : 0.058856, loss_ce: 0.000820, loss_dice: 0.116893
[12:24:41.709] TRAIN: iteration 28214 : loss : 0.084604, loss_ce: 0.001189, loss_dice: 0.168018
[12:24:41.926] TRAIN: iteration 28215 : loss : 0.168640, loss_ce: 0.003226, loss_dice: 0.334053
[12:24:42.135] TRAIN: iteration 28216 : loss : 0.105250, loss_ce: 0.002295, loss_dice: 0.208205
[12:24:42.346] TRAIN: iteration 28217 : loss : 0.250387, loss_ce: 0.000743, loss_dice: 0.500031
[12:24:42.554] TRAIN: iteration 28218 : loss : 0.052617, loss_ce: 0.001125, loss_dice: 0.104109
[12:24:42.762] TRAIN: iteration 28219 : loss : 0.250860, loss_ce: 0.001611, loss_dice: 0.500109
[12:24:43.020] TRAIN: iteration 28220 : loss : 0.204839, loss_ce: 0.003817, loss_dice: 0.405861
[12:24:43.254] TRAIN: iteration 28221 : loss : 0.250306, loss_ce: 0.000587, loss_dice: 0.500025
[12:24:43.462] TRAIN: iteration 28222 : loss : 0.159144, loss_ce: 0.002985, loss_dice: 0.315303
[12:24:43.670] TRAIN: iteration 28223 : loss : 0.250522, loss_ce: 0.001344, loss_dice: 0.499700
[12:24:43.883] TRAIN: iteration 28224 : loss : 0.153673, loss_ce: 0.004169, loss_dice: 0.303176
[12:24:44.092] TRAIN: iteration 28225 : loss : 0.107805, loss_ce: 0.008570, loss_dice: 0.207041
[12:24:44.300] TRAIN: iteration 28226 : loss : 0.141379, loss_ce: 0.002022, loss_dice: 0.280736
[12:24:44.508] TRAIN: iteration 28227 : loss : 0.023239, loss_ce: 0.001335, loss_dice: 0.045143
[12:24:44.719] TRAIN: iteration 28228 : loss : 0.092282, loss_ce: 0.001308, loss_dice: 0.183256
[12:24:44.929] TRAIN: iteration 28229 : loss : 0.250589, loss_ce: 0.001120, loss_dice: 0.500059
[12:24:45.138] TRAIN: iteration 28230 : loss : 0.073665, loss_ce: 0.006254, loss_dice: 0.141077
[12:24:45.351] TRAIN: iteration 28231 : loss : 0.081079, loss_ce: 0.001552, loss_dice: 0.160606
[12:24:45.559] TRAIN: iteration 28232 : loss : 0.233016, loss_ce: 0.006386, loss_dice: 0.459645
[12:24:45.773] TRAIN: iteration 28233 : loss : 0.240414, loss_ce: 0.001496, loss_dice: 0.479332
[12:24:45.983] TRAIN: iteration 28234 : loss : 0.033846, loss_ce: 0.005649, loss_dice: 0.062042
[12:24:46.194] TRAIN: iteration 28235 : loss : 0.067677, loss_ce: 0.002430, loss_dice: 0.132923
[12:24:46.404] TRAIN: iteration 28236 : loss : 0.186575, loss_ce: 0.004040, loss_dice: 0.369110
[12:24:46.613] TRAIN: iteration 28237 : loss : 0.075476, loss_ce: 0.004391, loss_dice: 0.146561
[12:24:46.821] TRAIN: iteration 28238 : loss : 0.126352, loss_ce: 0.004317, loss_dice: 0.248388
[12:24:47.032] TRAIN: iteration 28239 : loss : 0.250657, loss_ce: 0.001243, loss_dice: 0.500071
[12:24:47.242] TRAIN: iteration 28240 : loss : 0.207370, loss_ce: 0.001991, loss_dice: 0.412750
[12:24:47.484] TRAIN: iteration 28241 : loss : 0.175522, loss_ce: 0.001925, loss_dice: 0.349119
[12:24:47.693] TRAIN: iteration 28242 : loss : 0.133342, loss_ce: 0.003044, loss_dice: 0.263640
[12:24:47.900] TRAIN: iteration 28243 : loss : 0.014534, loss_ce: 0.001516, loss_dice: 0.027552
[12:24:48.108] TRAIN: iteration 28244 : loss : 0.044604, loss_ce: 0.005294, loss_dice: 0.083913
[12:24:48.322] TRAIN: iteration 28245 : loss : 0.114432, loss_ce: 0.003403, loss_dice: 0.225462
[12:24:48.530] TRAIN: iteration 28246 : loss : 0.045143, loss_ce: 0.003912, loss_dice: 0.086374
[12:24:48.745] TRAIN: iteration 28247 : loss : 0.199458, loss_ce: 0.001592, loss_dice: 0.397324
[12:24:48.957] TRAIN: iteration 28248 : loss : 0.081994, loss_ce: 0.003258, loss_dice: 0.160730
[12:24:49.169] TRAIN: iteration 28249 : loss : 0.212840, loss_ce: 0.001723, loss_dice: 0.423957
[12:24:50.321] TRAIN: iteration 28250 : loss : 0.095363, loss_ce: 0.003133, loss_dice: 0.187593
[12:24:50.531] TRAIN: iteration 28251 : loss : 0.126832, loss_ce: 0.002556, loss_dice: 0.251108
[12:24:50.747] TRAIN: iteration 28252 : loss : 0.122545, loss_ce: 0.003645, loss_dice: 0.241445
[12:24:50.956] TRAIN: iteration 28253 : loss : 0.250764, loss_ce: 0.001435, loss_dice: 0.500093
[12:24:51.165] TRAIN: iteration 28254 : loss : 0.251323, loss_ce: 0.002478, loss_dice: 0.500168
[12:24:51.375] TRAIN: iteration 28255 : loss : 0.251262, loss_ce: 0.002355, loss_dice: 0.500168
[12:24:51.585] TRAIN: iteration 28256 : loss : 0.065127, loss_ce: 0.000963, loss_dice: 0.129291
[12:24:51.800] TRAIN: iteration 28257 : loss : 0.082023, loss_ce: 0.003450, loss_dice: 0.160595
[12:24:52.008] TRAIN: iteration 28258 : loss : 0.250739, loss_ce: 0.001398, loss_dice: 0.500080
[12:24:52.216] TRAIN: iteration 28259 : loss : 0.047734, loss_ce: 0.002140, loss_dice: 0.093329
[12:24:52.424] TRAIN: iteration 28260 : loss : 0.219621, loss_ce: 0.001490, loss_dice: 0.437753
[12:24:52.667] TRAIN: iteration 28261 : loss : 0.019500, loss_ce: 0.001539, loss_dice: 0.037462
[12:24:52.875] TRAIN: iteration 28262 : loss : 0.250621, loss_ce: 0.001178, loss_dice: 0.500065
[12:24:53.122] TRAIN: iteration 28263 : loss : 0.094334, loss_ce: 0.002731, loss_dice: 0.185937
[12:24:53.338] TRAIN: iteration 28264 : loss : 0.064202, loss_ce: 0.000778, loss_dice: 0.127627
[12:24:53.551] TRAIN: iteration 28265 : loss : 0.067660, loss_ce: 0.000862, loss_dice: 0.134458
[12:24:53.762] TRAIN: iteration 28266 : loss : 0.042439, loss_ce: 0.000870, loss_dice: 0.084009
[12:24:53.969] TRAIN: iteration 28267 : loss : 0.018696, loss_ce: 0.000452, loss_dice: 0.036941
[12:24:54.177] TRAIN: iteration 28268 : loss : 0.056776, loss_ce: 0.000908, loss_dice: 0.112644
[12:24:54.385] TRAIN: iteration 28269 : loss : 0.046220, loss_ce: 0.002668, loss_dice: 0.089773
[12:24:54.596] TRAIN: iteration 28270 : loss : 0.060050, loss_ce: 0.001186, loss_dice: 0.118914
[12:24:54.803] TRAIN: iteration 28271 : loss : 0.036338, loss_ce: 0.002244, loss_dice: 0.070432
[12:24:55.017] TRAIN: iteration 28272 : loss : 0.056769, loss_ce: 0.001901, loss_dice: 0.111637
[12:24:55.227] TRAIN: iteration 28273 : loss : 0.038544, loss_ce: 0.001261, loss_dice: 0.075827
[12:24:55.437] TRAIN: iteration 28274 : loss : 0.068850, loss_ce: 0.000670, loss_dice: 0.137030
[12:24:55.645] TRAIN: iteration 28275 : loss : 0.037759, loss_ce: 0.000867, loss_dice: 0.074652
[12:24:55.853] TRAIN: iteration 28276 : loss : 0.104762, loss_ce: 0.015350, loss_dice: 0.194174
[12:24:56.067] TRAIN: iteration 28277 : loss : 0.093174, loss_ce: 0.001641, loss_dice: 0.184707
[12:24:56.277] TRAIN: iteration 28278 : loss : 0.130354, loss_ce: 0.015336, loss_dice: 0.245372
[12:24:56.486] TRAIN: iteration 28279 : loss : 0.251030, loss_ce: 0.001912, loss_dice: 0.500149
[12:24:56.695] TRAIN: iteration 28280 : loss : 0.250150, loss_ce: 0.004522, loss_dice: 0.495778
[12:24:56.940] TRAIN: iteration 28281 : loss : 0.110769, loss_ce: 0.002763, loss_dice: 0.218775
[12:24:57.150] TRAIN: iteration 28282 : loss : 0.089178, loss_ce: 0.002054, loss_dice: 0.176303
[12:24:57.359] TRAIN: iteration 28283 : loss : 0.078708, loss_ce: 0.002628, loss_dice: 0.154788
[12:24:57.566] TRAIN: iteration 28284 : loss : 0.087385, loss_ce: 0.002587, loss_dice: 0.172183
[12:24:57.774] TRAIN: iteration 28285 : loss : 0.034272, loss_ce: 0.002241, loss_dice: 0.066303
[12:24:57.980] TRAIN: iteration 28286 : loss : 0.053874, loss_ce: 0.001917, loss_dice: 0.105831
[12:24:58.188] TRAIN: iteration 28287 : loss : 0.048747, loss_ce: 0.004401, loss_dice: 0.093092
[12:24:58.395] TRAIN: iteration 28288 : loss : 0.196865, loss_ce: 0.001300, loss_dice: 0.392429
[12:24:58.603] TRAIN: iteration 28289 : loss : 0.033921, loss_ce: 0.000654, loss_dice: 0.067187
[12:24:58.812] TRAIN: iteration 28290 : loss : 0.058481, loss_ce: 0.002131, loss_dice: 0.114831
[12:24:59.021] TRAIN: iteration 28291 : loss : 0.128114, loss_ce: 0.004300, loss_dice: 0.251927
[12:24:59.229] TRAIN: iteration 28292 : loss : 0.043836, loss_ce: 0.002333, loss_dice: 0.085340
[12:24:59.436] TRAIN: iteration 28293 : loss : 0.057196, loss_ce: 0.002380, loss_dice: 0.112012
[12:24:59.643] TRAIN: iteration 28294 : loss : 0.104641, loss_ce: 0.000970, loss_dice: 0.208313
[12:24:59.852] TRAIN: iteration 28295 : loss : 0.233880, loss_ce: 0.002832, loss_dice: 0.464927
[12:25:00.061] TRAIN: iteration 28296 : loss : 0.250911, loss_ce: 0.001704, loss_dice: 0.500117
[12:25:00.270] TRAIN: iteration 28297 : loss : 0.251014, loss_ce: 0.001897, loss_dice: 0.500130
[12:25:00.478] TRAIN: iteration 28298 : loss : 0.064030, loss_ce: 0.003759, loss_dice: 0.124301
[12:25:00.686] TRAIN: iteration 28299 : loss : 0.065131, loss_ce: 0.008260, loss_dice: 0.122003
[12:25:00.894] TRAIN: iteration 28300 : loss : 0.071684, loss_ce: 0.004970, loss_dice: 0.138398
[12:25:01.133] TRAIN: iteration 28301 : loss : 0.146020, loss_ce: 0.002788, loss_dice: 0.289251
[12:25:01.342] TRAIN: iteration 28302 : loss : 0.039382, loss_ce: 0.003538, loss_dice: 0.075226
[12:25:01.550] TRAIN: iteration 28303 : loss : 0.070972, loss_ce: 0.001823, loss_dice: 0.140120
[12:25:01.758] TRAIN: iteration 28304 : loss : 0.032989, loss_ce: 0.001173, loss_dice: 0.064804
[12:25:01.967] TRAIN: iteration 28305 : loss : 0.158704, loss_ce: 0.001997, loss_dice: 0.315411
[12:25:02.178] TRAIN: iteration 28306 : loss : 0.164266, loss_ce: 0.002372, loss_dice: 0.326159
[12:25:02.386] TRAIN: iteration 28307 : loss : 0.089571, loss_ce: 0.002473, loss_dice: 0.176669
[12:25:02.593] TRAIN: iteration 28308 : loss : 0.059501, loss_ce: 0.005950, loss_dice: 0.113053
[12:25:02.801] TRAIN: iteration 28309 : loss : 0.052949, loss_ce: 0.001568, loss_dice: 0.104329
[12:25:03.011] TRAIN: iteration 28310 : loss : 0.034017, loss_ce: 0.005494, loss_dice: 0.062540
[12:25:03.219] TRAIN: iteration 28311 : loss : 0.207421, loss_ce: 0.000640, loss_dice: 0.414202
[12:25:03.427] TRAIN: iteration 28312 : loss : 0.071284, loss_ce: 0.004752, loss_dice: 0.137817
[12:25:03.634] TRAIN: iteration 28313 : loss : 0.085333, loss_ce: 0.003345, loss_dice: 0.167320
[12:25:03.842] TRAIN: iteration 28314 : loss : 0.054064, loss_ce: 0.002233, loss_dice: 0.105895
[12:25:04.050] TRAIN: iteration 28315 : loss : 0.091456, loss_ce: 0.001394, loss_dice: 0.181518
[12:25:04.258] TRAIN: iteration 28316 : loss : 0.034956, loss_ce: 0.001317, loss_dice: 0.068594
[12:25:04.465] TRAIN: iteration 28317 : loss : 0.053892, loss_ce: 0.004285, loss_dice: 0.103499
[12:25:04.674] TRAIN: iteration 28318 : loss : 0.253341, loss_ce: 0.010308, loss_dice: 0.496374
[12:25:04.884] TRAIN: iteration 28319 : loss : 0.025543, loss_ce: 0.000677, loss_dice: 0.050408
[12:25:05.092] TRAIN: iteration 28320 : loss : 0.157195, loss_ce: 0.023391, loss_dice: 0.290999
[12:25:05.338] TRAIN: iteration 28321 : loss : 0.234003, loss_ce: 0.010006, loss_dice: 0.457999
[12:25:05.552] TRAIN: iteration 28322 : loss : 0.086181, loss_ce: 0.008054, loss_dice: 0.164308
[12:25:05.763] TRAIN: iteration 28323 : loss : 0.085374, loss_ce: 0.001924, loss_dice: 0.168823
[12:25:05.971] TRAIN: iteration 28324 : loss : 0.033110, loss_ce: 0.006632, loss_dice: 0.059588
[12:25:06.181] TRAIN: iteration 28325 : loss : 0.211231, loss_ce: 0.000793, loss_dice: 0.421670
[12:25:06.395] TRAIN: iteration 28326 : loss : 0.167688, loss_ce: 0.002689, loss_dice: 0.332687
[12:25:06.610] TRAIN: iteration 28327 : loss : 0.254475, loss_ce: 0.009293, loss_dice: 0.499657
[12:25:06.827] TRAIN: iteration 28328 : loss : 0.109376, loss_ce: 0.001218, loss_dice: 0.217535
[12:25:07.041] TRAIN: iteration 28329 : loss : 0.116605, loss_ce: 0.001401, loss_dice: 0.231810
[12:25:07.260] TRAIN: iteration 28330 : loss : 0.032087, loss_ce: 0.001066, loss_dice: 0.063109
[12:25:07.470] TRAIN: iteration 28331 : loss : 0.087242, loss_ce: 0.002742, loss_dice: 0.171742
[12:25:07.681] TRAIN: iteration 28332 : loss : 0.041577, loss_ce: 0.002351, loss_dice: 0.080802
[12:25:07.890] TRAIN: iteration 28333 : loss : 0.022916, loss_ce: 0.001735, loss_dice: 0.044097
[12:25:08.098] TRAIN: iteration 28334 : loss : 0.065454, loss_ce: 0.002735, loss_dice: 0.128172
[12:25:08.311] TRAIN: iteration 28335 : loss : 0.150830, loss_ce: 0.010875, loss_dice: 0.290785
[12:25:08.527] TRAIN: iteration 28336 : loss : 0.054776, loss_ce: 0.000866, loss_dice: 0.108685
[12:25:08.745] TRAIN: iteration 28337 : loss : 0.050370, loss_ce: 0.003338, loss_dice: 0.097403
[12:25:08.953] TRAIN: iteration 28338 : loss : 0.155286, loss_ce: 0.002889, loss_dice: 0.307682
[12:25:09.168] TRAIN: iteration 28339 : loss : 0.232342, loss_ce: 0.001178, loss_dice: 0.463506
[12:25:09.376] TRAIN: iteration 28340 : loss : 0.250907, loss_ce: 0.001702, loss_dice: 0.500112
[12:25:09.618] TRAIN: iteration 28341 : loss : 0.032715, loss_ce: 0.002547, loss_dice: 0.062883
[12:25:10.132] TRAIN: iteration 28342 : loss : 0.082238, loss_ce: 0.003174, loss_dice: 0.161301
[12:25:10.345] TRAIN: iteration 28343 : loss : 0.089327, loss_ce: 0.006363, loss_dice: 0.172291
[12:25:10.555] TRAIN: iteration 28344 : loss : 0.058503, loss_ce: 0.002543, loss_dice: 0.114464
[12:25:10.763] TRAIN: iteration 28345 : loss : 0.035656, loss_ce: 0.004440, loss_dice: 0.066873
[12:25:10.971] TRAIN: iteration 28346 : loss : 0.028331, loss_ce: 0.002375, loss_dice: 0.054287
[12:25:11.178] TRAIN: iteration 28347 : loss : 0.058012, loss_ce: 0.006894, loss_dice: 0.109130
[12:25:11.386] TRAIN: iteration 28348 : loss : 0.078574, loss_ce: 0.008631, loss_dice: 0.148517
[12:25:11.602] TRAIN: iteration 28349 : loss : 0.024895, loss_ce: 0.001972, loss_dice: 0.047818
[12:25:11.816] TRAIN: iteration 28350 : loss : 0.068858, loss_ce: 0.006769, loss_dice: 0.130947
[12:25:12.025] TRAIN: iteration 28351 : loss : 0.085276, loss_ce: 0.007194, loss_dice: 0.163358
[12:25:12.235] TRAIN: iteration 28352 : loss : 0.207873, loss_ce: 0.001488, loss_dice: 0.414258
[12:25:12.449] TRAIN: iteration 28353 : loss : 0.126950, loss_ce: 0.002691, loss_dice: 0.251210
[12:25:12.659] TRAIN: iteration 28354 : loss : 0.021806, loss_ce: 0.002349, loss_dice: 0.041264
[12:25:12.868] TRAIN: iteration 28355 : loss : 0.052584, loss_ce: 0.001287, loss_dice: 0.103880
[12:25:13.076] TRAIN: iteration 28356 : loss : 0.113978, loss_ce: 0.000985, loss_dice: 0.226971
[12:25:13.291] TRAIN: iteration 28357 : loss : 0.115332, loss_ce: 0.000613, loss_dice: 0.230052
[12:25:13.499] TRAIN: iteration 28358 : loss : 0.247154, loss_ce: 0.000457, loss_dice: 0.493851
[12:25:13.706] TRAIN: iteration 28359 : loss : 0.023193, loss_ce: 0.003001, loss_dice: 0.043386
[12:25:14.117] TRAIN: iteration 28360 : loss : 0.250157, loss_ce: 0.000310, loss_dice: 0.500003
[12:25:14.132] NaN or Inf found in input tensor.
[12:25:14.363] TRAIN: iteration 28361 : loss : 0.016596, loss_ce: 0.000721, loss_dice: 0.032472
[12:25:14.570] TRAIN: iteration 28362 : loss : 0.108173, loss_ce: 0.005111, loss_dice: 0.211236
[12:25:14.777] TRAIN: iteration 28363 : loss : 0.096131, loss_ce: 0.001064, loss_dice: 0.191199
[12:25:14.985] TRAIN: iteration 28364 : loss : 0.244390, loss_ce: 0.002270, loss_dice: 0.486511
[12:25:15.193] TRAIN: iteration 28365 : loss : 0.240811, loss_ce: 0.000434, loss_dice: 0.481188
[12:25:15.407] TRAIN: iteration 28366 : loss : 0.099169, loss_ce: 0.003441, loss_dice: 0.194896
[12:25:15.614] TRAIN: iteration 28367 : loss : 0.027036, loss_ce: 0.001065, loss_dice: 0.053006
[12:25:15.823] TRAIN: iteration 28368 : loss : 0.058454, loss_ce: 0.001122, loss_dice: 0.115787
[12:25:16.041] TRAIN: iteration 28369 : loss : 0.250146, loss_ce: 0.000289, loss_dice: 0.500002
[12:25:16.249] TRAIN: iteration 28370 : loss : 0.244429, loss_ce: 0.000284, loss_dice: 0.488574
[12:25:16.457] TRAIN: iteration 28371 : loss : 0.253066, loss_ce: 0.009967, loss_dice: 0.496165
[12:25:16.671] TRAIN: iteration 28372 : loss : 0.048757, loss_ce: 0.002437, loss_dice: 0.095078
[12:25:16.880] TRAIN: iteration 28373 : loss : 0.059023, loss_ce: 0.001360, loss_dice: 0.116685
[12:25:17.089] TRAIN: iteration 28374 : loss : 0.250256, loss_ce: 0.000494, loss_dice: 0.500019
[12:25:17.300] TRAIN: iteration 28375 : loss : 0.224498, loss_ce: 0.030651, loss_dice: 0.418344
[12:25:17.508] TRAIN: iteration 28376 : loss : 0.188899, loss_ce: 0.002264, loss_dice: 0.375534
[12:25:17.731] TRAIN: iteration 28377 : loss : 0.134792, loss_ce: 0.011689, loss_dice: 0.257895
[12:25:18.763] TRAIN: iteration 28378 : loss : 0.131807, loss_ce: 0.004257, loss_dice: 0.259357
[12:25:18.980] TRAIN: iteration 28379 : loss : 0.100774, loss_ce: 0.004656, loss_dice: 0.196893
[12:25:19.189] TRAIN: iteration 28380 : loss : 0.107616, loss_ce: 0.003885, loss_dice: 0.211348
[12:25:19.423] TRAIN: iteration 28381 : loss : 0.083199, loss_ce: 0.001679, loss_dice: 0.164719
[12:25:19.632] TRAIN: iteration 28382 : loss : 0.051645, loss_ce: 0.002069, loss_dice: 0.101221
[12:25:19.843] TRAIN: iteration 28383 : loss : 0.050937, loss_ce: 0.003436, loss_dice: 0.098438
[12:25:20.053] TRAIN: iteration 28384 : loss : 0.100376, loss_ce: 0.007735, loss_dice: 0.193016
[12:25:20.260] TRAIN: iteration 28385 : loss : 0.156936, loss_ce: 0.011169, loss_dice: 0.302702
[12:25:21.085] TRAIN: iteration 28386 : loss : 0.074324, loss_ce: 0.003365, loss_dice: 0.145283
[12:25:21.295] TRAIN: iteration 28387 : loss : 0.103251, loss_ce: 0.001479, loss_dice: 0.205023
[12:25:21.518] TRAIN: iteration 28388 : loss : 0.039832, loss_ce: 0.002959, loss_dice: 0.076705
[12:25:21.737] TRAIN: iteration 28389 : loss : 0.116018, loss_ce: 0.006308, loss_dice: 0.225728
[12:25:21.946] TRAIN: iteration 28390 : loss : 0.250183, loss_ce: 0.000968, loss_dice: 0.499397
[12:25:22.154] TRAIN: iteration 28391 : loss : 0.064240, loss_ce: 0.003480, loss_dice: 0.125000
[12:25:22.374] TRAIN: iteration 28392 : loss : 0.037928, loss_ce: 0.001478, loss_dice: 0.074379
[12:25:22.581] TRAIN: iteration 28393 : loss : 0.055421, loss_ce: 0.004340, loss_dice: 0.106502
[12:25:22.789] TRAIN: iteration 28394 : loss : 0.055311, loss_ce: 0.005235, loss_dice: 0.105387
[12:25:23.001] TRAIN: iteration 28395 : loss : 0.031419, loss_ce: 0.003973, loss_dice: 0.058865
[12:25:23.211] TRAIN: iteration 28396 : loss : 0.069991, loss_ce: 0.006629, loss_dice: 0.133352
[12:25:23.419] TRAIN: iteration 28397 : loss : 0.181801, loss_ce: 0.001538, loss_dice: 0.362064
[12:25:23.628] TRAIN: iteration 28398 : loss : 0.068340, loss_ce: 0.003514, loss_dice: 0.133166
[12:25:23.836] TRAIN: iteration 28399 : loss : 0.103287, loss_ce: 0.005438, loss_dice: 0.201136
[12:25:24.044] TRAIN: iteration 28400 : loss : 0.032491, loss_ce: 0.001230, loss_dice: 0.063751
[12:25:24.281] TRAIN: iteration 28401 : loss : 0.052238, loss_ce: 0.003176, loss_dice: 0.101301
[12:25:24.490] TRAIN: iteration 28402 : loss : 0.042147, loss_ce: 0.003924, loss_dice: 0.080370
[12:25:24.701] TRAIN: iteration 28403 : loss : 0.023324, loss_ce: 0.003209, loss_dice: 0.043439
[12:25:24.960] TRAIN: iteration 28404 : loss : 0.161689, loss_ce: 0.001856, loss_dice: 0.321521
[12:25:25.992] TRAIN: iteration 28405 : loss : 0.055925, loss_ce: 0.007083, loss_dice: 0.104766
[12:25:26.202] TRAIN: iteration 28406 : loss : 0.069862, loss_ce: 0.001449, loss_dice: 0.138275
[12:25:26.413] TRAIN: iteration 28407 : loss : 0.101974, loss_ce: 0.003557, loss_dice: 0.200392
[12:25:26.622] TRAIN: iteration 28408 : loss : 0.173721, loss_ce: 0.004150, loss_dice: 0.343292
[12:25:26.830] TRAIN: iteration 28409 : loss : 0.094311, loss_ce: 0.002945, loss_dice: 0.185676
[12:25:27.040] TRAIN: iteration 28410 : loss : 0.111973, loss_ce: 0.001578, loss_dice: 0.222367
[12:25:27.248] TRAIN: iteration 28411 : loss : 0.080993, loss_ce: 0.004005, loss_dice: 0.157981
[12:25:27.466] TRAIN: iteration 28412 : loss : 0.142654, loss_ce: 0.003720, loss_dice: 0.281588
[12:25:27.677] TRAIN: iteration 28413 : loss : 0.250216, loss_ce: 0.000427, loss_dice: 0.500005
[12:25:27.885] TRAIN: iteration 28414 : loss : 0.248407, loss_ce: 0.003540, loss_dice: 0.493274
[12:25:28.093] TRAIN: iteration 28415 : loss : 0.053288, loss_ce: 0.007794, loss_dice: 0.098782
[12:25:28.300] TRAIN: iteration 28416 : loss : 0.046359, loss_ce: 0.005118, loss_dice: 0.087600
[12:25:28.508] TRAIN: iteration 28417 : loss : 0.075509, loss_ce: 0.005431, loss_dice: 0.145587
[12:25:28.747] TRAIN: iteration 28418 : loss : 0.250399, loss_ce: 0.000767, loss_dice: 0.500032
[12:25:28.956] TRAIN: iteration 28419 : loss : 0.116715, loss_ce: 0.003362, loss_dice: 0.230068
[12:25:29.164] TRAIN: iteration 28420 : loss : 0.140319, loss_ce: 0.003813, loss_dice: 0.276826
[12:25:29.403] TRAIN: iteration 28421 : loss : 0.106350, loss_ce: 0.007149, loss_dice: 0.205550
[12:25:29.613] TRAIN: iteration 28422 : loss : 0.102737, loss_ce: 0.002508, loss_dice: 0.202966
[12:25:29.820] TRAIN: iteration 28423 : loss : 0.040012, loss_ce: 0.001227, loss_dice: 0.078797
[12:25:30.030] TRAIN: iteration 28424 : loss : 0.073940, loss_ce: 0.001089, loss_dice: 0.146792
[12:25:30.241] TRAIN: iteration 28425 : loss : 0.091372, loss_ce: 0.005492, loss_dice: 0.177251
[12:25:30.878] TRAIN: iteration 28426 : loss : 0.131274, loss_ce: 0.049985, loss_dice: 0.212563
[12:25:31.093] TRAIN: iteration 28427 : loss : 0.250934, loss_ce: 0.001773, loss_dice: 0.500095
[12:25:31.300] TRAIN: iteration 28428 : loss : 0.250961, loss_ce: 0.001810, loss_dice: 0.500112
[12:25:31.511] TRAIN: iteration 28429 : loss : 0.190425, loss_ce: 0.001891, loss_dice: 0.378959
[12:25:31.719] TRAIN: iteration 28430 : loss : 0.067828, loss_ce: 0.004101, loss_dice: 0.131555
[12:25:31.928] TRAIN: iteration 28431 : loss : 0.150592, loss_ce: 0.006294, loss_dice: 0.294891
[12:25:32.137] TRAIN: iteration 28432 : loss : 0.090597, loss_ce: 0.004428, loss_dice: 0.176767
[12:25:32.354] TRAIN: iteration 28433 : loss : 0.046081, loss_ce: 0.001471, loss_dice: 0.090690
[12:25:32.565] TRAIN: iteration 28434 : loss : 0.147898, loss_ce: 0.003138, loss_dice: 0.292657
[12:25:32.775] TRAIN: iteration 28435 : loss : 0.119934, loss_ce: 0.005538, loss_dice: 0.234330
[12:25:32.982] TRAIN: iteration 28436 : loss : 0.246359, loss_ce: 0.006639, loss_dice: 0.486079
[12:25:33.191] TRAIN: iteration 28437 : loss : 0.082486, loss_ce: 0.004075, loss_dice: 0.160898
[12:25:33.398] TRAIN: iteration 28438 : loss : 0.074566, loss_ce: 0.003673, loss_dice: 0.145460
[12:25:33.605] TRAIN: iteration 28439 : loss : 0.056231, loss_ce: 0.005600, loss_dice: 0.106862
[12:25:33.813] TRAIN: iteration 28440 : loss : 0.071920, loss_ce: 0.002702, loss_dice: 0.141138
[12:25:34.131] TRAIN: iteration 28441 : loss : 0.251263, loss_ce: 0.002373, loss_dice: 0.500154
[12:25:34.338] TRAIN: iteration 28442 : loss : 0.069778, loss_ce: 0.001225, loss_dice: 0.138332
[12:25:34.547] TRAIN: iteration 28443 : loss : 0.025668, loss_ce: 0.001188, loss_dice: 0.050149
[12:25:34.757] TRAIN: iteration 28444 : loss : 0.250883, loss_ce: 0.001663, loss_dice: 0.500104
[12:25:34.966] TRAIN: iteration 28445 : loss : 0.096660, loss_ce: 0.002172, loss_dice: 0.191148
[12:25:35.176] TRAIN: iteration 28446 : loss : 0.186861, loss_ce: 0.000886, loss_dice: 0.372836
[12:25:35.383] TRAIN: iteration 28447 : loss : 0.111038, loss_ce: 0.012696, loss_dice: 0.209380
[12:25:35.592] TRAIN: iteration 28448 : loss : 0.254905, loss_ce: 0.020531, loss_dice: 0.489280
[12:25:35.799] TRAIN: iteration 28449 : loss : 0.251131, loss_ce: 0.002261, loss_dice: 0.500001
[12:25:36.009] TRAIN: iteration 28450 : loss : 0.192610, loss_ce: 0.001107, loss_dice: 0.384113
[12:25:36.219] TRAIN: iteration 28451 : loss : 0.096874, loss_ce: 0.002470, loss_dice: 0.191279
[12:25:36.427] TRAIN: iteration 28452 : loss : 0.198653, loss_ce: 0.005668, loss_dice: 0.391637
[12:25:36.639] TRAIN: iteration 28453 : loss : 0.155024, loss_ce: 0.004500, loss_dice: 0.305547
[12:25:36.846] TRAIN: iteration 28454 : loss : 0.038477, loss_ce: 0.002397, loss_dice: 0.074556
[12:25:37.054] TRAIN: iteration 28455 : loss : 0.045370, loss_ce: 0.006977, loss_dice: 0.083763
[12:25:37.267] TRAIN: iteration 28456 : loss : 0.054329, loss_ce: 0.003992, loss_dice: 0.104666
[12:25:37.484] TRAIN: iteration 28457 : loss : 0.250768, loss_ce: 0.001472, loss_dice: 0.500065
[12:25:37.691] TRAIN: iteration 28458 : loss : 0.182066, loss_ce: 0.003327, loss_dice: 0.360804
[12:25:38.643] TRAIN: iteration 28459 : loss : 0.079413, loss_ce: 0.006462, loss_dice: 0.152365
[12:25:38.852] TRAIN: iteration 28460 : loss : 0.031055, loss_ce: 0.002909, loss_dice: 0.059202
[12:25:39.070] TRAIN: iteration 28461 : loss : 0.070120, loss_ce: 0.003584, loss_dice: 0.136656
[12:25:39.280] TRAIN: iteration 28462 : loss : 0.038043, loss_ce: 0.002146, loss_dice: 0.073939
[12:25:39.514] TRAIN: iteration 28463 : loss : 0.063933, loss_ce: 0.005154, loss_dice: 0.122712
[12:25:39.722] TRAIN: iteration 28464 : loss : 0.075638, loss_ce: 0.005209, loss_dice: 0.146066
[12:25:39.933] TRAIN: iteration 28465 : loss : 0.252174, loss_ce: 0.004058, loss_dice: 0.500291
[12:25:40.143] TRAIN: iteration 28466 : loss : 0.146331, loss_ce: 0.001651, loss_dice: 0.291010
[12:25:40.352] TRAIN: iteration 28467 : loss : 0.224140, loss_ce: 0.003002, loss_dice: 0.445278
[12:25:40.562] TRAIN: iteration 28468 : loss : 0.251555, loss_ce: 0.002919, loss_dice: 0.500191
[12:25:40.770] TRAIN: iteration 28469 : loss : 0.200648, loss_ce: 0.004636, loss_dice: 0.396660
[12:25:40.977] TRAIN: iteration 28470 : loss : 0.250882, loss_ce: 0.004274, loss_dice: 0.497489
[12:25:41.187] TRAIN: iteration 28471 : loss : 0.066999, loss_ce: 0.001071, loss_dice: 0.132927
[12:25:41.400] TRAIN: iteration 28472 : loss : 0.099682, loss_ce: 0.002296, loss_dice: 0.197068
[12:25:41.608] TRAIN: iteration 28473 : loss : 0.054894, loss_ce: 0.002756, loss_dice: 0.107032
[12:25:41.825] TRAIN: iteration 28474 : loss : 0.153992, loss_ce: 0.002701, loss_dice: 0.305282
[12:25:42.042] TRAIN: iteration 28475 : loss : 0.094200, loss_ce: 0.007330, loss_dice: 0.181070
[12:25:42.253] TRAIN: iteration 28476 : loss : 0.061689, loss_ce: 0.004051, loss_dice: 0.119328
[12:25:42.461] TRAIN: iteration 28477 : loss : 0.029511, loss_ce: 0.002468, loss_dice: 0.056555
[12:25:42.682] TRAIN: iteration 28478 : loss : 0.023648, loss_ce: 0.000841, loss_dice: 0.046455
[12:25:42.890] TRAIN: iteration 28479 : loss : 0.023818, loss_ce: 0.000934, loss_dice: 0.046703
[12:25:43.105] TRAIN: iteration 28480 : loss : 0.116084, loss_ce: 0.003035, loss_dice: 0.229133
[12:25:43.363] TRAIN: iteration 28481 : loss : 0.236082, loss_ce: 0.002400, loss_dice: 0.469764
[12:25:43.574] TRAIN: iteration 28482 : loss : 0.038007, loss_ce: 0.003598, loss_dice: 0.072416
[12:25:43.786] TRAIN: iteration 28483 : loss : 0.080035, loss_ce: 0.001666, loss_dice: 0.158405
[12:25:43.997] TRAIN: iteration 28484 : loss : 0.069905, loss_ce: 0.012713, loss_dice: 0.127096
[12:25:44.211] TRAIN: iteration 28485 : loss : 0.189830, loss_ce: 0.005713, loss_dice: 0.373948
[12:25:44.427] TRAIN: iteration 28486 : loss : 0.055953, loss_ce: 0.008186, loss_dice: 0.103720
[12:25:44.634] TRAIN: iteration 28487 : loss : 0.063469, loss_ce: 0.002379, loss_dice: 0.124559
[12:25:44.841] TRAIN: iteration 28488 : loss : 0.054843, loss_ce: 0.005766, loss_dice: 0.103919
[12:25:45.049] TRAIN: iteration 28489 : loss : 0.210470, loss_ce: 0.001390, loss_dice: 0.419550
[12:25:45.258] TRAIN: iteration 28490 : loss : 0.033428, loss_ce: 0.001135, loss_dice: 0.065722
[12:25:45.466] TRAIN: iteration 28491 : loss : 0.065072, loss_ce: 0.001956, loss_dice: 0.128187
[12:25:45.686] TRAIN: iteration 28492 : loss : 0.047467, loss_ce: 0.002806, loss_dice: 0.092129
[12:25:45.893] TRAIN: iteration 28493 : loss : 0.095060, loss_ce: 0.003273, loss_dice: 0.186846
[12:25:46.101] TRAIN: iteration 28494 : loss : 0.036276, loss_ce: 0.004404, loss_dice: 0.068148
[12:25:46.309] TRAIN: iteration 28495 : loss : 0.115000, loss_ce: 0.004164, loss_dice: 0.225837
[12:25:46.523] TRAIN: iteration 28496 : loss : 0.249914, loss_ce: 0.002372, loss_dice: 0.497456
[12:25:46.735] TRAIN: iteration 28497 : loss : 0.066893, loss_ce: 0.003062, loss_dice: 0.130724
[12:25:46.942] TRAIN: iteration 28498 : loss : 0.111582, loss_ce: 0.002105, loss_dice: 0.221058
[12:25:47.157] TRAIN: iteration 28499 : loss : 0.118787, loss_ce: 0.004146, loss_dice: 0.233427
[12:25:47.369] TRAIN: iteration 28500 : loss : 0.198215, loss_ce: 0.002168, loss_dice: 0.394263
[12:25:48.238] TRAIN: iteration 28501 : loss : 0.252113, loss_ce: 0.003962, loss_dice: 0.500265
[12:25:48.445] TRAIN: iteration 28502 : loss : 0.237657, loss_ce: 0.004288, loss_dice: 0.471025
[12:25:48.655] TRAIN: iteration 28503 : loss : 0.046839, loss_ce: 0.005101, loss_dice: 0.088577
[12:25:48.865] TRAIN: iteration 28504 : loss : 0.057392, loss_ce: 0.001131, loss_dice: 0.113652
[12:25:49.076] TRAIN: iteration 28505 : loss : 0.046521, loss_ce: 0.003110, loss_dice: 0.089933
[12:25:49.285] TRAIN: iteration 28506 : loss : 0.251532, loss_ce: 0.002882, loss_dice: 0.500182
[12:25:49.495] TRAIN: iteration 28507 : loss : 0.063828, loss_ce: 0.001483, loss_dice: 0.126172
[12:25:49.703] TRAIN: iteration 28508 : loss : 0.194934, loss_ce: 0.001924, loss_dice: 0.387943
[12:25:49.911] TRAIN: iteration 28509 : loss : 0.079770, loss_ce: 0.008177, loss_dice: 0.151362
[12:25:50.119] TRAIN: iteration 28510 : loss : 0.017141, loss_ce: 0.000754, loss_dice: 0.033528
[12:25:50.325] TRAIN: iteration 28511 : loss : 0.105122, loss_ce: 0.001991, loss_dice: 0.208252
[12:25:50.534] TRAIN: iteration 28512 : loss : 0.069508, loss_ce: 0.003707, loss_dice: 0.135309
[12:25:50.742] TRAIN: iteration 28513 : loss : 0.089519, loss_ce: 0.004410, loss_dice: 0.174627
[12:25:50.950] TRAIN: iteration 28514 : loss : 0.191103, loss_ce: 0.002688, loss_dice: 0.379519
[12:25:51.161] TRAIN: iteration 28515 : loss : 0.101771, loss_ce: 0.006032, loss_dice: 0.197510
[12:25:51.369] TRAIN: iteration 28516 : loss : 0.049453, loss_ce: 0.003780, loss_dice: 0.095126
[12:25:54.160] TRAIN: iteration 28517 : loss : 0.036568, loss_ce: 0.002363, loss_dice: 0.070772
[12:25:54.369] TRAIN: iteration 28518 : loss : 0.171531, loss_ce: 0.010610, loss_dice: 0.332452
[12:25:54.578] TRAIN: iteration 28519 : loss : 0.192428, loss_ce: 0.001296, loss_dice: 0.383559
[12:25:54.785] TRAIN: iteration 28520 : loss : 0.046135, loss_ce: 0.000936, loss_dice: 0.091333
[12:25:55.030] TRAIN: iteration 28521 : loss : 0.106700, loss_ce: 0.005247, loss_dice: 0.208152
[12:25:55.241] TRAIN: iteration 28522 : loss : 0.032615, loss_ce: 0.002000, loss_dice: 0.063231
[12:25:55.449] TRAIN: iteration 28523 : loss : 0.033225, loss_ce: 0.001437, loss_dice: 0.065014
[12:25:55.658] TRAIN: iteration 28524 : loss : 0.188663, loss_ce: 0.001907, loss_dice: 0.375419
[12:25:56.372] TRAIN: iteration 28525 : loss : 0.250189, loss_ce: 0.003665, loss_dice: 0.496714
[12:25:56.579] TRAIN: iteration 28526 : loss : 0.094749, loss_ce: 0.002260, loss_dice: 0.187238
[12:25:56.787] TRAIN: iteration 28527 : loss : 0.122886, loss_ce: 0.004688, loss_dice: 0.241084
[12:25:56.997] TRAIN: iteration 28528 : loss : 0.080406, loss_ce: 0.001994, loss_dice: 0.158817
[12:25:57.213] TRAIN: iteration 28529 : loss : 0.028279, loss_ce: 0.000512, loss_dice: 0.056045
[12:25:57.425] TRAIN: iteration 28530 : loss : 0.007991, loss_ce: 0.000927, loss_dice: 0.015055
[12:25:57.632] TRAIN: iteration 28531 : loss : 0.251631, loss_ce: 0.003062, loss_dice: 0.500200
[12:25:57.841] TRAIN: iteration 28532 : loss : 0.081210, loss_ce: 0.006661, loss_dice: 0.155759
[12:25:58.049] TRAIN: iteration 28533 : loss : 0.043347, loss_ce: 0.003825, loss_dice: 0.082870
[12:25:58.263] TRAIN: iteration 28534 : loss : 0.073768, loss_ce: 0.001783, loss_dice: 0.145753
[12:25:58.471] TRAIN: iteration 28535 : loss : 0.058403, loss_ce: 0.001443, loss_dice: 0.115364
[12:25:58.680] TRAIN: iteration 28536 : loss : 0.113758, loss_ce: 0.003240, loss_dice: 0.224276
[12:25:58.889] TRAIN: iteration 28537 : loss : 0.029577, loss_ce: 0.002729, loss_dice: 0.056426
[12:25:59.096] TRAIN: iteration 28538 : loss : 0.111735, loss_ce: 0.004887, loss_dice: 0.218583
[12:25:59.365] TRAIN: iteration 28539 : loss : 0.060845, loss_ce: 0.003288, loss_dice: 0.118403
[12:25:59.575] TRAIN: iteration 28540 : loss : 0.162301, loss_ce: 0.001299, loss_dice: 0.323303
[12:25:59.814] TRAIN: iteration 28541 : loss : 0.184662, loss_ce: 0.001501, loss_dice: 0.367823
[12:26:00.022] TRAIN: iteration 28542 : loss : 0.134015, loss_ce: 0.006054, loss_dice: 0.261976
[12:26:00.230] TRAIN: iteration 28543 : loss : 0.097503, loss_ce: 0.002846, loss_dice: 0.192159
[12:26:00.437] TRAIN: iteration 28544 : loss : 0.020755, loss_ce: 0.000714, loss_dice: 0.040797
[12:26:00.645] TRAIN: iteration 28545 : loss : 0.250496, loss_ce: 0.000945, loss_dice: 0.500047
[12:26:00.854] TRAIN: iteration 28546 : loss : 0.043468, loss_ce: 0.001149, loss_dice: 0.085786
[12:26:01.062] TRAIN: iteration 28547 : loss : 0.250424, loss_ce: 0.000815, loss_dice: 0.500033
[12:26:01.274] TRAIN: iteration 28548 : loss : 0.056105, loss_ce: 0.004705, loss_dice: 0.107505
[12:26:01.482] TRAIN: iteration 28549 : loss : 0.087516, loss_ce: 0.004556, loss_dice: 0.170476
[12:26:01.693] TRAIN: iteration 28550 : loss : 0.255196, loss_ce: 0.009663, loss_dice: 0.500730
[12:26:01.906] TRAIN: iteration 28551 : loss : 0.250966, loss_ce: 0.001819, loss_dice: 0.500112
[12:26:03.034] TRAIN: iteration 28552 : loss : 0.037677, loss_ce: 0.000480, loss_dice: 0.074873
[12:26:03.241] TRAIN: iteration 28553 : loss : 0.065740, loss_ce: 0.002026, loss_dice: 0.129454
[12:26:03.450] TRAIN: iteration 28554 : loss : 0.250432, loss_ce: 0.000839, loss_dice: 0.500026
[12:26:03.665] TRAIN: iteration 28555 : loss : 0.145706, loss_ce: 0.001093, loss_dice: 0.290319
[12:26:04.053] TRAIN: iteration 28556 : loss : 0.056431, loss_ce: 0.001755, loss_dice: 0.111107
[12:26:04.268] TRAIN: iteration 28557 : loss : 0.194600, loss_ce: 0.001606, loss_dice: 0.387595
[12:26:04.480] TRAIN: iteration 28558 : loss : 0.036485, loss_ce: 0.002949, loss_dice: 0.070022
[12:26:04.688] TRAIN: iteration 28559 : loss : 0.108880, loss_ce: 0.001222, loss_dice: 0.216538
[12:26:04.895] TRAIN: iteration 28560 : loss : 0.250313, loss_ce: 0.000615, loss_dice: 0.500012
[12:26:05.132] TRAIN: iteration 28561 : loss : 0.113244, loss_ce: 0.000635, loss_dice: 0.225852
[12:26:05.341] TRAIN: iteration 28562 : loss : 0.099947, loss_ce: 0.005134, loss_dice: 0.194760
[12:26:05.558] TRAIN: iteration 28563 : loss : 0.250455, loss_ce: 0.000877, loss_dice: 0.500032
[12:26:05.767] TRAIN: iteration 28564 : loss : 0.082378, loss_ce: 0.003374, loss_dice: 0.161381
[12:26:05.978] TRAIN: iteration 28565 : loss : 0.085126, loss_ce: 0.003384, loss_dice: 0.166869
[12:26:06.191] TRAIN: iteration 28566 : loss : 0.218893, loss_ce: 0.001008, loss_dice: 0.436778
[12:26:06.580] TRAIN: iteration 28567 : loss : 0.048016, loss_ce: 0.002433, loss_dice: 0.093599
[12:26:06.788] TRAIN: iteration 28568 : loss : 0.140273, loss_ce: 0.004429, loss_dice: 0.276117
[12:26:06.998] TRAIN: iteration 28569 : loss : 0.028706, loss_ce: 0.003133, loss_dice: 0.054279
[12:26:07.208] TRAIN: iteration 28570 : loss : 0.246960, loss_ce: 0.001795, loss_dice: 0.492124
[12:26:07.418] TRAIN: iteration 28571 : loss : 0.072785, loss_ce: 0.001232, loss_dice: 0.144338
[12:26:07.748] TRAIN: iteration 28572 : loss : 0.088919, loss_ce: 0.006051, loss_dice: 0.171787
[12:26:07.958] TRAIN: iteration 28573 : loss : 0.093131, loss_ce: 0.009567, loss_dice: 0.176694
[12:26:08.169] TRAIN: iteration 28574 : loss : 0.253057, loss_ce: 0.008400, loss_dice: 0.497715
[12:26:08.579] TRAIN: iteration 28575 : loss : 0.250350, loss_ce: 0.000669, loss_dice: 0.500031
[12:26:08.791] TRAIN: iteration 28576 : loss : 0.250646, loss_ce: 0.001222, loss_dice: 0.500070
[12:26:09.003] TRAIN: iteration 28577 : loss : 0.100010, loss_ce: 0.006481, loss_dice: 0.193539
[12:26:09.217] TRAIN: iteration 28578 : loss : 0.091206, loss_ce: 0.008039, loss_dice: 0.174373
[12:26:09.428] TRAIN: iteration 28579 : loss : 0.250803, loss_ce: 0.001506, loss_dice: 0.500099
[12:26:09.639] TRAIN: iteration 28580 : loss : 0.048912, loss_ce: 0.001001, loss_dice: 0.096824
[12:26:09.876] TRAIN: iteration 28581 : loss : 0.084170, loss_ce: 0.009722, loss_dice: 0.158618
[12:26:10.087] TRAIN: iteration 28582 : loss : 0.070999, loss_ce: 0.005285, loss_dice: 0.136713
[12:26:10.296] TRAIN: iteration 28583 : loss : 0.110278, loss_ce: 0.001624, loss_dice: 0.218932
[12:26:10.505] TRAIN: iteration 28584 : loss : 0.077723, loss_ce: 0.010627, loss_dice: 0.144819
[12:26:10.717] TRAIN: iteration 28585 : loss : 0.034346, loss_ce: 0.002035, loss_dice: 0.066657
[12:26:10.928] TRAIN: iteration 28586 : loss : 0.063809, loss_ce: 0.003749, loss_dice: 0.123869
[12:26:11.138] TRAIN: iteration 28587 : loss : 0.128363, loss_ce: 0.008748, loss_dice: 0.247978
[12:26:11.345] TRAIN: iteration 28588 : loss : 0.024048, loss_ce: 0.003135, loss_dice: 0.044961
[12:26:11.554] TRAIN: iteration 28589 : loss : 0.091628, loss_ce: 0.003044, loss_dice: 0.180212
[12:26:11.768] TRAIN: iteration 28590 : loss : 0.060226, loss_ce: 0.006400, loss_dice: 0.114052
[12:26:11.978] TRAIN: iteration 28591 : loss : 0.069426, loss_ce: 0.003044, loss_dice: 0.135808
[12:26:12.188] TRAIN: iteration 28592 : loss : 0.250637, loss_ce: 0.001231, loss_dice: 0.500043
[12:26:12.398] TRAIN: iteration 28593 : loss : 0.055679, loss_ce: 0.002570, loss_dice: 0.108788
[12:26:12.607] TRAIN: iteration 28594 : loss : 0.255678, loss_ce: 0.011533, loss_dice: 0.499822
[12:26:12.841] TRAIN: iteration 28595 : loss : 0.021907, loss_ce: 0.002726, loss_dice: 0.041088
[12:26:13.050] TRAIN: iteration 28596 : loss : 0.096576, loss_ce: 0.008220, loss_dice: 0.184931
[12:26:13.259] TRAIN: iteration 28597 : loss : 0.049625, loss_ce: 0.006751, loss_dice: 0.092499
[12:26:13.466] TRAIN: iteration 28598 : loss : 0.049309, loss_ce: 0.002979, loss_dice: 0.095640
[12:26:15.127] TRAIN: iteration 28599 : loss : 0.053737, loss_ce: 0.001423, loss_dice: 0.106052
[12:26:15.337] TRAIN: iteration 28600 : loss : 0.247825, loss_ce: 0.004336, loss_dice: 0.491315
[12:26:15.577] TRAIN: iteration 28601 : loss : 0.118747, loss_ce: 0.005235, loss_dice: 0.232259
[12:26:15.833] TRAIN: iteration 28602 : loss : 0.236661, loss_ce: 0.005205, loss_dice: 0.468117
[12:26:16.043] TRAIN: iteration 28603 : loss : 0.045040, loss_ce: 0.005750, loss_dice: 0.084331
[12:26:16.253] TRAIN: iteration 28604 : loss : 0.081249, loss_ce: 0.002402, loss_dice: 0.160096
[12:26:16.465] TRAIN: iteration 28605 : loss : 0.082545, loss_ce: 0.001991, loss_dice: 0.163098
[12:26:16.673] TRAIN: iteration 28606 : loss : 0.082282, loss_ce: 0.008554, loss_dice: 0.156011
[12:26:16.886] TRAIN: iteration 28607 : loss : 0.092293, loss_ce: 0.004891, loss_dice: 0.179696
[12:26:17.102] TRAIN: iteration 28608 : loss : 0.237819, loss_ce: 0.003674, loss_dice: 0.471964
[12:26:17.311] TRAIN: iteration 28609 : loss : 0.074805, loss_ce: 0.004443, loss_dice: 0.145168
[12:26:17.519] TRAIN: iteration 28610 : loss : 0.057371, loss_ce: 0.006608, loss_dice: 0.108134
[12:26:17.727] TRAIN: iteration 28611 : loss : 0.187531, loss_ce: 0.000985, loss_dice: 0.374077
[12:26:17.937] TRAIN: iteration 28612 : loss : 0.092695, loss_ce: 0.002863, loss_dice: 0.182528
[12:26:18.147] TRAIN: iteration 28613 : loss : 0.097925, loss_ce: 0.005320, loss_dice: 0.190530
[12:26:18.356] TRAIN: iteration 28614 : loss : 0.086637, loss_ce: 0.009223, loss_dice: 0.164051
[12:26:18.566] TRAIN: iteration 28615 : loss : 0.089313, loss_ce: 0.002064, loss_dice: 0.176563
[12:26:18.776] TRAIN: iteration 28616 : loss : 0.057648, loss_ce: 0.003995, loss_dice: 0.111300
[12:26:18.988] TRAIN: iteration 28617 : loss : 0.050788, loss_ce: 0.001094, loss_dice: 0.100483
[12:26:19.197] TRAIN: iteration 28618 : loss : 0.026667, loss_ce: 0.002623, loss_dice: 0.050712
[12:26:19.405] TRAIN: iteration 28619 : loss : 0.024113, loss_ce: 0.001070, loss_dice: 0.047157
[12:26:19.614] TRAIN: iteration 28620 : loss : 0.067784, loss_ce: 0.001926, loss_dice: 0.133642
[12:26:20.088] TRAIN: iteration 28621 : loss : 0.117920, loss_ce: 0.003597, loss_dice: 0.232243
[12:26:20.296] TRAIN: iteration 28622 : loss : 0.073032, loss_ce: 0.003905, loss_dice: 0.142160
[12:26:20.583] TRAIN: iteration 28623 : loss : 0.061731, loss_ce: 0.002581, loss_dice: 0.120882
[12:26:20.790] TRAIN: iteration 28624 : loss : 0.036529, loss_ce: 0.005044, loss_dice: 0.068015
[12:26:20.998] TRAIN: iteration 28625 : loss : 0.155449, loss_ce: 0.008094, loss_dice: 0.302804
[12:26:21.206] TRAIN: iteration 28626 : loss : 0.026625, loss_ce: 0.001811, loss_dice: 0.051440
[12:26:21.414] TRAIN: iteration 28627 : loss : 0.187643, loss_ce: 0.003366, loss_dice: 0.371920
[12:26:21.622] TRAIN: iteration 28628 : loss : 0.155275, loss_ce: 0.001560, loss_dice: 0.308991
[12:26:22.542] TRAIN: iteration 28629 : loss : 0.029524, loss_ce: 0.004482, loss_dice: 0.054566
[12:26:22.758] TRAIN: iteration 28630 : loss : 0.066080, loss_ce: 0.004082, loss_dice: 0.128079
[12:26:22.969] TRAIN: iteration 28631 : loss : 0.023707, loss_ce: 0.001867, loss_dice: 0.045547
[12:26:23.178] TRAIN: iteration 28632 : loss : 0.216962, loss_ce: 0.000657, loss_dice: 0.433268
[12:26:23.387] TRAIN: iteration 28633 : loss : 0.080483, loss_ce: 0.008239, loss_dice: 0.152727
[12:26:23.596] TRAIN: iteration 28634 : loss : 0.064220, loss_ce: 0.002737, loss_dice: 0.125703
[12:26:23.804] TRAIN: iteration 28635 : loss : 0.072140, loss_ce: 0.004038, loss_dice: 0.140242
[12:26:24.013] TRAIN: iteration 28636 : loss : 0.035385, loss_ce: 0.001302, loss_dice: 0.069468
[12:26:24.255] TRAIN: iteration 28637 : loss : 0.222589, loss_ce: 0.002422, loss_dice: 0.442757
[12:26:24.463] TRAIN: iteration 28638 : loss : 0.250125, loss_ce: 0.000251, loss_dice: 0.499999
[12:26:24.671] TRAIN: iteration 28639 : loss : 0.020182, loss_ce: 0.001185, loss_dice: 0.039179
[12:26:24.881] TRAIN: iteration 28640 : loss : 0.035806, loss_ce: 0.003642, loss_dice: 0.067969
[12:26:25.129] TRAIN: iteration 28641 : loss : 0.044609, loss_ce: 0.005411, loss_dice: 0.083807
[12:26:25.339] TRAIN: iteration 28642 : loss : 0.056161, loss_ce: 0.004015, loss_dice: 0.108308
[12:26:25.547] TRAIN: iteration 28643 : loss : 0.250178, loss_ce: 0.000352, loss_dice: 0.500003
[12:26:25.758] TRAIN: iteration 28644 : loss : 0.141585, loss_ce: 0.002341, loss_dice: 0.280830
[12:26:25.969] TRAIN: iteration 28645 : loss : 0.136364, loss_ce: 0.011945, loss_dice: 0.260784
[12:26:26.179] TRAIN: iteration 28646 : loss : 0.046856, loss_ce: 0.002079, loss_dice: 0.091633
[12:26:26.389] TRAIN: iteration 28647 : loss : 0.036406, loss_ce: 0.001176, loss_dice: 0.071635
[12:26:26.602] TRAIN: iteration 28648 : loss : 0.058664, loss_ce: 0.003351, loss_dice: 0.113977
[12:26:26.812] TRAIN: iteration 28649 : loss : 0.048169, loss_ce: 0.003541, loss_dice: 0.092796
[12:26:27.023] TRAIN: iteration 28650 : loss : 0.080400, loss_ce: 0.002772, loss_dice: 0.158027
[12:26:27.245] TRAIN: iteration 28651 : loss : 0.034587, loss_ce: 0.000595, loss_dice: 0.068579
[12:26:27.454] TRAIN: iteration 28652 : loss : 0.038499, loss_ce: 0.000587, loss_dice: 0.076412
[12:26:28.494] TRAIN: iteration 28653 : loss : 0.100727, loss_ce: 0.002209, loss_dice: 0.199246
[12:26:28.704] TRAIN: iteration 28654 : loss : 0.249454, loss_ce: 0.001231, loss_dice: 0.497678
[12:26:28.912] TRAIN: iteration 28655 : loss : 0.044091, loss_ce: 0.008412, loss_dice: 0.079770
[12:26:29.122] TRAIN: iteration 28656 : loss : 0.072034, loss_ce: 0.001784, loss_dice: 0.142284
[12:26:29.328] TRAIN: iteration 28657 : loss : 0.078694, loss_ce: 0.003356, loss_dice: 0.154033
[12:26:29.538] TRAIN: iteration 28658 : loss : 0.251530, loss_ce: 0.002844, loss_dice: 0.500216
[12:26:29.749] TRAIN: iteration 28659 : loss : 0.091092, loss_ce: 0.002808, loss_dice: 0.179376
[12:26:29.957] TRAIN: iteration 28660 : loss : 0.093420, loss_ce: 0.005144, loss_dice: 0.181696
[12:26:30.195] TRAIN: iteration 28661 : loss : 0.070653, loss_ce: 0.002111, loss_dice: 0.139195
[12:26:30.402] TRAIN: iteration 28662 : loss : 0.051393, loss_ce: 0.000928, loss_dice: 0.101858
[12:26:30.611] TRAIN: iteration 28663 : loss : 0.030547, loss_ce: 0.001984, loss_dice: 0.059109
[12:26:30.819] TRAIN: iteration 28664 : loss : 0.158624, loss_ce: 0.002379, loss_dice: 0.314870
[12:26:31.770] TRAIN: iteration 28665 : loss : 0.124889, loss_ce: 0.003508, loss_dice: 0.246269
[12:26:31.979] TRAIN: iteration 28666 : loss : 0.083608, loss_ce: 0.002206, loss_dice: 0.165010
[12:26:32.193] TRAIN: iteration 28667 : loss : 0.060017, loss_ce: 0.003685, loss_dice: 0.116349
[12:26:32.401] TRAIN: iteration 28668 : loss : 0.047683, loss_ce: 0.004041, loss_dice: 0.091325
[12:26:34.700] TRAIN: iteration 28669 : loss : 0.024857, loss_ce: 0.000834, loss_dice: 0.048881
[12:26:34.907] TRAIN: iteration 28670 : loss : 0.100631, loss_ce: 0.003074, loss_dice: 0.198188
[12:26:35.124] TRAIN: iteration 28671 : loss : 0.163580, loss_ce: 0.002477, loss_dice: 0.324683
[12:26:35.331] TRAIN: iteration 28672 : loss : 0.152543, loss_ce: 0.000667, loss_dice: 0.304418
[12:26:35.538] TRAIN: iteration 28673 : loss : 0.138988, loss_ce: 0.001420, loss_dice: 0.276556
[12:26:35.745] TRAIN: iteration 28674 : loss : 0.091718, loss_ce: 0.001439, loss_dice: 0.181997
[12:26:35.951] TRAIN: iteration 28675 : loss : 0.252273, loss_ce: 0.005909, loss_dice: 0.498636
[12:26:36.166] TRAIN: iteration 28676 : loss : 0.250579, loss_ce: 0.001093, loss_dice: 0.500065
[12:26:36.850] TRAIN: iteration 28677 : loss : 0.075187, loss_ce: 0.006115, loss_dice: 0.144259
[12:26:37.059] TRAIN: iteration 28678 : loss : 0.250137, loss_ce: 0.000270, loss_dice: 0.500004
[12:26:37.266] TRAIN: iteration 28679 : loss : 0.016855, loss_ce: 0.001804, loss_dice: 0.031907
[12:26:37.474] TRAIN: iteration 28680 : loss : 0.076315, loss_ce: 0.004170, loss_dice: 0.148460
[12:26:37.713] TRAIN: iteration 28681 : loss : 0.043094, loss_ce: 0.001799, loss_dice: 0.084388
[12:26:37.919] TRAIN: iteration 28682 : loss : 0.231961, loss_ce: 0.006244, loss_dice: 0.457679
[12:26:38.127] TRAIN: iteration 28683 : loss : 0.095242, loss_ce: 0.004643, loss_dice: 0.185842
[12:26:38.334] TRAIN: iteration 28684 : loss : 0.056560, loss_ce: 0.001504, loss_dice: 0.111617
[12:26:38.542] TRAIN: iteration 28685 : loss : 0.250351, loss_ce: 0.000673, loss_dice: 0.500029
[12:26:38.749] TRAIN: iteration 28686 : loss : 0.077447, loss_ce: 0.002579, loss_dice: 0.152315
[12:26:38.960] TRAIN: iteration 28687 : loss : 0.250328, loss_ce: 0.000623, loss_dice: 0.500033
[12:26:39.168] TRAIN: iteration 28688 : loss : 0.034488, loss_ce: 0.002371, loss_dice: 0.066604
[12:26:39.375] TRAIN: iteration 28689 : loss : 0.187782, loss_ce: 0.002148, loss_dice: 0.373417
[12:26:39.583] TRAIN: iteration 28690 : loss : 0.038667, loss_ce: 0.005280, loss_dice: 0.072054
[12:26:39.790] TRAIN: iteration 28691 : loss : 0.101959, loss_ce: 0.004163, loss_dice: 0.199755
[12:26:39.998] TRAIN: iteration 28692 : loss : 0.109883, loss_ce: 0.003159, loss_dice: 0.216608
[12:26:40.205] TRAIN: iteration 28693 : loss : 0.076188, loss_ce: 0.005212, loss_dice: 0.147164
[12:26:40.412] TRAIN: iteration 28694 : loss : 0.016893, loss_ce: 0.000474, loss_dice: 0.033311
[12:26:40.621] TRAIN: iteration 28695 : loss : 0.033325, loss_ce: 0.001197, loss_dice: 0.065453
[12:26:40.829] TRAIN: iteration 28696 : loss : 0.112268, loss_ce: 0.005597, loss_dice: 0.218938
[12:26:41.039] TRAIN: iteration 28697 : loss : 0.065194, loss_ce: 0.001277, loss_dice: 0.129111
[12:26:42.791] TRAIN: iteration 28698 : loss : 0.053034, loss_ce: 0.002429, loss_dice: 0.103639
[12:26:42.999] TRAIN: iteration 28699 : loss : 0.043398, loss_ce: 0.002554, loss_dice: 0.084241
[12:26:43.207] TRAIN: iteration 28700 : loss : 0.151796, loss_ce: 0.003422, loss_dice: 0.300169
[12:26:43.446] TRAIN: iteration 28701 : loss : 0.060722, loss_ce: 0.007736, loss_dice: 0.113709
[12:26:43.658] TRAIN: iteration 28702 : loss : 0.060930, loss_ce: 0.009315, loss_dice: 0.112544
[12:26:43.866] TRAIN: iteration 28703 : loss : 0.060073, loss_ce: 0.002709, loss_dice: 0.117436
[12:26:44.074] TRAIN: iteration 28704 : loss : 0.096489, loss_ce: 0.002651, loss_dice: 0.190328
[12:26:44.283] TRAIN: iteration 28705 : loss : 0.250946, loss_ce: 0.001778, loss_dice: 0.500114
[12:26:45.018] TRAIN: iteration 28706 : loss : 0.217169, loss_ce: 0.001536, loss_dice: 0.432803
[12:26:45.226] TRAIN: iteration 28707 : loss : 0.078892, loss_ce: 0.003228, loss_dice: 0.154556
[12:26:45.433] TRAIN: iteration 28708 : loss : 0.073458, loss_ce: 0.004663, loss_dice: 0.142253
[12:26:45.642] TRAIN: iteration 28709 : loss : 0.250197, loss_ce: 0.003653, loss_dice: 0.496741
[12:26:45.853] TRAIN: iteration 28710 : loss : 0.062009, loss_ce: 0.005435, loss_dice: 0.118584
[12:26:46.130] TRAIN: iteration 28711 : loss : 0.023444, loss_ce: 0.000966, loss_dice: 0.045922
[12:26:46.339] TRAIN: iteration 28712 : loss : 0.154476, loss_ce: 0.000853, loss_dice: 0.308100
[12:26:46.546] TRAIN: iteration 28713 : loss : 0.148051, loss_ce: 0.001908, loss_dice: 0.294193
[12:26:46.757] TRAIN: iteration 28714 : loss : 0.087600, loss_ce: 0.006116, loss_dice: 0.169084
[12:26:46.966] TRAIN: iteration 28715 : loss : 0.037534, loss_ce: 0.003170, loss_dice: 0.071898
[12:26:47.174] TRAIN: iteration 28716 : loss : 0.090847, loss_ce: 0.011002, loss_dice: 0.170691
[12:26:47.384] TRAIN: iteration 28717 : loss : 0.250141, loss_ce: 0.000280, loss_dice: 0.500002
[12:26:47.592] TRAIN: iteration 28718 : loss : 0.132865, loss_ce: 0.002562, loss_dice: 0.263168
[12:26:47.929] TRAIN: iteration 28719 : loss : 0.154030, loss_ce: 0.004560, loss_dice: 0.303501
[12:26:48.137] TRAIN: iteration 28720 : loss : 0.085752, loss_ce: 0.001660, loss_dice: 0.169844
[12:26:48.378] TRAIN: iteration 28721 : loss : 0.065408, loss_ce: 0.001876, loss_dice: 0.128941
[12:26:48.587] TRAIN: iteration 28722 : loss : 0.250799, loss_ce: 0.001846, loss_dice: 0.499753
[12:26:48.797] TRAIN: iteration 28723 : loss : 0.225095, loss_ce: 0.003331, loss_dice: 0.446860
[12:26:49.007] TRAIN: iteration 28724 : loss : 0.082384, loss_ce: 0.002217, loss_dice: 0.162552
[12:26:49.217] TRAIN: iteration 28725 : loss : 0.063029, loss_ce: 0.000664, loss_dice: 0.125394
[12:26:49.424] TRAIN: iteration 28726 : loss : 0.156620, loss_ce: 0.006478, loss_dice: 0.306762
[12:26:50.034] TRAIN: iteration 28727 : loss : 0.038268, loss_ce: 0.002405, loss_dice: 0.074131
[12:26:50.242] TRAIN: iteration 28728 : loss : 0.250265, loss_ce: 0.007018, loss_dice: 0.493513
[12:26:50.455] TRAIN: iteration 28729 : loss : 0.075283, loss_ce: 0.006218, loss_dice: 0.144347
[12:26:50.696] TRAIN: iteration 28730 : loss : 0.127299, loss_ce: 0.002471, loss_dice: 0.252127
[12:26:50.903] TRAIN: iteration 28731 : loss : 0.254451, loss_ce: 0.009739, loss_dice: 0.499163
[12:26:51.112] TRAIN: iteration 28732 : loss : 0.136570, loss_ce: 0.000936, loss_dice: 0.272205
[12:26:51.326] TRAIN: iteration 28733 : loss : 0.229830, loss_ce: 0.007362, loss_dice: 0.452298
[12:26:51.536] TRAIN: iteration 28734 : loss : 0.043202, loss_ce: 0.003154, loss_dice: 0.083251
[12:26:53.004] TRAIN: iteration 28735 : loss : 0.058761, loss_ce: 0.006359, loss_dice: 0.111163
[12:26:53.214] TRAIN: iteration 28736 : loss : 0.087075, loss_ce: 0.001742, loss_dice: 0.172409
[12:26:53.424] TRAIN: iteration 28737 : loss : 0.053652, loss_ce: 0.004608, loss_dice: 0.102697
[12:26:53.632] TRAIN: iteration 28738 : loss : 0.021403, loss_ce: 0.002563, loss_dice: 0.040243
[12:26:53.841] TRAIN: iteration 28739 : loss : 0.249410, loss_ce: 0.006759, loss_dice: 0.492060
[12:26:54.053] TRAIN: iteration 28740 : loss : 0.099986, loss_ce: 0.001699, loss_dice: 0.198273
[12:26:54.055] NaN or Inf found in input tensor.
[12:26:54.273] TRAIN: iteration 28741 : loss : 0.055020, loss_ce: 0.001710, loss_dice: 0.108331
[12:26:54.482] TRAIN: iteration 28742 : loss : 0.073651, loss_ce: 0.003401, loss_dice: 0.143901
[12:26:55.583] TRAIN: iteration 28743 : loss : 0.069411, loss_ce: 0.002019, loss_dice: 0.136803
[12:26:55.797] TRAIN: iteration 28744 : loss : 0.251207, loss_ce: 0.002260, loss_dice: 0.500154
[12:26:56.006] TRAIN: iteration 28745 : loss : 0.250934, loss_ce: 0.001743, loss_dice: 0.500124
[12:26:56.214] TRAIN: iteration 28746 : loss : 0.040037, loss_ce: 0.007896, loss_dice: 0.072178
[12:26:56.427] TRAIN: iteration 28747 : loss : 0.058408, loss_ce: 0.003419, loss_dice: 0.113397
[12:26:56.637] TRAIN: iteration 28748 : loss : 0.249146, loss_ce: 0.001477, loss_dice: 0.496815
[12:26:56.846] TRAIN: iteration 28749 : loss : 0.044833, loss_ce: 0.001240, loss_dice: 0.088427
[12:26:57.056] TRAIN: iteration 28750 : loss : 0.079614, loss_ce: 0.001829, loss_dice: 0.157400
[12:26:57.265] TRAIN: iteration 28751 : loss : 0.244511, loss_ce: 0.002407, loss_dice: 0.486615
[12:26:57.477] TRAIN: iteration 28752 : loss : 0.250540, loss_ce: 0.001027, loss_dice: 0.500054
[12:26:57.685] TRAIN: iteration 28753 : loss : 0.090724, loss_ce: 0.001904, loss_dice: 0.179544
[12:26:57.893] TRAIN: iteration 28754 : loss : 0.023427, loss_ce: 0.000659, loss_dice: 0.046194
[12:26:58.101] TRAIN: iteration 28755 : loss : 0.040177, loss_ce: 0.002768, loss_dice: 0.077585
[12:26:58.310] TRAIN: iteration 28756 : loss : 0.250592, loss_ce: 0.001130, loss_dice: 0.500054
[12:26:58.519] TRAIN: iteration 28757 : loss : 0.251725, loss_ce: 0.003221, loss_dice: 0.500229
[12:26:58.727] TRAIN: iteration 28758 : loss : 0.031778, loss_ce: 0.001407, loss_dice: 0.062149
[12:26:58.934] TRAIN: iteration 28759 : loss : 0.231992, loss_ce: 0.002597, loss_dice: 0.461387
[12:26:59.148] TRAIN: iteration 28760 : loss : 0.134809, loss_ce: 0.004147, loss_dice: 0.265471
[12:26:59.386] TRAIN: iteration 28761 : loss : 0.174355, loss_ce: 0.002353, loss_dice: 0.346357
[12:26:59.593] TRAIN: iteration 28762 : loss : 0.152764, loss_ce: 0.002267, loss_dice: 0.303261
[12:27:00.137] TRAIN: iteration 28763 : loss : 0.022424, loss_ce: 0.003581, loss_dice: 0.041267
[12:27:00.344] TRAIN: iteration 28764 : loss : 0.048593, loss_ce: 0.003077, loss_dice: 0.094109
[12:27:00.552] TRAIN: iteration 28765 : loss : 0.250611, loss_ce: 0.001148, loss_dice: 0.500073
[12:27:00.764] TRAIN: iteration 28766 : loss : 0.251105, loss_ce: 0.002069, loss_dice: 0.500142
[12:27:00.973] TRAIN: iteration 28767 : loss : 0.067576, loss_ce: 0.003659, loss_dice: 0.131493
[12:27:01.187] TRAIN: iteration 28768 : loss : 0.251479, loss_ce: 0.002756, loss_dice: 0.500202
[12:27:01.398] TRAIN: iteration 28769 : loss : 0.083226, loss_ce: 0.003340, loss_dice: 0.163111
[12:27:01.606] TRAIN: iteration 28770 : loss : 0.032283, loss_ce: 0.001411, loss_dice: 0.063154
[12:27:03.883] TRAIN: iteration 28771 : loss : 0.250875, loss_ce: 0.001632, loss_dice: 0.500118
[12:27:04.091] TRAIN: iteration 28772 : loss : 0.041672, loss_ce: 0.004594, loss_dice: 0.078750
[12:27:04.298] TRAIN: iteration 28773 : loss : 0.164880, loss_ce: 0.001855, loss_dice: 0.327905
[12:27:04.506] TRAIN: iteration 28774 : loss : 0.083575, loss_ce: 0.007486, loss_dice: 0.159664
[12:27:04.716] TRAIN: iteration 28775 : loss : 0.139035, loss_ce: 0.006405, loss_dice: 0.271665
[12:27:04.925] TRAIN: iteration 28776 : loss : 0.019955, loss_ce: 0.000400, loss_dice: 0.039510
[12:27:05.134] TRAIN: iteration 28777 : loss : 0.051161, loss_ce: 0.005719, loss_dice: 0.096604
[12:27:05.341] TRAIN: iteration 28778 : loss : 0.114001, loss_ce: 0.018271, loss_dice: 0.209731
[12:27:05.548] TRAIN: iteration 28779 : loss : 0.250739, loss_ce: 0.001382, loss_dice: 0.500097
[12:27:05.755] TRAIN: iteration 28780 : loss : 0.156801, loss_ce: 0.003053, loss_dice: 0.310550
[12:27:05.997] TRAIN: iteration 28781 : loss : 0.045192, loss_ce: 0.002483, loss_dice: 0.087901
[12:27:06.205] TRAIN: iteration 28782 : loss : 0.056478, loss_ce: 0.001310, loss_dice: 0.111647
[12:27:06.411] TRAIN: iteration 28783 : loss : 0.250673, loss_ce: 0.001277, loss_dice: 0.500070
[12:27:06.619] TRAIN: iteration 28784 : loss : 0.214024, loss_ce: 0.000990, loss_dice: 0.427059
[12:27:06.832] TRAIN: iteration 28785 : loss : 0.150094, loss_ce: 0.020976, loss_dice: 0.279213
[12:27:07.043] TRAIN: iteration 28786 : loss : 0.040910, loss_ce: 0.002798, loss_dice: 0.079022
[12:27:07.867] TRAIN: iteration 28787 : loss : 0.154527, loss_ce: 0.012769, loss_dice: 0.296284
[12:27:08.076] TRAIN: iteration 28788 : loss : 0.128593, loss_ce: 0.000930, loss_dice: 0.256255
[12:27:08.287] TRAIN: iteration 28789 : loss : 0.103088, loss_ce: 0.002647, loss_dice: 0.203530
[12:27:08.499] TRAIN: iteration 28790 : loss : 0.054609, loss_ce: 0.005177, loss_dice: 0.104041
[12:27:08.708] TRAIN: iteration 28791 : loss : 0.019588, loss_ce: 0.001951, loss_dice: 0.037225
[12:27:08.917] TRAIN: iteration 28792 : loss : 0.124745, loss_ce: 0.005776, loss_dice: 0.243714
[12:27:09.820] TRAIN: iteration 28793 : loss : 0.048961, loss_ce: 0.007203, loss_dice: 0.090720
[12:27:10.036] TRAIN: iteration 28794 : loss : 0.121368, loss_ce: 0.002172, loss_dice: 0.240565
[12:27:10.254] TRAIN: iteration 28795 : loss : 0.032546, loss_ce: 0.004808, loss_dice: 0.060284
[12:27:10.462] TRAIN: iteration 28796 : loss : 0.050133, loss_ce: 0.003428, loss_dice: 0.096839
[12:27:10.670] TRAIN: iteration 28797 : loss : 0.140562, loss_ce: 0.002077, loss_dice: 0.279048
[12:27:10.883] TRAIN: iteration 28798 : loss : 0.192796, loss_ce: 0.002749, loss_dice: 0.382843
[12:27:11.096] TRAIN: iteration 28799 : loss : 0.239730, loss_ce: 0.001243, loss_dice: 0.478217
[12:27:11.305] TRAIN: iteration 28800 : loss : 0.065781, loss_ce: 0.006059, loss_dice: 0.125504
[12:27:11.982] TRAIN: iteration 28801 : loss : 0.164589, loss_ce: 0.002999, loss_dice: 0.326178
[12:27:12.191] TRAIN: iteration 28802 : loss : 0.029361, loss_ce: 0.002146, loss_dice: 0.056576
[12:27:13.572] TRAIN: iteration 28803 : loss : 0.103557, loss_ce: 0.001512, loss_dice: 0.205601
[12:27:13.779] TRAIN: iteration 28804 : loss : 0.126721, loss_ce: 0.007522, loss_dice: 0.245919
[12:27:13.987] TRAIN: iteration 28805 : loss : 0.031341, loss_ce: 0.002899, loss_dice: 0.059783
[12:27:14.195] TRAIN: iteration 28806 : loss : 0.033457, loss_ce: 0.003099, loss_dice: 0.063816
[12:27:14.402] TRAIN: iteration 28807 : loss : 0.031728, loss_ce: 0.000678, loss_dice: 0.062778
[12:27:14.609] TRAIN: iteration 28808 : loss : 0.242570, loss_ce: 0.001712, loss_dice: 0.483427
[12:27:15.575] TRAIN: iteration 28809 : loss : 0.032786, loss_ce: 0.002175, loss_dice: 0.063397
[12:27:15.783] TRAIN: iteration 28810 : loss : 0.047381, loss_ce: 0.002807, loss_dice: 0.091955
[12:27:16.303] TRAIN: iteration 28811 : loss : 0.179417, loss_ce: 0.003830, loss_dice: 0.355005
[12:27:16.510] TRAIN: iteration 28812 : loss : 0.063151, loss_ce: 0.007661, loss_dice: 0.118641
[12:27:16.717] TRAIN: iteration 28813 : loss : 0.011557, loss_ce: 0.001099, loss_dice: 0.022015
[12:27:16.923] TRAIN: iteration 28814 : loss : 0.076985, loss_ce: 0.004217, loss_dice: 0.149752
[12:27:17.131] TRAIN: iteration 28815 : loss : 0.076515, loss_ce: 0.004322, loss_dice: 0.148708
[12:27:17.338] TRAIN: iteration 28816 : loss : 0.102714, loss_ce: 0.002181, loss_dice: 0.203247
[12:27:18.944] TRAIN: iteration 28817 : loss : 0.189568, loss_ce: 0.004628, loss_dice: 0.374508
[12:27:19.152] TRAIN: iteration 28818 : loss : 0.186361, loss_ce: 0.003878, loss_dice: 0.368844
[12:27:19.438] TRAIN: iteration 28819 : loss : 0.033935, loss_ce: 0.004619, loss_dice: 0.063251
[12:27:19.653] TRAIN: iteration 28820 : loss : 0.015357, loss_ce: 0.000794, loss_dice: 0.029919
[12:27:19.891] TRAIN: iteration 28821 : loss : 0.052526, loss_ce: 0.001469, loss_dice: 0.103583
[12:27:20.101] TRAIN: iteration 28822 : loss : 0.069707, loss_ce: 0.011718, loss_dice: 0.127696
[12:27:20.316] TRAIN: iteration 28823 : loss : 0.038037, loss_ce: 0.004331, loss_dice: 0.071742
[12:27:20.526] TRAIN: iteration 28824 : loss : 0.074091, loss_ce: 0.006933, loss_dice: 0.141249
[12:27:21.227] TRAIN: iteration 28825 : loss : 0.075195, loss_ce: 0.001200, loss_dice: 0.149191
[12:27:21.434] TRAIN: iteration 28826 : loss : 0.012390, loss_ce: 0.000830, loss_dice: 0.023950
[12:27:21.759] TRAIN: iteration 28827 : loss : 0.154703, loss_ce: 0.001330, loss_dice: 0.308077
[12:27:21.967] TRAIN: iteration 28828 : loss : 0.036477, loss_ce: 0.003843, loss_dice: 0.069111
[12:27:22.176] TRAIN: iteration 28829 : loss : 0.074406, loss_ce: 0.005125, loss_dice: 0.143686
[12:27:22.384] TRAIN: iteration 28830 : loss : 0.083899, loss_ce: 0.005915, loss_dice: 0.161883
[12:27:22.596] TRAIN: iteration 28831 : loss : 0.162581, loss_ce: 0.006497, loss_dice: 0.318666
[12:27:22.805] TRAIN: iteration 28832 : loss : 0.070797, loss_ce: 0.000639, loss_dice: 0.140955
[12:27:23.011] TRAIN: iteration 28833 : loss : 0.205279, loss_ce: 0.002311, loss_dice: 0.408248
[12:27:23.220] TRAIN: iteration 28834 : loss : 0.043909, loss_ce: 0.004824, loss_dice: 0.082993
[12:27:25.266] TRAIN: iteration 28835 : loss : 0.072769, loss_ce: 0.003153, loss_dice: 0.142386
[12:27:25.474] TRAIN: iteration 28836 : loss : 0.221321, loss_ce: 0.003290, loss_dice: 0.439353
[12:27:25.682] TRAIN: iteration 28837 : loss : 0.050683, loss_ce: 0.001286, loss_dice: 0.100080
[12:27:25.889] TRAIN: iteration 28838 : loss : 0.120697, loss_ce: 0.002034, loss_dice: 0.239359
[12:27:26.097] TRAIN: iteration 28839 : loss : 0.103299, loss_ce: 0.003404, loss_dice: 0.203194
[12:27:26.308] TRAIN: iteration 28840 : loss : 0.042888, loss_ce: 0.000474, loss_dice: 0.085302
[12:27:26.550] TRAIN: iteration 28841 : loss : 0.174703, loss_ce: 0.001189, loss_dice: 0.348216
[12:27:26.763] TRAIN: iteration 28842 : loss : 0.057955, loss_ce: 0.008181, loss_dice: 0.107729
[12:27:26.970] TRAIN: iteration 28843 : loss : 0.127533, loss_ce: 0.000470, loss_dice: 0.254597
[12:27:27.184] TRAIN: iteration 28844 : loss : 0.049843, loss_ce: 0.000830, loss_dice: 0.098857
[12:27:27.392] TRAIN: iteration 28845 : loss : 0.030663, loss_ce: 0.001867, loss_dice: 0.059460
[12:27:27.603] TRAIN: iteration 28846 : loss : 0.104067, loss_ce: 0.003221, loss_dice: 0.204914
[12:27:27.815] TRAIN: iteration 28847 : loss : 0.023888, loss_ce: 0.005295, loss_dice: 0.042480
[12:27:28.023] TRAIN: iteration 28848 : loss : 0.085286, loss_ce: 0.010905, loss_dice: 0.159667
[12:27:28.236] TRAIN: iteration 28849 : loss : 0.154324, loss_ce: 0.001133, loss_dice: 0.307515
[12:27:28.446] TRAIN: iteration 28850 : loss : 0.125328, loss_ce: 0.002217, loss_dice: 0.248438
[12:27:28.761] TRAIN: iteration 28851 : loss : 0.031809, loss_ce: 0.000906, loss_dice: 0.062711
[12:27:28.970] TRAIN: iteration 28852 : loss : 0.048275, loss_ce: 0.001403, loss_dice: 0.095146
[12:27:30.957] TRAIN: iteration 28853 : loss : 0.159255, loss_ce: 0.008104, loss_dice: 0.310405
[12:27:31.173] TRAIN: iteration 28854 : loss : 0.252439, loss_ce: 0.005246, loss_dice: 0.499633
[12:27:31.383] TRAIN: iteration 28855 : loss : 0.250371, loss_ce: 0.000710, loss_dice: 0.500032
[12:27:31.591] TRAIN: iteration 28856 : loss : 0.073784, loss_ce: 0.007111, loss_dice: 0.140457
[12:27:31.803] TRAIN: iteration 28857 : loss : 0.135345, loss_ce: 0.002361, loss_dice: 0.268329
[12:27:32.019] TRAIN: iteration 28858 : loss : 0.220782, loss_ce: 0.004448, loss_dice: 0.437115
[12:27:32.228] TRAIN: iteration 28859 : loss : 0.231084, loss_ce: 0.002307, loss_dice: 0.459861
[12:27:32.440] TRAIN: iteration 28860 : loss : 0.138219, loss_ce: 0.004820, loss_dice: 0.271619
[12:27:32.441] NaN or Inf found in input tensor.
[12:27:32.658] TRAIN: iteration 28861 : loss : 0.093253, loss_ce: 0.004425, loss_dice: 0.182080
[12:27:32.866] TRAIN: iteration 28862 : loss : 0.045315, loss_ce: 0.001188, loss_dice: 0.089442
[12:27:33.075] TRAIN: iteration 28863 : loss : 0.039077, loss_ce: 0.001630, loss_dice: 0.076524
[12:27:33.282] TRAIN: iteration 28864 : loss : 0.052726, loss_ce: 0.004072, loss_dice: 0.101380
[12:27:33.493] TRAIN: iteration 28865 : loss : 0.082467, loss_ce: 0.004191, loss_dice: 0.160742
[12:27:33.701] TRAIN: iteration 28866 : loss : 0.107694, loss_ce: 0.003760, loss_dice: 0.211629
[12:27:33.911] TRAIN: iteration 28867 : loss : 0.041001, loss_ce: 0.007303, loss_dice: 0.074699
[12:27:34.120] TRAIN: iteration 28868 : loss : 0.127089, loss_ce: 0.006371, loss_dice: 0.247807
[12:27:34.329] TRAIN: iteration 28869 : loss : 0.041439, loss_ce: 0.002843, loss_dice: 0.080035
[12:27:34.536] TRAIN: iteration 28870 : loss : 0.114316, loss_ce: 0.004211, loss_dice: 0.224420
[12:27:35.472] TRAIN: iteration 28871 : loss : 0.155327, loss_ce: 0.015286, loss_dice: 0.295368
[12:27:35.679] TRAIN: iteration 28872 : loss : 0.171740, loss_ce: 0.001768, loss_dice: 0.341711
[12:27:35.885] TRAIN: iteration 28873 : loss : 0.154144, loss_ce: 0.008145, loss_dice: 0.300142
[12:27:36.094] TRAIN: iteration 28874 : loss : 0.163864, loss_ce: 0.004292, loss_dice: 0.323436
[12:27:36.301] TRAIN: iteration 28875 : loss : 0.245547, loss_ce: 0.002937, loss_dice: 0.488158
[12:27:36.512] TRAIN: iteration 28876 : loss : 0.035625, loss_ce: 0.003688, loss_dice: 0.067561
[12:27:36.719] TRAIN: iteration 28877 : loss : 0.175830, loss_ce: 0.007025, loss_dice: 0.344636
[12:27:36.925] TRAIN: iteration 28878 : loss : 0.056471, loss_ce: 0.005749, loss_dice: 0.107192
[12:27:37.136] TRAIN: iteration 28879 : loss : 0.168857, loss_ce: 0.007004, loss_dice: 0.330709
[12:27:37.343] TRAIN: iteration 28880 : loss : 0.061359, loss_ce: 0.003844, loss_dice: 0.118874
[12:27:38.635] TRAIN: iteration 28881 : loss : 0.050441, loss_ce: 0.002163, loss_dice: 0.098719
[12:27:38.843] TRAIN: iteration 28882 : loss : 0.058040, loss_ce: 0.004144, loss_dice: 0.111935
[12:27:39.050] TRAIN: iteration 28883 : loss : 0.251579, loss_ce: 0.002960, loss_dice: 0.500198
[12:27:39.257] TRAIN: iteration 28884 : loss : 0.040539, loss_ce: 0.002386, loss_dice: 0.078692
[12:27:39.500] TRAIN: iteration 28885 : loss : 0.219455, loss_ce: 0.001884, loss_dice: 0.437025
[12:27:39.709] TRAIN: iteration 28886 : loss : 0.250123, loss_ce: 0.002210, loss_dice: 0.498036
[12:27:39.916] TRAIN: iteration 28887 : loss : 0.030191, loss_ce: 0.001784, loss_dice: 0.058598
[12:27:40.124] TRAIN: iteration 28888 : loss : 0.110525, loss_ce: 0.004943, loss_dice: 0.216108
[12:27:42.020] TRAIN: iteration 28889 : loss : 0.203831, loss_ce: 0.005073, loss_dice: 0.402588
[12:27:42.228] TRAIN: iteration 28890 : loss : 0.056866, loss_ce: 0.004123, loss_dice: 0.109608
[12:27:42.439] TRAIN: iteration 28891 : loss : 0.200864, loss_ce: 0.002693, loss_dice: 0.399035
[12:27:42.668] TRAIN: iteration 28892 : loss : 0.104207, loss_ce: 0.003562, loss_dice: 0.204852
[12:27:42.876] TRAIN: iteration 28893 : loss : 0.250804, loss_ce: 0.001512, loss_dice: 0.500095
[12:27:43.092] TRAIN: iteration 28894 : loss : 0.250669, loss_ce: 0.001277, loss_dice: 0.500062
[12:27:43.299] TRAIN: iteration 28895 : loss : 0.151250, loss_ce: 0.005741, loss_dice: 0.296760
[12:27:43.507] TRAIN: iteration 28896 : loss : 0.029251, loss_ce: 0.002798, loss_dice: 0.055703
[12:27:43.998] TRAIN: iteration 28897 : loss : 0.136627, loss_ce: 0.002796, loss_dice: 0.270457
[12:27:44.212] TRAIN: iteration 28898 : loss : 0.048284, loss_ce: 0.006270, loss_dice: 0.090298
[12:27:44.422] TRAIN: iteration 28899 : loss : 0.168285, loss_ce: 0.004386, loss_dice: 0.332184
[12:27:44.631] TRAIN: iteration 28900 : loss : 0.068695, loss_ce: 0.003495, loss_dice: 0.133895
[12:27:44.867] TRAIN: iteration 28901 : loss : 0.125182, loss_ce: 0.030431, loss_dice: 0.219932
[12:27:45.082] TRAIN: iteration 28902 : loss : 0.057543, loss_ce: 0.002797, loss_dice: 0.112289
[12:27:45.289] TRAIN: iteration 28903 : loss : 0.250658, loss_ce: 0.001245, loss_dice: 0.500071
[12:27:45.496] TRAIN: iteration 28904 : loss : 0.035449, loss_ce: 0.002392, loss_dice: 0.068507
[12:27:48.736] TRAIN: iteration 28905 : loss : 0.029031, loss_ce: 0.001409, loss_dice: 0.056652
[12:27:48.951] TRAIN: iteration 28906 : loss : 0.065725, loss_ce: 0.005489, loss_dice: 0.125960
[12:27:49.159] TRAIN: iteration 28907 : loss : 0.152130, loss_ce: 0.009260, loss_dice: 0.295000
[12:27:49.368] TRAIN: iteration 28908 : loss : 0.035723, loss_ce: 0.000836, loss_dice: 0.070611
[12:27:49.580] TRAIN: iteration 28909 : loss : 0.084899, loss_ce: 0.003936, loss_dice: 0.165862
[12:27:49.790] TRAIN: iteration 28910 : loss : 0.050365, loss_ce: 0.005915, loss_dice: 0.094816
[12:27:49.997] TRAIN: iteration 28911 : loss : 0.155201, loss_ce: 0.002312, loss_dice: 0.308089
[12:27:50.210] TRAIN: iteration 28912 : loss : 0.123056, loss_ce: 0.002919, loss_dice: 0.243193
[12:27:50.964] TRAIN: iteration 28913 : loss : 0.062014, loss_ce: 0.004694, loss_dice: 0.119333
[12:27:51.173] TRAIN: iteration 28914 : loss : 0.032401, loss_ce: 0.004954, loss_dice: 0.059847
[12:27:51.382] TRAIN: iteration 28915 : loss : 0.243300, loss_ce: 0.002282, loss_dice: 0.484318
[12:27:51.589] TRAIN: iteration 28916 : loss : 0.060788, loss_ce: 0.005443, loss_dice: 0.116133
[12:27:51.798] TRAIN: iteration 28917 : loss : 0.100961, loss_ce: 0.008042, loss_dice: 0.193880
[12:27:52.005] TRAIN: iteration 28918 : loss : 0.133688, loss_ce: 0.010877, loss_dice: 0.256499
[12:27:52.212] TRAIN: iteration 28919 : loss : 0.031345, loss_ce: 0.005365, loss_dice: 0.057325
[12:27:52.419] TRAIN: iteration 28920 : loss : 0.250845, loss_ce: 0.001605, loss_dice: 0.500085
[12:27:52.898] TRAIN: iteration 28921 : loss : 0.026638, loss_ce: 0.001628, loss_dice: 0.051649
[12:27:53.108] TRAIN: iteration 28922 : loss : 0.092641, loss_ce: 0.004754, loss_dice: 0.180528
[12:27:53.314] TRAIN: iteration 28923 : loss : 0.095860, loss_ce: 0.004118, loss_dice: 0.187602
[12:27:53.522] TRAIN: iteration 28924 : loss : 0.095783, loss_ce: 0.006286, loss_dice: 0.185281
[12:27:53.731] TRAIN: iteration 28925 : loss : 0.244566, loss_ce: 0.013623, loss_dice: 0.475508
[12:27:53.939] TRAIN: iteration 28926 : loss : 0.058524, loss_ce: 0.002391, loss_dice: 0.114657
[12:27:54.147] TRAIN: iteration 28927 : loss : 0.078780, loss_ce: 0.003181, loss_dice: 0.154379
[12:27:54.355] TRAIN: iteration 28928 : loss : 0.035147, loss_ce: 0.003121, loss_dice: 0.067172
[12:27:57.393] TRAIN: iteration 28929 : loss : 0.101690, loss_ce: 0.004744, loss_dice: 0.198635
[12:27:57.608] TRAIN: iteration 28930 : loss : 0.024031, loss_ce: 0.001392, loss_dice: 0.046670
[12:27:57.816] TRAIN: iteration 28931 : loss : 0.215628, loss_ce: 0.001700, loss_dice: 0.429555
[12:27:58.033] TRAIN: iteration 28932 : loss : 0.059148, loss_ce: 0.003880, loss_dice: 0.114415
[12:27:58.243] TRAIN: iteration 28933 : loss : 0.034185, loss_ce: 0.000983, loss_dice: 0.067387
[12:27:58.454] TRAIN: iteration 28934 : loss : 0.078237, loss_ce: 0.007445, loss_dice: 0.149028
[12:27:58.753] TRAIN: iteration 28935 : loss : 0.117067, loss_ce: 0.004883, loss_dice: 0.229250
[12:27:58.963] TRAIN: iteration 28936 : loss : 0.155392, loss_ce: 0.009237, loss_dice: 0.301547
[12:28:01.660] TRAIN: iteration 28937 : loss : 0.102101, loss_ce: 0.003439, loss_dice: 0.200764
[12:28:01.873] TRAIN: iteration 28938 : loss : 0.020058, loss_ce: 0.001257, loss_dice: 0.038859
[12:28:02.090] TRAIN: iteration 28939 : loss : 0.095306, loss_ce: 0.011459, loss_dice: 0.179153
[12:28:02.298] TRAIN: iteration 28940 : loss : 0.077576, loss_ce: 0.003144, loss_dice: 0.152007
[12:28:02.529] TRAIN: iteration 28941 : loss : 0.120401, loss_ce: 0.003045, loss_dice: 0.237756
[12:28:02.744] TRAIN: iteration 28942 : loss : 0.116413, loss_ce: 0.007369, loss_dice: 0.225456
[12:28:02.958] TRAIN: iteration 28943 : loss : 0.059628, loss_ce: 0.010227, loss_dice: 0.109029
[12:28:03.167] TRAIN: iteration 28944 : loss : 0.208125, loss_ce: 0.001947, loss_dice: 0.414302
[12:28:03.374] TRAIN: iteration 28945 : loss : 0.120988, loss_ce: 0.002462, loss_dice: 0.239515
[12:28:03.584] TRAIN: iteration 28946 : loss : 0.036002, loss_ce: 0.007527, loss_dice: 0.064477
[12:28:03.791] TRAIN: iteration 28947 : loss : 0.096945, loss_ce: 0.003812, loss_dice: 0.190079
[12:28:03.998] TRAIN: iteration 28948 : loss : 0.104259, loss_ce: 0.005413, loss_dice: 0.203105
[12:28:04.215] TRAIN: iteration 28949 : loss : 0.126317, loss_ce: 0.004661, loss_dice: 0.247974
[12:28:04.423] TRAIN: iteration 28950 : loss : 0.210388, loss_ce: 0.006744, loss_dice: 0.414031
[12:28:04.630] TRAIN: iteration 28951 : loss : 0.251149, loss_ce: 0.002711, loss_dice: 0.499586
[12:28:04.838] TRAIN: iteration 28952 : loss : 0.100980, loss_ce: 0.004318, loss_dice: 0.197642
[12:28:05.048] TRAIN: iteration 28953 : loss : 0.017335, loss_ce: 0.000764, loss_dice: 0.033906
[12:28:06.508] TRAIN: iteration 28954 : loss : 0.183774, loss_ce: 0.001250, loss_dice: 0.366297
[12:28:06.717] TRAIN: iteration 28955 : loss : 0.091222, loss_ce: 0.003052, loss_dice: 0.179392
[12:28:06.926] TRAIN: iteration 28956 : loss : 0.222458, loss_ce: 0.001889, loss_dice: 0.443027
[12:28:07.136] TRAIN: iteration 28957 : loss : 0.250932, loss_ce: 0.001765, loss_dice: 0.500099
[12:28:07.459] TRAIN: iteration 28958 : loss : 0.041062, loss_ce: 0.005810, loss_dice: 0.076313
[12:28:07.669] TRAIN: iteration 28959 : loss : 0.059900, loss_ce: 0.002316, loss_dice: 0.117484
[12:28:07.877] TRAIN: iteration 28960 : loss : 0.117025, loss_ce: 0.003878, loss_dice: 0.230173
[12:28:08.228] TRAIN: iteration 28961 : loss : 0.037220, loss_ce: 0.001616, loss_dice: 0.072823
[12:28:08.436] TRAIN: iteration 28962 : loss : 0.047985, loss_ce: 0.004097, loss_dice: 0.091873
[12:28:08.647] TRAIN: iteration 28963 : loss : 0.040279, loss_ce: 0.004763, loss_dice: 0.075795
[12:28:08.856] TRAIN: iteration 28964 : loss : 0.094302, loss_ce: 0.002803, loss_dice: 0.185800
[12:28:09.072] TRAIN: iteration 28965 : loss : 0.099751, loss_ce: 0.004220, loss_dice: 0.195283
[12:28:09.280] TRAIN: iteration 28966 : loss : 0.039415, loss_ce: 0.001637, loss_dice: 0.077193
[12:28:09.488] TRAIN: iteration 28967 : loss : 0.037592, loss_ce: 0.001555, loss_dice: 0.073630
[12:28:09.696] TRAIN: iteration 28968 : loss : 0.102970, loss_ce: 0.004154, loss_dice: 0.201785
[12:28:11.176] TRAIN: iteration 28969 : loss : 0.041627, loss_ce: 0.001024, loss_dice: 0.082231
[12:28:11.384] TRAIN: iteration 28970 : loss : 0.251133, loss_ce: 0.002132, loss_dice: 0.500134
[12:28:11.592] TRAIN: iteration 28971 : loss : 0.156964, loss_ce: 0.006470, loss_dice: 0.307459
[12:28:12.347] TRAIN: iteration 28972 : loss : 0.076481, loss_ce: 0.001441, loss_dice: 0.151522
[12:28:12.554] TRAIN: iteration 28973 : loss : 0.224685, loss_ce: 0.002492, loss_dice: 0.446879
[12:28:12.762] TRAIN: iteration 28974 : loss : 0.054065, loss_ce: 0.003668, loss_dice: 0.104462
[12:28:12.968] TRAIN: iteration 28975 : loss : 0.109147, loss_ce: 0.002267, loss_dice: 0.216028
[12:28:13.176] TRAIN: iteration 28976 : loss : 0.105875, loss_ce: 0.008823, loss_dice: 0.202928
[12:28:14.327] TRAIN: iteration 28977 : loss : 0.018084, loss_ce: 0.000885, loss_dice: 0.035282
[12:28:15.998] TRAIN: iteration 28978 : loss : 0.178875, loss_ce: 0.001839, loss_dice: 0.355911
[12:28:16.208] TRAIN: iteration 28979 : loss : 0.250634, loss_ce: 0.001208, loss_dice: 0.500061
[12:28:16.414] TRAIN: iteration 28980 : loss : 0.051509, loss_ce: 0.002325, loss_dice: 0.100693
[12:28:16.649] TRAIN: iteration 28981 : loss : 0.048869, loss_ce: 0.002013, loss_dice: 0.095724
[12:28:16.858] TRAIN: iteration 28982 : loss : 0.043389, loss_ce: 0.001142, loss_dice: 0.085637
[12:28:17.132] TRAIN: iteration 28983 : loss : 0.050199, loss_ce: 0.004468, loss_dice: 0.095929
[12:28:17.346] TRAIN: iteration 28984 : loss : 0.165428, loss_ce: 0.001252, loss_dice: 0.329605
[12:28:19.589] TRAIN: iteration 28985 : loss : 0.250511, loss_ce: 0.000969, loss_dice: 0.500052
[12:28:19.795] TRAIN: iteration 28986 : loss : 0.094486, loss_ce: 0.003912, loss_dice: 0.185060
[12:28:20.003] TRAIN: iteration 28987 : loss : 0.117510, loss_ce: 0.001648, loss_dice: 0.233372
[12:28:20.210] TRAIN: iteration 28988 : loss : 0.095688, loss_ce: 0.001220, loss_dice: 0.190156
[12:28:20.417] TRAIN: iteration 28989 : loss : 0.183349, loss_ce: 0.000850, loss_dice: 0.365848
[12:28:20.627] TRAIN: iteration 28990 : loss : 0.087376, loss_ce: 0.005685, loss_dice: 0.169067
[12:28:20.833] TRAIN: iteration 28991 : loss : 0.234676, loss_ce: 0.011899, loss_dice: 0.457453
[12:28:21.043] TRAIN: iteration 28992 : loss : 0.071414, loss_ce: 0.004698, loss_dice: 0.138130
[12:28:21.896] TRAIN: iteration 28993 : loss : 0.158602, loss_ce: 0.013057, loss_dice: 0.304147
[12:28:22.107] TRAIN: iteration 28994 : loss : 0.231050, loss_ce: 0.001351, loss_dice: 0.460749
[12:28:22.318] TRAIN: iteration 28995 : loss : 0.250355, loss_ce: 0.001609, loss_dice: 0.499101
[12:28:22.525] TRAIN: iteration 28996 : loss : 0.023231, loss_ce: 0.000511, loss_dice: 0.045952
[12:28:22.738] TRAIN: iteration 28997 : loss : 0.165323, loss_ce: 0.002247, loss_dice: 0.328399
[12:28:22.946] TRAIN: iteration 28998 : loss : 0.128015, loss_ce: 0.002831, loss_dice: 0.253198
[12:28:23.161] TRAIN: iteration 28999 : loss : 0.251080, loss_ce: 0.002014, loss_dice: 0.500146
[12:28:23.369] TRAIN: iteration 29000 : loss : 0.031973, loss_ce: 0.003231, loss_dice: 0.060715
[12:28:23.608] TRAIN: iteration 29001 : loss : 0.103036, loss_ce: 0.004793, loss_dice: 0.201280
[12:28:23.815] TRAIN: iteration 29002 : loss : 0.141368, loss_ce: 0.002792, loss_dice: 0.279945
[12:28:24.027] TRAIN: iteration 29003 : loss : 0.118693, loss_ce: 0.009061, loss_dice: 0.228324
[12:28:27.233] TRAIN: iteration 29004 : loss : 0.038511, loss_ce: 0.002380, loss_dice: 0.074641
[12:28:27.440] TRAIN: iteration 29005 : loss : 0.097264, loss_ce: 0.001847, loss_dice: 0.192681
[12:28:27.647] TRAIN: iteration 29006 : loss : 0.066509, loss_ce: 0.005547, loss_dice: 0.127472
[12:28:27.855] TRAIN: iteration 29007 : loss : 0.246944, loss_ce: 0.000979, loss_dice: 0.492908
[12:28:28.064] TRAIN: iteration 29008 : loss : 0.073640, loss_ce: 0.003723, loss_dice: 0.143558
[12:28:28.964] TRAIN: iteration 29009 : loss : 0.019529, loss_ce: 0.002112, loss_dice: 0.036946
[12:28:29.177] TRAIN: iteration 29010 : loss : 0.234068, loss_ce: 0.002292, loss_dice: 0.465843
[12:28:29.384] TRAIN: iteration 29011 : loss : 0.080884, loss_ce: 0.003978, loss_dice: 0.157789
[12:28:29.592] TRAIN: iteration 29012 : loss : 0.108012, loss_ce: 0.003355, loss_dice: 0.212669
[12:28:29.799] TRAIN: iteration 29013 : loss : 0.094260, loss_ce: 0.005287, loss_dice: 0.183234
[12:28:30.007] TRAIN: iteration 29014 : loss : 0.250744, loss_ce: 0.001405, loss_dice: 0.500083
[12:28:30.216] TRAIN: iteration 29015 : loss : 0.152906, loss_ce: 0.003330, loss_dice: 0.302481
[12:28:30.425] TRAIN: iteration 29016 : loss : 0.100376, loss_ce: 0.003667, loss_dice: 0.197085
[12:28:33.556] TRAIN: iteration 29017 : loss : 0.082783, loss_ce: 0.001745, loss_dice: 0.163820
[12:28:33.763] TRAIN: iteration 29018 : loss : 0.068241, loss_ce: 0.000737, loss_dice: 0.135744
[12:28:33.972] TRAIN: iteration 29019 : loss : 0.250752, loss_ce: 0.001408, loss_dice: 0.500096
[12:28:34.182] TRAIN: iteration 29020 : loss : 0.086657, loss_ce: 0.004347, loss_dice: 0.168967
[12:28:34.420] TRAIN: iteration 29021 : loss : 0.117398, loss_ce: 0.002649, loss_dice: 0.232147
[12:28:34.628] TRAIN: iteration 29022 : loss : 0.242558, loss_ce: 0.002081, loss_dice: 0.483034
[12:28:34.837] TRAIN: iteration 29023 : loss : 0.079119, loss_ce: 0.004330, loss_dice: 0.153908
[12:28:35.044] TRAIN: iteration 29024 : loss : 0.229286, loss_ce: 0.004258, loss_dice: 0.454315
[12:28:36.642] TRAIN: iteration 29025 : loss : 0.073934, loss_ce: 0.002775, loss_dice: 0.145093
[12:28:36.850] TRAIN: iteration 29026 : loss : 0.036926, loss_ce: 0.002486, loss_dice: 0.071366
[12:28:37.058] TRAIN: iteration 29027 : loss : 0.044330, loss_ce: 0.003466, loss_dice: 0.085194
[12:28:37.266] TRAIN: iteration 29028 : loss : 0.119382, loss_ce: 0.002031, loss_dice: 0.236733
[12:28:37.473] TRAIN: iteration 29029 : loss : 0.199367, loss_ce: 0.001904, loss_dice: 0.396830
[12:28:37.680] TRAIN: iteration 29030 : loss : 0.250468, loss_ce: 0.000893, loss_dice: 0.500043
[12:28:37.895] TRAIN: iteration 29031 : loss : 0.070462, loss_ce: 0.001388, loss_dice: 0.139537
[12:28:38.103] TRAIN: iteration 29032 : loss : 0.250959, loss_ce: 0.001789, loss_dice: 0.500128
[12:28:39.547] TRAIN: iteration 29033 : loss : 0.101700, loss_ce: 0.000785, loss_dice: 0.202616
[12:28:39.935] TRAIN: iteration 29034 : loss : 0.071640, loss_ce: 0.001754, loss_dice: 0.141526
[12:28:40.142] TRAIN: iteration 29035 : loss : 0.035039, loss_ce: 0.000966, loss_dice: 0.069111
[12:28:40.349] TRAIN: iteration 29036 : loss : 0.060454, loss_ce: 0.001522, loss_dice: 0.119387
[12:28:40.558] TRAIN: iteration 29037 : loss : 0.073823, loss_ce: 0.007807, loss_dice: 0.139839
[12:28:40.768] TRAIN: iteration 29038 : loss : 0.251035, loss_ce: 0.001938, loss_dice: 0.500132
[12:28:40.975] TRAIN: iteration 29039 : loss : 0.033130, loss_ce: 0.000594, loss_dice: 0.065665
[12:28:41.183] TRAIN: iteration 29040 : loss : 0.244341, loss_ce: 0.001472, loss_dice: 0.487210
[12:28:45.540] TRAIN: iteration 29041 : loss : 0.141853, loss_ce: 0.008193, loss_dice: 0.275513
[12:28:45.748] TRAIN: iteration 29042 : loss : 0.065524, loss_ce: 0.003146, loss_dice: 0.127901
[12:28:45.957] TRAIN: iteration 29043 : loss : 0.242966, loss_ce: 0.001514, loss_dice: 0.484417
[12:28:46.164] TRAIN: iteration 29044 : loss : 0.175705, loss_ce: 0.001816, loss_dice: 0.349595
[12:28:46.373] TRAIN: iteration 29045 : loss : 0.181660, loss_ce: 0.001193, loss_dice: 0.362128
[12:28:46.588] TRAIN: iteration 29046 : loss : 0.081169, loss_ce: 0.002017, loss_dice: 0.160322
[12:28:46.797] TRAIN: iteration 29047 : loss : 0.094444, loss_ce: 0.006462, loss_dice: 0.182426
[12:28:47.050] TRAIN: iteration 29048 : loss : 0.219740, loss_ce: 0.001735, loss_dice: 0.437745
[12:28:49.720] TRAIN: iteration 29049 : loss : 0.207977, loss_ce: 0.004033, loss_dice: 0.411921
[12:28:49.929] TRAIN: iteration 29050 : loss : 0.079461, loss_ce: 0.001338, loss_dice: 0.157585
[12:28:50.139] TRAIN: iteration 29051 : loss : 0.061101, loss_ce: 0.001413, loss_dice: 0.120789
[12:28:50.347] TRAIN: iteration 29052 : loss : 0.019011, loss_ce: 0.000819, loss_dice: 0.037202
[12:28:50.554] TRAIN: iteration 29053 : loss : 0.082924, loss_ce: 0.003049, loss_dice: 0.162799
[12:28:50.763] TRAIN: iteration 29054 : loss : 0.050380, loss_ce: 0.003986, loss_dice: 0.096774
[12:28:50.973] TRAIN: iteration 29055 : loss : 0.054536, loss_ce: 0.006716, loss_dice: 0.102356
[12:28:51.183] TRAIN: iteration 29056 : loss : 0.076717, loss_ce: 0.002827, loss_dice: 0.150608
[12:28:53.347] TRAIN: iteration 29057 : loss : 0.246329, loss_ce: 0.003823, loss_dice: 0.488835
[12:28:53.556] TRAIN: iteration 29058 : loss : 0.062066, loss_ce: 0.002353, loss_dice: 0.121779
[12:28:53.764] TRAIN: iteration 29059 : loss : 0.074014, loss_ce: 0.001722, loss_dice: 0.146307
[12:28:53.973] TRAIN: iteration 29060 : loss : 0.058402, loss_ce: 0.004664, loss_dice: 0.112140
[12:28:54.211] TRAIN: iteration 29061 : loss : 0.250695, loss_ce: 0.001329, loss_dice: 0.500060
[12:28:54.433] TRAIN: iteration 29062 : loss : 0.052074, loss_ce: 0.004371, loss_dice: 0.099778
[12:28:54.642] TRAIN: iteration 29063 : loss : 0.071805, loss_ce: 0.006179, loss_dice: 0.137431
[12:28:54.849] TRAIN: iteration 29064 : loss : 0.046867, loss_ce: 0.001816, loss_dice: 0.091918
[12:28:56.082] TRAIN: iteration 29065 : loss : 0.074262, loss_ce: 0.002564, loss_dice: 0.145961
[12:28:56.289] TRAIN: iteration 29066 : loss : 0.108857, loss_ce: 0.003297, loss_dice: 0.214417
[12:28:56.496] TRAIN: iteration 29067 : loss : 0.250469, loss_ce: 0.000893, loss_dice: 0.500045
[12:28:56.703] TRAIN: iteration 29068 : loss : 0.045912, loss_ce: 0.004726, loss_dice: 0.087098
[12:28:56.916] TRAIN: iteration 29069 : loss : 0.057582, loss_ce: 0.002902, loss_dice: 0.112261
[12:28:57.123] TRAIN: iteration 29070 : loss : 0.042650, loss_ce: 0.003855, loss_dice: 0.081445
[12:28:57.333] TRAIN: iteration 29071 : loss : 0.044451, loss_ce: 0.004823, loss_dice: 0.084080
[12:28:57.542] TRAIN: iteration 29072 : loss : 0.025542, loss_ce: 0.001134, loss_dice: 0.049950
[12:29:00.642] TRAIN: iteration 29073 : loss : 0.190572, loss_ce: 0.006021, loss_dice: 0.375124
[12:29:00.851] TRAIN: iteration 29074 : loss : 0.251100, loss_ce: 0.002078, loss_dice: 0.500123
[12:29:01.061] TRAIN: iteration 29075 : loss : 0.046741, loss_ce: 0.001695, loss_dice: 0.091787
[12:29:01.275] TRAIN: iteration 29076 : loss : 0.250797, loss_ce: 0.001526, loss_dice: 0.500069
[12:29:01.482] TRAIN: iteration 29077 : loss : 0.060979, loss_ce: 0.000950, loss_dice: 0.121009
[12:29:01.689] TRAIN: iteration 29078 : loss : 0.022633, loss_ce: 0.002042, loss_dice: 0.043223
[12:29:01.897] TRAIN: iteration 29079 : loss : 0.068063, loss_ce: 0.004099, loss_dice: 0.132028
[12:29:02.112] TRAIN: iteration 29080 : loss : 0.073249, loss_ce: 0.005558, loss_dice: 0.140940
[12:29:03.012] TRAIN: iteration 29081 : loss : 0.074859, loss_ce: 0.011923, loss_dice: 0.137795
[12:29:03.221] TRAIN: iteration 29082 : loss : 0.250825, loss_ce: 0.001566, loss_dice: 0.500084
[12:29:03.430] TRAIN: iteration 29083 : loss : 0.048560, loss_ce: 0.001270, loss_dice: 0.095849
[12:29:03.638] TRAIN: iteration 29084 : loss : 0.032440, loss_ce: 0.001705, loss_dice: 0.063174
[12:29:03.856] TRAIN: iteration 29085 : loss : 0.082210, loss_ce: 0.003555, loss_dice: 0.160865
[12:29:04.069] TRAIN: iteration 29086 : loss : 0.024130, loss_ce: 0.001844, loss_dice: 0.046416
[12:29:04.277] TRAIN: iteration 29087 : loss : 0.049461, loss_ce: 0.003322, loss_dice: 0.095599
[12:29:04.486] TRAIN: iteration 29088 : loss : 0.027937, loss_ce: 0.005806, loss_dice: 0.050067
[12:29:07.997] TRAIN: iteration 29089 : loss : 0.163784, loss_ce: 0.005128, loss_dice: 0.322441
[12:29:08.212] TRAIN: iteration 29090 : loss : 0.149964, loss_ce: 0.005206, loss_dice: 0.294722
[12:29:08.425] TRAIN: iteration 29091 : loss : 0.055839, loss_ce: 0.004024, loss_dice: 0.107653
[12:29:08.633] TRAIN: iteration 29092 : loss : 0.044531, loss_ce: 0.005470, loss_dice: 0.083593
[12:29:08.863] TRAIN: iteration 29093 : loss : 0.061717, loss_ce: 0.003706, loss_dice: 0.119728
[12:29:09.071] TRAIN: iteration 29094 : loss : 0.220998, loss_ce: 0.001474, loss_dice: 0.440522
[12:29:09.279] TRAIN: iteration 29095 : loss : 0.026581, loss_ce: 0.001451, loss_dice: 0.051712
[12:29:09.487] TRAIN: iteration 29096 : loss : 0.250890, loss_ce: 0.001674, loss_dice: 0.500105
[12:29:09.697] TRAIN: iteration 29097 : loss : 0.085953, loss_ce: 0.003486, loss_dice: 0.168420
[12:29:09.907] TRAIN: iteration 29098 : loss : 0.025056, loss_ce: 0.001610, loss_dice: 0.048503
[12:29:10.116] TRAIN: iteration 29099 : loss : 0.077913, loss_ce: 0.001918, loss_dice: 0.153907
[12:29:10.332] TRAIN: iteration 29100 : loss : 0.038324, loss_ce: 0.001411, loss_dice: 0.075236
[12:29:10.333] NaN or Inf found in input tensor.
[12:29:10.548] TRAIN: iteration 29101 : loss : 0.111729, loss_ce: 0.008231, loss_dice: 0.215226
[12:29:10.756] TRAIN: iteration 29102 : loss : 0.033808, loss_ce: 0.004430, loss_dice: 0.063186
[12:29:10.964] TRAIN: iteration 29103 : loss : 0.250542, loss_ce: 0.001043, loss_dice: 0.500041
[12:29:11.174] TRAIN: iteration 29104 : loss : 0.065353, loss_ce: 0.006462, loss_dice: 0.124243
[12:29:13.342] TRAIN: iteration 29105 : loss : 0.040030, loss_ce: 0.003527, loss_dice: 0.076534
[12:29:13.550] TRAIN: iteration 29106 : loss : 0.049183, loss_ce: 0.002077, loss_dice: 0.096290
[12:29:13.758] TRAIN: iteration 29107 : loss : 0.072947, loss_ce: 0.002450, loss_dice: 0.143444
[12:29:13.968] TRAIN: iteration 29108 : loss : 0.086291, loss_ce: 0.007536, loss_dice: 0.165046
[12:29:14.174] TRAIN: iteration 29109 : loss : 0.069452, loss_ce: 0.001245, loss_dice: 0.137658
[12:29:14.382] TRAIN: iteration 29110 : loss : 0.027416, loss_ce: 0.002807, loss_dice: 0.052024
[12:29:14.590] TRAIN: iteration 29111 : loss : 0.066516, loss_ce: 0.001674, loss_dice: 0.131357
[12:29:14.798] TRAIN: iteration 29112 : loss : 0.087533, loss_ce: 0.009284, loss_dice: 0.165782
[12:29:15.388] TRAIN: iteration 29113 : loss : 0.184553, loss_ce: 0.001118, loss_dice: 0.367989
[12:29:15.595] TRAIN: iteration 29114 : loss : 0.086481, loss_ce: 0.003843, loss_dice: 0.169118
[12:29:15.803] TRAIN: iteration 29115 : loss : 0.005082, loss_ce: 0.000606, loss_dice: 0.009558
[12:29:16.361] TRAIN: iteration 29116 : loss : 0.142293, loss_ce: 0.002852, loss_dice: 0.281735
[12:29:16.569] TRAIN: iteration 29117 : loss : 0.032760, loss_ce: 0.002519, loss_dice: 0.063001
[12:29:18.810] TRAIN: iteration 29118 : loss : 0.029592, loss_ce: 0.000511, loss_dice: 0.058674
[12:29:19.018] TRAIN: iteration 29119 : loss : 0.173704, loss_ce: 0.003295, loss_dice: 0.344114
[12:29:19.232] TRAIN: iteration 29120 : loss : 0.127443, loss_ce: 0.001393, loss_dice: 0.253493
[12:29:19.232] NaN or Inf found in input tensor.
[12:29:19.449] TRAIN: iteration 29121 : loss : 0.250280, loss_ce: 0.000550, loss_dice: 0.500010
[12:29:19.656] TRAIN: iteration 29122 : loss : 0.061454, loss_ce: 0.004592, loss_dice: 0.118317
[12:29:19.862] TRAIN: iteration 29123 : loss : 0.150868, loss_ce: 0.000988, loss_dice: 0.300748
[12:29:20.070] TRAIN: iteration 29124 : loss : 0.196704, loss_ce: 0.001051, loss_dice: 0.392357
[12:29:20.321] TRAIN: iteration 29125 : loss : 0.085684, loss_ce: 0.009978, loss_dice: 0.161390
[12:29:20.527] TRAIN: iteration 29126 : loss : 0.038603, loss_ce: 0.001033, loss_dice: 0.076174
[12:29:20.735] TRAIN: iteration 29127 : loss : 0.072299, loss_ce: 0.003276, loss_dice: 0.141322
[12:29:21.054] TRAIN: iteration 29128 : loss : 0.021200, loss_ce: 0.001966, loss_dice: 0.040434
[12:29:23.355] TRAIN: iteration 29129 : loss : 0.041411, loss_ce: 0.001357, loss_dice: 0.081464
[12:29:23.566] TRAIN: iteration 29130 : loss : 0.071723, loss_ce: 0.001961, loss_dice: 0.141485
[12:29:25.499] TRAIN: iteration 29131 : loss : 0.056773, loss_ce: 0.007366, loss_dice: 0.106181
[12:29:25.711] TRAIN: iteration 29132 : loss : 0.125843, loss_ce: 0.001010, loss_dice: 0.250676
[12:29:25.917] TRAIN: iteration 29133 : loss : 0.107257, loss_ce: 0.002299, loss_dice: 0.212214
[12:29:26.125] TRAIN: iteration 29134 : loss : 0.147473, loss_ce: 0.001215, loss_dice: 0.293731
[12:29:26.332] TRAIN: iteration 29135 : loss : 0.250418, loss_ce: 0.000808, loss_dice: 0.500028
[12:29:26.538] TRAIN: iteration 29136 : loss : 0.047744, loss_ce: 0.004554, loss_dice: 0.090934
[12:29:27.490] TRAIN: iteration 29137 : loss : 0.162978, loss_ce: 0.002225, loss_dice: 0.323732
[12:29:27.700] TRAIN: iteration 29138 : loss : 0.084563, loss_ce: 0.000930, loss_dice: 0.168197
[12:29:27.908] TRAIN: iteration 29139 : loss : 0.094819, loss_ce: 0.003113, loss_dice: 0.186525
[12:29:29.417] TRAIN: iteration 29140 : loss : 0.056129, loss_ce: 0.003815, loss_dice: 0.108444
[12:29:29.657] TRAIN: iteration 29141 : loss : 0.117499, loss_ce: 0.001056, loss_dice: 0.233942
[12:29:29.923] TRAIN: iteration 29142 : loss : 0.071715, loss_ce: 0.004133, loss_dice: 0.139298
[12:29:30.131] TRAIN: iteration 29143 : loss : 0.127546, loss_ce: 0.004769, loss_dice: 0.250323
[12:29:30.508] TRAIN: iteration 29144 : loss : 0.121842, loss_ce: 0.001439, loss_dice: 0.242245
[12:29:32.792] TRAIN: iteration 29145 : loss : 0.236302, loss_ce: 0.002341, loss_dice: 0.470263
[12:29:33.002] TRAIN: iteration 29146 : loss : 0.050107, loss_ce: 0.004457, loss_dice: 0.095756
[12:29:33.209] TRAIN: iteration 29147 : loss : 0.127816, loss_ce: 0.003531, loss_dice: 0.252101
[12:29:33.822] TRAIN: iteration 29148 : loss : 0.147144, loss_ce: 0.005097, loss_dice: 0.289191
[12:29:34.029] TRAIN: iteration 29149 : loss : 0.074469, loss_ce: 0.002152, loss_dice: 0.146786
[12:29:34.237] TRAIN: iteration 29150 : loss : 0.096980, loss_ce: 0.003849, loss_dice: 0.190111
[12:29:34.448] TRAIN: iteration 29151 : loss : 0.090853, loss_ce: 0.001835, loss_dice: 0.179871
[12:29:34.655] TRAIN: iteration 29152 : loss : 0.062341, loss_ce: 0.004503, loss_dice: 0.120179
[12:29:36.638] TRAIN: iteration 29153 : loss : 0.032109, loss_ce: 0.002831, loss_dice: 0.061387
[12:29:36.846] TRAIN: iteration 29154 : loss : 0.087978, loss_ce: 0.003928, loss_dice: 0.172028
[12:29:37.053] TRAIN: iteration 29155 : loss : 0.199572, loss_ce: 0.001408, loss_dice: 0.397736
[12:29:37.262] TRAIN: iteration 29156 : loss : 0.076892, loss_ce: 0.001534, loss_dice: 0.152251
[12:29:37.476] TRAIN: iteration 29157 : loss : 0.250310, loss_ce: 0.002992, loss_dice: 0.497628
[12:29:37.684] TRAIN: iteration 29158 : loss : 0.189426, loss_ce: 0.004748, loss_dice: 0.374104
[12:29:37.893] TRAIN: iteration 29159 : loss : 0.209256, loss_ce: 0.011912, loss_dice: 0.406599
[12:29:39.366] TRAIN: iteration 29160 : loss : 0.229019, loss_ce: 0.004758, loss_dice: 0.453279
[12:29:40.864] TRAIN: iteration 29161 : loss : 0.223483, loss_ce: 0.005791, loss_dice: 0.441175
[12:29:41.078] TRAIN: iteration 29162 : loss : 0.041705, loss_ce: 0.001088, loss_dice: 0.082321
[12:29:41.289] TRAIN: iteration 29163 : loss : 0.121816, loss_ce: 0.001326, loss_dice: 0.242305
[12:29:41.496] TRAIN: iteration 29164 : loss : 0.244872, loss_ce: 0.001070, loss_dice: 0.488673
[12:29:41.704] TRAIN: iteration 29165 : loss : 0.208311, loss_ce: 0.002919, loss_dice: 0.413703
[12:29:41.912] TRAIN: iteration 29166 : loss : 0.053519, loss_ce: 0.002709, loss_dice: 0.104329
[12:29:42.119] TRAIN: iteration 29167 : loss : 0.097914, loss_ce: 0.001681, loss_dice: 0.194146
[12:29:42.519] TRAIN: iteration 29168 : loss : 0.085750, loss_ce: 0.004907, loss_dice: 0.166592
[12:29:47.168] TRAIN: iteration 29169 : loss : 0.251744, loss_ce: 0.005619, loss_dice: 0.497868
[12:29:47.377] TRAIN: iteration 29170 : loss : 0.051231, loss_ce: 0.001314, loss_dice: 0.101149
[12:29:47.585] TRAIN: iteration 29171 : loss : 0.031559, loss_ce: 0.002997, loss_dice: 0.060121
[12:29:47.793] TRAIN: iteration 29172 : loss : 0.088184, loss_ce: 0.003555, loss_dice: 0.172814
[12:29:48.009] TRAIN: iteration 29173 : loss : 0.074380, loss_ce: 0.002327, loss_dice: 0.146432
[12:29:48.217] TRAIN: iteration 29174 : loss : 0.250657, loss_ce: 0.001243, loss_dice: 0.500071
[12:29:48.424] TRAIN: iteration 29175 : loss : 0.040889, loss_ce: 0.002200, loss_dice: 0.079578
[12:29:48.638] TRAIN: iteration 29176 : loss : 0.156911, loss_ce: 0.002009, loss_dice: 0.311813
[12:29:51.385] TRAIN: iteration 29177 : loss : 0.045351, loss_ce: 0.003953, loss_dice: 0.086749
[12:29:51.592] TRAIN: iteration 29178 : loss : 0.053633, loss_ce: 0.001268, loss_dice: 0.105998
[12:29:51.801] TRAIN: iteration 29179 : loss : 0.250649, loss_ce: 0.001240, loss_dice: 0.500058
[12:29:52.010] TRAIN: iteration 29180 : loss : 0.027813, loss_ce: 0.002191, loss_dice: 0.053435
[12:29:52.248] TRAIN: iteration 29181 : loss : 0.214078, loss_ce: 0.002434, loss_dice: 0.425721
[12:29:52.457] TRAIN: iteration 29182 : loss : 0.197129, loss_ce: 0.007822, loss_dice: 0.386436
[12:29:52.676] TRAIN: iteration 29183 : loss : 0.154587, loss_ce: 0.001638, loss_dice: 0.307536
[12:29:52.884] TRAIN: iteration 29184 : loss : 0.068033, loss_ce: 0.000870, loss_dice: 0.135195
[12:29:55.188] TRAIN: iteration 29185 : loss : 0.041008, loss_ce: 0.003276, loss_dice: 0.078741
[12:29:55.395] TRAIN: iteration 29186 : loss : 0.106496, loss_ce: 0.001799, loss_dice: 0.211194
[12:29:55.601] TRAIN: iteration 29187 : loss : 0.138177, loss_ce: 0.001549, loss_dice: 0.274805
[12:29:55.869] TRAIN: iteration 29188 : loss : 0.251456, loss_ce: 0.002744, loss_dice: 0.500167
[12:29:56.076] TRAIN: iteration 29189 : loss : 0.158635, loss_ce: 0.002607, loss_dice: 0.314662
[12:29:56.283] TRAIN: iteration 29190 : loss : 0.250965, loss_ce: 0.001823, loss_dice: 0.500107
[12:29:56.490] TRAIN: iteration 29191 : loss : 0.175465, loss_ce: 0.002140, loss_dice: 0.348791
[12:29:56.698] TRAIN: iteration 29192 : loss : 0.187241, loss_ce: 0.006529, loss_dice: 0.367952
[12:29:59.473] TRAIN: iteration 29193 : loss : 0.059752, loss_ce: 0.001432, loss_dice: 0.118073
[12:29:59.683] TRAIN: iteration 29194 : loss : 0.066268, loss_ce: 0.005841, loss_dice: 0.126694
[12:29:59.893] TRAIN: iteration 29195 : loss : 0.059225, loss_ce: 0.005181, loss_dice: 0.113270
[12:30:00.101] TRAIN: iteration 29196 : loss : 0.131649, loss_ce: 0.011805, loss_dice: 0.251493
[12:30:00.820] TRAIN: iteration 29197 : loss : 0.038071, loss_ce: 0.004028, loss_dice: 0.072113
[12:30:01.026] TRAIN: iteration 29198 : loss : 0.168182, loss_ce: 0.003562, loss_dice: 0.332802
[12:30:01.243] TRAIN: iteration 29199 : loss : 0.063273, loss_ce: 0.003604, loss_dice: 0.122943
[12:30:01.452] TRAIN: iteration 29200 : loss : 0.125295, loss_ce: 0.003015, loss_dice: 0.247575
[12:30:03.704] TRAIN: iteration 29201 : loss : 0.156002, loss_ce: 0.001746, loss_dice: 0.310257
[12:30:03.913] TRAIN: iteration 29202 : loss : 0.043263, loss_ce: 0.002761, loss_dice: 0.083764
[12:30:04.122] TRAIN: iteration 29203 : loss : 0.256522, loss_ce: 0.014149, loss_dice: 0.498896
[12:30:04.328] TRAIN: iteration 29204 : loss : 0.026045, loss_ce: 0.002493, loss_dice: 0.049597
[12:30:04.535] TRAIN: iteration 29205 : loss : 0.092182, loss_ce: 0.003389, loss_dice: 0.180975
[12:30:04.742] TRAIN: iteration 29206 : loss : 0.046970, loss_ce: 0.001236, loss_dice: 0.092704
[12:30:04.953] TRAIN: iteration 29207 : loss : 0.064514, loss_ce: 0.001572, loss_dice: 0.127457
[12:30:05.160] TRAIN: iteration 29208 : loss : 0.051117, loss_ce: 0.004209, loss_dice: 0.098026
[12:30:08.692] TRAIN: iteration 29209 : loss : 0.240273, loss_ce: 0.002346, loss_dice: 0.478200
[12:30:08.905] TRAIN: iteration 29210 : loss : 0.048928, loss_ce: 0.006360, loss_dice: 0.091497
[12:30:09.114] TRAIN: iteration 29211 : loss : 0.164713, loss_ce: 0.002035, loss_dice: 0.327390
[12:30:09.327] TRAIN: iteration 29212 : loss : 0.143498, loss_ce: 0.003699, loss_dice: 0.283298
[12:30:09.534] TRAIN: iteration 29213 : loss : 0.033940, loss_ce: 0.002180, loss_dice: 0.065700
[12:30:11.007] TRAIN: iteration 29214 : loss : 0.208173, loss_ce: 0.001776, loss_dice: 0.414571
[12:30:11.214] TRAIN: iteration 29215 : loss : 0.097377, loss_ce: 0.003392, loss_dice: 0.191363
[12:30:12.700] TRAIN: iteration 29216 : loss : 0.045446, loss_ce: 0.002843, loss_dice: 0.088050
[12:30:13.093] TRAIN: iteration 29217 : loss : 0.250442, loss_ce: 0.000849, loss_dice: 0.500034
[12:30:13.300] TRAIN: iteration 29218 : loss : 0.250675, loss_ce: 0.001282, loss_dice: 0.500068
[12:30:13.507] TRAIN: iteration 29219 : loss : 0.250741, loss_ce: 0.001421, loss_dice: 0.500062
[12:30:14.057] TRAIN: iteration 29220 : loss : 0.141983, loss_ce: 0.002015, loss_dice: 0.281951
[12:30:14.058] NaN or Inf found in input tensor.
[12:30:14.273] TRAIN: iteration 29221 : loss : 0.020492, loss_ce: 0.001489, loss_dice: 0.039494
[12:30:17.492] TRAIN: iteration 29222 : loss : 0.161801, loss_ce: 0.001389, loss_dice: 0.322213
[12:30:17.700] TRAIN: iteration 29223 : loss : 0.071828, loss_ce: 0.004106, loss_dice: 0.139550
[12:30:17.907] TRAIN: iteration 29224 : loss : 0.035722, loss_ce: 0.001205, loss_dice: 0.070240
[12:30:18.115] TRAIN: iteration 29225 : loss : 0.071142, loss_ce: 0.002361, loss_dice: 0.139922
[12:30:18.321] TRAIN: iteration 29226 : loss : 0.087056, loss_ce: 0.005335, loss_dice: 0.168777
[12:30:18.533] TRAIN: iteration 29227 : loss : 0.031245, loss_ce: 0.000925, loss_dice: 0.061565
[12:30:20.163] TRAIN: iteration 29228 : loss : 0.038826, loss_ce: 0.003100, loss_dice: 0.074552
[12:30:20.370] TRAIN: iteration 29229 : loss : 0.047871, loss_ce: 0.007872, loss_dice: 0.087870
[12:30:20.578] TRAIN: iteration 29230 : loss : 0.096819, loss_ce: 0.007230, loss_dice: 0.186407
[12:30:20.787] TRAIN: iteration 29231 : loss : 0.213429, loss_ce: 0.001530, loss_dice: 0.425328
[12:30:20.994] TRAIN: iteration 29232 : loss : 0.042999, loss_ce: 0.001549, loss_dice: 0.084448
[12:30:21.201] TRAIN: iteration 29233 : loss : 0.122958, loss_ce: 0.001710, loss_dice: 0.244205
[12:30:21.410] TRAIN: iteration 29234 : loss : 0.056187, loss_ce: 0.009362, loss_dice: 0.103012
[12:30:21.618] TRAIN: iteration 29235 : loss : 0.081729, loss_ce: 0.003273, loss_dice: 0.160185
[12:30:25.595] TRAIN: iteration 29236 : loss : 0.029635, loss_ce: 0.001481, loss_dice: 0.057790
[12:30:25.804] TRAIN: iteration 29237 : loss : 0.071567, loss_ce: 0.004611, loss_dice: 0.138522
[12:30:26.011] TRAIN: iteration 29238 : loss : 0.033875, loss_ce: 0.001802, loss_dice: 0.065948
[12:30:26.220] TRAIN: iteration 29239 : loss : 0.250477, loss_ce: 0.000913, loss_dice: 0.500041
[12:30:27.500] TRAIN: iteration 29240 : loss : 0.044897, loss_ce: 0.001285, loss_dice: 0.088508
[12:30:27.800] TRAIN: iteration 29241 : loss : 0.031054, loss_ce: 0.002691, loss_dice: 0.059418
[12:30:28.009] TRAIN: iteration 29242 : loss : 0.239418, loss_ce: 0.005159, loss_dice: 0.473678
[12:30:28.215] TRAIN: iteration 29243 : loss : 0.045342, loss_ce: 0.001457, loss_dice: 0.089227
[12:30:28.550] TRAIN: iteration 29244 : loss : 0.143197, loss_ce: 0.005283, loss_dice: 0.281111
[12:30:28.757] TRAIN: iteration 29245 : loss : 0.062649, loss_ce: 0.001279, loss_dice: 0.124019
[12:30:28.965] TRAIN: iteration 29246 : loss : 0.083222, loss_ce: 0.008462, loss_dice: 0.157982
[12:30:29.173] TRAIN: iteration 29247 : loss : 0.136888, loss_ce: 0.011642, loss_dice: 0.262134
[12:30:33.592] TRAIN: iteration 29248 : loss : 0.056857, loss_ce: 0.003810, loss_dice: 0.109905
[12:30:33.862] TRAIN: iteration 29249 : loss : 0.064052, loss_ce: 0.001772, loss_dice: 0.126333
[12:30:34.070] TRAIN: iteration 29250 : loss : 0.111207, loss_ce: 0.001597, loss_dice: 0.220817
[12:30:34.277] TRAIN: iteration 29251 : loss : 0.034288, loss_ce: 0.001886, loss_dice: 0.066690
[12:30:34.485] TRAIN: iteration 29252 : loss : 0.250440, loss_ce: 0.001000, loss_dice: 0.499879
[12:30:34.694] TRAIN: iteration 29253 : loss : 0.092427, loss_ce: 0.001792, loss_dice: 0.183061
[12:30:34.903] TRAIN: iteration 29254 : loss : 0.062519, loss_ce: 0.002275, loss_dice: 0.122764
[12:30:35.111] TRAIN: iteration 29255 : loss : 0.033211, loss_ce: 0.006131, loss_dice: 0.060292
[12:30:40.464] TRAIN: iteration 29256 : loss : 0.250571, loss_ce: 0.001080, loss_dice: 0.500061
[12:30:40.675] TRAIN: iteration 29257 : loss : 0.069619, loss_ce: 0.003496, loss_dice: 0.135743
[12:30:40.883] TRAIN: iteration 29258 : loss : 0.206122, loss_ce: 0.003535, loss_dice: 0.408709
[12:30:41.092] TRAIN: iteration 29259 : loss : 0.081442, loss_ce: 0.001627, loss_dice: 0.161257
[12:30:41.301] TRAIN: iteration 29260 : loss : 0.050559, loss_ce: 0.001409, loss_dice: 0.099709
[12:30:41.538] TRAIN: iteration 29261 : loss : 0.227830, loss_ce: 0.001042, loss_dice: 0.454617
[12:30:41.746] TRAIN: iteration 29262 : loss : 0.250299, loss_ce: 0.000578, loss_dice: 0.500021
[12:30:41.953] TRAIN: iteration 29263 : loss : 0.040789, loss_ce: 0.002062, loss_dice: 0.079516
[12:30:44.533] TRAIN: iteration 29264 : loss : 0.041524, loss_ce: 0.003411, loss_dice: 0.079637
[12:30:44.740] TRAIN: iteration 29265 : loss : 0.042796, loss_ce: 0.001691, loss_dice: 0.083901
[12:30:44.947] TRAIN: iteration 29266 : loss : 0.050240, loss_ce: 0.001026, loss_dice: 0.099454
[12:30:45.156] TRAIN: iteration 29267 : loss : 0.045856, loss_ce: 0.001691, loss_dice: 0.090021
[12:30:45.364] TRAIN: iteration 29268 : loss : 0.054038, loss_ce: 0.002402, loss_dice: 0.105673
[12:30:45.570] TRAIN: iteration 29269 : loss : 0.178851, loss_ce: 0.001505, loss_dice: 0.356197
[12:30:45.777] TRAIN: iteration 29270 : loss : 0.250457, loss_ce: 0.000885, loss_dice: 0.500028
[12:30:45.986] TRAIN: iteration 29271 : loss : 0.250577, loss_ce: 0.001117, loss_dice: 0.500038
[12:30:51.120] TRAIN: iteration 29272 : loss : 0.134574, loss_ce: 0.004624, loss_dice: 0.264524
[12:30:51.329] TRAIN: iteration 29273 : loss : 0.057565, loss_ce: 0.002411, loss_dice: 0.112720
[12:30:51.536] TRAIN: iteration 29274 : loss : 0.250367, loss_ce: 0.000707, loss_dice: 0.500027
[12:30:51.744] TRAIN: iteration 29275 : loss : 0.068482, loss_ce: 0.002326, loss_dice: 0.134638
[12:30:51.953] TRAIN: iteration 29276 : loss : 0.036425, loss_ce: 0.001541, loss_dice: 0.071309
[12:30:52.410] TRAIN: iteration 29277 : loss : 0.159237, loss_ce: 0.001490, loss_dice: 0.316985
[12:30:52.619] TRAIN: iteration 29278 : loss : 0.091856, loss_ce: 0.000734, loss_dice: 0.182978
[12:30:52.826] TRAIN: iteration 29279 : loss : 0.093032, loss_ce: 0.002871, loss_dice: 0.183192
[12:30:55.161] TRAIN: iteration 29280 : loss : 0.041993, loss_ce: 0.000598, loss_dice: 0.083388
[12:30:56.667] TRAIN: iteration 29281 : loss : 0.233070, loss_ce: 0.003603, loss_dice: 0.462538
[12:30:56.875] TRAIN: iteration 29282 : loss : 0.013356, loss_ce: 0.000966, loss_dice: 0.025747
[12:30:57.082] TRAIN: iteration 29283 : loss : 0.133095, loss_ce: 0.001963, loss_dice: 0.264227
[12:30:59.962] TRAIN: iteration 29284 : loss : 0.199565, loss_ce: 0.002887, loss_dice: 0.396243
[12:31:00.173] TRAIN: iteration 29285 : loss : 0.137280, loss_ce: 0.005258, loss_dice: 0.269303
[12:31:00.381] TRAIN: iteration 29286 : loss : 0.191169, loss_ce: 0.004278, loss_dice: 0.378061
[12:31:00.589] TRAIN: iteration 29287 : loss : 0.065654, loss_ce: 0.002778, loss_dice: 0.128529
[12:31:01.420] TRAIN: iteration 29288 : loss : 0.140250, loss_ce: 0.001146, loss_dice: 0.279354
[12:31:01.721] TRAIN: iteration 29289 : loss : 0.157559, loss_ce: 0.005991, loss_dice: 0.309128
[12:31:01.929] TRAIN: iteration 29290 : loss : 0.065728, loss_ce: 0.004612, loss_dice: 0.126843
[12:31:02.138] TRAIN: iteration 29291 : loss : 0.150227, loss_ce: 0.001859, loss_dice: 0.298594
[12:31:06.955] TRAIN: iteration 29292 : loss : 0.145111, loss_ce: 0.003929, loss_dice: 0.286294
[12:31:07.171] TRAIN: iteration 29293 : loss : 0.041762, loss_ce: 0.001367, loss_dice: 0.082157
[12:31:07.378] TRAIN: iteration 29294 : loss : 0.044182, loss_ce: 0.003192, loss_dice: 0.085173
[12:31:07.585] TRAIN: iteration 29295 : loss : 0.053230, loss_ce: 0.000960, loss_dice: 0.105500
[12:31:07.792] TRAIN: iteration 29296 : loss : 0.032076, loss_ce: 0.005812, loss_dice: 0.058341
[12:31:08.002] TRAIN: iteration 29297 : loss : 0.150372, loss_ce: 0.005137, loss_dice: 0.295608
[12:31:08.209] TRAIN: iteration 29298 : loss : 0.022032, loss_ce: 0.001572, loss_dice: 0.042492
[12:31:08.528] TRAIN: iteration 29299 : loss : 0.138626, loss_ce: 0.004726, loss_dice: 0.272526
[12:31:12.339] TRAIN: iteration 29300 : loss : 0.029877, loss_ce: 0.001737, loss_dice: 0.058018
[12:31:12.573] TRAIN: iteration 29301 : loss : 0.238691, loss_ce: 0.000669, loss_dice: 0.476713
[12:31:12.789] TRAIN: iteration 29302 : loss : 0.145720, loss_ce: 0.015069, loss_dice: 0.276371
[12:31:13.001] TRAIN: iteration 29303 : loss : 0.066860, loss_ce: 0.003258, loss_dice: 0.130462
[12:31:13.476] TRAIN: iteration 29304 : loss : 0.250458, loss_ce: 0.000877, loss_dice: 0.500039
[12:31:15.831] TRAIN: iteration 29305 : loss : 0.041313, loss_ce: 0.003469, loss_dice: 0.079157
[12:31:16.043] TRAIN: iteration 29306 : loss : 0.034044, loss_ce: 0.001365, loss_dice: 0.066723
[12:31:16.256] TRAIN: iteration 29307 : loss : 0.249589, loss_ce: 0.001657, loss_dice: 0.497522
[12:31:19.130] TRAIN: iteration 29308 : loss : 0.037948, loss_ce: 0.003072, loss_dice: 0.072824
[12:31:19.337] TRAIN: iteration 29309 : loss : 0.083792, loss_ce: 0.002823, loss_dice: 0.164760
[12:31:19.544] TRAIN: iteration 29310 : loss : 0.029159, loss_ce: 0.001599, loss_dice: 0.056718
[12:31:19.752] TRAIN: iteration 29311 : loss : 0.052967, loss_ce: 0.002966, loss_dice: 0.102969
[12:31:20.014] TRAIN: iteration 29312 : loss : 0.130943, loss_ce: 0.003686, loss_dice: 0.258200
[12:31:22.364] TRAIN: iteration 29313 : loss : 0.106934, loss_ce: 0.002570, loss_dice: 0.211299
[12:31:22.573] TRAIN: iteration 29314 : loss : 0.053640, loss_ce: 0.003507, loss_dice: 0.103773
[12:31:22.781] TRAIN: iteration 29315 : loss : 0.120436, loss_ce: 0.003340, loss_dice: 0.237532
[12:31:24.556] TRAIN: iteration 29316 : loss : 0.250916, loss_ce: 0.001727, loss_dice: 0.500104
[12:31:24.765] TRAIN: iteration 29317 : loss : 0.044954, loss_ce: 0.003484, loss_dice: 0.086423
[12:31:24.973] TRAIN: iteration 29318 : loss : 0.049407, loss_ce: 0.005261, loss_dice: 0.093553
[12:31:25.185] TRAIN: iteration 29319 : loss : 0.248540, loss_ce: 0.001541, loss_dice: 0.495539
[12:31:25.973] TRAIN: iteration 29320 : loss : 0.250468, loss_ce: 0.000905, loss_dice: 0.500031
[12:31:30.792] TRAIN: iteration 29321 : loss : 0.180814, loss_ce: 0.006248, loss_dice: 0.355380
[12:31:30.999] TRAIN: iteration 29322 : loss : 0.037893, loss_ce: 0.005490, loss_dice: 0.070296
[12:31:31.206] TRAIN: iteration 29323 : loss : 0.073308, loss_ce: 0.001462, loss_dice: 0.145154
[12:31:32.142] TRAIN: iteration 29324 : loss : 0.250472, loss_ce: 0.000922, loss_dice: 0.500022
[12:31:32.349] TRAIN: iteration 29325 : loss : 0.030637, loss_ce: 0.004503, loss_dice: 0.056771
[12:31:32.556] TRAIN: iteration 29326 : loss : 0.056930, loss_ce: 0.001971, loss_dice: 0.111889
[12:31:32.764] TRAIN: iteration 29327 : loss : 0.091327, loss_ce: 0.010027, loss_dice: 0.172626
[12:31:34.033] TRAIN: iteration 29328 : loss : 0.031579, loss_ce: 0.002444, loss_dice: 0.060713
[12:31:36.286] TRAIN: iteration 29329 : loss : 0.042034, loss_ce: 0.005332, loss_dice: 0.078736
[12:31:36.494] TRAIN: iteration 29330 : loss : 0.149603, loss_ce: 0.001487, loss_dice: 0.297720
[12:31:36.702] TRAIN: iteration 29331 : loss : 0.090058, loss_ce: 0.003447, loss_dice: 0.176669
[12:31:40.177] TRAIN: iteration 29332 : loss : 0.059202, loss_ce: 0.001301, loss_dice: 0.117103
[12:31:40.388] TRAIN: iteration 29333 : loss : 0.146129, loss_ce: 0.001377, loss_dice: 0.290881
[12:31:40.595] TRAIN: iteration 29334 : loss : 0.250457, loss_ce: 0.000869, loss_dice: 0.500046
[12:31:40.802] TRAIN: iteration 29335 : loss : 0.047489, loss_ce: 0.001867, loss_dice: 0.093111
[12:31:41.015] TRAIN: iteration 29336 : loss : 0.185403, loss_ce: 0.003949, loss_dice: 0.366857
[12:31:42.374] TRAIN: iteration 29337 : loss : 0.059040, loss_ce: 0.000694, loss_dice: 0.117385
[12:31:42.583] TRAIN: iteration 29338 : loss : 0.250550, loss_ce: 0.001055, loss_dice: 0.500045
[12:31:43.726] TRAIN: iteration 29339 : loss : 0.091982, loss_ce: 0.003092, loss_dice: 0.180871
[12:31:46.287] TRAIN: iteration 29340 : loss : 0.250210, loss_ce: 0.000413, loss_dice: 0.500007
[12:31:46.522] TRAIN: iteration 29341 : loss : 0.098456, loss_ce: 0.001146, loss_dice: 0.195766
[12:31:46.729] TRAIN: iteration 29342 : loss : 0.250214, loss_ce: 0.000421, loss_dice: 0.500006
[12:31:46.935] TRAIN: iteration 29343 : loss : 0.052317, loss_ce: 0.002991, loss_dice: 0.101643
[12:31:48.031] TRAIN: iteration 29344 : loss : 0.187856, loss_ce: 0.001913, loss_dice: 0.373799
[12:31:49.813] TRAIN: iteration 29345 : loss : 0.126535, loss_ce: 0.002997, loss_dice: 0.250072
[12:31:50.022] TRAIN: iteration 29346 : loss : 0.079068, loss_ce: 0.003399, loss_dice: 0.154737
[12:31:50.230] TRAIN: iteration 29347 : loss : 0.250248, loss_ce: 0.000483, loss_dice: 0.500012
[12:31:54.889] TRAIN: iteration 29348 : loss : 0.092330, loss_ce: 0.003550, loss_dice: 0.181110
[12:31:55.098] TRAIN: iteration 29349 : loss : 0.070832, loss_ce: 0.013303, loss_dice: 0.128362
[12:31:55.306] TRAIN: iteration 29350 : loss : 0.048922, loss_ce: 0.001923, loss_dice: 0.095921
[12:31:55.516] TRAIN: iteration 29351 : loss : 0.044095, loss_ce: 0.003331, loss_dice: 0.084859
[12:31:55.724] TRAIN: iteration 29352 : loss : 0.077517, loss_ce: 0.005188, loss_dice: 0.149846
[12:31:56.860] TRAIN: iteration 29353 : loss : 0.046946, loss_ce: 0.004109, loss_dice: 0.089783
[12:31:57.071] TRAIN: iteration 29354 : loss : 0.123550, loss_ce: 0.002095, loss_dice: 0.245005
[12:31:58.352] TRAIN: iteration 29355 : loss : 0.149116, loss_ce: 0.002148, loss_dice: 0.296084
[12:32:02.134] TRAIN: iteration 29356 : loss : 0.061063, loss_ce: 0.004251, loss_dice: 0.117874
[12:32:02.342] TRAIN: iteration 29357 : loss : 0.105663, loss_ce: 0.002768, loss_dice: 0.208558
[12:32:02.553] TRAIN: iteration 29358 : loss : 0.114380, loss_ce: 0.002612, loss_dice: 0.226148
[12:32:03.363] TRAIN: iteration 29359 : loss : 0.080329, loss_ce: 0.002306, loss_dice: 0.158352
[12:32:03.570] TRAIN: iteration 29360 : loss : 0.242803, loss_ce: 0.002622, loss_dice: 0.482983
[12:32:04.542] TRAIN: iteration 29361 : loss : 0.063816, loss_ce: 0.003484, loss_dice: 0.124147
[12:32:04.753] TRAIN: iteration 29362 : loss : 0.058512, loss_ce: 0.008007, loss_dice: 0.109018
[12:32:04.960] TRAIN: iteration 29363 : loss : 0.057308, loss_ce: 0.006601, loss_dice: 0.108015
[12:32:06.847] TRAIN: iteration 29364 : loss : 0.041793, loss_ce: 0.005414, loss_dice: 0.078172
[12:32:09.857] TRAIN: iteration 29365 : loss : 0.046371, loss_ce: 0.007123, loss_dice: 0.085619
[12:32:10.064] TRAIN: iteration 29366 : loss : 0.106672, loss_ce: 0.003078, loss_dice: 0.210266
[12:32:13.072] TRAIN: iteration 29367 : loss : 0.143945, loss_ce: 0.006985, loss_dice: 0.280905
[12:32:13.280] TRAIN: iteration 29368 : loss : 0.039542, loss_ce: 0.002382, loss_dice: 0.076701
[12:32:13.924] TRAIN: iteration 29369 : loss : 0.051165, loss_ce: 0.002805, loss_dice: 0.099526
[12:32:14.131] TRAIN: iteration 29370 : loss : 0.250763, loss_ce: 0.002423, loss_dice: 0.499103
[12:32:14.339] TRAIN: iteration 29371 : loss : 0.069173, loss_ce: 0.003988, loss_dice: 0.134357
[12:32:16.505] TRAIN: iteration 29372 : loss : 0.117604, loss_ce: 0.001330, loss_dice: 0.233879
[12:32:17.946] TRAIN: iteration 29373 : loss : 0.029230, loss_ce: 0.002116, loss_dice: 0.056344
[12:32:18.155] TRAIN: iteration 29374 : loss : 0.046922, loss_ce: 0.001938, loss_dice: 0.091905
[12:32:18.363] TRAIN: iteration 29375 : loss : 0.109275, loss_ce: 0.004432, loss_dice: 0.214118
[12:32:18.569] TRAIN: iteration 29376 : loss : 0.027678, loss_ce: 0.001459, loss_dice: 0.053898
[12:32:23.146] TRAIN: iteration 29377 : loss : 0.093520, loss_ce: 0.000678, loss_dice: 0.186362
[12:32:23.359] TRAIN: iteration 29378 : loss : 0.142713, loss_ce: 0.002720, loss_dice: 0.282705
[12:32:23.566] TRAIN: iteration 29379 : loss : 0.069199, loss_ce: 0.001684, loss_dice: 0.136714
[12:32:26.857] TRAIN: iteration 29380 : loss : 0.246447, loss_ce: 0.001305, loss_dice: 0.491588
[12:32:27.094] TRAIN: iteration 29381 : loss : 0.151634, loss_ce: 0.001245, loss_dice: 0.302024
[12:32:27.303] TRAIN: iteration 29382 : loss : 0.199089, loss_ce: 0.002239, loss_dice: 0.395940
[12:32:27.509] TRAIN: iteration 29383 : loss : 0.041262, loss_ce: 0.001075, loss_dice: 0.081449
[12:32:27.716] TRAIN: iteration 29384 : loss : 0.113844, loss_ce: 0.004199, loss_dice: 0.223489
[12:32:32.161] TRAIN: iteration 29385 : loss : 0.061859, loss_ce: 0.001013, loss_dice: 0.122704
[12:32:32.372] TRAIN: iteration 29386 : loss : 0.032166, loss_ce: 0.001208, loss_dice: 0.063124
[12:32:32.581] TRAIN: iteration 29387 : loss : 0.152483, loss_ce: 0.001167, loss_dice: 0.303800
[12:32:35.973] TRAIN: iteration 29388 : loss : 0.246128, loss_ce: 0.002675, loss_dice: 0.489580
[12:32:36.180] TRAIN: iteration 29389 : loss : 0.229230, loss_ce: 0.002291, loss_dice: 0.456170
[12:32:36.388] TRAIN: iteration 29390 : loss : 0.054837, loss_ce: 0.001039, loss_dice: 0.108634
[12:32:36.594] TRAIN: iteration 29391 : loss : 0.250239, loss_ce: 0.000458, loss_dice: 0.500019
[12:32:36.802] TRAIN: iteration 29392 : loss : 0.088583, loss_ce: 0.005857, loss_dice: 0.171309
[12:32:38.431] TRAIN: iteration 29393 : loss : 0.121097, loss_ce: 0.004692, loss_dice: 0.237502
[12:32:38.640] TRAIN: iteration 29394 : loss : 0.250197, loss_ce: 0.000387, loss_dice: 0.500007
[12:32:38.846] TRAIN: iteration 29395 : loss : 0.102419, loss_ce: 0.031537, loss_dice: 0.173301
[12:32:46.424] TRAIN: iteration 29396 : loss : 0.251517, loss_ce: 0.002843, loss_dice: 0.500192
[12:32:46.634] TRAIN: iteration 29397 : loss : 0.105012, loss_ce: 0.004963, loss_dice: 0.205060
[12:32:46.846] TRAIN: iteration 29398 : loss : 0.130370, loss_ce: 0.002691, loss_dice: 0.258048
[12:32:47.054] TRAIN: iteration 29399 : loss : 0.122879, loss_ce: 0.000644, loss_dice: 0.245114
[12:32:47.262] TRAIN: iteration 29400 : loss : 0.250469, loss_ce: 0.000908, loss_dice: 0.500030
[12:32:48.286] TRAIN: iteration 29401 : loss : 0.206920, loss_ce: 0.000815, loss_dice: 0.413025
[12:32:48.495] TRAIN: iteration 29402 : loss : 0.245820, loss_ce: 0.001482, loss_dice: 0.490158
[12:32:48.702] TRAIN: iteration 29403 : loss : 0.030587, loss_ce: 0.002472, loss_dice: 0.058701
[12:32:53.168] TRAIN: iteration 29404 : loss : 0.250859, loss_ce: 0.001603, loss_dice: 0.500115
[12:32:53.382] TRAIN: iteration 29405 : loss : 0.019543, loss_ce: 0.000891, loss_dice: 0.038195
[12:32:53.602] TRAIN: iteration 29406 : loss : 0.073125, loss_ce: 0.003546, loss_dice: 0.142703
[12:32:53.809] TRAIN: iteration 29407 : loss : 0.250096, loss_ce: 0.000192, loss_dice: 0.499999
[12:32:54.018] TRAIN: iteration 29408 : loss : 0.065165, loss_ce: 0.005436, loss_dice: 0.124893
[12:32:54.277] TRAIN: iteration 29409 : loss : 0.086995, loss_ce: 0.001806, loss_dice: 0.172184
[12:32:54.484] TRAIN: iteration 29410 : loss : 0.107077, loss_ce: 0.002367, loss_dice: 0.211787
[12:32:57.519] TRAIN: iteration 29411 : loss : 0.056406, loss_ce: 0.001501, loss_dice: 0.111311
[12:33:02.207] TRAIN: iteration 29412 : loss : 0.026804, loss_ce: 0.001989, loss_dice: 0.051620
[12:33:02.420] TRAIN: iteration 29413 : loss : 0.047636, loss_ce: 0.001037, loss_dice: 0.094235
[12:33:02.627] TRAIN: iteration 29414 : loss : 0.033348, loss_ce: 0.007402, loss_dice: 0.059295
[12:33:02.834] TRAIN: iteration 29415 : loss : 0.097725, loss_ce: 0.010041, loss_dice: 0.185410
[12:33:03.042] TRAIN: iteration 29416 : loss : 0.101819, loss_ce: 0.002054, loss_dice: 0.201584
[12:33:03.254] TRAIN: iteration 29417 : loss : 0.077044, loss_ce: 0.004358, loss_dice: 0.149731
[12:33:03.462] TRAIN: iteration 29418 : loss : 0.105408, loss_ce: 0.008106, loss_dice: 0.202710
[12:33:04.793] TRAIN: iteration 29419 : loss : 0.111928, loss_ce: 0.003724, loss_dice: 0.220133
[12:33:09.309] TRAIN: iteration 29420 : loss : 0.129340, loss_ce: 0.015161, loss_dice: 0.243518
[12:33:09.310] NaN or Inf found in input tensor.
[12:33:09.525] TRAIN: iteration 29421 : loss : 0.074187, loss_ce: 0.005495, loss_dice: 0.142879
[12:33:09.733] TRAIN: iteration 29422 : loss : 0.185303, loss_ce: 0.011132, loss_dice: 0.359474
[12:33:09.942] TRAIN: iteration 29423 : loss : 0.085069, loss_ce: 0.000546, loss_dice: 0.169592
[12:33:10.151] TRAIN: iteration 29424 : loss : 0.078070, loss_ce: 0.003283, loss_dice: 0.152856
[12:33:11.903] TRAIN: iteration 29425 : loss : 0.033643, loss_ce: 0.001485, loss_dice: 0.065802
[12:33:12.110] TRAIN: iteration 29426 : loss : 0.242740, loss_ce: 0.001604, loss_dice: 0.483877
[12:33:12.318] TRAIN: iteration 29427 : loss : 0.063149, loss_ce: 0.004304, loss_dice: 0.121994
[12:33:18.048] TRAIN: iteration 29428 : loss : 0.250430, loss_ce: 0.000808, loss_dice: 0.500051
[12:33:18.256] TRAIN: iteration 29429 : loss : 0.072980, loss_ce: 0.001260, loss_dice: 0.144701
[12:33:18.463] TRAIN: iteration 29430 : loss : 0.123344, loss_ce: 0.002054, loss_dice: 0.244633
[12:33:18.671] TRAIN: iteration 29431 : loss : 0.090518, loss_ce: 0.006413, loss_dice: 0.174623
[12:33:18.878] TRAIN: iteration 29432 : loss : 0.253528, loss_ce: 0.007325, loss_dice: 0.499731
[12:33:21.802] TRAIN: iteration 29433 : loss : 0.034983, loss_ce: 0.001287, loss_dice: 0.068679
[12:33:22.009] TRAIN: iteration 29434 : loss : 0.066862, loss_ce: 0.001348, loss_dice: 0.132375
[12:33:22.217] TRAIN: iteration 29435 : loss : 0.251057, loss_ce: 0.002620, loss_dice: 0.499494
[12:33:28.025] TRAIN: iteration 29436 : loss : 0.250229, loss_ce: 0.000447, loss_dice: 0.500012
[12:33:28.232] TRAIN: iteration 29437 : loss : 0.118403, loss_ce: 0.001687, loss_dice: 0.235118
[12:33:28.441] TRAIN: iteration 29438 : loss : 0.038831, loss_ce: 0.001528, loss_dice: 0.076133
[12:33:28.648] TRAIN: iteration 29439 : loss : 0.249571, loss_ce: 0.000694, loss_dice: 0.498448
[12:33:28.861] TRAIN: iteration 29440 : loss : 0.022926, loss_ce: 0.001256, loss_dice: 0.044596
[12:33:28.861] NaN or Inf found in input tensor.
[12:33:30.651] TRAIN: iteration 29441 : loss : 0.172253, loss_ce: 0.009453, loss_dice: 0.335052
[12:33:30.859] TRAIN: iteration 29442 : loss : 0.033875, loss_ce: 0.003082, loss_dice: 0.064668
[12:33:31.066] TRAIN: iteration 29443 : loss : 0.250482, loss_ce: 0.000906, loss_dice: 0.500057
[12:33:36.969] TRAIN: iteration 29444 : loss : 0.100779, loss_ce: 0.001399, loss_dice: 0.200159
[12:33:37.179] TRAIN: iteration 29445 : loss : 0.063773, loss_ce: 0.003146, loss_dice: 0.124401
[12:33:37.387] TRAIN: iteration 29446 : loss : 0.080575, loss_ce: 0.006725, loss_dice: 0.154425
[12:33:37.595] TRAIN: iteration 29447 : loss : 0.103669, loss_ce: 0.013944, loss_dice: 0.193393
[12:33:37.802] TRAIN: iteration 29448 : loss : 0.061282, loss_ce: 0.003032, loss_dice: 0.119533
[12:33:39.700] TRAIN: iteration 29449 : loss : 0.037946, loss_ce: 0.003921, loss_dice: 0.071971
[12:33:39.908] TRAIN: iteration 29450 : loss : 0.037627, loss_ce: 0.002993, loss_dice: 0.072260
[12:33:40.116] TRAIN: iteration 29451 : loss : 0.068174, loss_ce: 0.000902, loss_dice: 0.135445
[12:33:45.388] TRAIN: iteration 29452 : loss : 0.246880, loss_ce: 0.001592, loss_dice: 0.492167
[12:33:45.598] TRAIN: iteration 29453 : loss : 0.203421, loss_ce: 0.000395, loss_dice: 0.406447
[12:33:45.806] TRAIN: iteration 29454 : loss : 0.046078, loss_ce: 0.006930, loss_dice: 0.085226
[12:33:46.012] TRAIN: iteration 29455 : loss : 0.075592, loss_ce: 0.003858, loss_dice: 0.147326
[12:33:46.221] TRAIN: iteration 29456 : loss : 0.204794, loss_ce: 0.003788, loss_dice: 0.405801
[12:33:47.809] TRAIN: iteration 29457 : loss : 0.110373, loss_ce: 0.001886, loss_dice: 0.218861
[12:33:48.016] TRAIN: iteration 29458 : loss : 0.068563, loss_ce: 0.000802, loss_dice: 0.136324
[12:33:48.224] TRAIN: iteration 29459 : loss : 0.049279, loss_ce: 0.001248, loss_dice: 0.097311
[12:33:54.036] TRAIN: iteration 29460 : loss : 0.022352, loss_ce: 0.001957, loss_dice: 0.042747
[12:33:54.274] TRAIN: iteration 29461 : loss : 0.029588, loss_ce: 0.001619, loss_dice: 0.057557
[12:33:54.481] TRAIN: iteration 29462 : loss : 0.114704, loss_ce: 0.005680, loss_dice: 0.223728
[12:33:54.688] TRAIN: iteration 29463 : loss : 0.036877, loss_ce: 0.005401, loss_dice: 0.068352
[12:33:54.895] TRAIN: iteration 29464 : loss : 0.054738, loss_ce: 0.004666, loss_dice: 0.104811
[12:33:56.684] TRAIN: iteration 29465 : loss : 0.248176, loss_ce: 0.001024, loss_dice: 0.495328
[12:33:56.891] TRAIN: iteration 29466 : loss : 0.092610, loss_ce: 0.002639, loss_dice: 0.182580
[12:33:57.162] TRAIN: iteration 29467 : loss : 0.068053, loss_ce: 0.005440, loss_dice: 0.130667
[12:34:01.296] TRAIN: iteration 29468 : loss : 0.235630, loss_ce: 0.010403, loss_dice: 0.460858
[12:34:01.503] TRAIN: iteration 29469 : loss : 0.049500, loss_ce: 0.002193, loss_dice: 0.096806
[12:34:01.712] TRAIN: iteration 29470 : loss : 0.163852, loss_ce: 0.001293, loss_dice: 0.326412
[12:34:01.920] TRAIN: iteration 29471 : loss : 0.022517, loss_ce: 0.002285, loss_dice: 0.042749
[12:34:02.133] TRAIN: iteration 29472 : loss : 0.059512, loss_ce: 0.002531, loss_dice: 0.116492
[12:34:06.685] TRAIN: iteration 29473 : loss : 0.250162, loss_ce: 0.000321, loss_dice: 0.500004
[12:34:06.893] TRAIN: iteration 29474 : loss : 0.071379, loss_ce: 0.016492, loss_dice: 0.126265
[12:34:07.105] TRAIN: iteration 29475 : loss : 0.007503, loss_ce: 0.000918, loss_dice: 0.014087
[12:34:08.080] TRAIN: iteration 29476 : loss : 0.052791, loss_ce: 0.002247, loss_dice: 0.103335
[12:34:10.499] TRAIN: iteration 29477 : loss : 0.222152, loss_ce: 0.000837, loss_dice: 0.443467
[12:34:10.707] TRAIN: iteration 29478 : loss : 0.070377, loss_ce: 0.003246, loss_dice: 0.137508
[12:34:10.916] TRAIN: iteration 29479 : loss : 0.076201, loss_ce: 0.012265, loss_dice: 0.140137
[12:34:11.124] TRAIN: iteration 29480 : loss : 0.145642, loss_ce: 0.004502, loss_dice: 0.286783
[12:34:16.815] TRAIN: iteration 29481 : loss : 0.051569, loss_ce: 0.008327, loss_dice: 0.094812
[12:34:17.021] TRAIN: iteration 29482 : loss : 0.242116, loss_ce: 0.007080, loss_dice: 0.477153
[12:34:17.228] TRAIN: iteration 29483 : loss : 0.080021, loss_ce: 0.007868, loss_dice: 0.152174
[12:34:18.801] TRAIN: iteration 29484 : loss : 0.063731, loss_ce: 0.005122, loss_dice: 0.122340
[12:34:19.013] TRAIN: iteration 29485 : loss : 0.154522, loss_ce: 0.008414, loss_dice: 0.300630
[12:34:19.220] TRAIN: iteration 29486 : loss : 0.148309, loss_ce: 0.001777, loss_dice: 0.294842
[12:34:19.428] TRAIN: iteration 29487 : loss : 0.043322, loss_ce: 0.004536, loss_dice: 0.082109
[12:34:19.636] TRAIN: iteration 29488 : loss : 0.051149, loss_ce: 0.003731, loss_dice: 0.098568
[12:34:26.085] TRAIN: iteration 29489 : loss : 0.086691, loss_ce: 0.002284, loss_dice: 0.171099
[12:34:26.299] TRAIN: iteration 29490 : loss : 0.077522, loss_ce: 0.002125, loss_dice: 0.152918
[12:34:26.508] TRAIN: iteration 29491 : loss : 0.029540, loss_ce: 0.002102, loss_dice: 0.056978
[12:34:27.528] TRAIN: iteration 29492 : loss : 0.051881, loss_ce: 0.008926, loss_dice: 0.094837
[12:34:27.735] TRAIN: iteration 29493 : loss : 0.082756, loss_ce: 0.001522, loss_dice: 0.163991
[12:34:27.942] TRAIN: iteration 29494 : loss : 0.250951, loss_ce: 0.001798, loss_dice: 0.500104
[12:34:28.037] TRAIN: iteration 29495 : loss : 0.250352, loss_ce: 0.000700, loss_dice: 0.500005
[12:39:42.581] VALIDATION: iteration 16 : loss : 0.113033, loss_ce: 0.004030, loss_dice: 0.222037
[12:39:44.553] TRAIN: iteration 29496 : loss : 0.039573, loss_ce: 0.000734, loss_dice: 0.078412
[12:39:45.749] TRAIN: iteration 29497 : loss : 0.025748, loss_ce: 0.002707, loss_dice: 0.048788
[12:39:45.956] TRAIN: iteration 29498 : loss : 0.044699, loss_ce: 0.003459, loss_dice: 0.085939
[12:39:46.163] TRAIN: iteration 29499 : loss : 0.063803, loss_ce: 0.007661, loss_dice: 0.119945
[12:39:46.372] TRAIN: iteration 29500 : loss : 0.111824, loss_ce: 0.006253, loss_dice: 0.217395
[12:39:46.620] TRAIN: iteration 29501 : loss : 0.242850, loss_ce: 0.003175, loss_dice: 0.482526
[12:39:46.833] TRAIN: iteration 29502 : loss : 0.051045, loss_ce: 0.002974, loss_dice: 0.099116
[12:39:47.042] TRAIN: iteration 29503 : loss : 0.250824, loss_ce: 0.001589, loss_dice: 0.500059
[12:39:47.251] TRAIN: iteration 29504 : loss : 0.069489, loss_ce: 0.001967, loss_dice: 0.137011
[12:39:47.699] TRAIN: iteration 29505 : loss : 0.170047, loss_ce: 0.001218, loss_dice: 0.338876
[12:39:47.908] TRAIN: iteration 29506 : loss : 0.200889, loss_ce: 0.003406, loss_dice: 0.398371
[12:39:48.117] TRAIN: iteration 29507 : loss : 0.247818, loss_ce: 0.002055, loss_dice: 0.493582
[12:39:48.325] TRAIN: iteration 29508 : loss : 0.250974, loss_ce: 0.001846, loss_dice: 0.500103
[12:39:48.535] TRAIN: iteration 29509 : loss : 0.041425, loss_ce: 0.002743, loss_dice: 0.080106
[12:39:48.743] TRAIN: iteration 29510 : loss : 0.022946, loss_ce: 0.001415, loss_dice: 0.044477
[12:39:49.111] TRAIN: iteration 29511 : loss : 0.250291, loss_ce: 0.000570, loss_dice: 0.500012
[12:39:49.319] TRAIN: iteration 29512 : loss : 0.250655, loss_ce: 0.001245, loss_dice: 0.500064
[12:39:49.529] TRAIN: iteration 29513 : loss : 0.098102, loss_ce: 0.001953, loss_dice: 0.194250
[12:39:49.740] TRAIN: iteration 29514 : loss : 0.018329, loss_ce: 0.000719, loss_dice: 0.035940
[12:39:49.952] TRAIN: iteration 29515 : loss : 0.045454, loss_ce: 0.005792, loss_dice: 0.085115
[12:39:50.162] TRAIN: iteration 29516 : loss : 0.078184, loss_ce: 0.001365, loss_dice: 0.155002
[12:39:50.372] TRAIN: iteration 29517 : loss : 0.027918, loss_ce: 0.003813, loss_dice: 0.052022
[12:39:50.580] TRAIN: iteration 29518 : loss : 0.093579, loss_ce: 0.002524, loss_dice: 0.184634
[12:39:50.788] TRAIN: iteration 29519 : loss : 0.250314, loss_ce: 0.000617, loss_dice: 0.500011
[12:39:51.008] TRAIN: iteration 29520 : loss : 0.057314, loss_ce: 0.007638, loss_dice: 0.106990
[12:39:51.009] NaN or Inf found in input tensor.
[12:39:51.238] TRAIN: iteration 29521 : loss : 0.124617, loss_ce: 0.001641, loss_dice: 0.247594
[12:39:51.447] TRAIN: iteration 29522 : loss : 0.048184, loss_ce: 0.004570, loss_dice: 0.091797
[12:39:51.661] TRAIN: iteration 29523 : loss : 0.250534, loss_ce: 0.002552, loss_dice: 0.498516
[12:39:51.873] TRAIN: iteration 29524 : loss : 0.144944, loss_ce: 0.001299, loss_dice: 0.288590
[12:39:52.088] TRAIN: iteration 29525 : loss : 0.132782, loss_ce: 0.005763, loss_dice: 0.259801
[12:39:52.299] TRAIN: iteration 29526 : loss : 0.247844, loss_ce: 0.000601, loss_dice: 0.495088
[12:39:52.512] TRAIN: iteration 29527 : loss : 0.118884, loss_ce: 0.001188, loss_dice: 0.236580
[12:39:52.719] TRAIN: iteration 29528 : loss : 0.250372, loss_ce: 0.000720, loss_dice: 0.500024
[12:39:52.929] TRAIN: iteration 29529 : loss : 0.250566, loss_ce: 0.001094, loss_dice: 0.500038
[12:39:53.137] TRAIN: iteration 29530 : loss : 0.241703, loss_ce: 0.002494, loss_dice: 0.480913
[12:39:53.347] TRAIN: iteration 29531 : loss : 0.250369, loss_ce: 0.000715, loss_dice: 0.500022
[12:39:53.557] TRAIN: iteration 29532 : loss : 0.036919, loss_ce: 0.005506, loss_dice: 0.068332
[12:39:53.774] TRAIN: iteration 29533 : loss : 0.069634, loss_ce: 0.001773, loss_dice: 0.137494
[12:39:53.988] TRAIN: iteration 29534 : loss : 0.049703, loss_ce: 0.006171, loss_dice: 0.093235
[12:39:54.200] TRAIN: iteration 29535 : loss : 0.158318, loss_ce: 0.006901, loss_dice: 0.309736
[12:39:54.416] TRAIN: iteration 29536 : loss : 0.100176, loss_ce: 0.005500, loss_dice: 0.194852
[12:39:54.627] TRAIN: iteration 29537 : loss : 0.250543, loss_ce: 0.001035, loss_dice: 0.500051
[12:39:54.842] TRAIN: iteration 29538 : loss : 0.060419, loss_ce: 0.004828, loss_dice: 0.116009
[12:39:55.054] TRAIN: iteration 29539 : loss : 0.251605, loss_ce: 0.003505, loss_dice: 0.499705
[12:39:55.265] TRAIN: iteration 29540 : loss : 0.174345, loss_ce: 0.002883, loss_dice: 0.345807
[12:39:55.510] TRAIN: iteration 29541 : loss : 0.037342, loss_ce: 0.003977, loss_dice: 0.070708
[12:39:55.719] TRAIN: iteration 29542 : loss : 0.068063, loss_ce: 0.003791, loss_dice: 0.132334
[12:39:55.928] TRAIN: iteration 29543 : loss : 0.103472, loss_ce: 0.005623, loss_dice: 0.201320
[12:39:56.137] TRAIN: iteration 29544 : loss : 0.061516, loss_ce: 0.002305, loss_dice: 0.120727
[12:39:56.348] TRAIN: iteration 29545 : loss : 0.204652, loss_ce: 0.005253, loss_dice: 0.404051
[12:39:56.556] TRAIN: iteration 29546 : loss : 0.062480, loss_ce: 0.005684, loss_dice: 0.119275
[12:39:56.764] TRAIN: iteration 29547 : loss : 0.033356, loss_ce: 0.004376, loss_dice: 0.062335
[12:39:56.972] TRAIN: iteration 29548 : loss : 0.232776, loss_ce: 0.003693, loss_dice: 0.461860
[12:39:57.180] TRAIN: iteration 29549 : loss : 0.049882, loss_ce: 0.005470, loss_dice: 0.094293
[12:39:57.388] TRAIN: iteration 29550 : loss : 0.061692, loss_ce: 0.001197, loss_dice: 0.122187
[12:39:57.595] TRAIN: iteration 29551 : loss : 0.057923, loss_ce: 0.002097, loss_dice: 0.113748
[12:39:57.802] TRAIN: iteration 29552 : loss : 0.241138, loss_ce: 0.003656, loss_dice: 0.478621
[12:39:58.012] TRAIN: iteration 29553 : loss : 0.042669, loss_ce: 0.001322, loss_dice: 0.084015
[12:39:58.220] TRAIN: iteration 29554 : loss : 0.063522, loss_ce: 0.003105, loss_dice: 0.123938
[12:39:58.427] TRAIN: iteration 29555 : loss : 0.173713, loss_ce: 0.011381, loss_dice: 0.336045
[12:39:58.637] TRAIN: iteration 29556 : loss : 0.166491, loss_ce: 0.001324, loss_dice: 0.331657
[12:39:59.482] TRAIN: iteration 29557 : loss : 0.250943, loss_ce: 0.001773, loss_dice: 0.500112
[12:39:59.690] TRAIN: iteration 29558 : loss : 0.201736, loss_ce: 0.001779, loss_dice: 0.401693
[12:39:59.901] TRAIN: iteration 29559 : loss : 0.060357, loss_ce: 0.001556, loss_dice: 0.119159
[12:40:00.110] TRAIN: iteration 29560 : loss : 0.061116, loss_ce: 0.003540, loss_dice: 0.118691
[12:40:00.345] TRAIN: iteration 29561 : loss : 0.225337, loss_ce: 0.001750, loss_dice: 0.448925
[12:40:00.554] TRAIN: iteration 29562 : loss : 0.105567, loss_ce: 0.004348, loss_dice: 0.206786
[12:40:00.764] TRAIN: iteration 29563 : loss : 0.087260, loss_ce: 0.001351, loss_dice: 0.173169
[12:40:00.979] TRAIN: iteration 29564 : loss : 0.047979, loss_ce: 0.004156, loss_dice: 0.091802
[12:40:01.187] TRAIN: iteration 29565 : loss : 0.036346, loss_ce: 0.002409, loss_dice: 0.070283
[12:40:01.395] TRAIN: iteration 29566 : loss : 0.057254, loss_ce: 0.000979, loss_dice: 0.113528
[12:40:01.602] TRAIN: iteration 29567 : loss : 0.250563, loss_ce: 0.001093, loss_dice: 0.500033
[12:40:01.810] TRAIN: iteration 29568 : loss : 0.072804, loss_ce: 0.004379, loss_dice: 0.141230
[12:40:02.018] TRAIN: iteration 29569 : loss : 0.189621, loss_ce: 0.001315, loss_dice: 0.377927
[12:40:02.228] TRAIN: iteration 29570 : loss : 0.109192, loss_ce: 0.007075, loss_dice: 0.211310
[12:40:02.436] TRAIN: iteration 29571 : loss : 0.250526, loss_ce: 0.001023, loss_dice: 0.500030
[12:40:02.648] TRAIN: iteration 29572 : loss : 0.089208, loss_ce: 0.003186, loss_dice: 0.175230
[12:40:02.855] TRAIN: iteration 29573 : loss : 0.149303, loss_ce: 0.003140, loss_dice: 0.295466
[12:40:03.062] TRAIN: iteration 29574 : loss : 0.147805, loss_ce: 0.004248, loss_dice: 0.291362
[12:40:03.270] TRAIN: iteration 29575 : loss : 0.161691, loss_ce: 0.007825, loss_dice: 0.315557
[12:40:03.477] TRAIN: iteration 29576 : loss : 0.251812, loss_ce: 0.003375, loss_dice: 0.500250
[12:40:03.686] TRAIN: iteration 29577 : loss : 0.058976, loss_ce: 0.009915, loss_dice: 0.108038
[12:40:03.894] TRAIN: iteration 29578 : loss : 0.035566, loss_ce: 0.003314, loss_dice: 0.067818
[12:40:04.104] TRAIN: iteration 29579 : loss : 0.026267, loss_ce: 0.001897, loss_dice: 0.050637
[12:40:04.381] TRAIN: iteration 29580 : loss : 0.104179, loss_ce: 0.006873, loss_dice: 0.201485
[12:40:04.624] TRAIN: iteration 29581 : loss : 0.028350, loss_ce: 0.002702, loss_dice: 0.053998
[12:40:04.834] TRAIN: iteration 29582 : loss : 0.179773, loss_ce: 0.003137, loss_dice: 0.356409
[12:40:05.047] TRAIN: iteration 29583 : loss : 0.039546, loss_ce: 0.002073, loss_dice: 0.077019
[12:40:05.258] TRAIN: iteration 29584 : loss : 0.085670, loss_ce: 0.004797, loss_dice: 0.166543
[12:40:05.465] TRAIN: iteration 29585 : loss : 0.041434, loss_ce: 0.002186, loss_dice: 0.080682
[12:40:05.673] TRAIN: iteration 29586 : loss : 0.015412, loss_ce: 0.001408, loss_dice: 0.029416
[12:40:05.881] TRAIN: iteration 29587 : loss : 0.087501, loss_ce: 0.021198, loss_dice: 0.153804
[12:40:06.095] TRAIN: iteration 29588 : loss : 0.076551, loss_ce: 0.006360, loss_dice: 0.146741
[12:40:06.302] TRAIN: iteration 29589 : loss : 0.054213, loss_ce: 0.002774, loss_dice: 0.105652
[12:40:06.515] TRAIN: iteration 29590 : loss : 0.181215, loss_ce: 0.019968, loss_dice: 0.342462
[12:40:06.722] TRAIN: iteration 29591 : loss : 0.219449, loss_ce: 0.001479, loss_dice: 0.437419
[12:40:06.929] TRAIN: iteration 29592 : loss : 0.035541, loss_ce: 0.001304, loss_dice: 0.069777
[12:40:07.144] TRAIN: iteration 29593 : loss : 0.141775, loss_ce: 0.005374, loss_dice: 0.278177
[12:40:07.356] TRAIN: iteration 29594 : loss : 0.032854, loss_ce: 0.000937, loss_dice: 0.064772
[12:40:07.567] TRAIN: iteration 29595 : loss : 0.026353, loss_ce: 0.000804, loss_dice: 0.051901
[12:40:07.777] TRAIN: iteration 29596 : loss : 0.225813, loss_ce: 0.002381, loss_dice: 0.449246
[12:40:07.987] TRAIN: iteration 29597 : loss : 0.046220, loss_ce: 0.005471, loss_dice: 0.086968
[12:40:08.195] TRAIN: iteration 29598 : loss : 0.056936, loss_ce: 0.001624, loss_dice: 0.112248
[12:40:08.404] TRAIN: iteration 29599 : loss : 0.089131, loss_ce: 0.002488, loss_dice: 0.175774
[12:40:08.612] TRAIN: iteration 29600 : loss : 0.250681, loss_ce: 0.001288, loss_dice: 0.500075
[12:40:08.854] TRAIN: iteration 29601 : loss : 0.077111, loss_ce: 0.001721, loss_dice: 0.152502
[12:40:09.065] TRAIN: iteration 29602 : loss : 0.247061, loss_ce: 0.000946, loss_dice: 0.493175
[12:40:09.277] TRAIN: iteration 29603 : loss : 0.159858, loss_ce: 0.001832, loss_dice: 0.317884
[12:40:09.488] TRAIN: iteration 29604 : loss : 0.052224, loss_ce: 0.003637, loss_dice: 0.100812
[12:40:09.697] TRAIN: iteration 29605 : loss : 0.030480, loss_ce: 0.001033, loss_dice: 0.059928
[12:40:09.908] TRAIN: iteration 29606 : loss : 0.056627, loss_ce: 0.001370, loss_dice: 0.111884
[12:40:10.115] TRAIN: iteration 29607 : loss : 0.053565, loss_ce: 0.005320, loss_dice: 0.101811
[12:40:10.324] TRAIN: iteration 29608 : loss : 0.250357, loss_ce: 0.000702, loss_dice: 0.500011
[12:40:10.532] TRAIN: iteration 29609 : loss : 0.022180, loss_ce: 0.004266, loss_dice: 0.040094
[12:40:10.741] TRAIN: iteration 29610 : loss : 0.060772, loss_ce: 0.002283, loss_dice: 0.119261
[12:40:10.950] TRAIN: iteration 29611 : loss : 0.118497, loss_ce: 0.003738, loss_dice: 0.233255
[12:40:11.159] TRAIN: iteration 29612 : loss : 0.041884, loss_ce: 0.004292, loss_dice: 0.079476
[12:40:11.367] TRAIN: iteration 29613 : loss : 0.254965, loss_ce: 0.009696, loss_dice: 0.500234
[12:40:11.578] TRAIN: iteration 29614 : loss : 0.048304, loss_ce: 0.003526, loss_dice: 0.093081
[12:40:11.786] TRAIN: iteration 29615 : loss : 0.027872, loss_ce: 0.004321, loss_dice: 0.051423
[12:40:11.995] TRAIN: iteration 29616 : loss : 0.251631, loss_ce: 0.003055, loss_dice: 0.500207
[12:40:12.204] TRAIN: iteration 29617 : loss : 0.084021, loss_ce: 0.005928, loss_dice: 0.162114
[12:40:12.412] TRAIN: iteration 29618 : loss : 0.049262, loss_ce: 0.000993, loss_dice: 0.097532
[12:40:12.627] TRAIN: iteration 29619 : loss : 0.055622, loss_ce: 0.004092, loss_dice: 0.107151
[12:40:12.834] TRAIN: iteration 29620 : loss : 0.046169, loss_ce: 0.001119, loss_dice: 0.091218
[12:40:13.072] TRAIN: iteration 29621 : loss : 0.050781, loss_ce: 0.001374, loss_dice: 0.100189
[12:40:13.280] TRAIN: iteration 29622 : loss : 0.251258, loss_ce: 0.002349, loss_dice: 0.500166
[12:40:13.490] TRAIN: iteration 29623 : loss : 0.043669, loss_ce: 0.002372, loss_dice: 0.084966
[12:40:13.697] TRAIN: iteration 29624 : loss : 0.250418, loss_ce: 0.000814, loss_dice: 0.500023
[12:40:13.907] TRAIN: iteration 29625 : loss : 0.196372, loss_ce: 0.001731, loss_dice: 0.391014
[12:40:14.114] TRAIN: iteration 29626 : loss : 0.081495, loss_ce: 0.003195, loss_dice: 0.159795
[12:40:14.331] TRAIN: iteration 29627 : loss : 0.251438, loss_ce: 0.002791, loss_dice: 0.500085
[12:40:14.540] TRAIN: iteration 29628 : loss : 0.250269, loss_ce: 0.000526, loss_dice: 0.500012
[12:40:14.751] TRAIN: iteration 29629 : loss : 0.094248, loss_ce: 0.003792, loss_dice: 0.184704
[12:40:14.959] TRAIN: iteration 29630 : loss : 0.250793, loss_ce: 0.001757, loss_dice: 0.499829
[12:40:15.167] TRAIN: iteration 29631 : loss : 0.070757, loss_ce: 0.004098, loss_dice: 0.137417
[12:40:15.556] TRAIN: iteration 29632 : loss : 0.250704, loss_ce: 0.001320, loss_dice: 0.500088
[12:40:15.765] TRAIN: iteration 29633 : loss : 0.025193, loss_ce: 0.002224, loss_dice: 0.048162
[12:40:15.972] TRAIN: iteration 29634 : loss : 0.039906, loss_ce: 0.001737, loss_dice: 0.078074
[12:40:16.181] TRAIN: iteration 29635 : loss : 0.015644, loss_ce: 0.000446, loss_dice: 0.030843
[12:40:16.388] TRAIN: iteration 29636 : loss : 0.068017, loss_ce: 0.001672, loss_dice: 0.134363
[12:40:16.596] TRAIN: iteration 29637 : loss : 0.103814, loss_ce: 0.003201, loss_dice: 0.204427
[12:40:16.820] TRAIN: iteration 29638 : loss : 0.068598, loss_ce: 0.005018, loss_dice: 0.132178
[12:40:17.031] TRAIN: iteration 29639 : loss : 0.065660, loss_ce: 0.001703, loss_dice: 0.129616
[12:40:17.242] TRAIN: iteration 29640 : loss : 0.026180, loss_ce: 0.001658, loss_dice: 0.050702
[12:40:17.492] TRAIN: iteration 29641 : loss : 0.050480, loss_ce: 0.007694, loss_dice: 0.093265
[12:40:17.699] TRAIN: iteration 29642 : loss : 0.052625, loss_ce: 0.001823, loss_dice: 0.103427
[12:40:17.906] TRAIN: iteration 29643 : loss : 0.250664, loss_ce: 0.001251, loss_dice: 0.500078
[12:40:18.114] TRAIN: iteration 29644 : loss : 0.250309, loss_ce: 0.001560, loss_dice: 0.499058
[12:40:18.323] TRAIN: iteration 29645 : loss : 0.058823, loss_ce: 0.003074, loss_dice: 0.114571
[12:40:18.532] TRAIN: iteration 29646 : loss : 0.056348, loss_ce: 0.001156, loss_dice: 0.111540
[12:40:18.743] TRAIN: iteration 29647 : loss : 0.080215, loss_ce: 0.001455, loss_dice: 0.158975
[12:40:18.959] TRAIN: iteration 29648 : loss : 0.149088, loss_ce: 0.001809, loss_dice: 0.296366
[12:40:19.168] TRAIN: iteration 29649 : loss : 0.058005, loss_ce: 0.003785, loss_dice: 0.112225
[12:40:19.379] TRAIN: iteration 29650 : loss : 0.038800, loss_ce: 0.001090, loss_dice: 0.076509
[12:40:19.588] TRAIN: iteration 29651 : loss : 0.063349, loss_ce: 0.007538, loss_dice: 0.119160
[12:40:19.836] TRAIN: iteration 29652 : loss : 0.250661, loss_ce: 0.001349, loss_dice: 0.499973
[12:40:20.047] TRAIN: iteration 29653 : loss : 0.055061, loss_ce: 0.001327, loss_dice: 0.108794
[12:40:20.259] TRAIN: iteration 29654 : loss : 0.070697, loss_ce: 0.003622, loss_dice: 0.137772
[12:40:20.468] TRAIN: iteration 29655 : loss : 0.020732, loss_ce: 0.001455, loss_dice: 0.040009
[12:40:20.683] TRAIN: iteration 29656 : loss : 0.250344, loss_ce: 0.001452, loss_dice: 0.499236
[12:40:20.913] TRAIN: iteration 29657 : loss : 0.107279, loss_ce: 0.002807, loss_dice: 0.211751
[12:40:21.123] TRAIN: iteration 29658 : loss : 0.066399, loss_ce: 0.003547, loss_dice: 0.129251
[12:40:21.333] TRAIN: iteration 29659 : loss : 0.102254, loss_ce: 0.001251, loss_dice: 0.203256
[12:40:21.543] TRAIN: iteration 29660 : loss : 0.189267, loss_ce: 0.001015, loss_dice: 0.377518
[12:40:21.780] TRAIN: iteration 29661 : loss : 0.048198, loss_ce: 0.007161, loss_dice: 0.089236
[12:40:21.998] TRAIN: iteration 29662 : loss : 0.038397, loss_ce: 0.004039, loss_dice: 0.072755
[12:40:22.209] TRAIN: iteration 29663 : loss : 0.072807, loss_ce: 0.004639, loss_dice: 0.140975
[12:40:22.426] TRAIN: iteration 29664 : loss : 0.166866, loss_ce: 0.002884, loss_dice: 0.330848
[12:40:22.634] TRAIN: iteration 29665 : loss : 0.103746, loss_ce: 0.003529, loss_dice: 0.203963
[12:40:22.841] TRAIN: iteration 29666 : loss : 0.227967, loss_ce: 0.002980, loss_dice: 0.452954
[12:40:23.053] TRAIN: iteration 29667 : loss : 0.054095, loss_ce: 0.002107, loss_dice: 0.106083
[12:40:23.264] TRAIN: iteration 29668 : loss : 0.028924, loss_ce: 0.002966, loss_dice: 0.054882
[12:40:23.479] TRAIN: iteration 29669 : loss : 0.207223, loss_ce: 0.001346, loss_dice: 0.413101
[12:40:23.686] TRAIN: iteration 29670 : loss : 0.025483, loss_ce: 0.002711, loss_dice: 0.048255
[12:40:23.896] TRAIN: iteration 29671 : loss : 0.037926, loss_ce: 0.004596, loss_dice: 0.071255
[12:40:24.110] TRAIN: iteration 29672 : loss : 0.018552, loss_ce: 0.001567, loss_dice: 0.035537
[12:40:24.318] TRAIN: iteration 29673 : loss : 0.035336, loss_ce: 0.004655, loss_dice: 0.066018
[12:40:24.527] TRAIN: iteration 29674 : loss : 0.145029, loss_ce: 0.004350, loss_dice: 0.285709
[12:40:24.739] TRAIN: iteration 29675 : loss : 0.165355, loss_ce: 0.005054, loss_dice: 0.325655
[12:40:24.947] TRAIN: iteration 29676 : loss : 0.071917, loss_ce: 0.003027, loss_dice: 0.140807
[12:40:25.156] TRAIN: iteration 29677 : loss : 0.051637, loss_ce: 0.001063, loss_dice: 0.102210
[12:40:25.364] TRAIN: iteration 29678 : loss : 0.230410, loss_ce: 0.008190, loss_dice: 0.452629
[12:40:25.574] TRAIN: iteration 29679 : loss : 0.058026, loss_ce: 0.002801, loss_dice: 0.113252
[12:40:25.783] TRAIN: iteration 29680 : loss : 0.211660, loss_ce: 0.008030, loss_dice: 0.415291
[12:40:26.022] TRAIN: iteration 29681 : loss : 0.051195, loss_ce: 0.003090, loss_dice: 0.099300
[12:40:26.231] TRAIN: iteration 29682 : loss : 0.250888, loss_ce: 0.001704, loss_dice: 0.500071
[12:40:26.440] TRAIN: iteration 29683 : loss : 0.052308, loss_ce: 0.001004, loss_dice: 0.103612
[12:40:26.650] TRAIN: iteration 29684 : loss : 0.070965, loss_ce: 0.006431, loss_dice: 0.135499
[12:40:26.858] TRAIN: iteration 29685 : loss : 0.023128, loss_ce: 0.001054, loss_dice: 0.045202
[12:40:27.067] TRAIN: iteration 29686 : loss : 0.187454, loss_ce: 0.002474, loss_dice: 0.372434
[12:40:27.280] TRAIN: iteration 29687 : loss : 0.077316, loss_ce: 0.002857, loss_dice: 0.151776
[12:40:27.490] TRAIN: iteration 29688 : loss : 0.042917, loss_ce: 0.005553, loss_dice: 0.080280
[12:40:27.712] TRAIN: iteration 29689 : loss : 0.250739, loss_ce: 0.001408, loss_dice: 0.500071
[12:40:27.920] TRAIN: iteration 29690 : loss : 0.039349, loss_ce: 0.002231, loss_dice: 0.076466
[12:40:28.129] TRAIN: iteration 29691 : loss : 0.167084, loss_ce: 0.003270, loss_dice: 0.330898
[12:40:28.339] TRAIN: iteration 29692 : loss : 0.147228, loss_ce: 0.005645, loss_dice: 0.288812
[12:40:28.548] TRAIN: iteration 29693 : loss : 0.067536, loss_ce: 0.001237, loss_dice: 0.133835
[12:40:28.756] TRAIN: iteration 29694 : loss : 0.078772, loss_ce: 0.005655, loss_dice: 0.151888
[12:40:28.963] TRAIN: iteration 29695 : loss : 0.114485, loss_ce: 0.001428, loss_dice: 0.227542
[12:40:29.170] TRAIN: iteration 29696 : loss : 0.127737, loss_ce: 0.003564, loss_dice: 0.251910
[12:40:29.379] TRAIN: iteration 29697 : loss : 0.081161, loss_ce: 0.005506, loss_dice: 0.156816
[12:40:29.589] TRAIN: iteration 29698 : loss : 0.250848, loss_ce: 0.001604, loss_dice: 0.500092
[12:40:29.797] TRAIN: iteration 29699 : loss : 0.022371, loss_ce: 0.002684, loss_dice: 0.042058
[12:40:30.013] TRAIN: iteration 29700 : loss : 0.074234, loss_ce: 0.002309, loss_dice: 0.146159
[12:40:30.255] TRAIN: iteration 29701 : loss : 0.033886, loss_ce: 0.002529, loss_dice: 0.065242
[12:40:30.471] TRAIN: iteration 29702 : loss : 0.069936, loss_ce: 0.005784, loss_dice: 0.134088
[12:40:30.679] TRAIN: iteration 29703 : loss : 0.250602, loss_ce: 0.001353, loss_dice: 0.499850
[12:40:30.893] TRAIN: iteration 29704 : loss : 0.257466, loss_ce: 0.015903, loss_dice: 0.499029
[12:40:31.102] TRAIN: iteration 29705 : loss : 0.110266, loss_ce: 0.007635, loss_dice: 0.212897
[12:40:31.316] TRAIN: iteration 29706 : loss : 0.072746, loss_ce: 0.003051, loss_dice: 0.142441
[12:40:31.530] TRAIN: iteration 29707 : loss : 0.103326, loss_ce: 0.001816, loss_dice: 0.204836
[12:40:31.746] TRAIN: iteration 29708 : loss : 0.240014, loss_ce: 0.001065, loss_dice: 0.478963
[12:40:31.956] TRAIN: iteration 29709 : loss : 0.137048, loss_ce: 0.022863, loss_dice: 0.251234
[12:40:32.167] TRAIN: iteration 29710 : loss : 0.031991, loss_ce: 0.002758, loss_dice: 0.061224
[12:40:32.376] TRAIN: iteration 29711 : loss : 0.226348, loss_ce: 0.002654, loss_dice: 0.450042
[12:40:32.586] TRAIN: iteration 29712 : loss : 0.034065, loss_ce: 0.003979, loss_dice: 0.064151
[12:40:32.794] TRAIN: iteration 29713 : loss : 0.198611, loss_ce: 0.002455, loss_dice: 0.394767
[12:40:33.002] TRAIN: iteration 29714 : loss : 0.093086, loss_ce: 0.005198, loss_dice: 0.180974
[12:40:33.213] TRAIN: iteration 29715 : loss : 0.163984, loss_ce: 0.001144, loss_dice: 0.326824
[12:40:33.422] TRAIN: iteration 29716 : loss : 0.238705, loss_ce: 0.004424, loss_dice: 0.472987
[12:40:33.634] TRAIN: iteration 29717 : loss : 0.033601, loss_ce: 0.003993, loss_dice: 0.063208
[12:40:33.843] TRAIN: iteration 29718 : loss : 0.201300, loss_ce: 0.003985, loss_dice: 0.398614
[12:40:34.052] TRAIN: iteration 29719 : loss : 0.219430, loss_ce: 0.003374, loss_dice: 0.435485
[12:40:34.262] TRAIN: iteration 29720 : loss : 0.118182, loss_ce: 0.002070, loss_dice: 0.234295
[12:40:34.501] TRAIN: iteration 29721 : loss : 0.090630, loss_ce: 0.005081, loss_dice: 0.176180
[12:40:34.710] TRAIN: iteration 29722 : loss : 0.054494, loss_ce: 0.001647, loss_dice: 0.107340
[12:40:34.917] TRAIN: iteration 29723 : loss : 0.063707, loss_ce: 0.004580, loss_dice: 0.122835
[12:40:35.129] TRAIN: iteration 29724 : loss : 0.039966, loss_ce: 0.005258, loss_dice: 0.074673
[12:40:35.338] TRAIN: iteration 29725 : loss : 0.200689, loss_ce: 0.002415, loss_dice: 0.398963
[12:40:35.544] TRAIN: iteration 29726 : loss : 0.250448, loss_ce: 0.000861, loss_dice: 0.500035
[12:40:35.752] TRAIN: iteration 29727 : loss : 0.066907, loss_ce: 0.003239, loss_dice: 0.130574
[12:40:35.959] TRAIN: iteration 29728 : loss : 0.250502, loss_ce: 0.000974, loss_dice: 0.500030
[12:40:36.173] TRAIN: iteration 29729 : loss : 0.094518, loss_ce: 0.010721, loss_dice: 0.178314
[12:40:36.383] TRAIN: iteration 29730 : loss : 0.034951, loss_ce: 0.007437, loss_dice: 0.062464
[12:40:36.598] TRAIN: iteration 29731 : loss : 0.055370, loss_ce: 0.006821, loss_dice: 0.103919
[12:40:36.808] TRAIN: iteration 29732 : loss : 0.250562, loss_ce: 0.001090, loss_dice: 0.500034
[12:40:37.015] TRAIN: iteration 29733 : loss : 0.093797, loss_ce: 0.001366, loss_dice: 0.186228
[12:40:37.226] TRAIN: iteration 29734 : loss : 0.199533, loss_ce: 0.006039, loss_dice: 0.393027
[12:40:37.436] TRAIN: iteration 29735 : loss : 0.079436, loss_ce: 0.004545, loss_dice: 0.154327
[12:40:37.652] TRAIN: iteration 29736 : loss : 0.039767, loss_ce: 0.003248, loss_dice: 0.076286
[12:40:37.871] TRAIN: iteration 29737 : loss : 0.055345, loss_ce: 0.001282, loss_dice: 0.109408
[12:40:38.515] TRAIN: iteration 29738 : loss : 0.110400, loss_ce: 0.006459, loss_dice: 0.214341
[12:40:38.726] TRAIN: iteration 29739 : loss : 0.092904, loss_ce: 0.005467, loss_dice: 0.180341
[12:40:38.934] TRAIN: iteration 29740 : loss : 0.162587, loss_ce: 0.001539, loss_dice: 0.323635
[12:40:39.170] TRAIN: iteration 29741 : loss : 0.090453, loss_ce: 0.005588, loss_dice: 0.175317
[12:40:39.378] TRAIN: iteration 29742 : loss : 0.037614, loss_ce: 0.008438, loss_dice: 0.066790
[12:40:39.587] TRAIN: iteration 29743 : loss : 0.102164, loss_ce: 0.001152, loss_dice: 0.203176
[12:40:39.801] TRAIN: iteration 29744 : loss : 0.083516, loss_ce: 0.002654, loss_dice: 0.164379
[12:40:40.016] TRAIN: iteration 29745 : loss : 0.242117, loss_ce: 0.002243, loss_dice: 0.481992
[12:40:40.708] TRAIN: iteration 29746 : loss : 0.065794, loss_ce: 0.004590, loss_dice: 0.126997
[12:40:40.916] TRAIN: iteration 29747 : loss : 0.113409, loss_ce: 0.001580, loss_dice: 0.225239
[12:40:41.124] TRAIN: iteration 29748 : loss : 0.034993, loss_ce: 0.001106, loss_dice: 0.068880
[12:40:41.331] TRAIN: iteration 29749 : loss : 0.250813, loss_ce: 0.001543, loss_dice: 0.500083
[12:40:41.540] TRAIN: iteration 29750 : loss : 0.076007, loss_ce: 0.002095, loss_dice: 0.149918
[12:40:41.748] TRAIN: iteration 29751 : loss : 0.089828, loss_ce: 0.001206, loss_dice: 0.178451
[12:40:41.958] TRAIN: iteration 29752 : loss : 0.063208, loss_ce: 0.002456, loss_dice: 0.123960
[12:40:42.169] TRAIN: iteration 29753 : loss : 0.088977, loss_ce: 0.001142, loss_dice: 0.176812
[12:40:44.842] TRAIN: iteration 29754 : loss : 0.073653, loss_ce: 0.002159, loss_dice: 0.145148
[12:40:45.050] TRAIN: iteration 29755 : loss : 0.150364, loss_ce: 0.001674, loss_dice: 0.299054
[12:40:45.258] TRAIN: iteration 29756 : loss : 0.215697, loss_ce: 0.001613, loss_dice: 0.429780
[12:40:45.468] TRAIN: iteration 29757 : loss : 0.019215, loss_ce: 0.001128, loss_dice: 0.037303
[12:40:45.678] TRAIN: iteration 29758 : loss : 0.030791, loss_ce: 0.001429, loss_dice: 0.060153
[12:40:45.888] TRAIN: iteration 29759 : loss : 0.145513, loss_ce: 0.001730, loss_dice: 0.289296
[12:40:46.099] TRAIN: iteration 29760 : loss : 0.250591, loss_ce: 0.001117, loss_dice: 0.500066
[12:40:46.347] TRAIN: iteration 29761 : loss : 0.067087, loss_ce: 0.001648, loss_dice: 0.132526
[12:40:46.554] TRAIN: iteration 29762 : loss : 0.250612, loss_ce: 0.001153, loss_dice: 0.500071
[12:40:46.763] TRAIN: iteration 29763 : loss : 0.203299, loss_ce: 0.002448, loss_dice: 0.404149
[12:40:46.974] TRAIN: iteration 29764 : loss : 0.250292, loss_ce: 0.000567, loss_dice: 0.500018
[12:40:47.188] TRAIN: iteration 29765 : loss : 0.064636, loss_ce: 0.001116, loss_dice: 0.128157
[12:40:47.396] TRAIN: iteration 29766 : loss : 0.065939, loss_ce: 0.001991, loss_dice: 0.129886
[12:40:47.605] TRAIN: iteration 29767 : loss : 0.064182, loss_ce: 0.001640, loss_dice: 0.126724
[12:40:47.814] TRAIN: iteration 29768 : loss : 0.079791, loss_ce: 0.002289, loss_dice: 0.157293
[12:40:48.023] TRAIN: iteration 29769 : loss : 0.046040, loss_ce: 0.003595, loss_dice: 0.088484
[12:40:48.234] TRAIN: iteration 29770 : loss : 0.179710, loss_ce: 0.004096, loss_dice: 0.355323
[12:40:48.448] TRAIN: iteration 29771 : loss : 0.166026, loss_ce: 0.003792, loss_dice: 0.328259
[12:40:48.657] TRAIN: iteration 29772 : loss : 0.129184, loss_ce: 0.003497, loss_dice: 0.254871
[12:40:48.866] TRAIN: iteration 29773 : loss : 0.034538, loss_ce: 0.001368, loss_dice: 0.067708
[12:40:49.073] TRAIN: iteration 29774 : loss : 0.027974, loss_ce: 0.000471, loss_dice: 0.055478
[12:40:49.281] TRAIN: iteration 29775 : loss : 0.118274, loss_ce: 0.000995, loss_dice: 0.235554
[12:40:49.489] TRAIN: iteration 29776 : loss : 0.059678, loss_ce: 0.001259, loss_dice: 0.118097
[12:40:49.698] TRAIN: iteration 29777 : loss : 0.027282, loss_ce: 0.001833, loss_dice: 0.052730
[12:40:49.909] TRAIN: iteration 29778 : loss : 0.034541, loss_ce: 0.001574, loss_dice: 0.067507
[12:40:50.119] TRAIN: iteration 29779 : loss : 0.250915, loss_ce: 0.001719, loss_dice: 0.500111
[12:40:50.327] TRAIN: iteration 29780 : loss : 0.250676, loss_ce: 0.001286, loss_dice: 0.500066
[12:40:50.566] TRAIN: iteration 29781 : loss : 0.135945, loss_ce: 0.002797, loss_dice: 0.269093
[12:40:50.779] TRAIN: iteration 29782 : loss : 0.106551, loss_ce: 0.002438, loss_dice: 0.210665
[12:40:50.988] TRAIN: iteration 29783 : loss : 0.250343, loss_ce: 0.000666, loss_dice: 0.500021
[12:40:51.196] TRAIN: iteration 29784 : loss : 0.046652, loss_ce: 0.000783, loss_dice: 0.092522
[12:40:51.406] TRAIN: iteration 29785 : loss : 0.032566, loss_ce: 0.000519, loss_dice: 0.064614
[12:40:51.614] TRAIN: iteration 29786 : loss : 0.179292, loss_ce: 0.001650, loss_dice: 0.356934
[12:40:51.822] TRAIN: iteration 29787 : loss : 0.078200, loss_ce: 0.014688, loss_dice: 0.141712
[12:40:52.031] TRAIN: iteration 29788 : loss : 0.110128, loss_ce: 0.004156, loss_dice: 0.216101
[12:40:52.240] TRAIN: iteration 29789 : loss : 0.029532, loss_ce: 0.004113, loss_dice: 0.054952
[12:40:52.448] TRAIN: iteration 29790 : loss : 0.250930, loss_ce: 0.001739, loss_dice: 0.500121
[12:40:52.657] TRAIN: iteration 29791 : loss : 0.078346, loss_ce: 0.010706, loss_dice: 0.145986
[12:40:52.867] TRAIN: iteration 29792 : loss : 0.067043, loss_ce: 0.001185, loss_dice: 0.132902
[12:40:53.075] TRAIN: iteration 29793 : loss : 0.152776, loss_ce: 0.001359, loss_dice: 0.304193
[12:40:53.283] TRAIN: iteration 29794 : loss : 0.111114, loss_ce: 0.000866, loss_dice: 0.221362
[12:40:53.491] TRAIN: iteration 29795 : loss : 0.095024, loss_ce: 0.001612, loss_dice: 0.188435
[12:40:53.700] TRAIN: iteration 29796 : loss : 0.182609, loss_ce: 0.001967, loss_dice: 0.363252
[12:40:53.907] TRAIN: iteration 29797 : loss : 0.150170, loss_ce: 0.001082, loss_dice: 0.299257
[12:40:54.116] TRAIN: iteration 29798 : loss : 0.250275, loss_ce: 0.000535, loss_dice: 0.500015
[12:40:54.328] TRAIN: iteration 29799 : loss : 0.104844, loss_ce: 0.003298, loss_dice: 0.206391
[12:40:54.539] TRAIN: iteration 29800 : loss : 0.071067, loss_ce: 0.001961, loss_dice: 0.140173
[12:40:54.778] TRAIN: iteration 29801 : loss : 0.216829, loss_ce: 0.002949, loss_dice: 0.430709
[12:40:54.989] TRAIN: iteration 29802 : loss : 0.038275, loss_ce: 0.004873, loss_dice: 0.071677
[12:40:55.205] TRAIN: iteration 29803 : loss : 0.084303, loss_ce: 0.002287, loss_dice: 0.166319
[12:40:55.417] TRAIN: iteration 29804 : loss : 0.250464, loss_ce: 0.000889, loss_dice: 0.500039
[12:40:55.626] TRAIN: iteration 29805 : loss : 0.066346, loss_ce: 0.001945, loss_dice: 0.130747
[12:40:55.835] TRAIN: iteration 29806 : loss : 0.088625, loss_ce: 0.002434, loss_dice: 0.174816
[12:40:56.047] TRAIN: iteration 29807 : loss : 0.247837, loss_ce: 0.000398, loss_dice: 0.495277
[12:40:56.262] TRAIN: iteration 29808 : loss : 0.251074, loss_ce: 0.002458, loss_dice: 0.499689
[12:40:56.469] TRAIN: iteration 29809 : loss : 0.054206, loss_ce: 0.001946, loss_dice: 0.106466
[12:40:56.684] TRAIN: iteration 29810 : loss : 0.084281, loss_ce: 0.001093, loss_dice: 0.167470
[12:40:56.894] TRAIN: iteration 29811 : loss : 0.133426, loss_ce: 0.007060, loss_dice: 0.259793
[12:40:57.103] TRAIN: iteration 29812 : loss : 0.250151, loss_ce: 0.000297, loss_dice: 0.500005
[12:40:57.311] TRAIN: iteration 29813 : loss : 0.088061, loss_ce: 0.001363, loss_dice: 0.174758
[12:40:57.518] TRAIN: iteration 29814 : loss : 0.074472, loss_ce: 0.003173, loss_dice: 0.145770
[12:40:57.725] TRAIN: iteration 29815 : loss : 0.036085, loss_ce: 0.001806, loss_dice: 0.070365
[12:40:57.937] TRAIN: iteration 29816 : loss : 0.250254, loss_ce: 0.000500, loss_dice: 0.500008
[12:40:58.145] TRAIN: iteration 29817 : loss : 0.058338, loss_ce: 0.000828, loss_dice: 0.115848
[12:40:58.353] TRAIN: iteration 29818 : loss : 0.045338, loss_ce: 0.001089, loss_dice: 0.089586
[12:40:58.561] TRAIN: iteration 29819 : loss : 0.047725, loss_ce: 0.002460, loss_dice: 0.092991
[12:40:58.769] TRAIN: iteration 29820 : loss : 0.022934, loss_ce: 0.001717, loss_dice: 0.044151
[12:40:59.040] TRAIN: iteration 29821 : loss : 0.087294, loss_ce: 0.006604, loss_dice: 0.167983
[12:40:59.249] TRAIN: iteration 29822 : loss : 0.250443, loss_ce: 0.000855, loss_dice: 0.500030
[12:40:59.457] TRAIN: iteration 29823 : loss : 0.040810, loss_ce: 0.001413, loss_dice: 0.080207
[12:40:59.664] TRAIN: iteration 29824 : loss : 0.052596, loss_ce: 0.001817, loss_dice: 0.103375
[12:40:59.985] TRAIN: iteration 29825 : loss : 0.137984, loss_ce: 0.001440, loss_dice: 0.274528
[12:41:00.193] TRAIN: iteration 29826 : loss : 0.098625, loss_ce: 0.007475, loss_dice: 0.189775
[12:41:00.402] TRAIN: iteration 29827 : loss : 0.056645, loss_ce: 0.001628, loss_dice: 0.111662
[12:41:00.646] TRAIN: iteration 29828 : loss : 0.224665, loss_ce: 0.001011, loss_dice: 0.448319
[12:41:01.458] TRAIN: iteration 29829 : loss : 0.080765, loss_ce: 0.002636, loss_dice: 0.158894
[12:41:01.665] TRAIN: iteration 29830 : loss : 0.036062, loss_ce: 0.002221, loss_dice: 0.069904
[12:41:01.872] TRAIN: iteration 29831 : loss : 0.211850, loss_ce: 0.003244, loss_dice: 0.420457
[12:41:02.079] TRAIN: iteration 29832 : loss : 0.159116, loss_ce: 0.003550, loss_dice: 0.314681
[12:41:02.287] TRAIN: iteration 29833 : loss : 0.232481, loss_ce: 0.003712, loss_dice: 0.461250
[12:41:02.495] TRAIN: iteration 29834 : loss : 0.072322, loss_ce: 0.002362, loss_dice: 0.142282
[12:41:02.702] TRAIN: iteration 29835 : loss : 0.070449, loss_ce: 0.002899, loss_dice: 0.138000
[12:41:02.918] TRAIN: iteration 29836 : loss : 0.075503, loss_ce: 0.001639, loss_dice: 0.149367
[12:41:03.227] TRAIN: iteration 29837 : loss : 0.116047, loss_ce: 0.010500, loss_dice: 0.221594
[12:41:03.434] TRAIN: iteration 29838 : loss : 0.071558, loss_ce: 0.001043, loss_dice: 0.142073
[12:41:03.642] TRAIN: iteration 29839 : loss : 0.075603, loss_ce: 0.001123, loss_dice: 0.150083
[12:41:03.855] TRAIN: iteration 29840 : loss : 0.089385, loss_ce: 0.001723, loss_dice: 0.177047
[12:41:04.108] TRAIN: iteration 29841 : loss : 0.088079, loss_ce: 0.003660, loss_dice: 0.172498
[12:41:04.317] TRAIN: iteration 29842 : loss : 0.250463, loss_ce: 0.000891, loss_dice: 0.500035
[12:41:04.525] TRAIN: iteration 29843 : loss : 0.121344, loss_ce: 0.022150, loss_dice: 0.220539
[12:41:04.740] TRAIN: iteration 29844 : loss : 0.055015, loss_ce: 0.004891, loss_dice: 0.105139
[12:41:06.079] TRAIN: iteration 29845 : loss : 0.064571, loss_ce: 0.000779, loss_dice: 0.128364
[12:41:06.288] TRAIN: iteration 29846 : loss : 0.088034, loss_ce: 0.004378, loss_dice: 0.171691
[12:41:06.497] TRAIN: iteration 29847 : loss : 0.250493, loss_ce: 0.000948, loss_dice: 0.500038
[12:41:06.706] TRAIN: iteration 29848 : loss : 0.093454, loss_ce: 0.009682, loss_dice: 0.177227
[12:41:06.916] TRAIN: iteration 29849 : loss : 0.249457, loss_ce: 0.001866, loss_dice: 0.497048
[12:41:07.135] TRAIN: iteration 29850 : loss : 0.122999, loss_ce: 0.008010, loss_dice: 0.237988
[12:41:07.344] TRAIN: iteration 29851 : loss : 0.034455, loss_ce: 0.003015, loss_dice: 0.065894
[12:41:07.559] TRAIN: iteration 29852 : loss : 0.067706, loss_ce: 0.004968, loss_dice: 0.130443
[12:41:08.483] TRAIN: iteration 29853 : loss : 0.057854, loss_ce: 0.004537, loss_dice: 0.111171
[12:41:08.693] TRAIN: iteration 29854 : loss : 0.061589, loss_ce: 0.001682, loss_dice: 0.121496
[12:41:08.902] TRAIN: iteration 29855 : loss : 0.030431, loss_ce: 0.002052, loss_dice: 0.058810
[12:41:09.115] TRAIN: iteration 29856 : loss : 0.051143, loss_ce: 0.004027, loss_dice: 0.098259
[12:41:09.346] TRAIN: iteration 29857 : loss : 0.055860, loss_ce: 0.006155, loss_dice: 0.105564
[12:41:09.558] TRAIN: iteration 29858 : loss : 0.024742, loss_ce: 0.001097, loss_dice: 0.048388
[12:41:09.766] TRAIN: iteration 29859 : loss : 0.031989, loss_ce: 0.001904, loss_dice: 0.062074
[12:41:09.973] TRAIN: iteration 29860 : loss : 0.160106, loss_ce: 0.006113, loss_dice: 0.314100
[12:41:10.217] TRAIN: iteration 29861 : loss : 0.077873, loss_ce: 0.002317, loss_dice: 0.153429
[12:41:10.424] TRAIN: iteration 29862 : loss : 0.250781, loss_ce: 0.001485, loss_dice: 0.500077
[12:41:10.634] TRAIN: iteration 29863 : loss : 0.094491, loss_ce: 0.021808, loss_dice: 0.167175
[12:41:10.842] TRAIN: iteration 29864 : loss : 0.117756, loss_ce: 0.001110, loss_dice: 0.234402
[12:41:11.051] TRAIN: iteration 29865 : loss : 0.033628, loss_ce: 0.007670, loss_dice: 0.059587
[12:41:11.259] TRAIN: iteration 29866 : loss : 0.250517, loss_ce: 0.001012, loss_dice: 0.500022
[12:41:11.469] TRAIN: iteration 29867 : loss : 0.027577, loss_ce: 0.000900, loss_dice: 0.054253
[12:41:11.679] TRAIN: iteration 29868 : loss : 0.021284, loss_ce: 0.001668, loss_dice: 0.040899
[12:41:11.889] TRAIN: iteration 29869 : loss : 0.133100, loss_ce: 0.004947, loss_dice: 0.261254
[12:41:12.105] TRAIN: iteration 29870 : loss : 0.250327, loss_ce: 0.000775, loss_dice: 0.499879
[12:41:12.315] TRAIN: iteration 29871 : loss : 0.122874, loss_ce: 0.024265, loss_dice: 0.221482
[12:41:12.523] TRAIN: iteration 29872 : loss : 0.111738, loss_ce: 0.011709, loss_dice: 0.211767
[12:41:12.734] TRAIN: iteration 29873 : loss : 0.025943, loss_ce: 0.002695, loss_dice: 0.049192
[12:41:12.944] TRAIN: iteration 29874 : loss : 0.250352, loss_ce: 0.000683, loss_dice: 0.500021
[12:41:13.152] TRAIN: iteration 29875 : loss : 0.103896, loss_ce: 0.006089, loss_dice: 0.201704
[12:41:13.360] TRAIN: iteration 29876 : loss : 0.061277, loss_ce: 0.009988, loss_dice: 0.112565
[12:41:13.568] TRAIN: iteration 29877 : loss : 0.101533, loss_ce: 0.005301, loss_dice: 0.197765
[12:41:13.777] TRAIN: iteration 29878 : loss : 0.221986, loss_ce: 0.001562, loss_dice: 0.442411
[12:41:13.987] TRAIN: iteration 29879 : loss : 0.031405, loss_ce: 0.004401, loss_dice: 0.058409
[12:41:14.195] TRAIN: iteration 29880 : loss : 0.021336, loss_ce: 0.000486, loss_dice: 0.042186
[12:41:14.434] TRAIN: iteration 29881 : loss : 0.114128, loss_ce: 0.013213, loss_dice: 0.215042
[12:41:14.645] TRAIN: iteration 29882 : loss : 0.063429, loss_ce: 0.002245, loss_dice: 0.124614
[12:41:14.855] TRAIN: iteration 29883 : loss : 0.175464, loss_ce: 0.006088, loss_dice: 0.344841
[12:41:15.065] TRAIN: iteration 29884 : loss : 0.073481, loss_ce: 0.004990, loss_dice: 0.141972
[12:41:15.276] TRAIN: iteration 29885 : loss : 0.250779, loss_ce: 0.001516, loss_dice: 0.500043
[12:41:15.489] TRAIN: iteration 29886 : loss : 0.175961, loss_ce: 0.003660, loss_dice: 0.348263
[12:41:15.705] TRAIN: iteration 29887 : loss : 0.229035, loss_ce: 0.001279, loss_dice: 0.456792
[12:41:15.914] TRAIN: iteration 29888 : loss : 0.043449, loss_ce: 0.004389, loss_dice: 0.082510
[12:41:16.123] TRAIN: iteration 29889 : loss : 0.094697, loss_ce: 0.009524, loss_dice: 0.179870
[12:41:16.331] TRAIN: iteration 29890 : loss : 0.066960, loss_ce: 0.004755, loss_dice: 0.129165
[12:41:16.538] TRAIN: iteration 29891 : loss : 0.054471, loss_ce: 0.001637, loss_dice: 0.107304
[12:41:16.746] TRAIN: iteration 29892 : loss : 0.069233, loss_ce: 0.005011, loss_dice: 0.133455
[12:41:16.954] TRAIN: iteration 29893 : loss : 0.250327, loss_ce: 0.000636, loss_dice: 0.500017
[12:41:17.168] TRAIN: iteration 29894 : loss : 0.250474, loss_ce: 0.000905, loss_dice: 0.500042
[12:41:17.377] TRAIN: iteration 29895 : loss : 0.084518, loss_ce: 0.003761, loss_dice: 0.165275
[12:41:17.928] TRAIN: iteration 29896 : loss : 0.066511, loss_ce: 0.002546, loss_dice: 0.130477
[12:41:18.136] TRAIN: iteration 29897 : loss : 0.041828, loss_ce: 0.002808, loss_dice: 0.080848
[12:41:18.343] TRAIN: iteration 29898 : loss : 0.251247, loss_ce: 0.002357, loss_dice: 0.500137
[12:41:18.551] TRAIN: iteration 29899 : loss : 0.066812, loss_ce: 0.003044, loss_dice: 0.130579
[12:41:18.761] TRAIN: iteration 29900 : loss : 0.092268, loss_ce: 0.004428, loss_dice: 0.180107
[12:41:18.997] TRAIN: iteration 29901 : loss : 0.077938, loss_ce: 0.008048, loss_dice: 0.147829
[12:41:19.204] TRAIN: iteration 29902 : loss : 0.117882, loss_ce: 0.006362, loss_dice: 0.229403
[12:41:19.414] TRAIN: iteration 29903 : loss : 0.049237, loss_ce: 0.005453, loss_dice: 0.093021
[12:41:19.626] TRAIN: iteration 29904 : loss : 0.213410, loss_ce: 0.005618, loss_dice: 0.421202
[12:41:19.842] TRAIN: iteration 29905 : loss : 0.035007, loss_ce: 0.003122, loss_dice: 0.066892
[12:41:20.050] TRAIN: iteration 29906 : loss : 0.025163, loss_ce: 0.002760, loss_dice: 0.047566
[12:41:20.260] TRAIN: iteration 29907 : loss : 0.054059, loss_ce: 0.001476, loss_dice: 0.106642
[12:41:20.467] TRAIN: iteration 29908 : loss : 0.034748, loss_ce: 0.004303, loss_dice: 0.065192
[12:41:20.674] TRAIN: iteration 29909 : loss : 0.038388, loss_ce: 0.001109, loss_dice: 0.075667
[12:41:20.886] TRAIN: iteration 29910 : loss : 0.131500, loss_ce: 0.004470, loss_dice: 0.258529
[12:41:21.100] TRAIN: iteration 29911 : loss : 0.228365, loss_ce: 0.001577, loss_dice: 0.455153
[12:41:21.496] TRAIN: iteration 29912 : loss : 0.250742, loss_ce: 0.001398, loss_dice: 0.500085
[12:41:21.705] TRAIN: iteration 29913 : loss : 0.138596, loss_ce: 0.001669, loss_dice: 0.275523
[12:41:21.914] TRAIN: iteration 29914 : loss : 0.217912, loss_ce: 0.005784, loss_dice: 0.430039
[12:41:22.125] TRAIN: iteration 29915 : loss : 0.240972, loss_ce: 0.003145, loss_dice: 0.478800
[12:41:22.332] TRAIN: iteration 29916 : loss : 0.237940, loss_ce: 0.013398, loss_dice: 0.462483
[12:41:22.540] TRAIN: iteration 29917 : loss : 0.026030, loss_ce: 0.001459, loss_dice: 0.050601
[12:41:22.748] TRAIN: iteration 29918 : loss : 0.040563, loss_ce: 0.005060, loss_dice: 0.076066
[12:41:22.955] TRAIN: iteration 29919 : loss : 0.223690, loss_ce: 0.002901, loss_dice: 0.444479
[12:41:23.163] TRAIN: iteration 29920 : loss : 0.251180, loss_ce: 0.002226, loss_dice: 0.500133
[12:41:23.164] NaN or Inf found in input tensor.
[12:41:23.382] TRAIN: iteration 29921 : loss : 0.177239, loss_ce: 0.003491, loss_dice: 0.350987
[12:41:23.861] TRAIN: iteration 29922 : loss : 0.037776, loss_ce: 0.001815, loss_dice: 0.073737
[12:41:24.068] TRAIN: iteration 29923 : loss : 0.222892, loss_ce: 0.001504, loss_dice: 0.444281
[12:41:24.277] TRAIN: iteration 29924 : loss : 0.062293, loss_ce: 0.001128, loss_dice: 0.123458
[12:41:24.484] TRAIN: iteration 29925 : loss : 0.257695, loss_ce: 0.015908, loss_dice: 0.499482
[12:41:24.692] TRAIN: iteration 29926 : loss : 0.076542, loss_ce: 0.005204, loss_dice: 0.147880
[12:41:24.899] TRAIN: iteration 29927 : loss : 0.073030, loss_ce: 0.003990, loss_dice: 0.142071
[12:41:25.823] TRAIN: iteration 29928 : loss : 0.045815, loss_ce: 0.001492, loss_dice: 0.090139
[12:41:26.030] TRAIN: iteration 29929 : loss : 0.052524, loss_ce: 0.001917, loss_dice: 0.103131
[12:41:26.238] TRAIN: iteration 29930 : loss : 0.251273, loss_ce: 0.002385, loss_dice: 0.500161
[12:41:26.446] TRAIN: iteration 29931 : loss : 0.038187, loss_ce: 0.003896, loss_dice: 0.072477
[12:41:26.654] TRAIN: iteration 29932 : loss : 0.124603, loss_ce: 0.002034, loss_dice: 0.247172
[12:41:26.861] TRAIN: iteration 29933 : loss : 0.247698, loss_ce: 0.003856, loss_dice: 0.491540
[12:41:27.073] TRAIN: iteration 29934 : loss : 0.100101, loss_ce: 0.009302, loss_dice: 0.190900
[12:41:27.280] TRAIN: iteration 29935 : loss : 0.023357, loss_ce: 0.001387, loss_dice: 0.045327
[12:41:28.039] TRAIN: iteration 29936 : loss : 0.055191, loss_ce: 0.006167, loss_dice: 0.104216
[12:41:28.248] TRAIN: iteration 29937 : loss : 0.081036, loss_ce: 0.003206, loss_dice: 0.158865
[12:41:28.458] TRAIN: iteration 29938 : loss : 0.059369, loss_ce: 0.003049, loss_dice: 0.115688
[12:41:28.666] TRAIN: iteration 29939 : loss : 0.031874, loss_ce: 0.002538, loss_dice: 0.061209
[12:41:28.875] TRAIN: iteration 29940 : loss : 0.176467, loss_ce: 0.001733, loss_dice: 0.351201
[12:41:29.116] TRAIN: iteration 29941 : loss : 0.229732, loss_ce: 0.005712, loss_dice: 0.453753
[12:41:29.327] TRAIN: iteration 29942 : loss : 0.095032, loss_ce: 0.003087, loss_dice: 0.186978
[12:41:29.535] TRAIN: iteration 29943 : loss : 0.240001, loss_ce: 0.003415, loss_dice: 0.476586
[12:41:29.743] TRAIN: iteration 29944 : loss : 0.081191, loss_ce: 0.005435, loss_dice: 0.156948
[12:41:29.952] TRAIN: iteration 29945 : loss : 0.053227, loss_ce: 0.001138, loss_dice: 0.105315
[12:41:30.160] TRAIN: iteration 29946 : loss : 0.106620, loss_ce: 0.012448, loss_dice: 0.200793
[12:41:30.368] TRAIN: iteration 29947 : loss : 0.250432, loss_ce: 0.001442, loss_dice: 0.499423
[12:41:30.576] TRAIN: iteration 29948 : loss : 0.158952, loss_ce: 0.002472, loss_dice: 0.315432
[12:41:30.783] TRAIN: iteration 29949 : loss : 0.061514, loss_ce: 0.004685, loss_dice: 0.118344
[12:41:30.991] TRAIN: iteration 29950 : loss : 0.189118, loss_ce: 0.006946, loss_dice: 0.371290
[12:41:31.199] TRAIN: iteration 29951 : loss : 0.083289, loss_ce: 0.003373, loss_dice: 0.163206
[12:41:31.410] TRAIN: iteration 29952 : loss : 0.251284, loss_ce: 0.002420, loss_dice: 0.500147
[12:41:31.621] TRAIN: iteration 29953 : loss : 0.026159, loss_ce: 0.001926, loss_dice: 0.050391
[12:41:31.828] TRAIN: iteration 29954 : loss : 0.021500, loss_ce: 0.001313, loss_dice: 0.041687
[12:41:32.039] TRAIN: iteration 29955 : loss : 0.251359, loss_ce: 0.002561, loss_dice: 0.500156
[12:41:32.255] TRAIN: iteration 29956 : loss : 0.128080, loss_ce: 0.006841, loss_dice: 0.249319
[12:41:32.465] TRAIN: iteration 29957 : loss : 0.043283, loss_ce: 0.001303, loss_dice: 0.085263
[12:41:32.673] TRAIN: iteration 29958 : loss : 0.089051, loss_ce: 0.001777, loss_dice: 0.176326
[12:41:32.883] TRAIN: iteration 29959 : loss : 0.046051, loss_ce: 0.006021, loss_dice: 0.086080
[12:41:33.095] TRAIN: iteration 29960 : loss : 0.075022, loss_ce: 0.003589, loss_dice: 0.146455
[12:41:33.340] TRAIN: iteration 29961 : loss : 0.052646, loss_ce: 0.010326, loss_dice: 0.094966
[12:41:33.550] TRAIN: iteration 29962 : loss : 0.053343, loss_ce: 0.004226, loss_dice: 0.102460
[12:41:33.764] TRAIN: iteration 29963 : loss : 0.088360, loss_ce: 0.002796, loss_dice: 0.173923
[12:41:33.974] TRAIN: iteration 29964 : loss : 0.034888, loss_ce: 0.001160, loss_dice: 0.068615
[12:41:34.183] TRAIN: iteration 29965 : loss : 0.061805, loss_ce: 0.006699, loss_dice: 0.116912
[12:41:34.392] TRAIN: iteration 29966 : loss : 0.042547, loss_ce: 0.003259, loss_dice: 0.081834
[12:41:34.603] TRAIN: iteration 29967 : loss : 0.140408, loss_ce: 0.001539, loss_dice: 0.279277
[12:41:34.813] TRAIN: iteration 29968 : loss : 0.026466, loss_ce: 0.002523, loss_dice: 0.050408
[12:41:35.023] TRAIN: iteration 29969 : loss : 0.251294, loss_ce: 0.002442, loss_dice: 0.500145
[12:41:35.234] TRAIN: iteration 29970 : loss : 0.066755, loss_ce: 0.004373, loss_dice: 0.129136
[12:41:35.443] TRAIN: iteration 29971 : loss : 0.198601, loss_ce: 0.008232, loss_dice: 0.388969
[12:41:35.659] TRAIN: iteration 29972 : loss : 0.034659, loss_ce: 0.002901, loss_dice: 0.066417
[12:41:35.872] TRAIN: iteration 29973 : loss : 0.087910, loss_ce: 0.001886, loss_dice: 0.173935
[12:41:36.080] TRAIN: iteration 29974 : loss : 0.053992, loss_ce: 0.010710, loss_dice: 0.097274
[12:41:36.288] TRAIN: iteration 29975 : loss : 0.250669, loss_ce: 0.001281, loss_dice: 0.500057
[12:41:36.503] TRAIN: iteration 29976 : loss : 0.045445, loss_ce: 0.001696, loss_dice: 0.089194
[12:41:36.711] TRAIN: iteration 29977 : loss : 0.039450, loss_ce: 0.003509, loss_dice: 0.075390
[12:41:36.925] TRAIN: iteration 29978 : loss : 0.021822, loss_ce: 0.000780, loss_dice: 0.042864
[12:41:37.133] TRAIN: iteration 29979 : loss : 0.095187, loss_ce: 0.006167, loss_dice: 0.184208
[12:41:37.340] TRAIN: iteration 29980 : loss : 0.046282, loss_ce: 0.006464, loss_dice: 0.086100
[12:41:37.578] TRAIN: iteration 29981 : loss : 0.249856, loss_ce: 0.003548, loss_dice: 0.496164
[12:41:37.786] TRAIN: iteration 29982 : loss : 0.100434, loss_ce: 0.007152, loss_dice: 0.193716
[12:41:37.999] TRAIN: iteration 29983 : loss : 0.245980, loss_ce: 0.001199, loss_dice: 0.490760
[12:41:38.207] TRAIN: iteration 29984 : loss : 0.058053, loss_ce: 0.005430, loss_dice: 0.110677
[12:41:38.427] TRAIN: iteration 29985 : loss : 0.035620, loss_ce: 0.004354, loss_dice: 0.066887
[12:41:38.637] TRAIN: iteration 29986 : loss : 0.249119, loss_ce: 0.001260, loss_dice: 0.496978
[12:41:38.846] TRAIN: iteration 29987 : loss : 0.060421, loss_ce: 0.002826, loss_dice: 0.118016
[12:41:39.055] TRAIN: iteration 29988 : loss : 0.213883, loss_ce: 0.001703, loss_dice: 0.426063
[12:41:39.269] TRAIN: iteration 29989 : loss : 0.252570, loss_ce: 0.004777, loss_dice: 0.500362
[12:41:39.477] TRAIN: iteration 29990 : loss : 0.022741, loss_ce: 0.001105, loss_dice: 0.044377
[12:41:39.689] TRAIN: iteration 29991 : loss : 0.159482, loss_ce: 0.001823, loss_dice: 0.317141
[12:41:39.899] TRAIN: iteration 29992 : loss : 0.072493, loss_ce: 0.002171, loss_dice: 0.142815
[12:41:40.108] TRAIN: iteration 29993 : loss : 0.046731, loss_ce: 0.003661, loss_dice: 0.089801
[12:41:40.317] TRAIN: iteration 29994 : loss : 0.250754, loss_ce: 0.001438, loss_dice: 0.500071
[12:41:40.528] TRAIN: iteration 29995 : loss : 0.052681, loss_ce: 0.000773, loss_dice: 0.104588
[12:41:40.737] TRAIN: iteration 29996 : loss : 0.044411, loss_ce: 0.002632, loss_dice: 0.086191
[12:41:40.946] TRAIN: iteration 29997 : loss : 0.071610, loss_ce: 0.000744, loss_dice: 0.142475
[12:41:41.162] TRAIN: iteration 29998 : loss : 0.061659, loss_ce: 0.001408, loss_dice: 0.121910
[12:41:41.383] TRAIN: iteration 29999 : loss : 0.057683, loss_ce: 0.001059, loss_dice: 0.114307
[12:41:41.590] TRAIN: iteration 30000 : loss : 0.148775, loss_ce: 0.002435, loss_dice: 0.295115
[12:41:41.825] TRAIN: iteration 30001 : loss : 0.196867, loss_ce: 0.000827, loss_dice: 0.392906
[12:41:42.033] TRAIN: iteration 30002 : loss : 0.049572, loss_ce: 0.001097, loss_dice: 0.098048
[12:41:42.241] TRAIN: iteration 30003 : loss : 0.061349, loss_ce: 0.002055, loss_dice: 0.120643
[12:41:42.448] TRAIN: iteration 30004 : loss : 0.121463, loss_ce: 0.011402, loss_dice: 0.231524
[12:41:42.671] TRAIN: iteration 30005 : loss : 0.063885, loss_ce: 0.002606, loss_dice: 0.125165
[12:41:42.882] TRAIN: iteration 30006 : loss : 0.250458, loss_ce: 0.000881, loss_dice: 0.500034
[12:41:43.090] TRAIN: iteration 30007 : loss : 0.058012, loss_ce: 0.001219, loss_dice: 0.114804
[12:41:43.298] TRAIN: iteration 30008 : loss : 0.039764, loss_ce: 0.003774, loss_dice: 0.075753
[12:41:44.235] TRAIN: iteration 30009 : loss : 0.059350, loss_ce: 0.003125, loss_dice: 0.115575
[12:41:44.443] TRAIN: iteration 30010 : loss : 0.199627, loss_ce: 0.005100, loss_dice: 0.394153
[12:41:44.659] TRAIN: iteration 30011 : loss : 0.187125, loss_ce: 0.006303, loss_dice: 0.367947
[12:41:44.868] TRAIN: iteration 30012 : loss : 0.233413, loss_ce: 0.001680, loss_dice: 0.465145
[12:41:45.080] TRAIN: iteration 30013 : loss : 0.058599, loss_ce: 0.002043, loss_dice: 0.115154
[12:41:45.290] TRAIN: iteration 30014 : loss : 0.084271, loss_ce: 0.002758, loss_dice: 0.165785
[12:41:45.499] TRAIN: iteration 30015 : loss : 0.171967, loss_ce: 0.001201, loss_dice: 0.342732
[12:41:45.714] TRAIN: iteration 30016 : loss : 0.083964, loss_ce: 0.003987, loss_dice: 0.163941
[12:41:45.921] TRAIN: iteration 30017 : loss : 0.083985, loss_ce: 0.002125, loss_dice: 0.165845
[12:41:46.130] TRAIN: iteration 30018 : loss : 0.251066, loss_ce: 0.002004, loss_dice: 0.500129
[12:41:46.345] TRAIN: iteration 30019 : loss : 0.042417, loss_ce: 0.004099, loss_dice: 0.080736
[12:41:46.553] TRAIN: iteration 30020 : loss : 0.250995, loss_ce: 0.001868, loss_dice: 0.500121
[12:41:46.786] TRAIN: iteration 30021 : loss : 0.104928, loss_ce: 0.002570, loss_dice: 0.207286
[12:41:47.001] TRAIN: iteration 30022 : loss : 0.075100, loss_ce: 0.001281, loss_dice: 0.148920
[12:41:47.246] TRAIN: iteration 30023 : loss : 0.193242, loss_ce: 0.012338, loss_dice: 0.374145
[12:41:47.458] TRAIN: iteration 30024 : loss : 0.148524, loss_ce: 0.002623, loss_dice: 0.294424
[12:41:48.052] TRAIN: iteration 30025 : loss : 0.253003, loss_ce: 0.007195, loss_dice: 0.498811
[12:41:48.262] TRAIN: iteration 30026 : loss : 0.044700, loss_ce: 0.000793, loss_dice: 0.088608
[12:41:48.472] TRAIN: iteration 30027 : loss : 0.066868, loss_ce: 0.002921, loss_dice: 0.130814
[12:41:48.681] TRAIN: iteration 30028 : loss : 0.062032, loss_ce: 0.009174, loss_dice: 0.114889
[12:41:48.891] TRAIN: iteration 30029 : loss : 0.065085, loss_ce: 0.003981, loss_dice: 0.126188
[12:41:49.099] TRAIN: iteration 30030 : loss : 0.052733, loss_ce: 0.005929, loss_dice: 0.099538
[12:41:49.307] TRAIN: iteration 30031 : loss : 0.039689, loss_ce: 0.001077, loss_dice: 0.078302
[12:41:49.520] TRAIN: iteration 30032 : loss : 0.142847, loss_ce: 0.006119, loss_dice: 0.279576
[12:41:49.731] TRAIN: iteration 30033 : loss : 0.038918, loss_ce: 0.001064, loss_dice: 0.076773
[12:41:49.940] TRAIN: iteration 30034 : loss : 0.175967, loss_ce: 0.001982, loss_dice: 0.349953
[12:41:50.150] TRAIN: iteration 30035 : loss : 0.036543, loss_ce: 0.000933, loss_dice: 0.072152
[12:41:50.357] TRAIN: iteration 30036 : loss : 0.135227, loss_ce: 0.007460, loss_dice: 0.262994
[12:41:50.567] TRAIN: iteration 30037 : loss : 0.070301, loss_ce: 0.006682, loss_dice: 0.133920
[12:41:50.779] TRAIN: iteration 30038 : loss : 0.231876, loss_ce: 0.001406, loss_dice: 0.462345
[12:41:50.987] TRAIN: iteration 30039 : loss : 0.073817, loss_ce: 0.001377, loss_dice: 0.146258
[12:41:51.196] TRAIN: iteration 30040 : loss : 0.250348, loss_ce: 0.000688, loss_dice: 0.500009
[12:41:52.225] TRAIN: iteration 30041 : loss : 0.090073, loss_ce: 0.002647, loss_dice: 0.177499
[12:41:52.435] TRAIN: iteration 30042 : loss : 0.057593, loss_ce: 0.002278, loss_dice: 0.112907
[12:41:52.643] TRAIN: iteration 30043 : loss : 0.250402, loss_ce: 0.000771, loss_dice: 0.500032
[12:41:52.852] TRAIN: iteration 30044 : loss : 0.051339, loss_ce: 0.005152, loss_dice: 0.097527
[12:41:53.061] TRAIN: iteration 30045 : loss : 0.078186, loss_ce: 0.001504, loss_dice: 0.154868
[12:41:53.278] TRAIN: iteration 30046 : loss : 0.247866, loss_ce: 0.001522, loss_dice: 0.494209
[12:41:53.486] TRAIN: iteration 30047 : loss : 0.030963, loss_ce: 0.006636, loss_dice: 0.055291
[12:41:53.694] TRAIN: iteration 30048 : loss : 0.233309, loss_ce: 0.003818, loss_dice: 0.462799
[12:41:53.903] TRAIN: iteration 30049 : loss : 0.127784, loss_ce: 0.005031, loss_dice: 0.250537
[12:41:54.114] TRAIN: iteration 30050 : loss : 0.041783, loss_ce: 0.002334, loss_dice: 0.081231
[12:41:54.323] TRAIN: iteration 30051 : loss : 0.166174, loss_ce: 0.004002, loss_dice: 0.328346
[12:41:54.530] TRAIN: iteration 30052 : loss : 0.089700, loss_ce: 0.007588, loss_dice: 0.171811
[12:41:54.738] TRAIN: iteration 30053 : loss : 0.038394, loss_ce: 0.001878, loss_dice: 0.074910
[12:41:54.946] TRAIN: iteration 30054 : loss : 0.050872, loss_ce: 0.006647, loss_dice: 0.095098
[12:41:55.167] TRAIN: iteration 30055 : loss : 0.250558, loss_ce: 0.001085, loss_dice: 0.500031
[12:41:55.373] TRAIN: iteration 30056 : loss : 0.193221, loss_ce: 0.002044, loss_dice: 0.384399
[12:41:55.580] TRAIN: iteration 30057 : loss : 0.114439, loss_ce: 0.011578, loss_dice: 0.217300
[12:41:55.788] TRAIN: iteration 30058 : loss : 0.054394, loss_ce: 0.005817, loss_dice: 0.102971
[12:41:56.003] TRAIN: iteration 30059 : loss : 0.145634, loss_ce: 0.007173, loss_dice: 0.284094
[12:41:56.212] TRAIN: iteration 30060 : loss : 0.142316, loss_ce: 0.008412, loss_dice: 0.276221
[12:41:56.449] TRAIN: iteration 30061 : loss : 0.023643, loss_ce: 0.002960, loss_dice: 0.044325
[12:41:56.658] TRAIN: iteration 30062 : loss : 0.018569, loss_ce: 0.001678, loss_dice: 0.035460
[12:41:56.866] TRAIN: iteration 30063 : loss : 0.202069, loss_ce: 0.001843, loss_dice: 0.402295
[12:41:57.075] TRAIN: iteration 30064 : loss : 0.044493, loss_ce: 0.002434, loss_dice: 0.086552
[12:41:57.289] TRAIN: iteration 30065 : loss : 0.134733, loss_ce: 0.006539, loss_dice: 0.262927
[12:41:57.501] TRAIN: iteration 30066 : loss : 0.112970, loss_ce: 0.001714, loss_dice: 0.224225
[12:41:57.721] TRAIN: iteration 30067 : loss : 0.103572, loss_ce: 0.004150, loss_dice: 0.202993
[12:41:57.929] TRAIN: iteration 30068 : loss : 0.076999, loss_ce: 0.009177, loss_dice: 0.144821
[12:41:58.138] TRAIN: iteration 30069 : loss : 0.068403, loss_ce: 0.001740, loss_dice: 0.135066
[12:41:58.346] TRAIN: iteration 30070 : loss : 0.207127, loss_ce: 0.002456, loss_dice: 0.411798
[12:41:58.553] TRAIN: iteration 30071 : loss : 0.060633, loss_ce: 0.007990, loss_dice: 0.113277
[12:41:58.760] TRAIN: iteration 30072 : loss : 0.081172, loss_ce: 0.006212, loss_dice: 0.156131
[12:41:58.967] TRAIN: iteration 30073 : loss : 0.058313, loss_ce: 0.002701, loss_dice: 0.113926
[12:41:59.175] TRAIN: iteration 30074 : loss : 0.037136, loss_ce: 0.000865, loss_dice: 0.073407
[12:41:59.382] TRAIN: iteration 30075 : loss : 0.043322, loss_ce: 0.001303, loss_dice: 0.085340
[12:41:59.590] TRAIN: iteration 30076 : loss : 0.101174, loss_ce: 0.002704, loss_dice: 0.199644
[12:41:59.797] TRAIN: iteration 30077 : loss : 0.042137, loss_ce: 0.002750, loss_dice: 0.081524
[12:42:00.136] TRAIN: iteration 30078 : loss : 0.250976, loss_ce: 0.001854, loss_dice: 0.500099
[12:42:00.343] TRAIN: iteration 30079 : loss : 0.058126, loss_ce: 0.006129, loss_dice: 0.110124
[12:42:00.551] TRAIN: iteration 30080 : loss : 0.075772, loss_ce: 0.005741, loss_dice: 0.145803
[12:42:00.789] TRAIN: iteration 30081 : loss : 0.065685, loss_ce: 0.002359, loss_dice: 0.129011
[12:42:00.997] TRAIN: iteration 30082 : loss : 0.112499, loss_ce: 0.003348, loss_dice: 0.221649
[12:42:01.207] TRAIN: iteration 30083 : loss : 0.095988, loss_ce: 0.003696, loss_dice: 0.188280
[12:42:01.416] TRAIN: iteration 30084 : loss : 0.076215, loss_ce: 0.002148, loss_dice: 0.150282
[12:42:01.624] TRAIN: iteration 30085 : loss : 0.050207, loss_ce: 0.004114, loss_dice: 0.096300
[12:42:02.284] TRAIN: iteration 30086 : loss : 0.067939, loss_ce: 0.002216, loss_dice: 0.133661
[12:42:02.491] TRAIN: iteration 30087 : loss : 0.113788, loss_ce: 0.004060, loss_dice: 0.223516
[12:42:02.698] TRAIN: iteration 30088 : loss : 0.094897, loss_ce: 0.002118, loss_dice: 0.187675
[12:42:02.905] TRAIN: iteration 30089 : loss : 0.062587, loss_ce: 0.010444, loss_dice: 0.114730
[12:42:03.116] TRAIN: iteration 30090 : loss : 0.096281, loss_ce: 0.003151, loss_dice: 0.189410
[12:42:03.323] TRAIN: iteration 30091 : loss : 0.124787, loss_ce: 0.002247, loss_dice: 0.247328
[12:42:03.530] TRAIN: iteration 30092 : loss : 0.225368, loss_ce: 0.002943, loss_dice: 0.447792
[12:42:03.740] TRAIN: iteration 30093 : loss : 0.082892, loss_ce: 0.002335, loss_dice: 0.163449
[12:42:03.947] TRAIN: iteration 30094 : loss : 0.250725, loss_ce: 0.001380, loss_dice: 0.500069
[12:42:04.155] TRAIN: iteration 30095 : loss : 0.020447, loss_ce: 0.001997, loss_dice: 0.038898
[12:42:04.363] TRAIN: iteration 30096 : loss : 0.187842, loss_ce: 0.001691, loss_dice: 0.373993
[12:42:04.571] TRAIN: iteration 30097 : loss : 0.058263, loss_ce: 0.002668, loss_dice: 0.113858
[12:42:04.778] TRAIN: iteration 30098 : loss : 0.177132, loss_ce: 0.001299, loss_dice: 0.352964
[12:42:04.985] TRAIN: iteration 30099 : loss : 0.078084, loss_ce: 0.006961, loss_dice: 0.149207
[12:42:05.194] TRAIN: iteration 30100 : loss : 0.052779, loss_ce: 0.003669, loss_dice: 0.101890
[12:42:05.431] TRAIN: iteration 30101 : loss : 0.214002, loss_ce: 0.002438, loss_dice: 0.425566
[12:42:07.284] TRAIN: iteration 30102 : loss : 0.059307, loss_ce: 0.004883, loss_dice: 0.113731
[12:42:07.491] TRAIN: iteration 30103 : loss : 0.067216, loss_ce: 0.005662, loss_dice: 0.128770
[12:42:07.699] TRAIN: iteration 30104 : loss : 0.111281, loss_ce: 0.003573, loss_dice: 0.218988
[12:42:07.905] TRAIN: iteration 30105 : loss : 0.022550, loss_ce: 0.001334, loss_dice: 0.043765
[12:42:08.113] TRAIN: iteration 30106 : loss : 0.085645, loss_ce: 0.001509, loss_dice: 0.169781
[12:42:08.320] TRAIN: iteration 30107 : loss : 0.018335, loss_ce: 0.004025, loss_dice: 0.032645
[12:42:08.528] TRAIN: iteration 30108 : loss : 0.072089, loss_ce: 0.005597, loss_dice: 0.138581
[12:42:08.735] TRAIN: iteration 30109 : loss : 0.047290, loss_ce: 0.003176, loss_dice: 0.091403
[12:42:08.946] TRAIN: iteration 30110 : loss : 0.065276, loss_ce: 0.004175, loss_dice: 0.126377
[12:42:09.162] TRAIN: iteration 30111 : loss : 0.121128, loss_ce: 0.006350, loss_dice: 0.235905
[12:42:09.370] TRAIN: iteration 30112 : loss : 0.144673, loss_ce: 0.007782, loss_dice: 0.281564
[12:42:09.577] TRAIN: iteration 30113 : loss : 0.032297, loss_ce: 0.001574, loss_dice: 0.063019
[12:42:09.786] TRAIN: iteration 30114 : loss : 0.044976, loss_ce: 0.005263, loss_dice: 0.084689
[12:42:09.995] TRAIN: iteration 30115 : loss : 0.071723, loss_ce: 0.001949, loss_dice: 0.141498
[12:42:10.207] TRAIN: iteration 30116 : loss : 0.022402, loss_ce: 0.001474, loss_dice: 0.043330
[12:42:10.422] TRAIN: iteration 30117 : loss : 0.096748, loss_ce: 0.002398, loss_dice: 0.191099
[12:42:10.633] TRAIN: iteration 30118 : loss : 0.054062, loss_ce: 0.003970, loss_dice: 0.104153
[12:42:11.480] TRAIN: iteration 30119 : loss : 0.047266, loss_ce: 0.000571, loss_dice: 0.093961
[12:42:11.690] TRAIN: iteration 30120 : loss : 0.072399, loss_ce: 0.005773, loss_dice: 0.139024
[12:42:11.927] TRAIN: iteration 30121 : loss : 0.188097, loss_ce: 0.003731, loss_dice: 0.372463
[12:42:12.138] TRAIN: iteration 30122 : loss : 0.034129, loss_ce: 0.001874, loss_dice: 0.066384
[12:42:12.350] TRAIN: iteration 30123 : loss : 0.034059, loss_ce: 0.002503, loss_dice: 0.065616
[12:42:12.557] TRAIN: iteration 30124 : loss : 0.075045, loss_ce: 0.004682, loss_dice: 0.145409
[12:42:12.766] TRAIN: iteration 30125 : loss : 0.182383, loss_ce: 0.001903, loss_dice: 0.362864
[12:42:12.976] TRAIN: iteration 30126 : loss : 0.031154, loss_ce: 0.006987, loss_dice: 0.055321
[12:42:13.188] TRAIN: iteration 30127 : loss : 0.213030, loss_ce: 0.001491, loss_dice: 0.424568
[12:42:13.397] TRAIN: iteration 30128 : loss : 0.110103, loss_ce: 0.009220, loss_dice: 0.210985
[12:42:13.604] TRAIN: iteration 30129 : loss : 0.079675, loss_ce: 0.001115, loss_dice: 0.158235
[12:42:13.818] TRAIN: iteration 30130 : loss : 0.088301, loss_ce: 0.010113, loss_dice: 0.166489
[12:42:14.027] TRAIN: iteration 30131 : loss : 0.062712, loss_ce: 0.002500, loss_dice: 0.122924
[12:42:14.234] TRAIN: iteration 30132 : loss : 0.087994, loss_ce: 0.007536, loss_dice: 0.168452
[12:42:14.451] TRAIN: iteration 30133 : loss : 0.032708, loss_ce: 0.000990, loss_dice: 0.064425
[12:42:14.669] TRAIN: iteration 30134 : loss : 0.088964, loss_ce: 0.005543, loss_dice: 0.172385
[12:42:14.877] TRAIN: iteration 30135 : loss : 0.076937, loss_ce: 0.006956, loss_dice: 0.146918
[12:42:15.086] TRAIN: iteration 30136 : loss : 0.058819, loss_ce: 0.004305, loss_dice: 0.113333
[12:42:15.294] TRAIN: iteration 30137 : loss : 0.107395, loss_ce: 0.002484, loss_dice: 0.212305
[12:42:15.502] TRAIN: iteration 30138 : loss : 0.061969, loss_ce: 0.001476, loss_dice: 0.122462
[12:42:15.710] TRAIN: iteration 30139 : loss : 0.047791, loss_ce: 0.004189, loss_dice: 0.091394
[12:42:15.917] TRAIN: iteration 30140 : loss : 0.056874, loss_ce: 0.001158, loss_dice: 0.112590
[12:42:16.156] TRAIN: iteration 30141 : loss : 0.117909, loss_ce: 0.002960, loss_dice: 0.232857
[12:42:16.364] TRAIN: iteration 30142 : loss : 0.178817, loss_ce: 0.001073, loss_dice: 0.356561
[12:42:16.573] TRAIN: iteration 30143 : loss : 0.250358, loss_ce: 0.000691, loss_dice: 0.500026
[12:42:16.786] TRAIN: iteration 30144 : loss : 0.252028, loss_ce: 0.003782, loss_dice: 0.500274
[12:42:17.396] TRAIN: iteration 30145 : loss : 0.052510, loss_ce: 0.003346, loss_dice: 0.101675
[12:42:17.611] TRAIN: iteration 30146 : loss : 0.250319, loss_ce: 0.000621, loss_dice: 0.500017
[12:42:17.819] TRAIN: iteration 30147 : loss : 0.043318, loss_ce: 0.008954, loss_dice: 0.077682
[12:42:18.026] TRAIN: iteration 30148 : loss : 0.250564, loss_ce: 0.001061, loss_dice: 0.500066
[12:42:18.237] TRAIN: iteration 30149 : loss : 0.187287, loss_ce: 0.001025, loss_dice: 0.373549
[12:42:18.447] TRAIN: iteration 30150 : loss : 0.135586, loss_ce: 0.001552, loss_dice: 0.269619
[12:42:18.654] TRAIN: iteration 30151 : loss : 0.250271, loss_ce: 0.000530, loss_dice: 0.500011
[12:42:18.862] TRAIN: iteration 30152 : loss : 0.064715, loss_ce: 0.003660, loss_dice: 0.125770
[12:42:19.605] TRAIN: iteration 30153 : loss : 0.055240, loss_ce: 0.015692, loss_dice: 0.094787
[12:42:19.814] TRAIN: iteration 30154 : loss : 0.056866, loss_ce: 0.001229, loss_dice: 0.112503
[12:42:20.025] TRAIN: iteration 30155 : loss : 0.078083, loss_ce: 0.002455, loss_dice: 0.153711
[12:42:20.232] TRAIN: iteration 30156 : loss : 0.074503, loss_ce: 0.008043, loss_dice: 0.140962
[12:42:20.443] TRAIN: iteration 30157 : loss : 0.029518, loss_ce: 0.000976, loss_dice: 0.058060
[12:42:20.654] TRAIN: iteration 30158 : loss : 0.064965, loss_ce: 0.006751, loss_dice: 0.123179
[12:42:20.863] TRAIN: iteration 30159 : loss : 0.012510, loss_ce: 0.001053, loss_dice: 0.023968
[12:42:21.074] TRAIN: iteration 30160 : loss : 0.070500, loss_ce: 0.001857, loss_dice: 0.139142
[12:42:21.314] TRAIN: iteration 30161 : loss : 0.072449, loss_ce: 0.006505, loss_dice: 0.138393
[12:42:21.528] TRAIN: iteration 30162 : loss : 0.143318, loss_ce: 0.001001, loss_dice: 0.285635
[12:42:21.739] TRAIN: iteration 30163 : loss : 0.094427, loss_ce: 0.006440, loss_dice: 0.182415
[12:42:21.950] TRAIN: iteration 30164 : loss : 0.069596, loss_ce: 0.001332, loss_dice: 0.137859
[12:42:22.159] TRAIN: iteration 30165 : loss : 0.031110, loss_ce: 0.001045, loss_dice: 0.061176
[12:42:22.442] TRAIN: iteration 30166 : loss : 0.083201, loss_ce: 0.001915, loss_dice: 0.164488
[12:42:22.653] TRAIN: iteration 30167 : loss : 0.144772, loss_ce: 0.002018, loss_dice: 0.287526
[12:42:22.862] TRAIN: iteration 30168 : loss : 0.071145, loss_ce: 0.001099, loss_dice: 0.141191
[12:42:23.072] TRAIN: iteration 30169 : loss : 0.046350, loss_ce: 0.009539, loss_dice: 0.083160
[12:42:23.282] TRAIN: iteration 30170 : loss : 0.056010, loss_ce: 0.004275, loss_dice: 0.107744
[12:42:23.490] TRAIN: iteration 30171 : loss : 0.163806, loss_ce: 0.001304, loss_dice: 0.326307
[12:42:23.701] TRAIN: iteration 30172 : loss : 0.113861, loss_ce: 0.000801, loss_dice: 0.226920
[12:42:23.909] TRAIN: iteration 30173 : loss : 0.250325, loss_ce: 0.000625, loss_dice: 0.500026
[12:42:24.117] TRAIN: iteration 30174 : loss : 0.255632, loss_ce: 0.011583, loss_dice: 0.499681
[12:42:24.326] TRAIN: iteration 30175 : loss : 0.087638, loss_ce: 0.002319, loss_dice: 0.172956
[12:42:24.533] TRAIN: iteration 30176 : loss : 0.124079, loss_ce: 0.002044, loss_dice: 0.246113
[12:42:24.742] TRAIN: iteration 30177 : loss : 0.043772, loss_ce: 0.004938, loss_dice: 0.082605
[12:42:24.949] TRAIN: iteration 30178 : loss : 0.064849, loss_ce: 0.003512, loss_dice: 0.126186
[12:42:25.157] TRAIN: iteration 30179 : loss : 0.056249, loss_ce: 0.001903, loss_dice: 0.110595
[12:42:25.365] TRAIN: iteration 30180 : loss : 0.123048, loss_ce: 0.002997, loss_dice: 0.243099
[12:42:25.604] TRAIN: iteration 30181 : loss : 0.018758, loss_ce: 0.001231, loss_dice: 0.036285
[12:42:25.811] TRAIN: iteration 30182 : loss : 0.230112, loss_ce: 0.001946, loss_dice: 0.458278
[12:42:26.025] TRAIN: iteration 30183 : loss : 0.043900, loss_ce: 0.002574, loss_dice: 0.085225
[12:42:26.232] TRAIN: iteration 30184 : loss : 0.103340, loss_ce: 0.003001, loss_dice: 0.203680
[12:42:26.440] TRAIN: iteration 30185 : loss : 0.061912, loss_ce: 0.005731, loss_dice: 0.118094
[12:42:26.646] TRAIN: iteration 30186 : loss : 0.231474, loss_ce: 0.003390, loss_dice: 0.459557
[12:42:26.854] TRAIN: iteration 30187 : loss : 0.251021, loss_ce: 0.001914, loss_dice: 0.500128
[12:42:27.062] TRAIN: iteration 30188 : loss : 0.039521, loss_ce: 0.004142, loss_dice: 0.074901
[12:42:27.276] TRAIN: iteration 30189 : loss : 0.035312, loss_ce: 0.001191, loss_dice: 0.069432
[12:42:27.487] TRAIN: iteration 30190 : loss : 0.051753, loss_ce: 0.004553, loss_dice: 0.098952
[12:42:27.702] TRAIN: iteration 30191 : loss : 0.012335, loss_ce: 0.001341, loss_dice: 0.023330
[12:42:27.917] TRAIN: iteration 30192 : loss : 0.249140, loss_ce: 0.002413, loss_dice: 0.495867
[12:42:28.125] TRAIN: iteration 30193 : loss : 0.026053, loss_ce: 0.000608, loss_dice: 0.051499
[12:42:28.333] TRAIN: iteration 30194 : loss : 0.077176, loss_ce: 0.001280, loss_dice: 0.153073
[12:42:28.543] TRAIN: iteration 30195 : loss : 0.252816, loss_ce: 0.007851, loss_dice: 0.497781
[12:42:28.754] TRAIN: iteration 30196 : loss : 0.113469, loss_ce: 0.002114, loss_dice: 0.224825
[12:42:28.970] TRAIN: iteration 30197 : loss : 0.250354, loss_ce: 0.000677, loss_dice: 0.500030
[12:42:29.179] TRAIN: iteration 30198 : loss : 0.029837, loss_ce: 0.000654, loss_dice: 0.059020
[12:42:29.408] TRAIN: iteration 30199 : loss : 0.250263, loss_ce: 0.000516, loss_dice: 0.500010
[12:42:29.617] TRAIN: iteration 30200 : loss : 0.067947, loss_ce: 0.001002, loss_dice: 0.134892
[12:42:29.851] TRAIN: iteration 30201 : loss : 0.034217, loss_ce: 0.003109, loss_dice: 0.065325
[12:42:30.059] TRAIN: iteration 30202 : loss : 0.053598, loss_ce: 0.001927, loss_dice: 0.105270
[12:42:30.269] TRAIN: iteration 30203 : loss : 0.059014, loss_ce: 0.004197, loss_dice: 0.113831
[12:42:30.476] TRAIN: iteration 30204 : loss : 0.249521, loss_ce: 0.001109, loss_dice: 0.497934
[12:42:30.692] TRAIN: iteration 30205 : loss : 0.061686, loss_ce: 0.000648, loss_dice: 0.122723
[12:42:31.293] TRAIN: iteration 30206 : loss : 0.250794, loss_ce: 0.001485, loss_dice: 0.500103
[12:42:31.501] TRAIN: iteration 30207 : loss : 0.102269, loss_ce: 0.006712, loss_dice: 0.197827
[12:42:31.709] TRAIN: iteration 30208 : loss : 0.038470, loss_ce: 0.003059, loss_dice: 0.073882
[12:42:31.920] TRAIN: iteration 30209 : loss : 0.044393, loss_ce: 0.002647, loss_dice: 0.086140
[12:42:32.131] TRAIN: iteration 30210 : loss : 0.086008, loss_ce: 0.004667, loss_dice: 0.167348
[12:42:32.338] TRAIN: iteration 30211 : loss : 0.257153, loss_ce: 0.013306, loss_dice: 0.501000
[12:42:32.545] TRAIN: iteration 30212 : loss : 0.093797, loss_ce: 0.001790, loss_dice: 0.185804
[12:42:32.759] TRAIN: iteration 30213 : loss : 0.208735, loss_ce: 0.010323, loss_dice: 0.407148
[12:42:32.966] TRAIN: iteration 30214 : loss : 0.083812, loss_ce: 0.001034, loss_dice: 0.166590
[12:42:33.175] TRAIN: iteration 30215 : loss : 0.071743, loss_ce: 0.002729, loss_dice: 0.140756
[12:42:33.383] TRAIN: iteration 30216 : loss : 0.208394, loss_ce: 0.001446, loss_dice: 0.415343
[12:42:33.591] TRAIN: iteration 30217 : loss : 0.107848, loss_ce: 0.004294, loss_dice: 0.211401
[12:42:33.799] TRAIN: iteration 30218 : loss : 0.197234, loss_ce: 0.002038, loss_dice: 0.392429
[12:42:34.008] TRAIN: iteration 30219 : loss : 0.144535, loss_ce: 0.001088, loss_dice: 0.287982
[12:42:34.569] TRAIN: iteration 30220 : loss : 0.128803, loss_ce: 0.003367, loss_dice: 0.254238
[12:42:34.808] TRAIN: iteration 30221 : loss : 0.098347, loss_ce: 0.003561, loss_dice: 0.193133
[12:42:35.017] TRAIN: iteration 30222 : loss : 0.054975, loss_ce: 0.003195, loss_dice: 0.106755
[12:42:35.231] TRAIN: iteration 30223 : loss : 0.065994, loss_ce: 0.001740, loss_dice: 0.130248
[12:42:35.442] TRAIN: iteration 30224 : loss : 0.098061, loss_ce: 0.004156, loss_dice: 0.191966
[12:42:35.651] TRAIN: iteration 30225 : loss : 0.242706, loss_ce: 0.001222, loss_dice: 0.484190
[12:42:35.859] TRAIN: iteration 30226 : loss : 0.037984, loss_ce: 0.006514, loss_dice: 0.069453
[12:42:36.067] TRAIN: iteration 30227 : loss : 0.075899, loss_ce: 0.003716, loss_dice: 0.148083
[12:42:36.277] TRAIN: iteration 30228 : loss : 0.088436, loss_ce: 0.021724, loss_dice: 0.155147
[12:42:36.486] TRAIN: iteration 30229 : loss : 0.070064, loss_ce: 0.002328, loss_dice: 0.137800
[12:42:36.693] TRAIN: iteration 30230 : loss : 0.162462, loss_ce: 0.001929, loss_dice: 0.322994
[12:42:36.903] TRAIN: iteration 30231 : loss : 0.025559, loss_ce: 0.002344, loss_dice: 0.048773
[12:42:37.112] TRAIN: iteration 30232 : loss : 0.166486, loss_ce: 0.002209, loss_dice: 0.330763
[12:42:37.320] TRAIN: iteration 30233 : loss : 0.043489, loss_ce: 0.004172, loss_dice: 0.082805
[12:42:37.527] TRAIN: iteration 30234 : loss : 0.197395, loss_ce: 0.002350, loss_dice: 0.392440
[12:42:37.735] TRAIN: iteration 30235 : loss : 0.032012, loss_ce: 0.000460, loss_dice: 0.063565
[12:42:37.979] TRAIN: iteration 30236 : loss : 0.237574, loss_ce: 0.003984, loss_dice: 0.471165
[12:42:38.190] TRAIN: iteration 30237 : loss : 0.067598, loss_ce: 0.002521, loss_dice: 0.132675
[12:42:38.400] TRAIN: iteration 30238 : loss : 0.049789, loss_ce: 0.002568, loss_dice: 0.097011
[12:42:38.610] TRAIN: iteration 30239 : loss : 0.071505, loss_ce: 0.004216, loss_dice: 0.138795
[12:42:38.818] TRAIN: iteration 30240 : loss : 0.031154, loss_ce: 0.004754, loss_dice: 0.057554
[12:42:39.056] TRAIN: iteration 30241 : loss : 0.055708, loss_ce: 0.011547, loss_dice: 0.099868
[12:42:39.265] TRAIN: iteration 30242 : loss : 0.180118, loss_ce: 0.001005, loss_dice: 0.359230
[12:42:39.472] TRAIN: iteration 30243 : loss : 0.053889, loss_ce: 0.003369, loss_dice: 0.104410
[12:42:39.681] TRAIN: iteration 30244 : loss : 0.091216, loss_ce: 0.001841, loss_dice: 0.180590
[12:42:39.889] TRAIN: iteration 30245 : loss : 0.121068, loss_ce: 0.019153, loss_dice: 0.222983
[12:42:40.099] TRAIN: iteration 30246 : loss : 0.086322, loss_ce: 0.003127, loss_dice: 0.169518
[12:42:40.307] TRAIN: iteration 30247 : loss : 0.023695, loss_ce: 0.002401, loss_dice: 0.044988
[12:42:40.515] TRAIN: iteration 30248 : loss : 0.222080, loss_ce: 0.001732, loss_dice: 0.442429
[12:42:40.725] TRAIN: iteration 30249 : loss : 0.250620, loss_ce: 0.001171, loss_dice: 0.500069
[12:42:40.941] TRAIN: iteration 30250 : loss : 0.063260, loss_ce: 0.005929, loss_dice: 0.120590
[12:42:42.053] TRAIN: iteration 30251 : loss : 0.048013, loss_ce: 0.002361, loss_dice: 0.093664
[12:42:42.260] TRAIN: iteration 30252 : loss : 0.052821, loss_ce: 0.002034, loss_dice: 0.103609
[12:42:42.467] TRAIN: iteration 30253 : loss : 0.060059, loss_ce: 0.000719, loss_dice: 0.119399
[12:42:42.674] TRAIN: iteration 30254 : loss : 0.240584, loss_ce: 0.003223, loss_dice: 0.477946
[12:42:42.881] TRAIN: iteration 30255 : loss : 0.131145, loss_ce: 0.001539, loss_dice: 0.260751
[12:42:43.095] TRAIN: iteration 30256 : loss : 0.173947, loss_ce: 0.001708, loss_dice: 0.346186
[12:42:43.303] TRAIN: iteration 30257 : loss : 0.240593, loss_ce: 0.001296, loss_dice: 0.479890
[12:42:43.511] TRAIN: iteration 30258 : loss : 0.065417, loss_ce: 0.001680, loss_dice: 0.129154
[12:42:43.720] TRAIN: iteration 30259 : loss : 0.095784, loss_ce: 0.002630, loss_dice: 0.188939
[12:42:43.929] TRAIN: iteration 30260 : loss : 0.167456, loss_ce: 0.000979, loss_dice: 0.333933
[12:42:44.286] TRAIN: iteration 30261 : loss : 0.223537, loss_ce: 0.002202, loss_dice: 0.444871
[12:42:44.494] TRAIN: iteration 30262 : loss : 0.060186, loss_ce: 0.000855, loss_dice: 0.119517
[12:42:44.706] TRAIN: iteration 30263 : loss : 0.051936, loss_ce: 0.002849, loss_dice: 0.101024
[12:42:44.914] TRAIN: iteration 30264 : loss : 0.036996, loss_ce: 0.000700, loss_dice: 0.073291
[12:42:45.129] TRAIN: iteration 30265 : loss : 0.111895, loss_ce: 0.003062, loss_dice: 0.220728
[12:42:45.336] TRAIN: iteration 30266 : loss : 0.169739, loss_ce: 0.000804, loss_dice: 0.338674
[12:42:45.545] TRAIN: iteration 30267 : loss : 0.114336, loss_ce: 0.006189, loss_dice: 0.222483
[12:42:45.759] TRAIN: iteration 30268 : loss : 0.026688, loss_ce: 0.001922, loss_dice: 0.051453
[12:42:45.972] TRAIN: iteration 30269 : loss : 0.044660, loss_ce: 0.007150, loss_dice: 0.082171
[12:42:46.183] TRAIN: iteration 30270 : loss : 0.018726, loss_ce: 0.000933, loss_dice: 0.036520
[12:42:46.392] TRAIN: iteration 30271 : loss : 0.062866, loss_ce: 0.001841, loss_dice: 0.123891
[12:42:46.603] TRAIN: iteration 30272 : loss : 0.069936, loss_ce: 0.000693, loss_dice: 0.139179
[12:42:46.810] TRAIN: iteration 30273 : loss : 0.106233, loss_ce: 0.000718, loss_dice: 0.211749
[12:42:47.019] TRAIN: iteration 30274 : loss : 0.105116, loss_ce: 0.004099, loss_dice: 0.206134
[12:42:47.228] TRAIN: iteration 30275 : loss : 0.208880, loss_ce: 0.005228, loss_dice: 0.412532
[12:42:47.443] TRAIN: iteration 30276 : loss : 0.107400, loss_ce: 0.008568, loss_dice: 0.206231
[12:42:47.994] TRAIN: iteration 30277 : loss : 0.040607, loss_ce: 0.005293, loss_dice: 0.075922
[12:42:48.202] TRAIN: iteration 30278 : loss : 0.079913, loss_ce: 0.001164, loss_dice: 0.158661
[12:42:48.409] TRAIN: iteration 30279 : loss : 0.020269, loss_ce: 0.001697, loss_dice: 0.038841
[12:42:48.625] TRAIN: iteration 30280 : loss : 0.013079, loss_ce: 0.001205, loss_dice: 0.024953
[12:42:48.873] TRAIN: iteration 30281 : loss : 0.099290, loss_ce: 0.003756, loss_dice: 0.194824
[12:42:49.082] TRAIN: iteration 30282 : loss : 0.081043, loss_ce: 0.001653, loss_dice: 0.160433
[12:42:49.294] TRAIN: iteration 30283 : loss : 0.100286, loss_ce: 0.000676, loss_dice: 0.199896
[12:42:49.503] TRAIN: iteration 30284 : loss : 0.250197, loss_ce: 0.000387, loss_dice: 0.500006
[12:42:49.710] TRAIN: iteration 30285 : loss : 0.064991, loss_ce: 0.001673, loss_dice: 0.128310
[12:42:49.920] TRAIN: iteration 30286 : loss : 0.128769, loss_ce: 0.002316, loss_dice: 0.255221
[12:42:50.268] TRAIN: iteration 30287 : loss : 0.250202, loss_ce: 0.000398, loss_dice: 0.500006
[12:42:50.475] TRAIN: iteration 30288 : loss : 0.035957, loss_ce: 0.001452, loss_dice: 0.070463
[12:42:50.687] TRAIN: iteration 30289 : loss : 0.070344, loss_ce: 0.001129, loss_dice: 0.139560
[12:42:50.900] TRAIN: iteration 30290 : loss : 0.089442, loss_ce: 0.000630, loss_dice: 0.178253
[12:42:51.108] TRAIN: iteration 30291 : loss : 0.073940, loss_ce: 0.002166, loss_dice: 0.145714
[12:42:51.314] TRAIN: iteration 30292 : loss : 0.113297, loss_ce: 0.004982, loss_dice: 0.221613
[12:42:51.710] TRAIN: iteration 30293 : loss : 0.065552, loss_ce: 0.002461, loss_dice: 0.128644
[12:42:51.926] TRAIN: iteration 30294 : loss : 0.027878, loss_ce: 0.001906, loss_dice: 0.053850
[12:42:52.133] TRAIN: iteration 30295 : loss : 0.115412, loss_ce: 0.000663, loss_dice: 0.230161
[12:42:52.340] TRAIN: iteration 30296 : loss : 0.131946, loss_ce: 0.004153, loss_dice: 0.259740
[12:42:53.282] TRAIN: iteration 30297 : loss : 0.034336, loss_ce: 0.006109, loss_dice: 0.062562
[12:42:53.491] TRAIN: iteration 30298 : loss : 0.069780, loss_ce: 0.002590, loss_dice: 0.136970
[12:42:53.700] TRAIN: iteration 30299 : loss : 0.090819, loss_ce: 0.002004, loss_dice: 0.179634
[12:42:53.907] TRAIN: iteration 30300 : loss : 0.031618, loss_ce: 0.000506, loss_dice: 0.062730
[12:42:55.375] TRAIN: iteration 30301 : loss : 0.131881, loss_ce: 0.000992, loss_dice: 0.262770
[12:42:55.585] TRAIN: iteration 30302 : loss : 0.090432, loss_ce: 0.003994, loss_dice: 0.176870
[12:42:55.792] TRAIN: iteration 30303 : loss : 0.136576, loss_ce: 0.015836, loss_dice: 0.257315
[12:42:56.006] TRAIN: iteration 30304 : loss : 0.033988, loss_ce: 0.004721, loss_dice: 0.063256
[12:42:56.216] TRAIN: iteration 30305 : loss : 0.124740, loss_ce: 0.000786, loss_dice: 0.248693
[12:42:56.423] TRAIN: iteration 30306 : loss : 0.049686, loss_ce: 0.000398, loss_dice: 0.098974
[12:42:56.631] TRAIN: iteration 30307 : loss : 0.014825, loss_ce: 0.000451, loss_dice: 0.029200
[12:42:56.840] TRAIN: iteration 30308 : loss : 0.217350, loss_ce: 0.002799, loss_dice: 0.431901
[12:42:57.073] TRAIN: iteration 30309 : loss : 0.072586, loss_ce: 0.002446, loss_dice: 0.142726
[12:42:57.281] TRAIN: iteration 30310 : loss : 0.030988, loss_ce: 0.006278, loss_dice: 0.055698
[12:42:57.491] TRAIN: iteration 30311 : loss : 0.042122, loss_ce: 0.004091, loss_dice: 0.080154
[12:42:57.699] TRAIN: iteration 30312 : loss : 0.062649, loss_ce: 0.001080, loss_dice: 0.124218
[12:42:57.911] TRAIN: iteration 30313 : loss : 0.250684, loss_ce: 0.001277, loss_dice: 0.500092
[12:42:58.120] TRAIN: iteration 30314 : loss : 0.026238, loss_ce: 0.002063, loss_dice: 0.050413
[12:42:58.329] TRAIN: iteration 30315 : loss : 0.064202, loss_ce: 0.002578, loss_dice: 0.125826
[12:42:58.536] TRAIN: iteration 30316 : loss : 0.042683, loss_ce: 0.000696, loss_dice: 0.084671
[12:42:58.745] TRAIN: iteration 30317 : loss : 0.025099, loss_ce: 0.001834, loss_dice: 0.048364
[12:42:58.956] TRAIN: iteration 30318 : loss : 0.078388, loss_ce: 0.001879, loss_dice: 0.154897
[12:42:59.173] TRAIN: iteration 30319 : loss : 0.033560, loss_ce: 0.006258, loss_dice: 0.060861
[12:42:59.496] TRAIN: iteration 30320 : loss : 0.153046, loss_ce: 0.001174, loss_dice: 0.304918
[12:42:59.733] TRAIN: iteration 30321 : loss : 0.052095, loss_ce: 0.002737, loss_dice: 0.101453
[12:42:59.941] TRAIN: iteration 30322 : loss : 0.098454, loss_ce: 0.001475, loss_dice: 0.195434
[12:43:00.163] TRAIN: iteration 30323 : loss : 0.041983, loss_ce: 0.003360, loss_dice: 0.080607
[12:43:00.373] TRAIN: iteration 30324 : loss : 0.096233, loss_ce: 0.006370, loss_dice: 0.186097
[12:43:00.580] TRAIN: iteration 30325 : loss : 0.037578, loss_ce: 0.002985, loss_dice: 0.072170
[12:43:00.795] TRAIN: iteration 30326 : loss : 0.030694, loss_ce: 0.003571, loss_dice: 0.057816
[12:43:01.002] TRAIN: iteration 30327 : loss : 0.119106, loss_ce: 0.000834, loss_dice: 0.237377
[12:43:01.210] TRAIN: iteration 30328 : loss : 0.056164, loss_ce: 0.000804, loss_dice: 0.111524
[12:43:01.422] TRAIN: iteration 30329 : loss : 0.053009, loss_ce: 0.005589, loss_dice: 0.100428
[12:43:01.630] TRAIN: iteration 30330 : loss : 0.050768, loss_ce: 0.003479, loss_dice: 0.098057
[12:43:01.839] TRAIN: iteration 30331 : loss : 0.037529, loss_ce: 0.001588, loss_dice: 0.073470
[12:43:02.047] TRAIN: iteration 30332 : loss : 0.107255, loss_ce: 0.001351, loss_dice: 0.213160
[12:43:02.258] TRAIN: iteration 30333 : loss : 0.205732, loss_ce: 0.000283, loss_dice: 0.411181
[12:43:02.466] TRAIN: iteration 30334 : loss : 0.081977, loss_ce: 0.000892, loss_dice: 0.163061
[12:43:02.676] TRAIN: iteration 30335 : loss : 0.039547, loss_ce: 0.003481, loss_dice: 0.075613
[12:43:02.891] TRAIN: iteration 30336 : loss : 0.028131, loss_ce: 0.000973, loss_dice: 0.055289
[12:43:03.103] TRAIN: iteration 30337 : loss : 0.073129, loss_ce: 0.005263, loss_dice: 0.140995
[12:43:03.335] TRAIN: iteration 30338 : loss : 0.050838, loss_ce: 0.001270, loss_dice: 0.100406
[12:43:03.544] TRAIN: iteration 30339 : loss : 0.072664, loss_ce: 0.002171, loss_dice: 0.143158
[12:43:03.757] TRAIN: iteration 30340 : loss : 0.148926, loss_ce: 0.006424, loss_dice: 0.291428
[12:43:03.997] TRAIN: iteration 30341 : loss : 0.122352, loss_ce: 0.007026, loss_dice: 0.237678
[12:43:04.205] TRAIN: iteration 30342 : loss : 0.068231, loss_ce: 0.001576, loss_dice: 0.134886
[12:43:04.420] TRAIN: iteration 30343 : loss : 0.033257, loss_ce: 0.000596, loss_dice: 0.065917
[12:43:04.629] TRAIN: iteration 30344 : loss : 0.057794, loss_ce: 0.002697, loss_dice: 0.112892
[12:43:04.843] TRAIN: iteration 30345 : loss : 0.031024, loss_ce: 0.000957, loss_dice: 0.061092
[12:43:05.053] TRAIN: iteration 30346 : loss : 0.033630, loss_ce: 0.000605, loss_dice: 0.066654
[12:43:05.265] TRAIN: iteration 30347 : loss : 0.121726, loss_ce: 0.002154, loss_dice: 0.241297
[12:43:06.124] TRAIN: iteration 30348 : loss : 0.056781, loss_ce: 0.009301, loss_dice: 0.104262
[12:43:06.332] TRAIN: iteration 30349 : loss : 0.115759, loss_ce: 0.002494, loss_dice: 0.229024
[12:43:06.543] TRAIN: iteration 30350 : loss : 0.035130, loss_ce: 0.001486, loss_dice: 0.068774
[12:43:06.751] TRAIN: iteration 30351 : loss : 0.035905, loss_ce: 0.000396, loss_dice: 0.071413
[12:43:06.959] TRAIN: iteration 30352 : loss : 0.050238, loss_ce: 0.000790, loss_dice: 0.099686
[12:43:07.170] TRAIN: iteration 30353 : loss : 0.249911, loss_ce: 0.003843, loss_dice: 0.495979
[12:43:07.380] TRAIN: iteration 30354 : loss : 0.045332, loss_ce: 0.002094, loss_dice: 0.088569
[12:43:07.588] TRAIN: iteration 30355 : loss : 0.037465, loss_ce: 0.001865, loss_dice: 0.073065
[12:43:08.558] TRAIN: iteration 30356 : loss : 0.045280, loss_ce: 0.002498, loss_dice: 0.088062
[12:43:08.767] TRAIN: iteration 30357 : loss : 0.236515, loss_ce: 0.001490, loss_dice: 0.471541
[12:43:08.977] TRAIN: iteration 30358 : loss : 0.020968, loss_ce: 0.002146, loss_dice: 0.039790
[12:43:09.187] TRAIN: iteration 30359 : loss : 0.044140, loss_ce: 0.002787, loss_dice: 0.085492
[12:43:09.442] TRAIN: iteration 30360 : loss : 0.035811, loss_ce: 0.006009, loss_dice: 0.065612
[12:43:09.682] TRAIN: iteration 30361 : loss : 0.071345, loss_ce: 0.002705, loss_dice: 0.139984
[12:43:09.897] TRAIN: iteration 30362 : loss : 0.247763, loss_ce: 0.001232, loss_dice: 0.494294
[12:43:10.116] TRAIN: iteration 30363 : loss : 0.130721, loss_ce: 0.013764, loss_dice: 0.247678
[12:43:10.669] TRAIN: iteration 30364 : loss : 0.250589, loss_ce: 0.001118, loss_dice: 0.500060
[12:43:10.880] TRAIN: iteration 30365 : loss : 0.135824, loss_ce: 0.006679, loss_dice: 0.264970
[12:43:11.090] TRAIN: iteration 30366 : loss : 0.251206, loss_ce: 0.002261, loss_dice: 0.500150
[12:43:11.307] TRAIN: iteration 30367 : loss : 0.029595, loss_ce: 0.001721, loss_dice: 0.057469
[12:43:11.564] TRAIN: iteration 30368 : loss : 0.146321, loss_ce: 0.009945, loss_dice: 0.282698
[12:43:11.771] TRAIN: iteration 30369 : loss : 0.227951, loss_ce: 0.002436, loss_dice: 0.453465
[12:43:11.978] TRAIN: iteration 30370 : loss : 0.120676, loss_ce: 0.007422, loss_dice: 0.233930
[12:43:12.187] TRAIN: iteration 30371 : loss : 0.093577, loss_ce: 0.004607, loss_dice: 0.182547
[12:43:12.399] TRAIN: iteration 30372 : loss : 0.251695, loss_ce: 0.003179, loss_dice: 0.500211
[12:43:12.608] TRAIN: iteration 30373 : loss : 0.036819, loss_ce: 0.001692, loss_dice: 0.071945
[12:43:12.816] TRAIN: iteration 30374 : loss : 0.045002, loss_ce: 0.003084, loss_dice: 0.086920
[12:43:13.033] TRAIN: iteration 30375 : loss : 0.087703, loss_ce: 0.001303, loss_dice: 0.174103
[12:43:13.245] TRAIN: iteration 30376 : loss : 0.059332, loss_ce: 0.001113, loss_dice: 0.117550
[12:43:13.456] TRAIN: iteration 30377 : loss : 0.251001, loss_ce: 0.001890, loss_dice: 0.500112
[12:43:13.666] TRAIN: iteration 30378 : loss : 0.250759, loss_ce: 0.001457, loss_dice: 0.500060
[12:43:13.934] TRAIN: iteration 30379 : loss : 0.027458, loss_ce: 0.003589, loss_dice: 0.051327
[12:43:14.143] TRAIN: iteration 30380 : loss : 0.250982, loss_ce: 0.001856, loss_dice: 0.500109
[12:43:14.380] TRAIN: iteration 30381 : loss : 0.066782, loss_ce: 0.001120, loss_dice: 0.132443
[12:43:14.751] TRAIN: iteration 30382 : loss : 0.056456, loss_ce: 0.001223, loss_dice: 0.111689
[12:43:14.964] TRAIN: iteration 30383 : loss : 0.074111, loss_ce: 0.003686, loss_dice: 0.144535
[12:43:15.172] TRAIN: iteration 30384 : loss : 0.066365, loss_ce: 0.001677, loss_dice: 0.131053
[12:43:15.380] TRAIN: iteration 30385 : loss : 0.250832, loss_ce: 0.001576, loss_dice: 0.500088
[12:43:15.588] TRAIN: iteration 30386 : loss : 0.056936, loss_ce: 0.001379, loss_dice: 0.112493
[12:43:15.795] TRAIN: iteration 30387 : loss : 0.053863, loss_ce: 0.002154, loss_dice: 0.105572
[12:43:16.010] TRAIN: iteration 30388 : loss : 0.053848, loss_ce: 0.002449, loss_dice: 0.105246
[12:43:16.218] TRAIN: iteration 30389 : loss : 0.123231, loss_ce: 0.003107, loss_dice: 0.243354
[12:43:16.426] TRAIN: iteration 30390 : loss : 0.170691, loss_ce: 0.001737, loss_dice: 0.339645
[12:43:16.641] TRAIN: iteration 30391 : loss : 0.047789, loss_ce: 0.002281, loss_dice: 0.093296
[12:43:16.848] TRAIN: iteration 30392 : loss : 0.090074, loss_ce: 0.001781, loss_dice: 0.178366
[12:43:17.056] TRAIN: iteration 30393 : loss : 0.057487, loss_ce: 0.003131, loss_dice: 0.111842
[12:43:17.268] TRAIN: iteration 30394 : loss : 0.087670, loss_ce: 0.005221, loss_dice: 0.170118
[12:43:18.456] TRAIN: iteration 30395 : loss : 0.096401, loss_ce: 0.001948, loss_dice: 0.190853
[12:43:18.666] TRAIN: iteration 30396 : loss : 0.182380, loss_ce: 0.002576, loss_dice: 0.362185
[12:43:18.877] TRAIN: iteration 30397 : loss : 0.050066, loss_ce: 0.001946, loss_dice: 0.098187
[12:43:19.089] TRAIN: iteration 30398 : loss : 0.034675, loss_ce: 0.003467, loss_dice: 0.065884
[12:43:19.300] TRAIN: iteration 30399 : loss : 0.042962, loss_ce: 0.001264, loss_dice: 0.084661
[12:43:19.509] TRAIN: iteration 30400 : loss : 0.057561, loss_ce: 0.001820, loss_dice: 0.113302
[12:43:19.750] TRAIN: iteration 30401 : loss : 0.107523, loss_ce: 0.005237, loss_dice: 0.209810
[12:43:19.958] TRAIN: iteration 30402 : loss : 0.067755, loss_ce: 0.003408, loss_dice: 0.132103
[12:43:20.164] TRAIN: iteration 30403 : loss : 0.182750, loss_ce: 0.004676, loss_dice: 0.360824
[12:43:20.373] TRAIN: iteration 30404 : loss : 0.250767, loss_ce: 0.001460, loss_dice: 0.500073
[12:43:20.581] TRAIN: iteration 30405 : loss : 0.087822, loss_ce: 0.004549, loss_dice: 0.171094
[12:43:20.790] TRAIN: iteration 30406 : loss : 0.151309, loss_ce: 0.002020, loss_dice: 0.300598
[12:43:21.000] TRAIN: iteration 30407 : loss : 0.241356, loss_ce: 0.001433, loss_dice: 0.481279
[12:43:21.210] TRAIN: iteration 30408 : loss : 0.181554, loss_ce: 0.001374, loss_dice: 0.361735
[12:43:21.417] TRAIN: iteration 30409 : loss : 0.027945, loss_ce: 0.002417, loss_dice: 0.053473
[12:43:21.630] TRAIN: iteration 30410 : loss : 0.251433, loss_ce: 0.005294, loss_dice: 0.497572
[12:43:21.916] TRAIN: iteration 30411 : loss : 0.083361, loss_ce: 0.008852, loss_dice: 0.157870
[12:43:22.147] TRAIN: iteration 30412 : loss : 0.046813, loss_ce: 0.003096, loss_dice: 0.090530
[12:43:22.362] TRAIN: iteration 30413 : loss : 0.044932, loss_ce: 0.002737, loss_dice: 0.087127
[12:43:22.570] TRAIN: iteration 30414 : loss : 0.033918, loss_ce: 0.004352, loss_dice: 0.063485
[12:43:22.782] TRAIN: iteration 30415 : loss : 0.086069, loss_ce: 0.005850, loss_dice: 0.166288
[12:43:22.994] TRAIN: iteration 30416 : loss : 0.123668, loss_ce: 0.007519, loss_dice: 0.239817
[12:43:23.208] TRAIN: iteration 30417 : loss : 0.074269, loss_ce: 0.002262, loss_dice: 0.146277
[12:43:23.416] TRAIN: iteration 30418 : loss : 0.093340, loss_ce: 0.001786, loss_dice: 0.184894
[12:43:23.629] TRAIN: iteration 30419 : loss : 0.251013, loss_ce: 0.001929, loss_dice: 0.500097
[12:43:23.841] TRAIN: iteration 30420 : loss : 0.033897, loss_ce: 0.006661, loss_dice: 0.061133
[12:43:24.078] TRAIN: iteration 30421 : loss : 0.067741, loss_ce: 0.002307, loss_dice: 0.133175
[12:43:24.315] TRAIN: iteration 30422 : loss : 0.117232, loss_ce: 0.002360, loss_dice: 0.232103
[12:43:24.523] TRAIN: iteration 30423 : loss : 0.208718, loss_ce: 0.005669, loss_dice: 0.411766
[12:43:24.730] TRAIN: iteration 30424 : loss : 0.109504, loss_ce: 0.003327, loss_dice: 0.215682
[12:43:24.938] TRAIN: iteration 30425 : loss : 0.250342, loss_ce: 0.000671, loss_dice: 0.500012
[12:43:25.145] TRAIN: iteration 30426 : loss : 0.150008, loss_ce: 0.001798, loss_dice: 0.298217
[12:43:25.354] TRAIN: iteration 30427 : loss : 0.042292, loss_ce: 0.001143, loss_dice: 0.083440
[12:43:25.564] TRAIN: iteration 30428 : loss : 0.065318, loss_ce: 0.001268, loss_dice: 0.129367
[12:43:25.862] TRAIN: iteration 30429 : loss : 0.249144, loss_ce: 0.000960, loss_dice: 0.497328
[12:43:26.070] TRAIN: iteration 30430 : loss : 0.186929, loss_ce: 0.017019, loss_dice: 0.356839
[12:43:26.282] TRAIN: iteration 30431 : loss : 0.082697, loss_ce: 0.001584, loss_dice: 0.163811
[12:43:26.493] TRAIN: iteration 30432 : loss : 0.146766, loss_ce: 0.001115, loss_dice: 0.292418
[12:43:26.705] TRAIN: iteration 30433 : loss : 0.066740, loss_ce: 0.001860, loss_dice: 0.131620
[12:43:26.912] TRAIN: iteration 30434 : loss : 0.250277, loss_ce: 0.000538, loss_dice: 0.500016
[12:43:27.125] TRAIN: iteration 30435 : loss : 0.048448, loss_ce: 0.001549, loss_dice: 0.095348
[12:43:27.333] TRAIN: iteration 30436 : loss : 0.250682, loss_ce: 0.001940, loss_dice: 0.499424
[12:43:27.542] TRAIN: iteration 30437 : loss : 0.083019, loss_ce: 0.002016, loss_dice: 0.164021
[12:43:27.749] TRAIN: iteration 30438 : loss : 0.250537, loss_ce: 0.001018, loss_dice: 0.500056
[12:43:27.961] TRAIN: iteration 30439 : loss : 0.242527, loss_ce: 0.001056, loss_dice: 0.483998
[12:43:28.173] TRAIN: iteration 30440 : loss : 0.054225, loss_ce: 0.008817, loss_dice: 0.099633
[12:43:28.414] TRAIN: iteration 30441 : loss : 0.121524, loss_ce: 0.003488, loss_dice: 0.239559
[12:43:28.733] TRAIN: iteration 30442 : loss : 0.113967, loss_ce: 0.002283, loss_dice: 0.225651
[12:43:28.943] TRAIN: iteration 30443 : loss : 0.107037, loss_ce: 0.000982, loss_dice: 0.213093
[12:43:29.153] TRAIN: iteration 30444 : loss : 0.172238, loss_ce: 0.005394, loss_dice: 0.339082
[12:43:29.359] TRAIN: iteration 30445 : loss : 0.246936, loss_ce: 0.001097, loss_dice: 0.492775
[12:43:29.570] TRAIN: iteration 30446 : loss : 0.104577, loss_ce: 0.005423, loss_dice: 0.203730
[12:43:29.781] TRAIN: iteration 30447 : loss : 0.168527, loss_ce: 0.003722, loss_dice: 0.333332
[12:43:29.989] TRAIN: iteration 30448 : loss : 0.063592, loss_ce: 0.000997, loss_dice: 0.126188
[12:43:30.203] TRAIN: iteration 30449 : loss : 0.076899, loss_ce: 0.001886, loss_dice: 0.151911
[12:43:30.412] TRAIN: iteration 30450 : loss : 0.047930, loss_ce: 0.010776, loss_dice: 0.085085
[12:43:30.979] TRAIN: iteration 30451 : loss : 0.043747, loss_ce: 0.004196, loss_dice: 0.083299
[12:43:31.188] TRAIN: iteration 30452 : loss : 0.058912, loss_ce: 0.007371, loss_dice: 0.110452
[12:43:31.395] TRAIN: iteration 30453 : loss : 0.098012, loss_ce: 0.005489, loss_dice: 0.190535
[12:43:31.607] TRAIN: iteration 30454 : loss : 0.040350, loss_ce: 0.003251, loss_dice: 0.077450
[12:43:31.816] TRAIN: iteration 30455 : loss : 0.250496, loss_ce: 0.000968, loss_dice: 0.500024
[12:43:32.023] TRAIN: iteration 30456 : loss : 0.048306, loss_ce: 0.001028, loss_dice: 0.095585
[12:43:32.233] TRAIN: iteration 30457 : loss : 0.250613, loss_ce: 0.001182, loss_dice: 0.500044
[12:43:32.473] TRAIN: iteration 30458 : loss : 0.103478, loss_ce: 0.001696, loss_dice: 0.205260
[12:43:33.281] TRAIN: iteration 30459 : loss : 0.048171, loss_ce: 0.003082, loss_dice: 0.093261
[12:43:33.487] TRAIN: iteration 30460 : loss : 0.251393, loss_ce: 0.002611, loss_dice: 0.500175
[12:43:33.734] TRAIN: iteration 30461 : loss : 0.250719, loss_ce: 0.001358, loss_dice: 0.500080
[12:43:34.040] TRAIN: iteration 30462 : loss : 0.055847, loss_ce: 0.002396, loss_dice: 0.109298
[12:43:34.271] TRAIN: iteration 30463 : loss : 0.063989, loss_ce: 0.004121, loss_dice: 0.123857
[12:43:34.479] TRAIN: iteration 30464 : loss : 0.215607, loss_ce: 0.001171, loss_dice: 0.430043
[12:43:34.693] TRAIN: iteration 30465 : loss : 0.035792, loss_ce: 0.000871, loss_dice: 0.070713
[12:43:34.902] TRAIN: iteration 30466 : loss : 0.166763, loss_ce: 0.003587, loss_dice: 0.329938
[12:43:35.110] TRAIN: iteration 30467 : loss : 0.092099, loss_ce: 0.004308, loss_dice: 0.179890
[12:43:35.317] TRAIN: iteration 30468 : loss : 0.058028, loss_ce: 0.001793, loss_dice: 0.114262
[12:43:35.531] TRAIN: iteration 30469 : loss : 0.065044, loss_ce: 0.007220, loss_dice: 0.122868
[12:43:36.140] TRAIN: iteration 30470 : loss : 0.250432, loss_ce: 0.000846, loss_dice: 0.500017
[12:43:36.356] TRAIN: iteration 30471 : loss : 0.074242, loss_ce: 0.001855, loss_dice: 0.146629
[12:43:36.563] TRAIN: iteration 30472 : loss : 0.236815, loss_ce: 0.001705, loss_dice: 0.471925
[12:43:36.771] TRAIN: iteration 30473 : loss : 0.250417, loss_ce: 0.000802, loss_dice: 0.500033
[12:43:36.979] TRAIN: iteration 30474 : loss : 0.098483, loss_ce: 0.003771, loss_dice: 0.193196
[12:43:37.919] TRAIN: iteration 30475 : loss : 0.032653, loss_ce: 0.001378, loss_dice: 0.063929
[12:43:38.126] TRAIN: iteration 30476 : loss : 0.027311, loss_ce: 0.002145, loss_dice: 0.052477
[12:43:38.334] TRAIN: iteration 30477 : loss : 0.250468, loss_ce: 0.000906, loss_dice: 0.500030
[12:43:38.543] TRAIN: iteration 30478 : loss : 0.250352, loss_ce: 0.000684, loss_dice: 0.500019
[12:43:38.751] TRAIN: iteration 30479 : loss : 0.072588, loss_ce: 0.005720, loss_dice: 0.139456
[12:43:38.958] TRAIN: iteration 30480 : loss : 0.081589, loss_ce: 0.000815, loss_dice: 0.162364
[12:43:39.193] TRAIN: iteration 30481 : loss : 0.250532, loss_ce: 0.001025, loss_dice: 0.500039
[12:43:39.402] TRAIN: iteration 30482 : loss : 0.050486, loss_ce: 0.003789, loss_dice: 0.097183
[12:43:39.611] TRAIN: iteration 30483 : loss : 0.099068, loss_ce: 0.001487, loss_dice: 0.196650
[12:43:40.421] TRAIN: iteration 30484 : loss : 0.020325, loss_ce: 0.002313, loss_dice: 0.038336
[12:43:40.628] TRAIN: iteration 30485 : loss : 0.250453, loss_ce: 0.000871, loss_dice: 0.500034
[12:43:40.836] TRAIN: iteration 30486 : loss : 0.085930, loss_ce: 0.000681, loss_dice: 0.171179
[12:43:41.043] TRAIN: iteration 30487 : loss : 0.029560, loss_ce: 0.000959, loss_dice: 0.058161
[12:43:42.154] TRAIN: iteration 30488 : loss : 0.124620, loss_ce: 0.003925, loss_dice: 0.245315
[12:43:42.368] TRAIN: iteration 30489 : loss : 0.078981, loss_ce: 0.001570, loss_dice: 0.156393
[12:43:43.386] TRAIN: iteration 30490 : loss : 0.190728, loss_ce: 0.001048, loss_dice: 0.380408
[12:43:43.599] TRAIN: iteration 30491 : loss : 0.052951, loss_ce: 0.005333, loss_dice: 0.100568
[12:43:43.806] TRAIN: iteration 30492 : loss : 0.250217, loss_ce: 0.000425, loss_dice: 0.500009
[12:43:44.016] TRAIN: iteration 30493 : loss : 0.074032, loss_ce: 0.004917, loss_dice: 0.143148
[12:43:44.224] TRAIN: iteration 30494 : loss : 0.154694, loss_ce: 0.001520, loss_dice: 0.307868
[12:43:44.433] TRAIN: iteration 30495 : loss : 0.092597, loss_ce: 0.001013, loss_dice: 0.184180
[12:43:44.798] TRAIN: iteration 30496 : loss : 0.080531, loss_ce: 0.001063, loss_dice: 0.159998
[12:43:45.005] TRAIN: iteration 30497 : loss : 0.075594, loss_ce: 0.005209, loss_dice: 0.145979
[12:43:45.346] TRAIN: iteration 30498 : loss : 0.073589, loss_ce: 0.002314, loss_dice: 0.144865
[12:43:45.553] TRAIN: iteration 30499 : loss : 0.038827, loss_ce: 0.002308, loss_dice: 0.075345
[12:43:45.762] TRAIN: iteration 30500 : loss : 0.134703, loss_ce: 0.008592, loss_dice: 0.260813
[12:43:46.000] TRAIN: iteration 30501 : loss : 0.071842, loss_ce: 0.008227, loss_dice: 0.135457
[12:43:46.217] TRAIN: iteration 30502 : loss : 0.215096, loss_ce: 0.000652, loss_dice: 0.429540
[12:43:46.425] TRAIN: iteration 30503 : loss : 0.067550, loss_ce: 0.009563, loss_dice: 0.125537
[12:43:46.633] TRAIN: iteration 30504 : loss : 0.131729, loss_ce: 0.002112, loss_dice: 0.261346
[12:43:46.842] TRAIN: iteration 30505 : loss : 0.029613, loss_ce: 0.002792, loss_dice: 0.056434
[12:43:49.575] TRAIN: iteration 30506 : loss : 0.199196, loss_ce: 0.000692, loss_dice: 0.397699
[12:43:49.786] TRAIN: iteration 30507 : loss : 0.232268, loss_ce: 0.002197, loss_dice: 0.462338
[12:43:49.996] TRAIN: iteration 30508 : loss : 0.073421, loss_ce: 0.004422, loss_dice: 0.142421
[12:43:50.207] TRAIN: iteration 30509 : loss : 0.250319, loss_ce: 0.000626, loss_dice: 0.500012
[12:43:50.414] TRAIN: iteration 30510 : loss : 0.015934, loss_ce: 0.002720, loss_dice: 0.029148
[12:43:50.626] TRAIN: iteration 30511 : loss : 0.037989, loss_ce: 0.002815, loss_dice: 0.073164
[12:43:50.834] TRAIN: iteration 30512 : loss : 0.026993, loss_ce: 0.003229, loss_dice: 0.050758
[12:43:51.041] TRAIN: iteration 30513 : loss : 0.076744, loss_ce: 0.001250, loss_dice: 0.152238
[12:43:51.267] TRAIN: iteration 30514 : loss : 0.254416, loss_ce: 0.009171, loss_dice: 0.499662
[12:43:51.475] TRAIN: iteration 30515 : loss : 0.033571, loss_ce: 0.005125, loss_dice: 0.062017
[12:43:51.683] TRAIN: iteration 30516 : loss : 0.084359, loss_ce: 0.003221, loss_dice: 0.165498
[12:43:51.892] TRAIN: iteration 30517 : loss : 0.079821, loss_ce: 0.005628, loss_dice: 0.154014
[12:43:52.110] TRAIN: iteration 30518 : loss : 0.056994, loss_ce: 0.006950, loss_dice: 0.107038
[12:43:52.318] TRAIN: iteration 30519 : loss : 0.078400, loss_ce: 0.001235, loss_dice: 0.155564
[12:43:52.526] TRAIN: iteration 30520 : loss : 0.250538, loss_ce: 0.001036, loss_dice: 0.500039
[12:43:52.765] TRAIN: iteration 30521 : loss : 0.102508, loss_ce: 0.004587, loss_dice: 0.200429
[12:43:52.973] TRAIN: iteration 30522 : loss : 0.164685, loss_ce: 0.004177, loss_dice: 0.325193
[12:43:53.181] TRAIN: iteration 30523 : loss : 0.093622, loss_ce: 0.001797, loss_dice: 0.185448
[12:43:53.388] TRAIN: iteration 30524 : loss : 0.119409, loss_ce: 0.005865, loss_dice: 0.232953
[12:43:53.595] TRAIN: iteration 30525 : loss : 0.250172, loss_ce: 0.000342, loss_dice: 0.500002
[12:43:53.807] TRAIN: iteration 30526 : loss : 0.050180, loss_ce: 0.004625, loss_dice: 0.095735
[12:43:54.014] TRAIN: iteration 30527 : loss : 0.075245, loss_ce: 0.001916, loss_dice: 0.148574
[12:43:54.222] TRAIN: iteration 30528 : loss : 0.209402, loss_ce: 0.002077, loss_dice: 0.416728
[12:43:54.429] TRAIN: iteration 30529 : loss : 0.084701, loss_ce: 0.011014, loss_dice: 0.158389
[12:43:54.731] TRAIN: iteration 30530 : loss : 0.056568, loss_ce: 0.002931, loss_dice: 0.110206
[12:43:54.940] TRAIN: iteration 30531 : loss : 0.054757, loss_ce: 0.003248, loss_dice: 0.106266
[12:43:55.148] TRAIN: iteration 30532 : loss : 0.041440, loss_ce: 0.005229, loss_dice: 0.077650
[12:43:55.356] TRAIN: iteration 30533 : loss : 0.025553, loss_ce: 0.002292, loss_dice: 0.048814
[12:43:55.566] TRAIN: iteration 30534 : loss : 0.045101, loss_ce: 0.000716, loss_dice: 0.089486
[12:43:55.776] TRAIN: iteration 30535 : loss : 0.122110, loss_ce: 0.002498, loss_dice: 0.241723
[12:43:55.984] TRAIN: iteration 30536 : loss : 0.025860, loss_ce: 0.003819, loss_dice: 0.047902
[12:43:56.192] TRAIN: iteration 30537 : loss : 0.146123, loss_ce: 0.001559, loss_dice: 0.290686
[12:43:56.964] TRAIN: iteration 30538 : loss : 0.212621, loss_ce: 0.001168, loss_dice: 0.424074
[12:43:57.172] TRAIN: iteration 30539 : loss : 0.088912, loss_ce: 0.010272, loss_dice: 0.167552
[12:43:58.078] TRAIN: iteration 30540 : loss : 0.042842, loss_ce: 0.004345, loss_dice: 0.081338
[12:43:58.318] TRAIN: iteration 30541 : loss : 0.211060, loss_ce: 0.003078, loss_dice: 0.419041
[12:43:58.528] TRAIN: iteration 30542 : loss : 0.069598, loss_ce: 0.002622, loss_dice: 0.136574
[12:43:58.737] TRAIN: iteration 30543 : loss : 0.027813, loss_ce: 0.001397, loss_dice: 0.054229
[12:43:58.945] TRAIN: iteration 30544 : loss : 0.148894, loss_ce: 0.002754, loss_dice: 0.295034
[12:43:59.154] TRAIN: iteration 30545 : loss : 0.056693, loss_ce: 0.000608, loss_dice: 0.112779
[12:44:00.005] TRAIN: iteration 30546 : loss : 0.210991, loss_ce: 0.003498, loss_dice: 0.418485
[12:44:00.213] TRAIN: iteration 30547 : loss : 0.031215, loss_ce: 0.000653, loss_dice: 0.061778
[12:44:00.539] TRAIN: iteration 30548 : loss : 0.080756, loss_ce: 0.003264, loss_dice: 0.158249
[12:44:00.747] TRAIN: iteration 30549 : loss : 0.158866, loss_ce: 0.006690, loss_dice: 0.311042
[12:44:00.965] TRAIN: iteration 30550 : loss : 0.065041, loss_ce: 0.003532, loss_dice: 0.126551
[12:44:01.173] TRAIN: iteration 30551 : loss : 0.014519, loss_ce: 0.000367, loss_dice: 0.028670
[12:44:01.381] TRAIN: iteration 30552 : loss : 0.250380, loss_ce: 0.000725, loss_dice: 0.500034
[12:44:01.588] TRAIN: iteration 30553 : loss : 0.043765, loss_ce: 0.001821, loss_dice: 0.085710
[12:44:04.034] TRAIN: iteration 30554 : loss : 0.232968, loss_ce: 0.001067, loss_dice: 0.464868
[12:44:04.242] TRAIN: iteration 30555 : loss : 0.075155, loss_ce: 0.002273, loss_dice: 0.148037
[12:44:04.450] TRAIN: iteration 30556 : loss : 0.052187, loss_ce: 0.002508, loss_dice: 0.101867
[12:44:04.662] TRAIN: iteration 30557 : loss : 0.047949, loss_ce: 0.005086, loss_dice: 0.090813
[12:44:04.869] TRAIN: iteration 30558 : loss : 0.039596, loss_ce: 0.001802, loss_dice: 0.077390
[12:44:05.077] TRAIN: iteration 30559 : loss : 0.122302, loss_ce: 0.004343, loss_dice: 0.240261
[12:44:05.287] TRAIN: iteration 30560 : loss : 0.101211, loss_ce: 0.001861, loss_dice: 0.200560
[12:44:05.521] TRAIN: iteration 30561 : loss : 0.211923, loss_ce: 0.004008, loss_dice: 0.419838
[12:44:05.735] TRAIN: iteration 30562 : loss : 0.100285, loss_ce: 0.011367, loss_dice: 0.189204
[12:44:05.942] TRAIN: iteration 30563 : loss : 0.250764, loss_ce: 0.001447, loss_dice: 0.500081
[12:44:06.156] TRAIN: iteration 30564 : loss : 0.056303, loss_ce: 0.001786, loss_dice: 0.110819
[12:44:06.371] TRAIN: iteration 30565 : loss : 0.088016, loss_ce: 0.003569, loss_dice: 0.172464
[12:44:06.578] TRAIN: iteration 30566 : loss : 0.060864, loss_ce: 0.001305, loss_dice: 0.120424
[12:44:06.785] TRAIN: iteration 30567 : loss : 0.060315, loss_ce: 0.001758, loss_dice: 0.118872
[12:44:06.992] TRAIN: iteration 30568 : loss : 0.150992, loss_ce: 0.001252, loss_dice: 0.300732
[12:44:07.199] TRAIN: iteration 30569 : loss : 0.072667, loss_ce: 0.001738, loss_dice: 0.143596
[12:44:07.407] TRAIN: iteration 30570 : loss : 0.032528, loss_ce: 0.002566, loss_dice: 0.062490
[12:44:07.614] TRAIN: iteration 30571 : loss : 0.065824, loss_ce: 0.004436, loss_dice: 0.127213
[12:44:07.869] TRAIN: iteration 30572 : loss : 0.250776, loss_ce: 0.001470, loss_dice: 0.500081
[12:44:08.076] TRAIN: iteration 30573 : loss : 0.092985, loss_ce: 0.004580, loss_dice: 0.181390
[12:44:08.284] TRAIN: iteration 30574 : loss : 0.035649, loss_ce: 0.006429, loss_dice: 0.064868
[12:44:08.492] TRAIN: iteration 30575 : loss : 0.174541, loss_ce: 0.002457, loss_dice: 0.346625
[12:44:08.700] TRAIN: iteration 30576 : loss : 0.056796, loss_ce: 0.001053, loss_dice: 0.112538
[12:44:08.915] TRAIN: iteration 30577 : loss : 0.063301, loss_ce: 0.010808, loss_dice: 0.115795
[12:44:09.142] TRAIN: iteration 30578 : loss : 0.250913, loss_ce: 0.001707, loss_dice: 0.500120
[12:44:09.352] TRAIN: iteration 30579 : loss : 0.037336, loss_ce: 0.001182, loss_dice: 0.073490
[12:44:09.559] TRAIN: iteration 30580 : loss : 0.140602, loss_ce: 0.001600, loss_dice: 0.279603
[12:44:09.806] TRAIN: iteration 30581 : loss : 0.086576, loss_ce: 0.003880, loss_dice: 0.169272
[12:44:10.014] TRAIN: iteration 30582 : loss : 0.120055, loss_ce: 0.001690, loss_dice: 0.238421
[12:44:10.250] TRAIN: iteration 30583 : loss : 0.060662, loss_ce: 0.009162, loss_dice: 0.112161
[12:44:10.796] TRAIN: iteration 30584 : loss : 0.189896, loss_ce: 0.002034, loss_dice: 0.377758
[12:44:11.008] TRAIN: iteration 30585 : loss : 0.033977, loss_ce: 0.001411, loss_dice: 0.066542
[12:44:11.364] TRAIN: iteration 30586 : loss : 0.037010, loss_ce: 0.001576, loss_dice: 0.072445
[12:44:11.575] TRAIN: iteration 30587 : loss : 0.028809, loss_ce: 0.000892, loss_dice: 0.056725
[12:44:11.782] TRAIN: iteration 30588 : loss : 0.131427, loss_ce: 0.000933, loss_dice: 0.261921
[12:44:11.993] TRAIN: iteration 30589 : loss : 0.126216, loss_ce: 0.002319, loss_dice: 0.250113
[12:44:12.205] TRAIN: iteration 30590 : loss : 0.098554, loss_ce: 0.004207, loss_dice: 0.192901
[12:44:12.413] TRAIN: iteration 30591 : loss : 0.250254, loss_ce: 0.002955, loss_dice: 0.497553
[12:44:14.801] TRAIN: iteration 30592 : loss : 0.077968, loss_ce: 0.000648, loss_dice: 0.155289
[12:44:15.014] TRAIN: iteration 30593 : loss : 0.180954, loss_ce: 0.001405, loss_dice: 0.360503
[12:44:15.227] TRAIN: iteration 30594 : loss : 0.110982, loss_ce: 0.004275, loss_dice: 0.217688
[12:44:15.436] TRAIN: iteration 30595 : loss : 0.121829, loss_ce: 0.002607, loss_dice: 0.241050
[12:44:15.643] TRAIN: iteration 30596 : loss : 0.068913, loss_ce: 0.003426, loss_dice: 0.134399
[12:44:15.852] TRAIN: iteration 30597 : loss : 0.085931, loss_ce: 0.003262, loss_dice: 0.168601
[12:44:16.060] TRAIN: iteration 30598 : loss : 0.251004, loss_ce: 0.001890, loss_dice: 0.500118
[12:44:16.270] TRAIN: iteration 30599 : loss : 0.071699, loss_ce: 0.004660, loss_dice: 0.138739
[12:44:16.695] TRAIN: iteration 30600 : loss : 0.070400, loss_ce: 0.000715, loss_dice: 0.140084
[12:44:16.931] TRAIN: iteration 30601 : loss : 0.028740, loss_ce: 0.005328, loss_dice: 0.052152
[12:44:17.139] TRAIN: iteration 30602 : loss : 0.250737, loss_ce: 0.001387, loss_dice: 0.500087
[12:44:17.346] TRAIN: iteration 30603 : loss : 0.078177, loss_ce: 0.010716, loss_dice: 0.145637
[12:44:17.555] TRAIN: iteration 30604 : loss : 0.087627, loss_ce: 0.004733, loss_dice: 0.170521
[12:44:17.764] TRAIN: iteration 30605 : loss : 0.250562, loss_ce: 0.001066, loss_dice: 0.500058
[12:44:18.002] TRAIN: iteration 30606 : loss : 0.250321, loss_ce: 0.000621, loss_dice: 0.500021
[12:44:18.210] TRAIN: iteration 30607 : loss : 0.045762, loss_ce: 0.003482, loss_dice: 0.088042
[12:44:19.533] TRAIN: iteration 30608 : loss : 0.071530, loss_ce: 0.002952, loss_dice: 0.140108
[12:44:19.741] TRAIN: iteration 30609 : loss : 0.101353, loss_ce: 0.001026, loss_dice: 0.201680
[12:44:19.949] TRAIN: iteration 30610 : loss : 0.250356, loss_ce: 0.000690, loss_dice: 0.500023
[12:44:20.164] TRAIN: iteration 30611 : loss : 0.037543, loss_ce: 0.001527, loss_dice: 0.073559
[12:44:21.143] TRAIN: iteration 30612 : loss : 0.085446, loss_ce: 0.004067, loss_dice: 0.166826
[12:44:21.350] TRAIN: iteration 30613 : loss : 0.061794, loss_ce: 0.002646, loss_dice: 0.120943
[12:44:21.559] TRAIN: iteration 30614 : loss : 0.064382, loss_ce: 0.002966, loss_dice: 0.125797
[12:44:21.767] TRAIN: iteration 30615 : loss : 0.048647, loss_ce: 0.004123, loss_dice: 0.093171
[12:44:22.394] TRAIN: iteration 30616 : loss : 0.118575, loss_ce: 0.004018, loss_dice: 0.233132
[12:44:22.602] TRAIN: iteration 30617 : loss : 0.171117, loss_ce: 0.000698, loss_dice: 0.341535
[12:44:22.827] TRAIN: iteration 30618 : loss : 0.099305, loss_ce: 0.001935, loss_dice: 0.196675
[12:44:23.037] TRAIN: iteration 30619 : loss : 0.121031, loss_ce: 0.001674, loss_dice: 0.240388
[12:44:23.246] TRAIN: iteration 30620 : loss : 0.073544, loss_ce: 0.002214, loss_dice: 0.144875
[12:44:23.494] TRAIN: iteration 30621 : loss : 0.062640, loss_ce: 0.004166, loss_dice: 0.121114
[12:44:23.703] TRAIN: iteration 30622 : loss : 0.250110, loss_ce: 0.000221, loss_dice: 0.499999
[12:44:23.925] TRAIN: iteration 30623 : loss : 0.164515, loss_ce: 0.008268, loss_dice: 0.320762
[12:44:24.132] TRAIN: iteration 30624 : loss : 0.050228, loss_ce: 0.005809, loss_dice: 0.094647
[12:44:24.340] TRAIN: iteration 30625 : loss : 0.040666, loss_ce: 0.000882, loss_dice: 0.080450
[12:44:24.548] TRAIN: iteration 30626 : loss : 0.070292, loss_ce: 0.002754, loss_dice: 0.137829
[12:44:24.758] TRAIN: iteration 30627 : loss : 0.200090, loss_ce: 0.007945, loss_dice: 0.392234
[12:44:24.966] TRAIN: iteration 30628 : loss : 0.250495, loss_ce: 0.000941, loss_dice: 0.500050
[12:44:28.998] TRAIN: iteration 30629 : loss : 0.098040, loss_ce: 0.000872, loss_dice: 0.195208
[12:44:29.205] TRAIN: iteration 30630 : loss : 0.052385, loss_ce: 0.001678, loss_dice: 0.103092
[12:44:29.411] TRAIN: iteration 30631 : loss : 0.110805, loss_ce: 0.007025, loss_dice: 0.214586
[12:44:29.618] TRAIN: iteration 30632 : loss : 0.044527, loss_ce: 0.000864, loss_dice: 0.088190
[12:44:29.825] TRAIN: iteration 30633 : loss : 0.080900, loss_ce: 0.001328, loss_dice: 0.160473
[12:44:30.033] TRAIN: iteration 30634 : loss : 0.048479, loss_ce: 0.006009, loss_dice: 0.090948
[12:44:30.241] TRAIN: iteration 30635 : loss : 0.060942, loss_ce: 0.001817, loss_dice: 0.120067
[12:44:30.452] TRAIN: iteration 30636 : loss : 0.057392, loss_ce: 0.003146, loss_dice: 0.111638
[12:44:30.659] TRAIN: iteration 30637 : loss : 0.148048, loss_ce: 0.001658, loss_dice: 0.294438
[12:44:30.880] TRAIN: iteration 30638 : loss : 0.042342, loss_ce: 0.001133, loss_dice: 0.083551
[12:44:31.095] TRAIN: iteration 30639 : loss : 0.250305, loss_ce: 0.000598, loss_dice: 0.500011
[12:44:34.696] TRAIN: iteration 30640 : loss : 0.170487, loss_ce: 0.006408, loss_dice: 0.334566
[12:44:34.938] TRAIN: iteration 30641 : loss : 0.074982, loss_ce: 0.007547, loss_dice: 0.142417
[12:44:35.147] TRAIN: iteration 30642 : loss : 0.189877, loss_ce: 0.002390, loss_dice: 0.377364
[12:44:35.355] TRAIN: iteration 30643 : loss : 0.057038, loss_ce: 0.003982, loss_dice: 0.110094
[12:44:35.565] TRAIN: iteration 30644 : loss : 0.250617, loss_ce: 0.001164, loss_dice: 0.500069
[12:44:35.780] TRAIN: iteration 30645 : loss : 0.041157, loss_ce: 0.003702, loss_dice: 0.078612
[12:44:35.994] TRAIN: iteration 30646 : loss : 0.054276, loss_ce: 0.000848, loss_dice: 0.107703
[12:44:36.205] TRAIN: iteration 30647 : loss : 0.034045, loss_ce: 0.000497, loss_dice: 0.067592
[12:44:37.776] TRAIN: iteration 30648 : loss : 0.244296, loss_ce: 0.002762, loss_dice: 0.485831
[12:44:37.984] TRAIN: iteration 30649 : loss : 0.084975, loss_ce: 0.001653, loss_dice: 0.168298
[12:44:38.192] TRAIN: iteration 30650 : loss : 0.054405, loss_ce: 0.002170, loss_dice: 0.106640
[12:44:38.400] TRAIN: iteration 30651 : loss : 0.076884, loss_ce: 0.008048, loss_dice: 0.145719
[12:44:38.608] TRAIN: iteration 30652 : loss : 0.250835, loss_ce: 0.001774, loss_dice: 0.499897
[12:44:38.817] TRAIN: iteration 30653 : loss : 0.062770, loss_ce: 0.002274, loss_dice: 0.123266
[12:44:39.032] TRAIN: iteration 30654 : loss : 0.195705, loss_ce: 0.002356, loss_dice: 0.389054
[12:44:39.242] TRAIN: iteration 30655 : loss : 0.053143, loss_ce: 0.003809, loss_dice: 0.102477
[12:44:40.396] TRAIN: iteration 30656 : loss : 0.078407, loss_ce: 0.003563, loss_dice: 0.153250
[12:44:40.723] TRAIN: iteration 30657 : loss : 0.242387, loss_ce: 0.003046, loss_dice: 0.481729
[12:44:40.932] TRAIN: iteration 30658 : loss : 0.250467, loss_ce: 0.000898, loss_dice: 0.500036
[12:44:41.141] TRAIN: iteration 30659 : loss : 0.061076, loss_ce: 0.002664, loss_dice: 0.119488
[12:44:41.348] TRAIN: iteration 30660 : loss : 0.250648, loss_ce: 0.001239, loss_dice: 0.500057
[12:44:41.593] TRAIN: iteration 30661 : loss : 0.243639, loss_ce: 0.002609, loss_dice: 0.484669
[12:44:41.800] TRAIN: iteration 30662 : loss : 0.111680, loss_ce: 0.005006, loss_dice: 0.218354
[12:44:42.007] TRAIN: iteration 30663 : loss : 0.250634, loss_ce: 0.001190, loss_dice: 0.500077
[12:44:42.214] TRAIN: iteration 30664 : loss : 0.098831, loss_ce: 0.001062, loss_dice: 0.196600
[12:44:42.422] TRAIN: iteration 30665 : loss : 0.108498, loss_ce: 0.003902, loss_dice: 0.213094
[12:44:42.629] TRAIN: iteration 30666 : loss : 0.028188, loss_ce: 0.002260, loss_dice: 0.054116
[12:44:42.836] TRAIN: iteration 30667 : loss : 0.051503, loss_ce: 0.001591, loss_dice: 0.101414
[12:44:43.046] TRAIN: iteration 30668 : loss : 0.219544, loss_ce: 0.002051, loss_dice: 0.437037
[12:44:43.254] TRAIN: iteration 30669 : loss : 0.031636, loss_ce: 0.002807, loss_dice: 0.060464
[12:44:43.463] TRAIN: iteration 30670 : loss : 0.250373, loss_ce: 0.000717, loss_dice: 0.500028
[12:44:43.670] TRAIN: iteration 30671 : loss : 0.209234, loss_ce: 0.002251, loss_dice: 0.416217
[12:44:43.878] TRAIN: iteration 30672 : loss : 0.120492, loss_ce: 0.010029, loss_dice: 0.230954
[12:44:44.086] TRAIN: iteration 30673 : loss : 0.070018, loss_ce: 0.004869, loss_dice: 0.135166
[12:44:44.293] TRAIN: iteration 30674 : loss : 0.056546, loss_ce: 0.004841, loss_dice: 0.108252
[12:44:44.500] TRAIN: iteration 30675 : loss : 0.068586, loss_ce: 0.002973, loss_dice: 0.134198
[12:44:44.707] TRAIN: iteration 30676 : loss : 0.046233, loss_ce: 0.005515, loss_dice: 0.086952
[12:44:45.102] TRAIN: iteration 30677 : loss : 0.118345, loss_ce: 0.009677, loss_dice: 0.227012
[12:44:45.310] TRAIN: iteration 30678 : loss : 0.183812, loss_ce: 0.002094, loss_dice: 0.365530
[12:44:45.517] TRAIN: iteration 30679 : loss : 0.031925, loss_ce: 0.003206, loss_dice: 0.060644
[12:44:45.874] TRAIN: iteration 30680 : loss : 0.041783, loss_ce: 0.002113, loss_dice: 0.081453
[12:44:46.791] TRAIN: iteration 30681 : loss : 0.250897, loss_ce: 0.001712, loss_dice: 0.500083
[12:44:46.999] TRAIN: iteration 30682 : loss : 0.038170, loss_ce: 0.002526, loss_dice: 0.073814
[12:44:47.207] TRAIN: iteration 30683 : loss : 0.072884, loss_ce: 0.003478, loss_dice: 0.142290
[12:44:48.615] TRAIN: iteration 30684 : loss : 0.107463, loss_ce: 0.002316, loss_dice: 0.212611
[12:44:48.826] TRAIN: iteration 30685 : loss : 0.112978, loss_ce: 0.001205, loss_dice: 0.224750
[12:44:49.034] TRAIN: iteration 30686 : loss : 0.099152, loss_ce: 0.003771, loss_dice: 0.194533
[12:44:49.246] TRAIN: iteration 30687 : loss : 0.179026, loss_ce: 0.003826, loss_dice: 0.354227
[12:44:49.468] TRAIN: iteration 30688 : loss : 0.043378, loss_ce: 0.006896, loss_dice: 0.079860
[12:44:49.674] TRAIN: iteration 30689 : loss : 0.134577, loss_ce: 0.004358, loss_dice: 0.264797
[12:44:49.883] TRAIN: iteration 30690 : loss : 0.065893, loss_ce: 0.002242, loss_dice: 0.129545
[12:44:50.093] TRAIN: iteration 30691 : loss : 0.162094, loss_ce: 0.002861, loss_dice: 0.321327
[12:44:51.280] TRAIN: iteration 30692 : loss : 0.106611, loss_ce: 0.010641, loss_dice: 0.202581
[12:44:51.488] TRAIN: iteration 30693 : loss : 0.171625, loss_ce: 0.001241, loss_dice: 0.342009
[12:44:51.702] TRAIN: iteration 30694 : loss : 0.045873, loss_ce: 0.002207, loss_dice: 0.089539
[12:44:51.911] TRAIN: iteration 30695 : loss : 0.014881, loss_ce: 0.000506, loss_dice: 0.029256
[12:44:52.125] TRAIN: iteration 30696 : loss : 0.090641, loss_ce: 0.002731, loss_dice: 0.178551
[12:44:52.333] TRAIN: iteration 30697 : loss : 0.058695, loss_ce: 0.003381, loss_dice: 0.114009
[12:44:52.541] TRAIN: iteration 30698 : loss : 0.065685, loss_ce: 0.002530, loss_dice: 0.128839
[12:44:52.748] TRAIN: iteration 30699 : loss : 0.052231, loss_ce: 0.004497, loss_dice: 0.099966
[12:44:52.959] TRAIN: iteration 30700 : loss : 0.017158, loss_ce: 0.000565, loss_dice: 0.033751
[12:44:53.980] TRAIN: iteration 30701 : loss : 0.147757, loss_ce: 0.002121, loss_dice: 0.293394
[12:44:54.188] TRAIN: iteration 30702 : loss : 0.036819, loss_ce: 0.000598, loss_dice: 0.073039
[12:44:54.845] TRAIN: iteration 30703 : loss : 0.045202, loss_ce: 0.010273, loss_dice: 0.080130
[12:44:55.063] TRAIN: iteration 30704 : loss : 0.080884, loss_ce: 0.003278, loss_dice: 0.158491
[12:44:56.607] TRAIN: iteration 30705 : loss : 0.039044, loss_ce: 0.000749, loss_dice: 0.077339
[12:44:56.813] TRAIN: iteration 30706 : loss : 0.049471, loss_ce: 0.004013, loss_dice: 0.094929
[12:44:57.021] TRAIN: iteration 30707 : loss : 0.075018, loss_ce: 0.005010, loss_dice: 0.145026
[12:44:57.232] TRAIN: iteration 30708 : loss : 0.078239, loss_ce: 0.007543, loss_dice: 0.148935
[12:44:58.599] TRAIN: iteration 30709 : loss : 0.040385, loss_ce: 0.001836, loss_dice: 0.078934
[12:44:58.806] TRAIN: iteration 30710 : loss : 0.250322, loss_ce: 0.000614, loss_dice: 0.500030
[12:44:59.014] TRAIN: iteration 30711 : loss : 0.076578, loss_ce: 0.000707, loss_dice: 0.152448
[12:44:59.224] TRAIN: iteration 30712 : loss : 0.127421, loss_ce: 0.004291, loss_dice: 0.250551
[12:44:59.433] TRAIN: iteration 30713 : loss : 0.030961, loss_ce: 0.000847, loss_dice: 0.061075
[12:44:59.642] TRAIN: iteration 30714 : loss : 0.128221, loss_ce: 0.003504, loss_dice: 0.252938
[12:44:59.852] TRAIN: iteration 30715 : loss : 0.043745, loss_ce: 0.002668, loss_dice: 0.084823
[12:45:00.061] TRAIN: iteration 30716 : loss : 0.024606, loss_ce: 0.001003, loss_dice: 0.048209
[12:45:00.319] TRAIN: iteration 30717 : loss : 0.055191, loss_ce: 0.002151, loss_dice: 0.108231
[12:45:01.275] TRAIN: iteration 30718 : loss : 0.023283, loss_ce: 0.003977, loss_dice: 0.042588
[12:45:01.727] TRAIN: iteration 30719 : loss : 0.236331, loss_ce: 0.003906, loss_dice: 0.468756
[12:45:01.934] TRAIN: iteration 30720 : loss : 0.155576, loss_ce: 0.005912, loss_dice: 0.305239
[12:45:02.404] TRAIN: iteration 30721 : loss : 0.051102, loss_ce: 0.006681, loss_dice: 0.095523
[12:45:02.612] TRAIN: iteration 30722 : loss : 0.155010, loss_ce: 0.003625, loss_dice: 0.306395
[12:45:02.823] TRAIN: iteration 30723 : loss : 0.079999, loss_ce: 0.003904, loss_dice: 0.156095
[12:45:03.030] TRAIN: iteration 30724 : loss : 0.025841, loss_ce: 0.002456, loss_dice: 0.049226
[12:45:03.237] TRAIN: iteration 30725 : loss : 0.093315, loss_ce: 0.007413, loss_dice: 0.179217
[12:45:03.445] TRAIN: iteration 30726 : loss : 0.049183, loss_ce: 0.002194, loss_dice: 0.096172
[12:45:05.325] TRAIN: iteration 30727 : loss : 0.033909, loss_ce: 0.002644, loss_dice: 0.065174
[12:45:05.532] TRAIN: iteration 30728 : loss : 0.250453, loss_ce: 0.000864, loss_dice: 0.500042
[12:45:06.206] TRAIN: iteration 30729 : loss : 0.185844, loss_ce: 0.001421, loss_dice: 0.370267
[12:45:06.413] TRAIN: iteration 30730 : loss : 0.162566, loss_ce: 0.001214, loss_dice: 0.323917
[12:45:06.621] TRAIN: iteration 30731 : loss : 0.060271, loss_ce: 0.002872, loss_dice: 0.117671
[12:45:06.829] TRAIN: iteration 30732 : loss : 0.101171, loss_ce: 0.004517, loss_dice: 0.197825
[12:45:07.039] TRAIN: iteration 30733 : loss : 0.157320, loss_ce: 0.002183, loss_dice: 0.312456
[12:45:07.247] TRAIN: iteration 30734 : loss : 0.051366, loss_ce: 0.004334, loss_dice: 0.098399
[12:45:09.816] TRAIN: iteration 30735 : loss : 0.023811, loss_ce: 0.001543, loss_dice: 0.046079
[12:45:10.023] TRAIN: iteration 30736 : loss : 0.058179, loss_ce: 0.001112, loss_dice: 0.115246
[12:45:10.230] TRAIN: iteration 30737 : loss : 0.109051, loss_ce: 0.002647, loss_dice: 0.215455
[12:45:10.438] TRAIN: iteration 30738 : loss : 0.194659, loss_ce: 0.019605, loss_dice: 0.369713
[12:45:10.647] TRAIN: iteration 30739 : loss : 0.035920, loss_ce: 0.000903, loss_dice: 0.070938
[12:45:10.862] TRAIN: iteration 30740 : loss : 0.029013, loss_ce: 0.001279, loss_dice: 0.056746
[12:45:11.104] TRAIN: iteration 30741 : loss : 0.068999, loss_ce: 0.002974, loss_dice: 0.135023
[12:45:11.313] TRAIN: iteration 30742 : loss : 0.089804, loss_ce: 0.002379, loss_dice: 0.177228
[12:45:11.521] TRAIN: iteration 30743 : loss : 0.146029, loss_ce: 0.006543, loss_dice: 0.285515
[12:45:11.729] TRAIN: iteration 30744 : loss : 0.115925, loss_ce: 0.005567, loss_dice: 0.226284
[12:45:11.959] TRAIN: iteration 30745 : loss : 0.021518, loss_ce: 0.001644, loss_dice: 0.041392
[12:45:12.167] TRAIN: iteration 30746 : loss : 0.096906, loss_ce: 0.001638, loss_dice: 0.192174
[12:45:12.375] TRAIN: iteration 30747 : loss : 0.250776, loss_ce: 0.001466, loss_dice: 0.500087
[12:45:12.585] TRAIN: iteration 30748 : loss : 0.113974, loss_ce: 0.004308, loss_dice: 0.223639
[12:45:12.794] TRAIN: iteration 30749 : loss : 0.228412, loss_ce: 0.001349, loss_dice: 0.455475
[12:45:13.006] TRAIN: iteration 30750 : loss : 0.251487, loss_ce: 0.002782, loss_dice: 0.500192
[12:45:15.617] TRAIN: iteration 30751 : loss : 0.081556, loss_ce: 0.000995, loss_dice: 0.162117
[12:45:15.824] TRAIN: iteration 30752 : loss : 0.149767, loss_ce: 0.006279, loss_dice: 0.293255
[12:45:16.032] TRAIN: iteration 30753 : loss : 0.081984, loss_ce: 0.003135, loss_dice: 0.160834
[12:45:16.239] TRAIN: iteration 30754 : loss : 0.225477, loss_ce: 0.003490, loss_dice: 0.447465
[12:45:16.450] TRAIN: iteration 30755 : loss : 0.038456, loss_ce: 0.000996, loss_dice: 0.075916
[12:45:16.657] TRAIN: iteration 30756 : loss : 0.039794, loss_ce: 0.007458, loss_dice: 0.072130
[12:45:16.872] TRAIN: iteration 30757 : loss : 0.169523, loss_ce: 0.002776, loss_dice: 0.336269
[12:45:17.079] TRAIN: iteration 30758 : loss : 0.073598, loss_ce: 0.002107, loss_dice: 0.145089
[12:45:18.468] TRAIN: iteration 30759 : loss : 0.198967, loss_ce: 0.001991, loss_dice: 0.395943
[12:45:19.399] TRAIN: iteration 30760 : loss : 0.067796, loss_ce: 0.002717, loss_dice: 0.132875
[12:45:19.643] TRAIN: iteration 30761 : loss : 0.254217, loss_ce: 0.007834, loss_dice: 0.500601
[12:45:19.850] TRAIN: iteration 30762 : loss : 0.037232, loss_ce: 0.002929, loss_dice: 0.071534
[12:45:20.058] TRAIN: iteration 30763 : loss : 0.043760, loss_ce: 0.004454, loss_dice: 0.083066
[12:45:20.265] TRAIN: iteration 30764 : loss : 0.110544, loss_ce: 0.002444, loss_dice: 0.218645
[12:45:20.475] TRAIN: iteration 30765 : loss : 0.023995, loss_ce: 0.002310, loss_dice: 0.045680
[12:45:20.686] TRAIN: iteration 30766 : loss : 0.030436, loss_ce: 0.004737, loss_dice: 0.056136
[12:45:21.040] TRAIN: iteration 30767 : loss : 0.071381, loss_ce: 0.001171, loss_dice: 0.141592
[12:45:21.253] TRAIN: iteration 30768 : loss : 0.075600, loss_ce: 0.001758, loss_dice: 0.149441
[12:45:21.461] TRAIN: iteration 30769 : loss : 0.136019, loss_ce: 0.006195, loss_dice: 0.265842
[12:45:21.670] TRAIN: iteration 30770 : loss : 0.042575, loss_ce: 0.005212, loss_dice: 0.079939
[12:45:21.879] TRAIN: iteration 30771 : loss : 0.044376, loss_ce: 0.005829, loss_dice: 0.082924
[12:45:22.099] TRAIN: iteration 30772 : loss : 0.092126, loss_ce: 0.003459, loss_dice: 0.180793
[12:45:22.306] TRAIN: iteration 30773 : loss : 0.105276, loss_ce: 0.000775, loss_dice: 0.209777
[12:45:22.513] TRAIN: iteration 30774 : loss : 0.235791, loss_ce: 0.001794, loss_dice: 0.469787
[12:45:22.982] TRAIN: iteration 30775 : loss : 0.035665, loss_ce: 0.001216, loss_dice: 0.070114
[12:45:23.261] TRAIN: iteration 30776 : loss : 0.231059, loss_ce: 0.005176, loss_dice: 0.456943
[12:45:23.468] TRAIN: iteration 30777 : loss : 0.038880, loss_ce: 0.004490, loss_dice: 0.073269
[12:45:23.676] TRAIN: iteration 30778 : loss : 0.126953, loss_ce: 0.006412, loss_dice: 0.247495
[12:45:23.892] TRAIN: iteration 30779 : loss : 0.035602, loss_ce: 0.001656, loss_dice: 0.069548
[12:45:24.099] TRAIN: iteration 30780 : loss : 0.082350, loss_ce: 0.005813, loss_dice: 0.158887
[12:45:25.712] TRAIN: iteration 30781 : loss : 0.092247, loss_ce: 0.002166, loss_dice: 0.182327
[12:45:25.919] TRAIN: iteration 30782 : loss : 0.109418, loss_ce: 0.005757, loss_dice: 0.213079
[12:45:28.104] TRAIN: iteration 30783 : loss : 0.047037, loss_ce: 0.001036, loss_dice: 0.093039
[12:45:28.315] TRAIN: iteration 30784 : loss : 0.058348, loss_ce: 0.008102, loss_dice: 0.108594
[12:45:28.523] TRAIN: iteration 30785 : loss : 0.021587, loss_ce: 0.001654, loss_dice: 0.041521
[12:45:28.730] TRAIN: iteration 30786 : loss : 0.061384, loss_ce: 0.012878, loss_dice: 0.109891
[12:45:28.937] TRAIN: iteration 30787 : loss : 0.038673, loss_ce: 0.002007, loss_dice: 0.075339
[12:45:29.144] TRAIN: iteration 30788 : loss : 0.192840, loss_ce: 0.001656, loss_dice: 0.384025
[12:45:29.356] TRAIN: iteration 30789 : loss : 0.163624, loss_ce: 0.004980, loss_dice: 0.322268
[12:45:29.565] TRAIN: iteration 30790 : loss : 0.072220, loss_ce: 0.004506, loss_dice: 0.139934
[12:45:31.169] TRAIN: iteration 30791 : loss : 0.081071, loss_ce: 0.002459, loss_dice: 0.159684
[12:45:31.379] TRAIN: iteration 30792 : loss : 0.250793, loss_ce: 0.001490, loss_dice: 0.500095
[12:45:31.588] TRAIN: iteration 30793 : loss : 0.076775, loss_ce: 0.003579, loss_dice: 0.149972
[12:45:31.901] TRAIN: iteration 30794 : loss : 0.020957, loss_ce: 0.004801, loss_dice: 0.037113
[12:45:32.108] TRAIN: iteration 30795 : loss : 0.036423, loss_ce: 0.002231, loss_dice: 0.070614
[12:45:32.315] TRAIN: iteration 30796 : loss : 0.243915, loss_ce: 0.002077, loss_dice: 0.485752
[12:45:32.524] TRAIN: iteration 30797 : loss : 0.074961, loss_ce: 0.002956, loss_dice: 0.146965
[12:45:32.731] TRAIN: iteration 30798 : loss : 0.044854, loss_ce: 0.001599, loss_dice: 0.088110
[12:45:32.939] TRAIN: iteration 30799 : loss : 0.067125, loss_ce: 0.001725, loss_dice: 0.132525
[12:45:37.649] TRAIN: iteration 30800 : loss : 0.038487, loss_ce: 0.004048, loss_dice: 0.072926
[12:45:37.889] TRAIN: iteration 30801 : loss : 0.049368, loss_ce: 0.002468, loss_dice: 0.096268
[12:45:38.099] TRAIN: iteration 30802 : loss : 0.250942, loss_ce: 0.001792, loss_dice: 0.500091
[12:45:38.309] TRAIN: iteration 30803 : loss : 0.041541, loss_ce: 0.010092, loss_dice: 0.072989
[12:45:38.517] TRAIN: iteration 30804 : loss : 0.250658, loss_ce: 0.001269, loss_dice: 0.500048
[12:45:38.727] TRAIN: iteration 30805 : loss : 0.101867, loss_ce: 0.020779, loss_dice: 0.182955
[12:45:38.936] TRAIN: iteration 30806 : loss : 0.061341, loss_ce: 0.001959, loss_dice: 0.120723
[12:45:39.144] TRAIN: iteration 30807 : loss : 0.119851, loss_ce: 0.005036, loss_dice: 0.234667
[12:45:41.035] TRAIN: iteration 30808 : loss : 0.128205, loss_ce: 0.003386, loss_dice: 0.253025
[12:45:41.249] TRAIN: iteration 30809 : loss : 0.117438, loss_ce: 0.004978, loss_dice: 0.229898
[12:45:41.458] TRAIN: iteration 30810 : loss : 0.070033, loss_ce: 0.005159, loss_dice: 0.134907
[12:45:41.665] TRAIN: iteration 30811 : loss : 0.052369, loss_ce: 0.001911, loss_dice: 0.102827
[12:45:41.874] TRAIN: iteration 30812 : loss : 0.078925, loss_ce: 0.001860, loss_dice: 0.155991
[12:45:42.087] TRAIN: iteration 30813 : loss : 0.250956, loss_ce: 0.001822, loss_dice: 0.500089
[12:45:42.294] TRAIN: iteration 30814 : loss : 0.250553, loss_ce: 0.001058, loss_dice: 0.500048
[12:45:42.501] TRAIN: iteration 30815 : loss : 0.077879, loss_ce: 0.001014, loss_dice: 0.154743
[12:45:45.894] TRAIN: iteration 30816 : loss : 0.090751, loss_ce: 0.001441, loss_dice: 0.180060
[12:45:46.101] TRAIN: iteration 30817 : loss : 0.129501, loss_ce: 0.002530, loss_dice: 0.256473
[12:45:46.308] TRAIN: iteration 30818 : loss : 0.059984, loss_ce: 0.004701, loss_dice: 0.115268
[12:45:46.515] TRAIN: iteration 30819 : loss : 0.250643, loss_ce: 0.001215, loss_dice: 0.500071
[12:45:46.723] TRAIN: iteration 30820 : loss : 0.024461, loss_ce: 0.001350, loss_dice: 0.047571
[12:45:46.961] TRAIN: iteration 30821 : loss : 0.022554, loss_ce: 0.001884, loss_dice: 0.043224
[12:45:47.169] TRAIN: iteration 30822 : loss : 0.062252, loss_ce: 0.001765, loss_dice: 0.122739
[12:45:47.377] TRAIN: iteration 30823 : loss : 0.023439, loss_ce: 0.001047, loss_dice: 0.045832
[12:45:50.464] TRAIN: iteration 30824 : loss : 0.128499, loss_ce: 0.003117, loss_dice: 0.253881
[12:45:50.672] TRAIN: iteration 30825 : loss : 0.080043, loss_ce: 0.004627, loss_dice: 0.155459
[12:45:50.881] TRAIN: iteration 30826 : loss : 0.045967, loss_ce: 0.000891, loss_dice: 0.091043
[12:45:51.090] TRAIN: iteration 30827 : loss : 0.248795, loss_ce: 0.002758, loss_dice: 0.494832
[12:45:51.298] TRAIN: iteration 30828 : loss : 0.049731, loss_ce: 0.005367, loss_dice: 0.094094
[12:45:51.506] TRAIN: iteration 30829 : loss : 0.094469, loss_ce: 0.001360, loss_dice: 0.187578
[12:45:51.714] TRAIN: iteration 30830 : loss : 0.032325, loss_ce: 0.003608, loss_dice: 0.061042
[12:45:51.922] TRAIN: iteration 30831 : loss : 0.119255, loss_ce: 0.002101, loss_dice: 0.236409
[12:45:52.334] TRAIN: iteration 30832 : loss : 0.039666, loss_ce: 0.002959, loss_dice: 0.076373
[12:45:52.543] TRAIN: iteration 30833 : loss : 0.250276, loss_ce: 0.000539, loss_dice: 0.500013
[12:45:52.805] TRAIN: iteration 30834 : loss : 0.146407, loss_ce: 0.003131, loss_dice: 0.289683
[12:45:53.013] TRAIN: iteration 30835 : loss : 0.248882, loss_ce: 0.005915, loss_dice: 0.491849
[12:45:53.476] TRAIN: iteration 30836 : loss : 0.122671, loss_ce: 0.003933, loss_dice: 0.241408
[12:45:53.684] TRAIN: iteration 30837 : loss : 0.083136, loss_ce: 0.002652, loss_dice: 0.163621
[12:45:53.891] TRAIN: iteration 30838 : loss : 0.250498, loss_ce: 0.000948, loss_dice: 0.500048
[12:45:54.101] TRAIN: iteration 30839 : loss : 0.196923, loss_ce: 0.023596, loss_dice: 0.370250
[12:45:55.397] TRAIN: iteration 30840 : loss : 0.078690, loss_ce: 0.002215, loss_dice: 0.155166
[12:45:55.630] TRAIN: iteration 30841 : loss : 0.239822, loss_ce: 0.002344, loss_dice: 0.477301
[12:45:55.837] TRAIN: iteration 30842 : loss : 0.250483, loss_ce: 0.000919, loss_dice: 0.500047
[12:45:56.045] TRAIN: iteration 30843 : loss : 0.100569, loss_ce: 0.002522, loss_dice: 0.198616
[12:45:56.261] TRAIN: iteration 30844 : loss : 0.060008, loss_ce: 0.001464, loss_dice: 0.118551
[12:45:56.468] TRAIN: iteration 30845 : loss : 0.250680, loss_ce: 0.001604, loss_dice: 0.499757
[12:45:56.677] TRAIN: iteration 30846 : loss : 0.115228, loss_ce: 0.001092, loss_dice: 0.229364
[12:45:56.884] TRAIN: iteration 30847 : loss : 0.134027, loss_ce: 0.005033, loss_dice: 0.263020
[12:46:00.313] TRAIN: iteration 30848 : loss : 0.239002, loss_ce: 0.002516, loss_dice: 0.475487
[12:46:00.522] TRAIN: iteration 30849 : loss : 0.023383, loss_ce: 0.001322, loss_dice: 0.045444
[12:46:00.731] TRAIN: iteration 30850 : loss : 0.032487, loss_ce: 0.002705, loss_dice: 0.062269
[12:46:00.938] TRAIN: iteration 30851 : loss : 0.069697, loss_ce: 0.005670, loss_dice: 0.133725
[12:46:01.146] TRAIN: iteration 30852 : loss : 0.043325, loss_ce: 0.002233, loss_dice: 0.084417
[12:46:01.353] TRAIN: iteration 30853 : loss : 0.050011, loss_ce: 0.002914, loss_dice: 0.097109
[12:46:01.562] TRAIN: iteration 30854 : loss : 0.046912, loss_ce: 0.004096, loss_dice: 0.089729
[12:46:01.776] TRAIN: iteration 30855 : loss : 0.092051, loss_ce: 0.003086, loss_dice: 0.181016
[12:46:01.983] TRAIN: iteration 30856 : loss : 0.051075, loss_ce: 0.003723, loss_dice: 0.098427
[12:46:02.191] TRAIN: iteration 30857 : loss : 0.041251, loss_ce: 0.001128, loss_dice: 0.081375
[12:46:02.399] TRAIN: iteration 30858 : loss : 0.073606, loss_ce: 0.001207, loss_dice: 0.146004
[12:46:02.877] TRAIN: iteration 30859 : loss : 0.049234, loss_ce: 0.002153, loss_dice: 0.096314
[12:46:03.090] TRAIN: iteration 30860 : loss : 0.023961, loss_ce: 0.005617, loss_dice: 0.042306
[12:46:03.379] TRAIN: iteration 30861 : loss : 0.042789, loss_ce: 0.002734, loss_dice: 0.082844
[12:46:03.585] TRAIN: iteration 30862 : loss : 0.042481, loss_ce: 0.001587, loss_dice: 0.083374
[12:46:03.792] TRAIN: iteration 30863 : loss : 0.112216, loss_ce: 0.000844, loss_dice: 0.223589
[12:46:06.239] TRAIN: iteration 30864 : loss : 0.250273, loss_ce: 0.000534, loss_dice: 0.500012
[12:46:06.599] TRAIN: iteration 30865 : loss : 0.250734, loss_ce: 0.001381, loss_dice: 0.500087
[12:46:06.806] TRAIN: iteration 30866 : loss : 0.062592, loss_ce: 0.004202, loss_dice: 0.120982
[12:46:07.239] TRAIN: iteration 30867 : loss : 0.123079, loss_ce: 0.002298, loss_dice: 0.243861
[12:46:07.447] TRAIN: iteration 30868 : loss : 0.108896, loss_ce: 0.001743, loss_dice: 0.216048
[12:46:07.657] TRAIN: iteration 30869 : loss : 0.029088, loss_ce: 0.001130, loss_dice: 0.057045
[12:46:07.864] TRAIN: iteration 30870 : loss : 0.025225, loss_ce: 0.002461, loss_dice: 0.047988
[12:46:08.102] TRAIN: iteration 30871 : loss : 0.025474, loss_ce: 0.000605, loss_dice: 0.050344
[12:46:12.371] TRAIN: iteration 30872 : loss : 0.250176, loss_ce: 0.000348, loss_dice: 0.500004
[12:46:12.585] TRAIN: iteration 30873 : loss : 0.157295, loss_ce: 0.015540, loss_dice: 0.299050
[12:46:12.798] TRAIN: iteration 30874 : loss : 0.237922, loss_ce: 0.001369, loss_dice: 0.474474
[12:46:13.546] TRAIN: iteration 30875 : loss : 0.111947, loss_ce: 0.003703, loss_dice: 0.220190
[12:46:13.754] TRAIN: iteration 30876 : loss : 0.036164, loss_ce: 0.001227, loss_dice: 0.071101
[12:46:13.963] TRAIN: iteration 30877 : loss : 0.249263, loss_ce: 0.000857, loss_dice: 0.497668
[12:46:14.171] TRAIN: iteration 30878 : loss : 0.249970, loss_ce: 0.000719, loss_dice: 0.499222
[12:46:14.380] TRAIN: iteration 30879 : loss : 0.013236, loss_ce: 0.000793, loss_dice: 0.025678
[12:46:15.726] TRAIN: iteration 30880 : loss : 0.094969, loss_ce: 0.002494, loss_dice: 0.187445
[12:46:15.942] TRAIN: iteration 30881 : loss : 0.249961, loss_ce: 0.000929, loss_dice: 0.498993
[12:46:16.151] TRAIN: iteration 30882 : loss : 0.067249, loss_ce: 0.002890, loss_dice: 0.131607
[12:46:16.359] TRAIN: iteration 30883 : loss : 0.055917, loss_ce: 0.001082, loss_dice: 0.110751
[12:46:16.567] TRAIN: iteration 30884 : loss : 0.115050, loss_ce: 0.004560, loss_dice: 0.225539
[12:46:16.775] TRAIN: iteration 30885 : loss : 0.113794, loss_ce: 0.004246, loss_dice: 0.223342
[12:46:17.583] TRAIN: iteration 30886 : loss : 0.087076, loss_ce: 0.002036, loss_dice: 0.172116
[12:46:17.792] TRAIN: iteration 30887 : loss : 0.056005, loss_ce: 0.003006, loss_dice: 0.109004
[12:46:18.001] TRAIN: iteration 30888 : loss : 0.091779, loss_ce: 0.003725, loss_dice: 0.179833
[12:46:18.208] TRAIN: iteration 30889 : loss : 0.023185, loss_ce: 0.000976, loss_dice: 0.045393
[12:46:18.915] TRAIN: iteration 30890 : loss : 0.250398, loss_ce: 0.000771, loss_dice: 0.500024
[12:46:19.123] TRAIN: iteration 30891 : loss : 0.080032, loss_ce: 0.006044, loss_dice: 0.154020
[12:46:19.362] TRAIN: iteration 30892 : loss : 0.101762, loss_ce: 0.008264, loss_dice: 0.195260
[12:46:19.574] TRAIN: iteration 30893 : loss : 0.103697, loss_ce: 0.003553, loss_dice: 0.203841
[12:46:21.342] TRAIN: iteration 30894 : loss : 0.018575, loss_ce: 0.001621, loss_dice: 0.035529
[12:46:21.555] TRAIN: iteration 30895 : loss : 0.210368, loss_ce: 0.000752, loss_dice: 0.419984
[12:46:22.232] TRAIN: iteration 30896 : loss : 0.069618, loss_ce: 0.005383, loss_dice: 0.133853
[12:46:22.439] TRAIN: iteration 30897 : loss : 0.034926, loss_ce: 0.001010, loss_dice: 0.068841
[12:46:22.646] TRAIN: iteration 30898 : loss : 0.020341, loss_ce: 0.000398, loss_dice: 0.040285
[12:46:22.854] TRAIN: iteration 30899 : loss : 0.236344, loss_ce: 0.002193, loss_dice: 0.470496
[12:46:23.911] TRAIN: iteration 30900 : loss : 0.130475, loss_ce: 0.001771, loss_dice: 0.259180
[12:46:24.160] TRAIN: iteration 30901 : loss : 0.030429, loss_ce: 0.004768, loss_dice: 0.056090
[12:46:27.914] TRAIN: iteration 30902 : loss : 0.053430, loss_ce: 0.004280, loss_dice: 0.102580
[12:46:28.123] TRAIN: iteration 30903 : loss : 0.047556, loss_ce: 0.001075, loss_dice: 0.094037
[12:46:28.333] TRAIN: iteration 30904 : loss : 0.204803, loss_ce: 0.007835, loss_dice: 0.401770
[12:46:28.666] TRAIN: iteration 30905 : loss : 0.038261, loss_ce: 0.003959, loss_dice: 0.072564
[12:46:28.876] TRAIN: iteration 30906 : loss : 0.111368, loss_ce: 0.001184, loss_dice: 0.221551
[12:46:29.086] TRAIN: iteration 30907 : loss : 0.033459, loss_ce: 0.003412, loss_dice: 0.063506
[12:46:29.414] TRAIN: iteration 30908 : loss : 0.052189, loss_ce: 0.002964, loss_dice: 0.101414
[12:46:29.622] TRAIN: iteration 30909 : loss : 0.171025, loss_ce: 0.004240, loss_dice: 0.337811
[12:46:34.325] TRAIN: iteration 30910 : loss : 0.045352, loss_ce: 0.001056, loss_dice: 0.089648
[12:46:34.540] TRAIN: iteration 30911 : loss : 0.060809, loss_ce: 0.002346, loss_dice: 0.119271
[12:46:34.746] TRAIN: iteration 30912 : loss : 0.022261, loss_ce: 0.000722, loss_dice: 0.043801
[12:46:34.952] TRAIN: iteration 30913 : loss : 0.076818, loss_ce: 0.005834, loss_dice: 0.147803
[12:46:35.159] TRAIN: iteration 30914 : loss : 0.250407, loss_ce: 0.000829, loss_dice: 0.499985
[12:46:35.367] TRAIN: iteration 30915 : loss : 0.251179, loss_ce: 0.002215, loss_dice: 0.500143
[12:46:35.599] TRAIN: iteration 30916 : loss : 0.105989, loss_ce: 0.007014, loss_dice: 0.204963
[12:46:35.806] TRAIN: iteration 30917 : loss : 0.250499, loss_ce: 0.000974, loss_dice: 0.500024
[12:46:39.434] TRAIN: iteration 30918 : loss : 0.175012, loss_ce: 0.003503, loss_dice: 0.346521
[12:46:39.642] TRAIN: iteration 30919 : loss : 0.040413, loss_ce: 0.002169, loss_dice: 0.078656
[12:46:39.851] TRAIN: iteration 30920 : loss : 0.076125, loss_ce: 0.000839, loss_dice: 0.151410
[12:46:40.087] TRAIN: iteration 30921 : loss : 0.250389, loss_ce: 0.000762, loss_dice: 0.500016
[12:46:40.295] TRAIN: iteration 30922 : loss : 0.240303, loss_ce: 0.000855, loss_dice: 0.479751
[12:46:40.503] TRAIN: iteration 30923 : loss : 0.022823, loss_ce: 0.000476, loss_dice: 0.045170
[12:46:40.712] TRAIN: iteration 30924 : loss : 0.121097, loss_ce: 0.008000, loss_dice: 0.234194
[12:46:40.920] TRAIN: iteration 30925 : loss : 0.054497, loss_ce: 0.007695, loss_dice: 0.101298
[12:46:44.797] TRAIN: iteration 30926 : loss : 0.221712, loss_ce: 0.002333, loss_dice: 0.441090
[12:46:45.003] TRAIN: iteration 30927 : loss : 0.108273, loss_ce: 0.008782, loss_dice: 0.207764
[12:46:45.210] TRAIN: iteration 30928 : loss : 0.087758, loss_ce: 0.005339, loss_dice: 0.170177
[12:46:45.417] TRAIN: iteration 30929 : loss : 0.250477, loss_ce: 0.006286, loss_dice: 0.494669
[12:46:45.624] TRAIN: iteration 30930 : loss : 0.110520, loss_ce: 0.000929, loss_dice: 0.220111
[12:46:45.833] TRAIN: iteration 30931 : loss : 0.086297, loss_ce: 0.003599, loss_dice: 0.168995
[12:46:46.040] TRAIN: iteration 30932 : loss : 0.119963, loss_ce: 0.001755, loss_dice: 0.238170
[12:46:46.249] TRAIN: iteration 30933 : loss : 0.099517, loss_ce: 0.004918, loss_dice: 0.194116
[12:46:51.430] TRAIN: iteration 30934 : loss : 0.103745, loss_ce: 0.003398, loss_dice: 0.204093
[12:46:51.637] TRAIN: iteration 30935 : loss : 0.034585, loss_ce: 0.003391, loss_dice: 0.065778
[12:46:51.850] TRAIN: iteration 30936 : loss : 0.046644, loss_ce: 0.003771, loss_dice: 0.089517
[12:46:52.060] TRAIN: iteration 30937 : loss : 0.115869, loss_ce: 0.004674, loss_dice: 0.227063
[12:46:52.271] TRAIN: iteration 30938 : loss : 0.251208, loss_ce: 0.002287, loss_dice: 0.500128
[12:46:52.481] TRAIN: iteration 30939 : loss : 0.250405, loss_ce: 0.000793, loss_dice: 0.500016
[12:46:52.690] TRAIN: iteration 30940 : loss : 0.214739, loss_ce: 0.001028, loss_dice: 0.428449
[12:46:52.928] TRAIN: iteration 30941 : loss : 0.031368, loss_ce: 0.000700, loss_dice: 0.062035
[12:46:54.244] TRAIN: iteration 30942 : loss : 0.095526, loss_ce: 0.007562, loss_dice: 0.183489
[12:46:54.451] TRAIN: iteration 30943 : loss : 0.029739, loss_ce: 0.002824, loss_dice: 0.056654
[12:46:54.660] TRAIN: iteration 30944 : loss : 0.250593, loss_ce: 0.001138, loss_dice: 0.500048
[12:46:54.867] TRAIN: iteration 30945 : loss : 0.046498, loss_ce: 0.001701, loss_dice: 0.091294
[12:46:55.075] TRAIN: iteration 30946 : loss : 0.247572, loss_ce: 0.001880, loss_dice: 0.493264
[12:46:55.287] TRAIN: iteration 30947 : loss : 0.068866, loss_ce: 0.002516, loss_dice: 0.135216
[12:46:55.497] TRAIN: iteration 30948 : loss : 0.052624, loss_ce: 0.002502, loss_dice: 0.102746
[12:46:55.708] TRAIN: iteration 30949 : loss : 0.105937, loss_ce: 0.005418, loss_dice: 0.206455
[12:46:58.406] TRAIN: iteration 30950 : loss : 0.080519, loss_ce: 0.001103, loss_dice: 0.159935
[12:46:58.619] TRAIN: iteration 30951 : loss : 0.087680, loss_ce: 0.006206, loss_dice: 0.169153
[12:46:58.827] TRAIN: iteration 30952 : loss : 0.150471, loss_ce: 0.007736, loss_dice: 0.293205
[12:46:59.268] TRAIN: iteration 30953 : loss : 0.028281, loss_ce: 0.003866, loss_dice: 0.052696
[12:46:59.475] TRAIN: iteration 30954 : loss : 0.214141, loss_ce: 0.002143, loss_dice: 0.426139
[12:46:59.689] TRAIN: iteration 30955 : loss : 0.041065, loss_ce: 0.001909, loss_dice: 0.080220
[12:46:59.895] TRAIN: iteration 30956 : loss : 0.090300, loss_ce: 0.002415, loss_dice: 0.178186
[12:47:00.103] TRAIN: iteration 30957 : loss : 0.091425, loss_ce: 0.006309, loss_dice: 0.176541
[12:47:02.399] TRAIN: iteration 30958 : loss : 0.063341, loss_ce: 0.001614, loss_dice: 0.125068
[12:47:02.608] TRAIN: iteration 30959 : loss : 0.194288, loss_ce: 0.001889, loss_dice: 0.386687
[12:47:06.886] TRAIN: iteration 30960 : loss : 0.062036, loss_ce: 0.004640, loss_dice: 0.119432
[12:47:07.122] TRAIN: iteration 30961 : loss : 0.238379, loss_ce: 0.000681, loss_dice: 0.476077
[12:47:07.332] TRAIN: iteration 30962 : loss : 0.193127, loss_ce: 0.004885, loss_dice: 0.381368
[12:47:07.540] TRAIN: iteration 30963 : loss : 0.025968, loss_ce: 0.005475, loss_dice: 0.046461
[12:47:07.747] TRAIN: iteration 30964 : loss : 0.236639, loss_ce: 0.001677, loss_dice: 0.471602
[12:47:07.958] TRAIN: iteration 30965 : loss : 0.106452, loss_ce: 0.004605, loss_dice: 0.208299
[12:47:08.165] TRAIN: iteration 30966 : loss : 0.137355, loss_ce: 0.003137, loss_dice: 0.271574
[12:47:08.371] TRAIN: iteration 30967 : loss : 0.236947, loss_ce: 0.001505, loss_dice: 0.472389
[12:47:09.961] TRAIN: iteration 30968 : loss : 0.037733, loss_ce: 0.002570, loss_dice: 0.072896
[12:47:10.168] TRAIN: iteration 30969 : loss : 0.231299, loss_ce: 0.003101, loss_dice: 0.459497
[12:47:10.374] TRAIN: iteration 30970 : loss : 0.217756, loss_ce: 0.003557, loss_dice: 0.431955
[12:47:10.581] TRAIN: iteration 30971 : loss : 0.046787, loss_ce: 0.000758, loss_dice: 0.092815
[12:47:10.790] TRAIN: iteration 30972 : loss : 0.042891, loss_ce: 0.003328, loss_dice: 0.082455
[12:47:10.997] TRAIN: iteration 30973 : loss : 0.040252, loss_ce: 0.003135, loss_dice: 0.077370
[12:47:11.205] TRAIN: iteration 30974 : loss : 0.138248, loss_ce: 0.001637, loss_dice: 0.274858
[12:47:11.412] TRAIN: iteration 30975 : loss : 0.037767, loss_ce: 0.001733, loss_dice: 0.073800
[12:47:15.844] TRAIN: iteration 30976 : loss : 0.044067, loss_ce: 0.004317, loss_dice: 0.083816
[12:47:16.052] TRAIN: iteration 30977 : loss : 0.076165, loss_ce: 0.001516, loss_dice: 0.150813
[12:47:16.259] TRAIN: iteration 30978 : loss : 0.063266, loss_ce: 0.004255, loss_dice: 0.122276
[12:47:16.468] TRAIN: iteration 30979 : loss : 0.077853, loss_ce: 0.004233, loss_dice: 0.151472
[12:47:16.677] TRAIN: iteration 30980 : loss : 0.073927, loss_ce: 0.006016, loss_dice: 0.141839
[12:47:16.915] TRAIN: iteration 30981 : loss : 0.250536, loss_ce: 0.001042, loss_dice: 0.500030
[12:47:17.123] TRAIN: iteration 30982 : loss : 0.151300, loss_ce: 0.001806, loss_dice: 0.300793
[12:47:17.332] TRAIN: iteration 30983 : loss : 0.149385, loss_ce: 0.001100, loss_dice: 0.297670
[12:47:20.881] TRAIN: iteration 30984 : loss : 0.035046, loss_ce: 0.002497, loss_dice: 0.067596
[12:47:21.092] TRAIN: iteration 30985 : loss : 0.077352, loss_ce: 0.005312, loss_dice: 0.149392
[12:47:21.300] TRAIN: iteration 30986 : loss : 0.068135, loss_ce: 0.004133, loss_dice: 0.132137
[12:47:21.513] TRAIN: iteration 30987 : loss : 0.161702, loss_ce: 0.006938, loss_dice: 0.316466
[12:47:21.721] TRAIN: iteration 30988 : loss : 0.077348, loss_ce: 0.001109, loss_dice: 0.153586
[12:47:21.929] TRAIN: iteration 30989 : loss : 0.041945, loss_ce: 0.003810, loss_dice: 0.080081
[12:47:25.585] TRAIN: iteration 30990 : loss : 0.066144, loss_ce: 0.002142, loss_dice: 0.130146
[12:47:25.798] TRAIN: iteration 30991 : loss : 0.022728, loss_ce: 0.001096, loss_dice: 0.044360
[12:47:26.318] TRAIN: iteration 30992 : loss : 0.202939, loss_ce: 0.002440, loss_dice: 0.403439
[12:47:26.524] TRAIN: iteration 30993 : loss : 0.053099, loss_ce: 0.001190, loss_dice: 0.105008
[12:47:26.732] TRAIN: iteration 30994 : loss : 0.046169, loss_ce: 0.000979, loss_dice: 0.091358
[12:47:26.942] TRAIN: iteration 30995 : loss : 0.154132, loss_ce: 0.003030, loss_dice: 0.305234
[12:47:27.151] TRAIN: iteration 30996 : loss : 0.249785, loss_ce: 0.011041, loss_dice: 0.488528
[12:47:27.361] TRAIN: iteration 30997 : loss : 0.103847, loss_ce: 0.000989, loss_dice: 0.206705
[12:47:33.664] TRAIN: iteration 30998 : loss : 0.250715, loss_ce: 0.001356, loss_dice: 0.500074
[12:47:33.871] TRAIN: iteration 30999 : loss : 0.036436, loss_ce: 0.003961, loss_dice: 0.068911
[12:47:34.079] TRAIN: iteration 31000 : loss : 0.049077, loss_ce: 0.002949, loss_dice: 0.095205
[12:47:34.319] TRAIN: iteration 31001 : loss : 0.062181, loss_ce: 0.010338, loss_dice: 0.114023
[12:47:34.527] TRAIN: iteration 31002 : loss : 0.116725, loss_ce: 0.001367, loss_dice: 0.232084
[12:47:34.735] TRAIN: iteration 31003 : loss : 0.074368, loss_ce: 0.010405, loss_dice: 0.138331
[12:47:34.944] TRAIN: iteration 31004 : loss : 0.031829, loss_ce: 0.001206, loss_dice: 0.062452
[12:47:35.157] TRAIN: iteration 31005 : loss : 0.043688, loss_ce: 0.002953, loss_dice: 0.084422
[12:47:39.738] TRAIN: iteration 31006 : loss : 0.198739, loss_ce: 0.000719, loss_dice: 0.396760
[12:47:39.945] TRAIN: iteration 31007 : loss : 0.111882, loss_ce: 0.001455, loss_dice: 0.222309
[12:47:40.157] TRAIN: iteration 31008 : loss : 0.113195, loss_ce: 0.001465, loss_dice: 0.224926
[12:47:40.366] TRAIN: iteration 31009 : loss : 0.168500, loss_ce: 0.002330, loss_dice: 0.334669
[12:47:40.575] TRAIN: iteration 31010 : loss : 0.250440, loss_ce: 0.000836, loss_dice: 0.500044
[12:47:40.782] TRAIN: iteration 31011 : loss : 0.072603, loss_ce: 0.006254, loss_dice: 0.138952
[12:47:40.990] TRAIN: iteration 31012 : loss : 0.027191, loss_ce: 0.002681, loss_dice: 0.051702
[12:47:41.197] TRAIN: iteration 31013 : loss : 0.067747, loss_ce: 0.004589, loss_dice: 0.130905
[12:47:46.599] TRAIN: iteration 31014 : loss : 0.068378, loss_ce: 0.002727, loss_dice: 0.134029
[12:47:46.807] TRAIN: iteration 31015 : loss : 0.032394, loss_ce: 0.001509, loss_dice: 0.063278
[12:47:47.068] TRAIN: iteration 31016 : loss : 0.205394, loss_ce: 0.001050, loss_dice: 0.409738
[12:47:47.283] TRAIN: iteration 31017 : loss : 0.041782, loss_ce: 0.005551, loss_dice: 0.078012
[12:47:47.490] TRAIN: iteration 31018 : loss : 0.056118, loss_ce: 0.000493, loss_dice: 0.111744
[12:47:47.697] TRAIN: iteration 31019 : loss : 0.030984, loss_ce: 0.003962, loss_dice: 0.058006
[12:47:47.907] TRAIN: iteration 31020 : loss : 0.106638, loss_ce: 0.001230, loss_dice: 0.212046
[12:47:48.147] TRAIN: iteration 31021 : loss : 0.162441, loss_ce: 0.001290, loss_dice: 0.323592
[12:47:50.610] TRAIN: iteration 31022 : loss : 0.051014, loss_ce: 0.001109, loss_dice: 0.100919
[12:47:50.819] TRAIN: iteration 31023 : loss : 0.196770, loss_ce: 0.000856, loss_dice: 0.392685
[12:47:53.642] TRAIN: iteration 31024 : loss : 0.099127, loss_ce: 0.006380, loss_dice: 0.191875
[12:47:53.849] TRAIN: iteration 31025 : loss : 0.077370, loss_ce: 0.005882, loss_dice: 0.148859
[12:47:54.058] TRAIN: iteration 31026 : loss : 0.207434, loss_ce: 0.003949, loss_dice: 0.410919
[12:47:54.266] TRAIN: iteration 31027 : loss : 0.047729, loss_ce: 0.001545, loss_dice: 0.093913
[12:47:54.473] TRAIN: iteration 31028 : loss : 0.113737, loss_ce: 0.003188, loss_dice: 0.224285
[12:47:54.681] TRAIN: iteration 31029 : loss : 0.049689, loss_ce: 0.002331, loss_dice: 0.097048
[12:47:56.095] TRAIN: iteration 31030 : loss : 0.081974, loss_ce: 0.002517, loss_dice: 0.161431
[12:47:56.303] TRAIN: iteration 31031 : loss : 0.130065, loss_ce: 0.001032, loss_dice: 0.259097
[12:48:02.211] TRAIN: iteration 31032 : loss : 0.022150, loss_ce: 0.002752, loss_dice: 0.041548
[12:48:02.420] TRAIN: iteration 31033 : loss : 0.250355, loss_ce: 0.000697, loss_dice: 0.500013
[12:48:02.628] TRAIN: iteration 31034 : loss : 0.250665, loss_ce: 0.001268, loss_dice: 0.500063
[12:48:02.840] TRAIN: iteration 31035 : loss : 0.205560, loss_ce: 0.004525, loss_dice: 0.406595
[12:48:03.048] TRAIN: iteration 31036 : loss : 0.148609, loss_ce: 0.002987, loss_dice: 0.294231
[12:48:03.257] TRAIN: iteration 31037 : loss : 0.031807, loss_ce: 0.001122, loss_dice: 0.062493
[12:48:03.471] TRAIN: iteration 31038 : loss : 0.084472, loss_ce: 0.004347, loss_dice: 0.164598
[12:48:03.680] TRAIN: iteration 31039 : loss : 0.250619, loss_ce: 0.001189, loss_dice: 0.500049
[12:48:07.047] TRAIN: iteration 31040 : loss : 0.082901, loss_ce: 0.004072, loss_dice: 0.161730
[12:48:07.284] TRAIN: iteration 31041 : loss : 0.215642, loss_ce: 0.002721, loss_dice: 0.428563
[12:48:07.494] TRAIN: iteration 31042 : loss : 0.095106, loss_ce: 0.006336, loss_dice: 0.183876
[12:48:07.701] TRAIN: iteration 31043 : loss : 0.115458, loss_ce: 0.005046, loss_dice: 0.225870
[12:48:07.908] TRAIN: iteration 31044 : loss : 0.037910, loss_ce: 0.005577, loss_dice: 0.070242
[12:48:08.116] TRAIN: iteration 31045 : loss : 0.250825, loss_ce: 0.001570, loss_dice: 0.500080
[12:48:10.179] TRAIN: iteration 31046 : loss : 0.021898, loss_ce: 0.003605, loss_dice: 0.040191
[12:48:10.385] TRAIN: iteration 31047 : loss : 0.059842, loss_ce: 0.003145, loss_dice: 0.116540
[12:48:13.168] TRAIN: iteration 31048 : loss : 0.095442, loss_ce: 0.003943, loss_dice: 0.186940
[12:48:13.509] TRAIN: iteration 31049 : loss : 0.024595, loss_ce: 0.001288, loss_dice: 0.047902
[12:48:13.716] TRAIN: iteration 31050 : loss : 0.029542, loss_ce: 0.003255, loss_dice: 0.055829
[12:48:13.923] TRAIN: iteration 31051 : loss : 0.023145, loss_ce: 0.001336, loss_dice: 0.044954
[12:48:14.130] TRAIN: iteration 31052 : loss : 0.079716, loss_ce: 0.001567, loss_dice: 0.157864
[12:48:14.337] TRAIN: iteration 31053 : loss : 0.044098, loss_ce: 0.001928, loss_dice: 0.086267
[12:48:15.466] TRAIN: iteration 31054 : loss : 0.014089, loss_ce: 0.001670, loss_dice: 0.026509
[12:48:17.791] TRAIN: iteration 31055 : loss : 0.054458, loss_ce: 0.001907, loss_dice: 0.107009
[12:48:18.936] TRAIN: iteration 31056 : loss : 0.122802, loss_ce: 0.008368, loss_dice: 0.237236
[12:48:19.144] TRAIN: iteration 31057 : loss : 0.213699, loss_ce: 0.015302, loss_dice: 0.412096
[12:48:19.351] TRAIN: iteration 31058 : loss : 0.033586, loss_ce: 0.003192, loss_dice: 0.063980
[12:48:19.561] TRAIN: iteration 31059 : loss : 0.215322, loss_ce: 0.004387, loss_dice: 0.426257
[12:48:21.587] TRAIN: iteration 31060 : loss : 0.028816, loss_ce: 0.001682, loss_dice: 0.055950
[12:48:21.825] TRAIN: iteration 31061 : loss : 0.042338, loss_ce: 0.002802, loss_dice: 0.081874
[12:48:22.136] TRAIN: iteration 31062 : loss : 0.250930, loss_ce: 0.001781, loss_dice: 0.500078
[12:48:23.659] TRAIN: iteration 31063 : loss : 0.238469, loss_ce: 0.000989, loss_dice: 0.475950
[12:48:27.726] TRAIN: iteration 31064 : loss : 0.070608, loss_ce: 0.005923, loss_dice: 0.135292
[12:48:27.940] TRAIN: iteration 31065 : loss : 0.020469, loss_ce: 0.000726, loss_dice: 0.040212
[12:48:28.150] TRAIN: iteration 31066 : loss : 0.040480, loss_ce: 0.001697, loss_dice: 0.079263
[12:48:28.358] TRAIN: iteration 31067 : loss : 0.122504, loss_ce: 0.002762, loss_dice: 0.242245
[12:48:30.277] TRAIN: iteration 31068 : loss : 0.039667, loss_ce: 0.008629, loss_dice: 0.070706
[12:48:30.490] TRAIN: iteration 31069 : loss : 0.070661, loss_ce: 0.002259, loss_dice: 0.139063
[12:48:30.698] TRAIN: iteration 31070 : loss : 0.109112, loss_ce: 0.002832, loss_dice: 0.215393
[12:48:30.906] TRAIN: iteration 31071 : loss : 0.029519, loss_ce: 0.001199, loss_dice: 0.057839
[12:48:32.455] TRAIN: iteration 31072 : loss : 0.019369, loss_ce: 0.001402, loss_dice: 0.037336
[12:48:32.661] TRAIN: iteration 31073 : loss : 0.044790, loss_ce: 0.002467, loss_dice: 0.087113
[12:48:32.868] TRAIN: iteration 31074 : loss : 0.045922, loss_ce: 0.002286, loss_dice: 0.089558
[12:48:33.107] TRAIN: iteration 31075 : loss : 0.023768, loss_ce: 0.002513, loss_dice: 0.045023
[12:48:37.750] TRAIN: iteration 31076 : loss : 0.085719, loss_ce: 0.002320, loss_dice: 0.169117
[12:48:37.957] TRAIN: iteration 31077 : loss : 0.048977, loss_ce: 0.004291, loss_dice: 0.093663
[12:48:38.165] TRAIN: iteration 31078 : loss : 0.250439, loss_ce: 0.000853, loss_dice: 0.500025
[12:48:38.373] TRAIN: iteration 31079 : loss : 0.038066, loss_ce: 0.001229, loss_dice: 0.074903
[12:48:40.531] TRAIN: iteration 31080 : loss : 0.100674, loss_ce: 0.003511, loss_dice: 0.197836
[12:48:40.764] TRAIN: iteration 31081 : loss : 0.104460, loss_ce: 0.006465, loss_dice: 0.202456
[12:48:40.971] TRAIN: iteration 31082 : loss : 0.047896, loss_ce: 0.009711, loss_dice: 0.086082
[12:48:41.181] TRAIN: iteration 31083 : loss : 0.205970, loss_ce: 0.008762, loss_dice: 0.403177
[12:48:44.505] TRAIN: iteration 31084 : loss : 0.146579, loss_ce: 0.001637, loss_dice: 0.291521
[12:48:44.711] TRAIN: iteration 31085 : loss : 0.065406, loss_ce: 0.006784, loss_dice: 0.124027
[12:48:44.920] TRAIN: iteration 31086 : loss : 0.222719, loss_ce: 0.002192, loss_dice: 0.443246
[12:48:45.127] TRAIN: iteration 31087 : loss : 0.075732, loss_ce: 0.002134, loss_dice: 0.149330
[12:48:46.377] TRAIN: iteration 31088 : loss : 0.037118, loss_ce: 0.001084, loss_dice: 0.073153
[12:48:46.585] TRAIN: iteration 31089 : loss : 0.050146, loss_ce: 0.003651, loss_dice: 0.096642
[12:48:46.793] TRAIN: iteration 31090 : loss : 0.083024, loss_ce: 0.001337, loss_dice: 0.164712
[12:48:47.001] TRAIN: iteration 31091 : loss : 0.043207, loss_ce: 0.001461, loss_dice: 0.084952
[12:48:51.420] TRAIN: iteration 31092 : loss : 0.054448, loss_ce: 0.001750, loss_dice: 0.107146
[12:48:51.631] TRAIN: iteration 31093 : loss : 0.046828, loss_ce: 0.015700, loss_dice: 0.077956
[12:48:51.839] TRAIN: iteration 31094 : loss : 0.056701, loss_ce: 0.006987, loss_dice: 0.106416
[12:48:52.050] TRAIN: iteration 31095 : loss : 0.250439, loss_ce: 0.015390, loss_dice: 0.485487
[12:48:55.324] TRAIN: iteration 31096 : loss : 0.063410, loss_ce: 0.004197, loss_dice: 0.122623
[12:48:55.534] TRAIN: iteration 31097 : loss : 0.069153, loss_ce: 0.002267, loss_dice: 0.136039
[12:48:55.741] TRAIN: iteration 31098 : loss : 0.052735, loss_ce: 0.001449, loss_dice: 0.104022
[12:48:55.951] TRAIN: iteration 31099 : loss : 0.121296, loss_ce: 0.001795, loss_dice: 0.240796
[12:48:59.650] TRAIN: iteration 31100 : loss : 0.052548, loss_ce: 0.002337, loss_dice: 0.102758
[12:48:59.888] TRAIN: iteration 31101 : loss : 0.035776, loss_ce: 0.005918, loss_dice: 0.065634
[12:49:00.103] TRAIN: iteration 31102 : loss : 0.084268, loss_ce: 0.002506, loss_dice: 0.166029
[12:49:00.311] TRAIN: iteration 31103 : loss : 0.053285, loss_ce: 0.002888, loss_dice: 0.103682
[12:49:00.946] TRAIN: iteration 31104 : loss : 0.140205, loss_ce: 0.006266, loss_dice: 0.274144
[12:49:01.154] TRAIN: iteration 31105 : loss : 0.073345, loss_ce: 0.002883, loss_dice: 0.143808
[12:49:01.362] TRAIN: iteration 31106 : loss : 0.065738, loss_ce: 0.006077, loss_dice: 0.125399
[12:49:01.570] TRAIN: iteration 31107 : loss : 0.027527, loss_ce: 0.000729, loss_dice: 0.054325
[12:49:05.232] TRAIN: iteration 31108 : loss : 0.093568, loss_ce: 0.002342, loss_dice: 0.184793
[12:49:05.439] TRAIN: iteration 31109 : loss : 0.043607, loss_ce: 0.000911, loss_dice: 0.086304
[12:49:05.647] TRAIN: iteration 31110 : loss : 0.055920, loss_ce: 0.007336, loss_dice: 0.104504
[12:49:06.613] TRAIN: iteration 31111 : loss : 0.092397, loss_ce: 0.002736, loss_dice: 0.182058
[12:49:09.129] TRAIN: iteration 31112 : loss : 0.200235, loss_ce: 0.002160, loss_dice: 0.398309
[12:49:09.336] TRAIN: iteration 31113 : loss : 0.199222, loss_ce: 0.007700, loss_dice: 0.390743
[12:49:09.543] TRAIN: iteration 31114 : loss : 0.137088, loss_ce: 0.003446, loss_dice: 0.270729
[12:49:09.750] TRAIN: iteration 31115 : loss : 0.050138, loss_ce: 0.001421, loss_dice: 0.098856
[12:49:10.355] TRAIN: iteration 31116 : loss : 0.026261, loss_ce: 0.000705, loss_dice: 0.051817
[12:49:10.569] TRAIN: iteration 31117 : loss : 0.221968, loss_ce: 0.001643, loss_dice: 0.442294
[12:49:10.777] TRAIN: iteration 31118 : loss : 0.250788, loss_ce: 0.001489, loss_dice: 0.500086
[12:49:13.709] TRAIN: iteration 31119 : loss : 0.076481, loss_ce: 0.001877, loss_dice: 0.151085
[12:49:15.957] TRAIN: iteration 31120 : loss : 0.045200, loss_ce: 0.002615, loss_dice: 0.087785
[12:49:16.185] TRAIN: iteration 31121 : loss : 0.251030, loss_ce: 0.001925, loss_dice: 0.500135
[12:49:16.394] TRAIN: iteration 31122 : loss : 0.154745, loss_ce: 0.002634, loss_dice: 0.306855
[12:49:16.602] TRAIN: iteration 31123 : loss : 0.250651, loss_ce: 0.001243, loss_dice: 0.500060
[12:49:19.106] TRAIN: iteration 31124 : loss : 0.060087, loss_ce: 0.002788, loss_dice: 0.117386
[12:49:19.313] TRAIN: iteration 31125 : loss : 0.062716, loss_ce: 0.002046, loss_dice: 0.123386
[12:49:19.521] TRAIN: iteration 31126 : loss : 0.037903, loss_ce: 0.004826, loss_dice: 0.070980
[12:49:22.303] TRAIN: iteration 31127 : loss : 0.084595, loss_ce: 0.002175, loss_dice: 0.167016
[12:49:24.425] TRAIN: iteration 31128 : loss : 0.109476, loss_ce: 0.000875, loss_dice: 0.218077
[12:49:24.632] TRAIN: iteration 31129 : loss : 0.149882, loss_ce: 0.001380, loss_dice: 0.298383
[12:49:24.841] TRAIN: iteration 31130 : loss : 0.022727, loss_ce: 0.005704, loss_dice: 0.039750
[12:49:25.049] TRAIN: iteration 31131 : loss : 0.051377, loss_ce: 0.004028, loss_dice: 0.098727
[12:49:25.570] TRAIN: iteration 31132 : loss : 0.061631, loss_ce: 0.002731, loss_dice: 0.120530
[12:49:25.777] TRAIN: iteration 31133 : loss : 0.031901, loss_ce: 0.000617, loss_dice: 0.063184
[12:49:25.986] TRAIN: iteration 31134 : loss : 0.250467, loss_ce: 0.000899, loss_dice: 0.500034
[12:49:29.177] TRAIN: iteration 31135 : loss : 0.044675, loss_ce: 0.000634, loss_dice: 0.088716
[12:49:33.057] TRAIN: iteration 31136 : loss : 0.250491, loss_ce: 0.000945, loss_dice: 0.500037
[12:49:33.266] TRAIN: iteration 31137 : loss : 0.173115, loss_ce: 0.001600, loss_dice: 0.344629
[12:49:33.473] TRAIN: iteration 31138 : loss : 0.250308, loss_ce: 0.000598, loss_dice: 0.500019
[12:49:33.681] TRAIN: iteration 31139 : loss : 0.077510, loss_ce: 0.002818, loss_dice: 0.152201
[12:49:34.279] TRAIN: iteration 31140 : loss : 0.068563, loss_ce: 0.002700, loss_dice: 0.134426
[12:49:34.516] TRAIN: iteration 31141 : loss : 0.036710, loss_ce: 0.001036, loss_dice: 0.072384
[12:49:34.723] TRAIN: iteration 31142 : loss : 0.071113, loss_ce: 0.000758, loss_dice: 0.141469
[12:49:36.541] TRAIN: iteration 31143 : loss : 0.138210, loss_ce: 0.006536, loss_dice: 0.269885
[12:49:39.175] TRAIN: iteration 31144 : loss : 0.179215, loss_ce: 0.004172, loss_dice: 0.354258
[12:49:39.389] TRAIN: iteration 31145 : loss : 0.210567, loss_ce: 0.007412, loss_dice: 0.413723
[12:49:39.595] TRAIN: iteration 31146 : loss : 0.019616, loss_ce: 0.002389, loss_dice: 0.036842
[12:49:39.803] TRAIN: iteration 31147 : loss : 0.250643, loss_ce: 0.001229, loss_dice: 0.500057
[12:49:42.234] TRAIN: iteration 31148 : loss : 0.040728, loss_ce: 0.002920, loss_dice: 0.078536
[12:49:42.442] TRAIN: iteration 31149 : loss : 0.249248, loss_ce: 0.003178, loss_dice: 0.495318
[12:49:42.700] TRAIN: iteration 31150 : loss : 0.069684, loss_ce: 0.001087, loss_dice: 0.138281
[12:49:44.565] TRAIN: iteration 31151 : loss : 0.216855, loss_ce: 0.001179, loss_dice: 0.432531
[12:49:49.990] TRAIN: iteration 31152 : loss : 0.035595, loss_ce: 0.002495, loss_dice: 0.068694
[12:49:50.199] TRAIN: iteration 31153 : loss : 0.067339, loss_ce: 0.002224, loss_dice: 0.132454
[12:49:50.411] TRAIN: iteration 31154 : loss : 0.250362, loss_ce: 0.000699, loss_dice: 0.500024
[12:49:50.619] TRAIN: iteration 31155 : loss : 0.226184, loss_ce: 0.006891, loss_dice: 0.445478
[12:49:51.912] TRAIN: iteration 31156 : loss : 0.052383, loss_ce: 0.004578, loss_dice: 0.100188
[12:49:52.121] TRAIN: iteration 31157 : loss : 0.255449, loss_ce: 0.011367, loss_dice: 0.499531
[12:49:52.330] TRAIN: iteration 31158 : loss : 0.251041, loss_ce: 0.001941, loss_dice: 0.500142
[12:49:53.624] TRAIN: iteration 31159 : loss : 0.049424, loss_ce: 0.003168, loss_dice: 0.095680
[12:49:58.778] TRAIN: iteration 31160 : loss : 0.035191, loss_ce: 0.001084, loss_dice: 0.069298
[12:49:59.011] TRAIN: iteration 31161 : loss : 0.102950, loss_ce: 0.001445, loss_dice: 0.204456
[12:49:59.218] TRAIN: iteration 31162 : loss : 0.058995, loss_ce: 0.003382, loss_dice: 0.114608
[12:49:59.426] TRAIN: iteration 31163 : loss : 0.138482, loss_ce: 0.002791, loss_dice: 0.274173
[12:50:02.081] TRAIN: iteration 31164 : loss : 0.130093, loss_ce: 0.002797, loss_dice: 0.257388
[12:50:02.296] TRAIN: iteration 31165 : loss : 0.084895, loss_ce: 0.001025, loss_dice: 0.168765
[12:50:02.504] TRAIN: iteration 31166 : loss : 0.103866, loss_ce: 0.001153, loss_dice: 0.206579
[12:50:02.711] TRAIN: iteration 31167 : loss : 0.142855, loss_ce: 0.002678, loss_dice: 0.283031
[12:50:06.614] TRAIN: iteration 31168 : loss : 0.085798, loss_ce: 0.002639, loss_dice: 0.168958
[12:50:06.826] TRAIN: iteration 31169 : loss : 0.045710, loss_ce: 0.001571, loss_dice: 0.089848
[12:50:07.034] TRAIN: iteration 31170 : loss : 0.136476, loss_ce: 0.003359, loss_dice: 0.269593
[12:50:07.246] TRAIN: iteration 31171 : loss : 0.250354, loss_ce: 0.000682, loss_dice: 0.500025
[12:50:10.854] TRAIN: iteration 31172 : loss : 0.038424, loss_ce: 0.002281, loss_dice: 0.074566
[12:50:11.064] TRAIN: iteration 31173 : loss : 0.046610, loss_ce: 0.004337, loss_dice: 0.088883
[12:50:11.272] TRAIN: iteration 31174 : loss : 0.039738, loss_ce: 0.003851, loss_dice: 0.075625
[12:50:11.677] TRAIN: iteration 31175 : loss : 0.112148, loss_ce: 0.002454, loss_dice: 0.221842
[12:50:14.846] TRAIN: iteration 31176 : loss : 0.126828, loss_ce: 0.001710, loss_dice: 0.251947
[12:50:15.054] TRAIN: iteration 31177 : loss : 0.081001, loss_ce: 0.003634, loss_dice: 0.158368
[12:50:15.264] TRAIN: iteration 31178 : loss : 0.250652, loss_ce: 0.001230, loss_dice: 0.500073
[12:50:15.473] TRAIN: iteration 31179 : loss : 0.042889, loss_ce: 0.003438, loss_dice: 0.082339
[12:50:19.041] TRAIN: iteration 31180 : loss : 0.075981, loss_ce: 0.001355, loss_dice: 0.150608
[12:50:19.278] TRAIN: iteration 31181 : loss : 0.030689, loss_ce: 0.002744, loss_dice: 0.058635
[12:50:19.486] TRAIN: iteration 31182 : loss : 0.193225, loss_ce: 0.005925, loss_dice: 0.380526
[12:50:20.445] TRAIN: iteration 31183 : loss : 0.235388, loss_ce: 0.008119, loss_dice: 0.462656
[12:50:23.114] TRAIN: iteration 31184 : loss : 0.083044, loss_ce: 0.003459, loss_dice: 0.162628
[12:50:23.322] TRAIN: iteration 31185 : loss : 0.055428, loss_ce: 0.001985, loss_dice: 0.108872
[12:50:23.530] TRAIN: iteration 31186 : loss : 0.038469, loss_ce: 0.003072, loss_dice: 0.073865
[12:50:23.740] TRAIN: iteration 31187 : loss : 0.171433, loss_ce: 0.003020, loss_dice: 0.339845
[12:50:28.822] TRAIN: iteration 31188 : loss : 0.039298, loss_ce: 0.001966, loss_dice: 0.076630
[12:50:29.029] TRAIN: iteration 31189 : loss : 0.039470, loss_ce: 0.005768, loss_dice: 0.073173
[12:50:29.242] TRAIN: iteration 31190 : loss : 0.124788, loss_ce: 0.006263, loss_dice: 0.243312
[12:50:29.452] TRAIN: iteration 31191 : loss : 0.047165, loss_ce: 0.001294, loss_dice: 0.093037
[12:50:33.034] TRAIN: iteration 31192 : loss : 0.026932, loss_ce: 0.004738, loss_dice: 0.049126
[12:50:33.242] TRAIN: iteration 31193 : loss : 0.250494, loss_ce: 0.000951, loss_dice: 0.500036
[12:50:33.451] TRAIN: iteration 31194 : loss : 0.043796, loss_ce: 0.004371, loss_dice: 0.083222
[12:50:33.661] TRAIN: iteration 31195 : loss : 0.051876, loss_ce: 0.005191, loss_dice: 0.098562
[12:50:38.450] TRAIN: iteration 31196 : loss : 0.051834, loss_ce: 0.000859, loss_dice: 0.102809
[12:50:38.658] TRAIN: iteration 31197 : loss : 0.025304, loss_ce: 0.000695, loss_dice: 0.049914
[12:50:38.866] TRAIN: iteration 31198 : loss : 0.119535, loss_ce: 0.012407, loss_dice: 0.226663
[12:50:39.073] TRAIN: iteration 31199 : loss : 0.206604, loss_ce: 0.001949, loss_dice: 0.411259
[12:50:39.487] TRAIN: iteration 31200 : loss : 0.049826, loss_ce: 0.001592, loss_dice: 0.098060
[12:50:39.722] TRAIN: iteration 31201 : loss : 0.161616, loss_ce: 0.001695, loss_dice: 0.321536
[12:50:39.930] TRAIN: iteration 31202 : loss : 0.102922, loss_ce: 0.005147, loss_dice: 0.200698
[12:50:40.138] TRAIN: iteration 31203 : loss : 0.189456, loss_ce: 0.001211, loss_dice: 0.377701
[12:50:46.307] TRAIN: iteration 31204 : loss : 0.041282, loss_ce: 0.002330, loss_dice: 0.080234
[12:50:46.514] TRAIN: iteration 31205 : loss : 0.070287, loss_ce: 0.001511, loss_dice: 0.139062
[12:50:46.721] TRAIN: iteration 31206 : loss : 0.233512, loss_ce: 0.003511, loss_dice: 0.463514
[12:50:46.928] TRAIN: iteration 31207 : loss : 0.043550, loss_ce: 0.002302, loss_dice: 0.084799
[12:50:47.218] TRAIN: iteration 31208 : loss : 0.130576, loss_ce: 0.020423, loss_dice: 0.240730
[12:50:47.425] TRAIN: iteration 31209 : loss : 0.117067, loss_ce: 0.002633, loss_dice: 0.231501
[12:50:47.632] TRAIN: iteration 31210 : loss : 0.097608, loss_ce: 0.002009, loss_dice: 0.193207
[12:50:47.839] TRAIN: iteration 31211 : loss : 0.092390, loss_ce: 0.001632, loss_dice: 0.183148
[12:50:53.475] TRAIN: iteration 31212 : loss : 0.250922, loss_ce: 0.001736, loss_dice: 0.500108
[12:50:53.683] TRAIN: iteration 31213 : loss : 0.104397, loss_ce: 0.002354, loss_dice: 0.206440
[12:50:53.893] TRAIN: iteration 31214 : loss : 0.036321, loss_ce: 0.001761, loss_dice: 0.070881
[12:50:55.557] TRAIN: iteration 31215 : loss : 0.040695, loss_ce: 0.001916, loss_dice: 0.079474
[12:50:56.051] TRAIN: iteration 31216 : loss : 0.158066, loss_ce: 0.006118, loss_dice: 0.310014
[12:50:56.258] TRAIN: iteration 31217 : loss : 0.055287, loss_ce: 0.002088, loss_dice: 0.108486
[12:50:56.467] TRAIN: iteration 31218 : loss : 0.093676, loss_ce: 0.002734, loss_dice: 0.184618
[12:50:56.673] TRAIN: iteration 31219 : loss : 0.072429, loss_ce: 0.004323, loss_dice: 0.140536
[12:51:02.203] TRAIN: iteration 31220 : loss : 0.099324, loss_ce: 0.002862, loss_dice: 0.195787
[12:51:02.204] NaN or Inf found in input tensor.
[12:51:02.419] TRAIN: iteration 31221 : loss : 0.037258, loss_ce: 0.000690, loss_dice: 0.073827
[12:51:02.627] TRAIN: iteration 31222 : loss : 0.090486, loss_ce: 0.011524, loss_dice: 0.169448
[12:51:05.381] TRAIN: iteration 31223 : loss : 0.227810, loss_ce: 0.001638, loss_dice: 0.453983
[12:51:05.589] TRAIN: iteration 31224 : loss : 0.041523, loss_ce: 0.005066, loss_dice: 0.077980
[12:51:05.800] TRAIN: iteration 31225 : loss : 0.250471, loss_ce: 0.000913, loss_dice: 0.500028
[12:51:06.008] TRAIN: iteration 31226 : loss : 0.034639, loss_ce: 0.001512, loss_dice: 0.067766
[12:51:06.216] TRAIN: iteration 31227 : loss : 0.043442, loss_ce: 0.002895, loss_dice: 0.083988
[12:51:12.851] TRAIN: iteration 31228 : loss : 0.230037, loss_ce: 0.005151, loss_dice: 0.454924
[12:51:13.057] TRAIN: iteration 31229 : loss : 0.034650, loss_ce: 0.002241, loss_dice: 0.067060
[12:51:13.155] TRAIN: iteration 31230 : loss : 0.067238, loss_ce: 0.003884, loss_dice: 0.130592
[12:56:31.610] VALIDATION: iteration 17 : loss : 0.107403, loss_ce: 0.003617, loss_dice: 0.211188
[12:56:34.355] TRAIN: iteration 31231 : loss : 0.051284, loss_ce: 0.000777, loss_dice: 0.101791
[12:56:34.567] TRAIN: iteration 31232 : loss : 0.066024, loss_ce: 0.003649, loss_dice: 0.128399
[12:56:34.775] TRAIN: iteration 31233 : loss : 0.034186, loss_ce: 0.007784, loss_dice: 0.060588
[12:56:34.991] TRAIN: iteration 31234 : loss : 0.133773, loss_ce: 0.004918, loss_dice: 0.262628
[12:56:35.205] TRAIN: iteration 31235 : loss : 0.033002, loss_ce: 0.004963, loss_dice: 0.061041
[12:56:35.416] TRAIN: iteration 31236 : loss : 0.102708, loss_ce: 0.001192, loss_dice: 0.204224
[12:56:35.632] TRAIN: iteration 31237 : loss : 0.062446, loss_ce: 0.003920, loss_dice: 0.120971
[12:56:35.840] TRAIN: iteration 31238 : loss : 0.079917, loss_ce: 0.005694, loss_dice: 0.154141
[12:56:36.050] TRAIN: iteration 31239 : loss : 0.048281, loss_ce: 0.008485, loss_dice: 0.088077
[12:56:36.259] TRAIN: iteration 31240 : loss : 0.078761, loss_ce: 0.005105, loss_dice: 0.152416
[12:56:36.504] TRAIN: iteration 31241 : loss : 0.042458, loss_ce: 0.004319, loss_dice: 0.080597
[12:56:36.716] TRAIN: iteration 31242 : loss : 0.056472, loss_ce: 0.005688, loss_dice: 0.107256
[12:56:36.925] TRAIN: iteration 31243 : loss : 0.096311, loss_ce: 0.001361, loss_dice: 0.191261
[12:56:37.133] TRAIN: iteration 31244 : loss : 0.063274, loss_ce: 0.005752, loss_dice: 0.120797
[12:56:37.354] TRAIN: iteration 31245 : loss : 0.110786, loss_ce: 0.001525, loss_dice: 0.220047
[12:56:37.566] TRAIN: iteration 31246 : loss : 0.152676, loss_ce: 0.004929, loss_dice: 0.300422
[12:56:37.774] TRAIN: iteration 31247 : loss : 0.048159, loss_ce: 0.002753, loss_dice: 0.093566
[12:56:37.987] TRAIN: iteration 31248 : loss : 0.083080, loss_ce: 0.002080, loss_dice: 0.164079
[12:56:38.195] TRAIN: iteration 31249 : loss : 0.251455, loss_ce: 0.002723, loss_dice: 0.500186
[12:56:38.407] TRAIN: iteration 31250 : loss : 0.021178, loss_ce: 0.004191, loss_dice: 0.038165
[12:56:38.614] TRAIN: iteration 31251 : loss : 0.031828, loss_ce: 0.001455, loss_dice: 0.062201
[12:56:38.821] TRAIN: iteration 31252 : loss : 0.062599, loss_ce: 0.007682, loss_dice: 0.117516
[12:56:39.028] TRAIN: iteration 31253 : loss : 0.250625, loss_ce: 0.001195, loss_dice: 0.500054
[12:56:39.237] TRAIN: iteration 31254 : loss : 0.250428, loss_ce: 0.000827, loss_dice: 0.500029
[12:56:39.444] TRAIN: iteration 31255 : loss : 0.137792, loss_ce: 0.002097, loss_dice: 0.273486
[12:56:39.653] TRAIN: iteration 31256 : loss : 0.033340, loss_ce: 0.007153, loss_dice: 0.059528
[12:56:39.861] TRAIN: iteration 31257 : loss : 0.091336, loss_ce: 0.003515, loss_dice: 0.179157
[12:56:40.071] TRAIN: iteration 31258 : loss : 0.163020, loss_ce: 0.002890, loss_dice: 0.323149
[12:56:40.283] TRAIN: iteration 31259 : loss : 0.042710, loss_ce: 0.003592, loss_dice: 0.081828
[12:56:40.491] TRAIN: iteration 31260 : loss : 0.069355, loss_ce: 0.003481, loss_dice: 0.135229
[12:56:40.729] TRAIN: iteration 31261 : loss : 0.096221, loss_ce: 0.011622, loss_dice: 0.180820
[12:56:40.939] TRAIN: iteration 31262 : loss : 0.061772, loss_ce: 0.001330, loss_dice: 0.122214
[12:56:41.153] TRAIN: iteration 31263 : loss : 0.087745, loss_ce: 0.006269, loss_dice: 0.169221
[12:56:41.362] TRAIN: iteration 31264 : loss : 0.118504, loss_ce: 0.001679, loss_dice: 0.235328
[12:56:41.569] TRAIN: iteration 31265 : loss : 0.098867, loss_ce: 0.005086, loss_dice: 0.192647
[12:56:41.776] TRAIN: iteration 31266 : loss : 0.226425, loss_ce: 0.002946, loss_dice: 0.449903
[12:56:41.986] TRAIN: iteration 31267 : loss : 0.139368, loss_ce: 0.003781, loss_dice: 0.274955
[12:56:42.194] TRAIN: iteration 31268 : loss : 0.021438, loss_ce: 0.001795, loss_dice: 0.041080
[12:56:42.410] TRAIN: iteration 31269 : loss : 0.065889, loss_ce: 0.004106, loss_dice: 0.127671
[12:56:42.626] TRAIN: iteration 31270 : loss : 0.060341, loss_ce: 0.005884, loss_dice: 0.114799
[12:56:42.836] TRAIN: iteration 31271 : loss : 0.067784, loss_ce: 0.002519, loss_dice: 0.133048
[12:56:43.045] TRAIN: iteration 31272 : loss : 0.251380, loss_ce: 0.005211, loss_dice: 0.497550
[12:56:43.257] TRAIN: iteration 31273 : loss : 0.030492, loss_ce: 0.004741, loss_dice: 0.056242
[12:56:43.466] TRAIN: iteration 31274 : loss : 0.066422, loss_ce: 0.001986, loss_dice: 0.130858
[12:56:43.674] TRAIN: iteration 31275 : loss : 0.175399, loss_ce: 0.003069, loss_dice: 0.347729
[12:56:43.884] TRAIN: iteration 31276 : loss : 0.042517, loss_ce: 0.000987, loss_dice: 0.084047
[12:56:44.092] TRAIN: iteration 31277 : loss : 0.041278, loss_ce: 0.001244, loss_dice: 0.081313
[12:56:44.302] TRAIN: iteration 31278 : loss : 0.097454, loss_ce: 0.007950, loss_dice: 0.186957
[12:56:44.512] TRAIN: iteration 31279 : loss : 0.065126, loss_ce: 0.004798, loss_dice: 0.125454
[12:56:44.727] TRAIN: iteration 31280 : loss : 0.137475, loss_ce: 0.005914, loss_dice: 0.269037
[12:56:44.967] TRAIN: iteration 31281 : loss : 0.082475, loss_ce: 0.003050, loss_dice: 0.161899
[12:56:45.183] TRAIN: iteration 31282 : loss : 0.030965, loss_ce: 0.004900, loss_dice: 0.057031
[12:56:45.391] TRAIN: iteration 31283 : loss : 0.041508, loss_ce: 0.001112, loss_dice: 0.081905
[12:56:45.600] TRAIN: iteration 31284 : loss : 0.237577, loss_ce: 0.010887, loss_dice: 0.464267
[12:56:45.808] TRAIN: iteration 31285 : loss : 0.067637, loss_ce: 0.000839, loss_dice: 0.134435
[12:56:46.020] TRAIN: iteration 31286 : loss : 0.056328, loss_ce: 0.003392, loss_dice: 0.109263
[12:56:46.237] TRAIN: iteration 31287 : loss : 0.119670, loss_ce: 0.002219, loss_dice: 0.237121
[12:56:46.454] TRAIN: iteration 31288 : loss : 0.009967, loss_ce: 0.001208, loss_dice: 0.018725
[12:56:46.663] TRAIN: iteration 31289 : loss : 0.030462, loss_ce: 0.000521, loss_dice: 0.060404
[12:56:46.871] TRAIN: iteration 31290 : loss : 0.020119, loss_ce: 0.003937, loss_dice: 0.036302
[12:56:47.081] TRAIN: iteration 31291 : loss : 0.110336, loss_ce: 0.001886, loss_dice: 0.218787
[12:56:47.288] TRAIN: iteration 31292 : loss : 0.129620, loss_ce: 0.001466, loss_dice: 0.257775
[12:56:47.495] TRAIN: iteration 31293 : loss : 0.043510, loss_ce: 0.004184, loss_dice: 0.082836
[12:56:47.702] TRAIN: iteration 31294 : loss : 0.163295, loss_ce: 0.008846, loss_dice: 0.317745
[12:56:47.910] TRAIN: iteration 31295 : loss : 0.248235, loss_ce: 0.002356, loss_dice: 0.494114
[12:56:48.123] TRAIN: iteration 31296 : loss : 0.058774, loss_ce: 0.001184, loss_dice: 0.116364
[12:56:48.331] TRAIN: iteration 31297 : loss : 0.042426, loss_ce: 0.002519, loss_dice: 0.082333
[12:56:48.559] TRAIN: iteration 31298 : loss : 0.030360, loss_ce: 0.000533, loss_dice: 0.060188
[12:56:48.768] TRAIN: iteration 31299 : loss : 0.057790, loss_ce: 0.001066, loss_dice: 0.114514
[12:56:48.977] TRAIN: iteration 31300 : loss : 0.037176, loss_ce: 0.004231, loss_dice: 0.070121
[12:56:49.217] TRAIN: iteration 31301 : loss : 0.095009, loss_ce: 0.002871, loss_dice: 0.187148
[12:56:49.451] TRAIN: iteration 31302 : loss : 0.126283, loss_ce: 0.006127, loss_dice: 0.246439
[12:56:49.662] TRAIN: iteration 31303 : loss : 0.093910, loss_ce: 0.002831, loss_dice: 0.184990
[12:56:49.872] TRAIN: iteration 31304 : loss : 0.091016, loss_ce: 0.005042, loss_dice: 0.176989
[12:56:50.080] TRAIN: iteration 31305 : loss : 0.057947, loss_ce: 0.001597, loss_dice: 0.114298
[12:56:50.289] TRAIN: iteration 31306 : loss : 0.036346, loss_ce: 0.002408, loss_dice: 0.070284
[12:56:50.495] TRAIN: iteration 31307 : loss : 0.156890, loss_ce: 0.012477, loss_dice: 0.301303
[12:56:50.704] TRAIN: iteration 31308 : loss : 0.064575, loss_ce: 0.001048, loss_dice: 0.128102
[12:56:50.911] TRAIN: iteration 31309 : loss : 0.077618, loss_ce: 0.003906, loss_dice: 0.151331
[12:56:51.123] TRAIN: iteration 31310 : loss : 0.114671, loss_ce: 0.007201, loss_dice: 0.222142
[12:56:51.333] TRAIN: iteration 31311 : loss : 0.051535, loss_ce: 0.001577, loss_dice: 0.101494
[12:56:51.541] TRAIN: iteration 31312 : loss : 0.251699, loss_ce: 0.004327, loss_dice: 0.499071
[12:56:51.748] TRAIN: iteration 31313 : loss : 0.117607, loss_ce: 0.024691, loss_dice: 0.210523
[12:56:51.956] TRAIN: iteration 31314 : loss : 0.237688, loss_ce: 0.001283, loss_dice: 0.474094
[12:56:52.166] TRAIN: iteration 31315 : loss : 0.051282, loss_ce: 0.003755, loss_dice: 0.098810
[12:56:52.373] TRAIN: iteration 31316 : loss : 0.049584, loss_ce: 0.004862, loss_dice: 0.094305
[12:56:52.581] TRAIN: iteration 31317 : loss : 0.077741, loss_ce: 0.010761, loss_dice: 0.144720
[12:56:52.788] TRAIN: iteration 31318 : loss : 0.051598, loss_ce: 0.001798, loss_dice: 0.101399
[12:56:52.998] TRAIN: iteration 31319 : loss : 0.030642, loss_ce: 0.002477, loss_dice: 0.058808
[12:56:53.206] TRAIN: iteration 31320 : loss : 0.146908, loss_ce: 0.009383, loss_dice: 0.284432
[12:56:53.458] TRAIN: iteration 31321 : loss : 0.093708, loss_ce: 0.002502, loss_dice: 0.184915
[12:56:53.673] TRAIN: iteration 31322 : loss : 0.051723, loss_ce: 0.004223, loss_dice: 0.099223
[12:56:53.882] TRAIN: iteration 31323 : loss : 0.250807, loss_ce: 0.001528, loss_dice: 0.500086
[12:56:54.089] TRAIN: iteration 31324 : loss : 0.251389, loss_ce: 0.002597, loss_dice: 0.500182
[12:56:54.297] TRAIN: iteration 31325 : loss : 0.022561, loss_ce: 0.001997, loss_dice: 0.043125
[12:56:54.505] TRAIN: iteration 31326 : loss : 0.055224, loss_ce: 0.001421, loss_dice: 0.109026
[12:56:54.715] TRAIN: iteration 31327 : loss : 0.073026, loss_ce: 0.002217, loss_dice: 0.143834
[12:56:54.923] TRAIN: iteration 31328 : loss : 0.239928, loss_ce: 0.006366, loss_dice: 0.473490
[12:56:55.130] TRAIN: iteration 31329 : loss : 0.052591, loss_ce: 0.003878, loss_dice: 0.101305
[12:56:55.339] TRAIN: iteration 31330 : loss : 0.065095, loss_ce: 0.001279, loss_dice: 0.128911
[12:56:55.546] TRAIN: iteration 31331 : loss : 0.079322, loss_ce: 0.002348, loss_dice: 0.156296
[12:56:55.754] TRAIN: iteration 31332 : loss : 0.067916, loss_ce: 0.001958, loss_dice: 0.133874
[12:56:55.963] TRAIN: iteration 31333 : loss : 0.250422, loss_ce: 0.000816, loss_dice: 0.500028
[12:56:56.174] TRAIN: iteration 31334 : loss : 0.044000, loss_ce: 0.002495, loss_dice: 0.085504
[12:56:56.382] TRAIN: iteration 31335 : loss : 0.031172, loss_ce: 0.001790, loss_dice: 0.060554
[12:56:56.591] TRAIN: iteration 31336 : loss : 0.102141, loss_ce: 0.003145, loss_dice: 0.201137
[12:56:56.801] TRAIN: iteration 31337 : loss : 0.027504, loss_ce: 0.000709, loss_dice: 0.054300
[12:56:57.011] TRAIN: iteration 31338 : loss : 0.130511, loss_ce: 0.002323, loss_dice: 0.258698
[12:56:57.222] TRAIN: iteration 31339 : loss : 0.044756, loss_ce: 0.005644, loss_dice: 0.083868
[12:56:57.431] TRAIN: iteration 31340 : loss : 0.042708, loss_ce: 0.000967, loss_dice: 0.084449
[12:56:57.681] TRAIN: iteration 31341 : loss : 0.250550, loss_ce: 0.001053, loss_dice: 0.500047
[12:56:57.891] TRAIN: iteration 31342 : loss : 0.217716, loss_ce: 0.001196, loss_dice: 0.434236
[12:56:58.102] TRAIN: iteration 31343 : loss : 0.174877, loss_ce: 0.005833, loss_dice: 0.343921
[12:56:58.312] TRAIN: iteration 31344 : loss : 0.117677, loss_ce: 0.003689, loss_dice: 0.231665
[12:56:58.525] TRAIN: iteration 31345 : loss : 0.088690, loss_ce: 0.002034, loss_dice: 0.175347
[12:56:58.735] TRAIN: iteration 31346 : loss : 0.091910, loss_ce: 0.010450, loss_dice: 0.173369
[12:56:58.943] TRAIN: iteration 31347 : loss : 0.136524, loss_ce: 0.008578, loss_dice: 0.264469
[12:56:59.152] TRAIN: iteration 31348 : loss : 0.109756, loss_ce: 0.011354, loss_dice: 0.208157
[12:56:59.363] TRAIN: iteration 31349 : loss : 0.083777, loss_ce: 0.005991, loss_dice: 0.161563
[12:56:59.575] TRAIN: iteration 31350 : loss : 0.031135, loss_ce: 0.001920, loss_dice: 0.060350
[12:56:59.788] TRAIN: iteration 31351 : loss : 0.092340, loss_ce: 0.000960, loss_dice: 0.183719
[12:56:59.996] TRAIN: iteration 31352 : loss : 0.183934, loss_ce: 0.003602, loss_dice: 0.364266
[12:57:00.205] TRAIN: iteration 31353 : loss : 0.047280, loss_ce: 0.004445, loss_dice: 0.090114
[12:57:00.412] TRAIN: iteration 31354 : loss : 0.081012, loss_ce: 0.008933, loss_dice: 0.153092
[12:57:00.621] TRAIN: iteration 31355 : loss : 0.251250, loss_ce: 0.002336, loss_dice: 0.500165
[12:57:00.836] TRAIN: iteration 31356 : loss : 0.106002, loss_ce: 0.018516, loss_dice: 0.193487
[12:57:01.045] TRAIN: iteration 31357 : loss : 0.096776, loss_ce: 0.005244, loss_dice: 0.188308
[12:57:01.253] TRAIN: iteration 31358 : loss : 0.115212, loss_ce: 0.002139, loss_dice: 0.228285
[12:57:01.466] TRAIN: iteration 31359 : loss : 0.249653, loss_ce: 0.007147, loss_dice: 0.492158
[12:57:01.676] TRAIN: iteration 31360 : loss : 0.109897, loss_ce: 0.005291, loss_dice: 0.214502
[12:57:01.928] TRAIN: iteration 31361 : loss : 0.247837, loss_ce: 0.001792, loss_dice: 0.493881
[12:57:02.136] TRAIN: iteration 31362 : loss : 0.126485, loss_ce: 0.002198, loss_dice: 0.250772
[12:57:02.345] TRAIN: iteration 31363 : loss : 0.078229, loss_ce: 0.002005, loss_dice: 0.154453
[12:57:02.555] TRAIN: iteration 31364 : loss : 0.250829, loss_ce: 0.005580, loss_dice: 0.496077
[12:57:02.764] TRAIN: iteration 31365 : loss : 0.030260, loss_ce: 0.001525, loss_dice: 0.058996
[12:57:02.999] TRAIN: iteration 31366 : loss : 0.055501, loss_ce: 0.001373, loss_dice: 0.109628
[12:57:03.209] TRAIN: iteration 31367 : loss : 0.190888, loss_ce: 0.001883, loss_dice: 0.379892
[12:57:03.418] TRAIN: iteration 31368 : loss : 0.089196, loss_ce: 0.010674, loss_dice: 0.167719
[12:57:03.626] TRAIN: iteration 31369 : loss : 0.011635, loss_ce: 0.000609, loss_dice: 0.022661
[12:57:03.835] TRAIN: iteration 31370 : loss : 0.059497, loss_ce: 0.002688, loss_dice: 0.116306
[12:57:04.064] TRAIN: iteration 31371 : loss : 0.035162, loss_ce: 0.000560, loss_dice: 0.069763
[12:57:04.272] TRAIN: iteration 31372 : loss : 0.231939, loss_ce: 0.001208, loss_dice: 0.462670
[12:57:04.479] TRAIN: iteration 31373 : loss : 0.103670, loss_ce: 0.000559, loss_dice: 0.206781
[12:57:04.688] TRAIN: iteration 31374 : loss : 0.048839, loss_ce: 0.003890, loss_dice: 0.093788
[12:57:04.909] TRAIN: iteration 31375 : loss : 0.209903, loss_ce: 0.002007, loss_dice: 0.417800
[12:57:05.117] TRAIN: iteration 31376 : loss : 0.061384, loss_ce: 0.002197, loss_dice: 0.120571
[12:57:05.325] TRAIN: iteration 31377 : loss : 0.235786, loss_ce: 0.002221, loss_dice: 0.469351
[12:57:05.534] TRAIN: iteration 31378 : loss : 0.091436, loss_ce: 0.000631, loss_dice: 0.182242
[12:57:05.741] TRAIN: iteration 31379 : loss : 0.194296, loss_ce: 0.001190, loss_dice: 0.387402
[12:57:05.949] TRAIN: iteration 31380 : loss : 0.253866, loss_ce: 0.011084, loss_dice: 0.496648
[12:57:06.178] TRAIN: iteration 31381 : loss : 0.027752, loss_ce: 0.001901, loss_dice: 0.053602
[12:57:06.386] TRAIN: iteration 31382 : loss : 0.205421, loss_ce: 0.000918, loss_dice: 0.409925
[12:57:06.596] TRAIN: iteration 31383 : loss : 0.039853, loss_ce: 0.001280, loss_dice: 0.078426
[12:57:06.804] TRAIN: iteration 31384 : loss : 0.145468, loss_ce: 0.002663, loss_dice: 0.288273
[12:57:07.013] TRAIN: iteration 31385 : loss : 0.250367, loss_ce: 0.000713, loss_dice: 0.500020
[12:57:07.220] TRAIN: iteration 31386 : loss : 0.058753, loss_ce: 0.001278, loss_dice: 0.116228
[12:57:07.427] TRAIN: iteration 31387 : loss : 0.041122, loss_ce: 0.000871, loss_dice: 0.081374
[12:57:07.636] TRAIN: iteration 31388 : loss : 0.120530, loss_ce: 0.003099, loss_dice: 0.237960
[12:57:07.844] TRAIN: iteration 31389 : loss : 0.065530, loss_ce: 0.002819, loss_dice: 0.128241
[12:57:08.053] TRAIN: iteration 31390 : loss : 0.048040, loss_ce: 0.002848, loss_dice: 0.093232
[12:57:08.261] TRAIN: iteration 31391 : loss : 0.068806, loss_ce: 0.000988, loss_dice: 0.136624
[12:57:08.471] TRAIN: iteration 31392 : loss : 0.023391, loss_ce: 0.001556, loss_dice: 0.045227
[12:57:08.678] TRAIN: iteration 31393 : loss : 0.089428, loss_ce: 0.008948, loss_dice: 0.169908
[12:57:08.887] TRAIN: iteration 31394 : loss : 0.252369, loss_ce: 0.005340, loss_dice: 0.499399
[12:57:09.125] TRAIN: iteration 31395 : loss : 0.101454, loss_ce: 0.001936, loss_dice: 0.200972
[12:57:09.333] TRAIN: iteration 31396 : loss : 0.213714, loss_ce: 0.001446, loss_dice: 0.425982
[12:57:09.541] TRAIN: iteration 31397 : loss : 0.105638, loss_ce: 0.005760, loss_dice: 0.205517
[12:57:09.753] TRAIN: iteration 31398 : loss : 0.136631, loss_ce: 0.003100, loss_dice: 0.270161
[12:57:09.967] TRAIN: iteration 31399 : loss : 0.037410, loss_ce: 0.002499, loss_dice: 0.072320
[12:57:10.180] TRAIN: iteration 31400 : loss : 0.250421, loss_ce: 0.000819, loss_dice: 0.500024
[12:57:10.413] TRAIN: iteration 31401 : loss : 0.038024, loss_ce: 0.000790, loss_dice: 0.075257
[12:57:10.620] TRAIN: iteration 31402 : loss : 0.068853, loss_ce: 0.004840, loss_dice: 0.132866
[12:57:10.828] TRAIN: iteration 31403 : loss : 0.067936, loss_ce: 0.002642, loss_dice: 0.133229
[12:57:11.043] TRAIN: iteration 31404 : loss : 0.250697, loss_ce: 0.001323, loss_dice: 0.500071
[12:57:11.256] TRAIN: iteration 31405 : loss : 0.087406, loss_ce: 0.004483, loss_dice: 0.170329
[12:57:11.467] TRAIN: iteration 31406 : loss : 0.145156, loss_ce: 0.002737, loss_dice: 0.287574
[12:57:11.683] TRAIN: iteration 31407 : loss : 0.062814, loss_ce: 0.003177, loss_dice: 0.122452
[12:57:11.892] TRAIN: iteration 31408 : loss : 0.076570, loss_ce: 0.002319, loss_dice: 0.150821
[12:57:12.102] TRAIN: iteration 31409 : loss : 0.058587, loss_ce: 0.001925, loss_dice: 0.115249
[12:57:12.311] TRAIN: iteration 31410 : loss : 0.032785, loss_ce: 0.002828, loss_dice: 0.062743
[12:57:12.527] TRAIN: iteration 31411 : loss : 0.026743, loss_ce: 0.003185, loss_dice: 0.050302
[12:57:12.741] TRAIN: iteration 31412 : loss : 0.146587, loss_ce: 0.003871, loss_dice: 0.289304
[12:57:12.950] TRAIN: iteration 31413 : loss : 0.117522, loss_ce: 0.005691, loss_dice: 0.229354
[12:57:13.161] TRAIN: iteration 31414 : loss : 0.073653, loss_ce: 0.002833, loss_dice: 0.144473
[12:57:13.374] TRAIN: iteration 31415 : loss : 0.250398, loss_ce: 0.000781, loss_dice: 0.500016
[12:57:13.585] TRAIN: iteration 31416 : loss : 0.061731, loss_ce: 0.002041, loss_dice: 0.121422
[12:57:13.801] TRAIN: iteration 31417 : loss : 0.043954, loss_ce: 0.002211, loss_dice: 0.085697
[12:57:14.010] TRAIN: iteration 31418 : loss : 0.250622, loss_ce: 0.001183, loss_dice: 0.500060
[12:57:14.218] TRAIN: iteration 31419 : loss : 0.025404, loss_ce: 0.004888, loss_dice: 0.045920
[12:57:14.429] TRAIN: iteration 31420 : loss : 0.096050, loss_ce: 0.004045, loss_dice: 0.188056
[12:57:14.674] TRAIN: iteration 31421 : loss : 0.037597, loss_ce: 0.000718, loss_dice: 0.074477
[12:57:14.884] TRAIN: iteration 31422 : loss : 0.250180, loss_ce: 0.002180, loss_dice: 0.498180
[12:57:15.094] TRAIN: iteration 31423 : loss : 0.217324, loss_ce: 0.001465, loss_dice: 0.433183
[12:57:15.306] TRAIN: iteration 31424 : loss : 0.060416, loss_ce: 0.000968, loss_dice: 0.119864
[12:57:15.514] TRAIN: iteration 31425 : loss : 0.147272, loss_ce: 0.005699, loss_dice: 0.288844
[12:57:15.724] TRAIN: iteration 31426 : loss : 0.034332, loss_ce: 0.000875, loss_dice: 0.067789
[12:57:15.931] TRAIN: iteration 31427 : loss : 0.046406, loss_ce: 0.001683, loss_dice: 0.091128
[12:57:16.139] TRAIN: iteration 31428 : loss : 0.237403, loss_ce: 0.002492, loss_dice: 0.472315
[12:57:16.346] TRAIN: iteration 31429 : loss : 0.234351, loss_ce: 0.002544, loss_dice: 0.466158
[12:57:16.555] TRAIN: iteration 31430 : loss : 0.048639, loss_ce: 0.007955, loss_dice: 0.089324
[12:57:16.764] TRAIN: iteration 31431 : loss : 0.056258, loss_ce: 0.002105, loss_dice: 0.110411
[12:57:16.973] TRAIN: iteration 31432 : loss : 0.024022, loss_ce: 0.003585, loss_dice: 0.044458
[12:57:17.183] TRAIN: iteration 31433 : loss : 0.074340, loss_ce: 0.002403, loss_dice: 0.146276
[12:57:17.391] TRAIN: iteration 31434 : loss : 0.233770, loss_ce: 0.006819, loss_dice: 0.460722
[12:57:17.602] TRAIN: iteration 31435 : loss : 0.054465, loss_ce: 0.005511, loss_dice: 0.103420
[12:57:17.809] TRAIN: iteration 31436 : loss : 0.043876, loss_ce: 0.001171, loss_dice: 0.086580
[12:57:18.017] TRAIN: iteration 31437 : loss : 0.140482, loss_ce: 0.006387, loss_dice: 0.274578
[12:57:18.228] TRAIN: iteration 31438 : loss : 0.071686, loss_ce: 0.004484, loss_dice: 0.138889
[12:57:18.435] TRAIN: iteration 31439 : loss : 0.108779, loss_ce: 0.004780, loss_dice: 0.212778
[12:57:18.654] TRAIN: iteration 31440 : loss : 0.053666, loss_ce: 0.002417, loss_dice: 0.104915
[12:57:18.893] TRAIN: iteration 31441 : loss : 0.052311, loss_ce: 0.002715, loss_dice: 0.101907
[12:57:19.106] TRAIN: iteration 31442 : loss : 0.036716, loss_ce: 0.003025, loss_dice: 0.070407
[12:57:19.317] TRAIN: iteration 31443 : loss : 0.055415, loss_ce: 0.002931, loss_dice: 0.107899
[12:57:19.524] TRAIN: iteration 31444 : loss : 0.024044, loss_ce: 0.001398, loss_dice: 0.046689
[12:57:19.735] TRAIN: iteration 31445 : loss : 0.030288, loss_ce: 0.003975, loss_dice: 0.056601
[12:57:19.945] TRAIN: iteration 31446 : loss : 0.035836, loss_ce: 0.000684, loss_dice: 0.070987
[12:57:21.216] TRAIN: iteration 31447 : loss : 0.065109, loss_ce: 0.002646, loss_dice: 0.127572
[12:57:21.427] TRAIN: iteration 31448 : loss : 0.079317, loss_ce: 0.002666, loss_dice: 0.155968
[12:57:21.636] TRAIN: iteration 31449 : loss : 0.041639, loss_ce: 0.004082, loss_dice: 0.079196
[12:57:21.848] TRAIN: iteration 31450 : loss : 0.050688, loss_ce: 0.004440, loss_dice: 0.096935
[12:57:22.058] TRAIN: iteration 31451 : loss : 0.155858, loss_ce: 0.002150, loss_dice: 0.309566
[12:57:22.268] TRAIN: iteration 31452 : loss : 0.099905, loss_ce: 0.001297, loss_dice: 0.198513
[12:57:22.475] TRAIN: iteration 31453 : loss : 0.053478, loss_ce: 0.002799, loss_dice: 0.104158
[12:57:22.685] TRAIN: iteration 31454 : loss : 0.111199, loss_ce: 0.002053, loss_dice: 0.220346
[12:57:22.895] TRAIN: iteration 31455 : loss : 0.251290, loss_ce: 0.002411, loss_dice: 0.500169
[12:57:23.109] TRAIN: iteration 31456 : loss : 0.250542, loss_ce: 0.001048, loss_dice: 0.500037
[12:57:23.323] TRAIN: iteration 31457 : loss : 0.222295, loss_ce: 0.001818, loss_dice: 0.442771
[12:57:23.536] TRAIN: iteration 31458 : loss : 0.025409, loss_ce: 0.003097, loss_dice: 0.047721
[12:57:23.743] TRAIN: iteration 31459 : loss : 0.124633, loss_ce: 0.005980, loss_dice: 0.243286
[12:57:23.951] TRAIN: iteration 31460 : loss : 0.250599, loss_ce: 0.001163, loss_dice: 0.500036
[12:57:24.189] TRAIN: iteration 31461 : loss : 0.042473, loss_ce: 0.001862, loss_dice: 0.083085
[12:57:24.398] TRAIN: iteration 31462 : loss : 0.026963, loss_ce: 0.003521, loss_dice: 0.050405
[12:57:24.606] TRAIN: iteration 31463 : loss : 0.250528, loss_ce: 0.001024, loss_dice: 0.500031
[12:57:24.816] TRAIN: iteration 31464 : loss : 0.187169, loss_ce: 0.004606, loss_dice: 0.369732
[12:57:25.025] TRAIN: iteration 31465 : loss : 0.063478, loss_ce: 0.004659, loss_dice: 0.122298
[12:57:25.232] TRAIN: iteration 31466 : loss : 0.184026, loss_ce: 0.001017, loss_dice: 0.367036
[12:57:25.443] TRAIN: iteration 31467 : loss : 0.062970, loss_ce: 0.000743, loss_dice: 0.125197
[12:57:25.657] TRAIN: iteration 31468 : loss : 0.250641, loss_ce: 0.001239, loss_dice: 0.500042
[12:57:25.876] TRAIN: iteration 31469 : loss : 0.058346, loss_ce: 0.000800, loss_dice: 0.115892
[12:57:26.084] TRAIN: iteration 31470 : loss : 0.037606, loss_ce: 0.001661, loss_dice: 0.073551
[12:57:26.293] TRAIN: iteration 31471 : loss : 0.026879, loss_ce: 0.004798, loss_dice: 0.048960
[12:57:26.501] TRAIN: iteration 31472 : loss : 0.183589, loss_ce: 0.002326, loss_dice: 0.364852
[12:57:26.711] TRAIN: iteration 31473 : loss : 0.030133, loss_ce: 0.004375, loss_dice: 0.055891
[12:57:26.921] TRAIN: iteration 31474 : loss : 0.098005, loss_ce: 0.006247, loss_dice: 0.189762
[12:57:27.128] TRAIN: iteration 31475 : loss : 0.204082, loss_ce: 0.003196, loss_dice: 0.404968
[12:57:27.342] TRAIN: iteration 31476 : loss : 0.044707, loss_ce: 0.000770, loss_dice: 0.088643
[12:57:27.554] TRAIN: iteration 31477 : loss : 0.056269, loss_ce: 0.004319, loss_dice: 0.108218
[12:57:27.763] TRAIN: iteration 31478 : loss : 0.144526, loss_ce: 0.001040, loss_dice: 0.288011
[12:57:27.971] TRAIN: iteration 31479 : loss : 0.169292, loss_ce: 0.001080, loss_dice: 0.337504
[12:57:28.180] TRAIN: iteration 31480 : loss : 0.250556, loss_ce: 0.001062, loss_dice: 0.500051
[12:57:28.416] TRAIN: iteration 31481 : loss : 0.196593, loss_ce: 0.001338, loss_dice: 0.391849
[12:57:28.627] TRAIN: iteration 31482 : loss : 0.139692, loss_ce: 0.001682, loss_dice: 0.277701
[12:57:28.842] TRAIN: iteration 31483 : loss : 0.250268, loss_ce: 0.000525, loss_dice: 0.500010
[12:57:29.053] TRAIN: iteration 31484 : loss : 0.141434, loss_ce: 0.001025, loss_dice: 0.281843
[12:57:29.267] TRAIN: iteration 31485 : loss : 0.030627, loss_ce: 0.001040, loss_dice: 0.060213
[12:57:29.475] TRAIN: iteration 31486 : loss : 0.056394, loss_ce: 0.002760, loss_dice: 0.110028
[12:57:29.689] TRAIN: iteration 31487 : loss : 0.036406, loss_ce: 0.000762, loss_dice: 0.072049
[12:57:29.898] TRAIN: iteration 31488 : loss : 0.051736, loss_ce: 0.000819, loss_dice: 0.102653
[12:57:30.106] TRAIN: iteration 31489 : loss : 0.106916, loss_ce: 0.007484, loss_dice: 0.206349
[12:57:30.315] TRAIN: iteration 31490 : loss : 0.250305, loss_ce: 0.000595, loss_dice: 0.500014
[12:57:30.523] TRAIN: iteration 31491 : loss : 0.250193, loss_ce: 0.000382, loss_dice: 0.500004
[12:57:30.737] TRAIN: iteration 31492 : loss : 0.044794, loss_ce: 0.001623, loss_dice: 0.087965
[12:57:30.949] TRAIN: iteration 31493 : loss : 0.032204, loss_ce: 0.000512, loss_dice: 0.063895
[12:57:31.160] TRAIN: iteration 31494 : loss : 0.184951, loss_ce: 0.001043, loss_dice: 0.368859
[12:57:31.370] TRAIN: iteration 31495 : loss : 0.148236, loss_ce: 0.001708, loss_dice: 0.294764
[12:57:31.590] TRAIN: iteration 31496 : loss : 0.012937, loss_ce: 0.000455, loss_dice: 0.025420
[12:57:31.797] TRAIN: iteration 31497 : loss : 0.078176, loss_ce: 0.005495, loss_dice: 0.150857
[12:57:32.005] TRAIN: iteration 31498 : loss : 0.250297, loss_ce: 0.000575, loss_dice: 0.500018
[12:57:32.215] TRAIN: iteration 31499 : loss : 0.087694, loss_ce: 0.009633, loss_dice: 0.165755
[12:57:32.422] TRAIN: iteration 31500 : loss : 0.057645, loss_ce: 0.002231, loss_dice: 0.113059
[12:57:32.663] TRAIN: iteration 31501 : loss : 0.092493, loss_ce: 0.002957, loss_dice: 0.182029
[12:57:32.874] TRAIN: iteration 31502 : loss : 0.094919, loss_ce: 0.001049, loss_dice: 0.188789
[12:57:33.082] TRAIN: iteration 31503 : loss : 0.043026, loss_ce: 0.003248, loss_dice: 0.082803
[12:57:33.290] TRAIN: iteration 31504 : loss : 0.018607, loss_ce: 0.003066, loss_dice: 0.034148
[12:57:33.506] TRAIN: iteration 31505 : loss : 0.219628, loss_ce: 0.004503, loss_dice: 0.434753
[12:57:33.714] TRAIN: iteration 31506 : loss : 0.252250, loss_ce: 0.004578, loss_dice: 0.499922
[12:57:33.922] TRAIN: iteration 31507 : loss : 0.093490, loss_ce: 0.001399, loss_dice: 0.185580
[12:57:34.131] TRAIN: iteration 31508 : loss : 0.137938, loss_ce: 0.000519, loss_dice: 0.275356
[12:57:34.341] TRAIN: iteration 31509 : loss : 0.031415, loss_ce: 0.000353, loss_dice: 0.062477
[12:57:34.550] TRAIN: iteration 31510 : loss : 0.132418, loss_ce: 0.000771, loss_dice: 0.264064
[12:57:34.764] TRAIN: iteration 31511 : loss : 0.055767, loss_ce: 0.003431, loss_dice: 0.108103
[12:57:34.972] TRAIN: iteration 31512 : loss : 0.162635, loss_ce: 0.002144, loss_dice: 0.323126
[12:57:35.180] TRAIN: iteration 31513 : loss : 0.034567, loss_ce: 0.002398, loss_dice: 0.066736
[12:57:35.388] TRAIN: iteration 31514 : loss : 0.104148, loss_ce: 0.003982, loss_dice: 0.204314
[12:57:35.597] TRAIN: iteration 31515 : loss : 0.039502, loss_ce: 0.003143, loss_dice: 0.075860
[12:57:35.806] TRAIN: iteration 31516 : loss : 0.153021, loss_ce: 0.003353, loss_dice: 0.302690
[12:57:36.023] TRAIN: iteration 31517 : loss : 0.026020, loss_ce: 0.004705, loss_dice: 0.047335
[12:57:36.234] TRAIN: iteration 31518 : loss : 0.250213, loss_ce: 0.000420, loss_dice: 0.500006
[12:57:36.442] TRAIN: iteration 31519 : loss : 0.042681, loss_ce: 0.002576, loss_dice: 0.082787
[12:57:36.658] TRAIN: iteration 31520 : loss : 0.059877, loss_ce: 0.005339, loss_dice: 0.114414
[12:57:36.659] NaN or Inf found in input tensor.
[12:57:36.872] TRAIN: iteration 31521 : loss : 0.037558, loss_ce: 0.000559, loss_dice: 0.074556
[12:57:37.080] TRAIN: iteration 31522 : loss : 0.051843, loss_ce: 0.003214, loss_dice: 0.100472
[12:57:37.288] TRAIN: iteration 31523 : loss : 0.132594, loss_ce: 0.007006, loss_dice: 0.258182
[12:57:37.497] TRAIN: iteration 31524 : loss : 0.082018, loss_ce: 0.003483, loss_dice: 0.160553
[12:57:37.704] TRAIN: iteration 31525 : loss : 0.083276, loss_ce: 0.004903, loss_dice: 0.161650
[12:57:37.912] TRAIN: iteration 31526 : loss : 0.044141, loss_ce: 0.004226, loss_dice: 0.084056
[12:57:38.121] TRAIN: iteration 31527 : loss : 0.047402, loss_ce: 0.007245, loss_dice: 0.087559
[12:57:38.337] TRAIN: iteration 31528 : loss : 0.037194, loss_ce: 0.001135, loss_dice: 0.073254
[12:57:38.551] TRAIN: iteration 31529 : loss : 0.204302, loss_ce: 0.001236, loss_dice: 0.407369
[12:57:38.759] TRAIN: iteration 31530 : loss : 0.111886, loss_ce: 0.004294, loss_dice: 0.219479
[12:57:38.965] TRAIN: iteration 31531 : loss : 0.077027, loss_ce: 0.003562, loss_dice: 0.150491
[12:57:39.173] TRAIN: iteration 31532 : loss : 0.072600, loss_ce: 0.001088, loss_dice: 0.144113
[12:57:39.381] TRAIN: iteration 31533 : loss : 0.069302, loss_ce: 0.002361, loss_dice: 0.136242
[12:57:39.588] TRAIN: iteration 31534 : loss : 0.021994, loss_ce: 0.003482, loss_dice: 0.040506
[12:57:39.799] TRAIN: iteration 31535 : loss : 0.030575, loss_ce: 0.000504, loss_dice: 0.060646
[12:57:40.006] TRAIN: iteration 31536 : loss : 0.070132, loss_ce: 0.002667, loss_dice: 0.137596
[12:57:40.215] TRAIN: iteration 31537 : loss : 0.250591, loss_ce: 0.001119, loss_dice: 0.500064
[12:57:40.422] TRAIN: iteration 31538 : loss : 0.059682, loss_ce: 0.001678, loss_dice: 0.117686
[12:57:40.631] TRAIN: iteration 31539 : loss : 0.048924, loss_ce: 0.000891, loss_dice: 0.096956
[12:57:40.838] TRAIN: iteration 31540 : loss : 0.035566, loss_ce: 0.001793, loss_dice: 0.069338
[12:57:41.056] TRAIN: iteration 31541 : loss : 0.069440, loss_ce: 0.004982, loss_dice: 0.133897
[12:57:41.266] TRAIN: iteration 31542 : loss : 0.057281, loss_ce: 0.001418, loss_dice: 0.113144
[12:57:41.480] TRAIN: iteration 31543 : loss : 0.068779, loss_ce: 0.006576, loss_dice: 0.130983
[12:57:41.688] TRAIN: iteration 31544 : loss : 0.136947, loss_ce: 0.002237, loss_dice: 0.271658
[12:57:41.898] TRAIN: iteration 31545 : loss : 0.164499, loss_ce: 0.002773, loss_dice: 0.326226
[12:57:42.108] TRAIN: iteration 31546 : loss : 0.048211, loss_ce: 0.003681, loss_dice: 0.092740
[12:57:42.316] TRAIN: iteration 31547 : loss : 0.089029, loss_ce: 0.010849, loss_dice: 0.167209
[12:57:42.523] TRAIN: iteration 31548 : loss : 0.078162, loss_ce: 0.002147, loss_dice: 0.154178
[12:57:42.731] TRAIN: iteration 31549 : loss : 0.032774, loss_ce: 0.002155, loss_dice: 0.063393
[12:57:42.948] TRAIN: iteration 31550 : loss : 0.075949, loss_ce: 0.001572, loss_dice: 0.150326
[12:57:43.156] TRAIN: iteration 31551 : loss : 0.041126, loss_ce: 0.000433, loss_dice: 0.081819
[12:57:44.168] TRAIN: iteration 31552 : loss : 0.055870, loss_ce: 0.007552, loss_dice: 0.104189
[12:57:44.382] TRAIN: iteration 31553 : loss : 0.048304, loss_ce: 0.006394, loss_dice: 0.090215
[12:57:44.598] TRAIN: iteration 31554 : loss : 0.169555, loss_ce: 0.000826, loss_dice: 0.338284
[12:57:44.808] TRAIN: iteration 31555 : loss : 0.060016, loss_ce: 0.004281, loss_dice: 0.115750
[12:57:45.015] TRAIN: iteration 31556 : loss : 0.213587, loss_ce: 0.001543, loss_dice: 0.425632
[12:57:45.227] TRAIN: iteration 31557 : loss : 0.027728, loss_ce: 0.001639, loss_dice: 0.053816
[12:57:45.441] TRAIN: iteration 31558 : loss : 0.147220, loss_ce: 0.003040, loss_dice: 0.291400
[12:57:45.687] TRAIN: iteration 31559 : loss : 0.250486, loss_ce: 0.001036, loss_dice: 0.499936
[12:57:45.896] TRAIN: iteration 31560 : loss : 0.059368, loss_ce: 0.003132, loss_dice: 0.115605
[12:57:46.135] TRAIN: iteration 31561 : loss : 0.260835, loss_ce: 0.021280, loss_dice: 0.500391
[12:57:46.350] TRAIN: iteration 31562 : loss : 0.134996, loss_ce: 0.004424, loss_dice: 0.265567
[12:57:46.561] TRAIN: iteration 31563 : loss : 0.081808, loss_ce: 0.001455, loss_dice: 0.162161
[12:57:46.771] TRAIN: iteration 31564 : loss : 0.068944, loss_ce: 0.003508, loss_dice: 0.134380
[12:57:46.988] TRAIN: iteration 31565 : loss : 0.155455, loss_ce: 0.001336, loss_dice: 0.309575
[12:57:47.202] TRAIN: iteration 31566 : loss : 0.115541, loss_ce: 0.001317, loss_dice: 0.229764
[12:57:47.412] TRAIN: iteration 31567 : loss : 0.121067, loss_ce: 0.006284, loss_dice: 0.235850
[12:57:47.622] TRAIN: iteration 31568 : loss : 0.206242, loss_ce: 0.002088, loss_dice: 0.410397
[12:57:47.833] TRAIN: iteration 31569 : loss : 0.054679, loss_ce: 0.005125, loss_dice: 0.104233
[12:57:48.044] TRAIN: iteration 31570 : loss : 0.076099, loss_ce: 0.003725, loss_dice: 0.148473
[12:57:48.253] TRAIN: iteration 31571 : loss : 0.092301, loss_ce: 0.004057, loss_dice: 0.180544
[12:57:48.460] TRAIN: iteration 31572 : loss : 0.245628, loss_ce: 0.001653, loss_dice: 0.489604
[12:57:48.668] TRAIN: iteration 31573 : loss : 0.096468, loss_ce: 0.004819, loss_dice: 0.188117
[12:57:48.878] TRAIN: iteration 31574 : loss : 0.074590, loss_ce: 0.001314, loss_dice: 0.147867
[12:57:49.088] TRAIN: iteration 31575 : loss : 0.245243, loss_ce: 0.001466, loss_dice: 0.489019
[12:57:49.296] TRAIN: iteration 31576 : loss : 0.116397, loss_ce: 0.009048, loss_dice: 0.223745
[12:57:49.510] TRAIN: iteration 31577 : loss : 0.142876, loss_ce: 0.003994, loss_dice: 0.281759
[12:57:49.746] TRAIN: iteration 31578 : loss : 0.019089, loss_ce: 0.000589, loss_dice: 0.037590
[12:57:49.954] TRAIN: iteration 31579 : loss : 0.125943, loss_ce: 0.002196, loss_dice: 0.249690
[12:57:50.165] TRAIN: iteration 31580 : loss : 0.071478, loss_ce: 0.002654, loss_dice: 0.140301
[12:57:50.410] TRAIN: iteration 31581 : loss : 0.118905, loss_ce: 0.005637, loss_dice: 0.232173
[12:57:50.620] TRAIN: iteration 31582 : loss : 0.227181, loss_ce: 0.001926, loss_dice: 0.452435
[12:57:50.833] TRAIN: iteration 31583 : loss : 0.039900, loss_ce: 0.002473, loss_dice: 0.077327
[12:57:51.041] TRAIN: iteration 31584 : loss : 0.036735, loss_ce: 0.001615, loss_dice: 0.071854
[12:57:51.252] TRAIN: iteration 31585 : loss : 0.250237, loss_ce: 0.000468, loss_dice: 0.500007
[12:57:51.460] TRAIN: iteration 31586 : loss : 0.161913, loss_ce: 0.003648, loss_dice: 0.320177
[12:57:51.676] TRAIN: iteration 31587 : loss : 0.038687, loss_ce: 0.005163, loss_dice: 0.072210
[12:57:51.885] TRAIN: iteration 31588 : loss : 0.236837, loss_ce: 0.002242, loss_dice: 0.471433
[12:57:52.093] TRAIN: iteration 31589 : loss : 0.120486, loss_ce: 0.001396, loss_dice: 0.239575
[12:57:52.303] TRAIN: iteration 31590 : loss : 0.250845, loss_ce: 0.001598, loss_dice: 0.500092
[12:57:52.511] TRAIN: iteration 31591 : loss : 0.127048, loss_ce: 0.001628, loss_dice: 0.252468
[12:57:52.718] TRAIN: iteration 31592 : loss : 0.034238, loss_ce: 0.001291, loss_dice: 0.067185
[12:57:52.926] TRAIN: iteration 31593 : loss : 0.088110, loss_ce: 0.003289, loss_dice: 0.172932
[12:57:53.134] TRAIN: iteration 31594 : loss : 0.247555, loss_ce: 0.000820, loss_dice: 0.494290
[12:57:53.344] TRAIN: iteration 31595 : loss : 0.247608, loss_ce: 0.001136, loss_dice: 0.494080
[12:57:53.551] TRAIN: iteration 31596 : loss : 0.138134, loss_ce: 0.002904, loss_dice: 0.273364
[12:57:53.760] TRAIN: iteration 31597 : loss : 0.026978, loss_ce: 0.001892, loss_dice: 0.052065
[12:57:53.970] TRAIN: iteration 31598 : loss : 0.111150, loss_ce: 0.001063, loss_dice: 0.221237
[12:57:54.181] TRAIN: iteration 31599 : loss : 0.062685, loss_ce: 0.002701, loss_dice: 0.122670
[12:57:54.392] TRAIN: iteration 31600 : loss : 0.028193, loss_ce: 0.001446, loss_dice: 0.054940
[12:57:54.625] TRAIN: iteration 31601 : loss : 0.250619, loss_ce: 0.001181, loss_dice: 0.500057
[12:57:54.854] TRAIN: iteration 31602 : loss : 0.047795, loss_ce: 0.004652, loss_dice: 0.090938
[12:57:55.062] TRAIN: iteration 31603 : loss : 0.203211, loss_ce: 0.001922, loss_dice: 0.404500
[12:57:55.271] TRAIN: iteration 31604 : loss : 0.024684, loss_ce: 0.001256, loss_dice: 0.048113
[12:57:55.485] TRAIN: iteration 31605 : loss : 0.057165, loss_ce: 0.002896, loss_dice: 0.111433
[12:57:55.697] TRAIN: iteration 31606 : loss : 0.153656, loss_ce: 0.004057, loss_dice: 0.303256
[12:57:55.906] TRAIN: iteration 31607 : loss : 0.082065, loss_ce: 0.001234, loss_dice: 0.162895
[12:57:56.115] TRAIN: iteration 31608 : loss : 0.108672, loss_ce: 0.005578, loss_dice: 0.211765
[12:57:56.324] TRAIN: iteration 31609 : loss : 0.038343, loss_ce: 0.000479, loss_dice: 0.076207
[12:57:56.531] TRAIN: iteration 31610 : loss : 0.047520, loss_ce: 0.004913, loss_dice: 0.090128
[12:57:56.738] TRAIN: iteration 31611 : loss : 0.183048, loss_ce: 0.005216, loss_dice: 0.360880
[12:57:56.947] TRAIN: iteration 31612 : loss : 0.249597, loss_ce: 0.000547, loss_dice: 0.498648
[12:57:57.161] TRAIN: iteration 31613 : loss : 0.115999, loss_ce: 0.003610, loss_dice: 0.228389
[12:57:57.371] TRAIN: iteration 31614 : loss : 0.250292, loss_ce: 0.000573, loss_dice: 0.500010
[12:57:57.579] TRAIN: iteration 31615 : loss : 0.125364, loss_ce: 0.004927, loss_dice: 0.245800
[12:57:57.794] TRAIN: iteration 31616 : loss : 0.047053, loss_ce: 0.001819, loss_dice: 0.092287
[12:57:58.010] TRAIN: iteration 31617 : loss : 0.124971, loss_ce: 0.001766, loss_dice: 0.248176
[12:57:58.221] TRAIN: iteration 31618 : loss : 0.250328, loss_ce: 0.000629, loss_dice: 0.500028
[12:57:58.430] TRAIN: iteration 31619 : loss : 0.152041, loss_ce: 0.002132, loss_dice: 0.301949
[12:57:58.641] TRAIN: iteration 31620 : loss : 0.146784, loss_ce: 0.002008, loss_dice: 0.291561
[12:57:58.881] TRAIN: iteration 31621 : loss : 0.037667, loss_ce: 0.001011, loss_dice: 0.074323
[12:57:59.089] TRAIN: iteration 31622 : loss : 0.034579, loss_ce: 0.000994, loss_dice: 0.068163
[12:57:59.298] TRAIN: iteration 31623 : loss : 0.026277, loss_ce: 0.000491, loss_dice: 0.052063
[12:57:59.513] TRAIN: iteration 31624 : loss : 0.040219, loss_ce: 0.005240, loss_dice: 0.075198
[12:57:59.728] TRAIN: iteration 31625 : loss : 0.032824, loss_ce: 0.003592, loss_dice: 0.062056
[12:57:59.935] TRAIN: iteration 31626 : loss : 0.070431, loss_ce: 0.002705, loss_dice: 0.138157
[12:58:00.143] TRAIN: iteration 31627 : loss : 0.067792, loss_ce: 0.002152, loss_dice: 0.133433
[12:58:00.354] TRAIN: iteration 31628 : loss : 0.082311, loss_ce: 0.003021, loss_dice: 0.161601
[12:58:00.569] TRAIN: iteration 31629 : loss : 0.029476, loss_ce: 0.003470, loss_dice: 0.055483
[12:58:00.780] TRAIN: iteration 31630 : loss : 0.087290, loss_ce: 0.003444, loss_dice: 0.171135
[12:58:00.994] TRAIN: iteration 31631 : loss : 0.007657, loss_ce: 0.000868, loss_dice: 0.014446
[12:58:01.202] TRAIN: iteration 31632 : loss : 0.026009, loss_ce: 0.002333, loss_dice: 0.049686
[12:58:01.410] TRAIN: iteration 31633 : loss : 0.046039, loss_ce: 0.001638, loss_dice: 0.090441
[12:58:01.617] TRAIN: iteration 31634 : loss : 0.138095, loss_ce: 0.004816, loss_dice: 0.271374
[12:58:01.824] TRAIN: iteration 31635 : loss : 0.032210, loss_ce: 0.001015, loss_dice: 0.063405
[12:58:02.033] TRAIN: iteration 31636 : loss : 0.042502, loss_ce: 0.001042, loss_dice: 0.083962
[12:58:02.247] TRAIN: iteration 31637 : loss : 0.078869, loss_ce: 0.002686, loss_dice: 0.155051
[12:58:02.454] TRAIN: iteration 31638 : loss : 0.025401, loss_ce: 0.000693, loss_dice: 0.050109
[12:58:02.661] TRAIN: iteration 31639 : loss : 0.053186, loss_ce: 0.001777, loss_dice: 0.104594
[12:58:02.869] TRAIN: iteration 31640 : loss : 0.072691, loss_ce: 0.001494, loss_dice: 0.143887
[12:58:03.120] TRAIN: iteration 31641 : loss : 0.228590, loss_ce: 0.002398, loss_dice: 0.454782
[12:58:03.336] TRAIN: iteration 31642 : loss : 0.045993, loss_ce: 0.001453, loss_dice: 0.090533
[12:58:03.543] TRAIN: iteration 31643 : loss : 0.018634, loss_ce: 0.000890, loss_dice: 0.036378
[12:58:03.750] TRAIN: iteration 31644 : loss : 0.059213, loss_ce: 0.006205, loss_dice: 0.112221
[12:58:03.958] TRAIN: iteration 31645 : loss : 0.097819, loss_ce: 0.001482, loss_dice: 0.194156
[12:58:04.168] TRAIN: iteration 31646 : loss : 0.251741, loss_ce: 0.003247, loss_dice: 0.500236
[12:58:04.377] TRAIN: iteration 31647 : loss : 0.166239, loss_ce: 0.002347, loss_dice: 0.330132
[12:58:04.586] TRAIN: iteration 31648 : loss : 0.121196, loss_ce: 0.001430, loss_dice: 0.240962
[12:58:04.797] TRAIN: iteration 31649 : loss : 0.054878, loss_ce: 0.002507, loss_dice: 0.107250
[12:58:05.013] TRAIN: iteration 31650 : loss : 0.041754, loss_ce: 0.001370, loss_dice: 0.082138
[12:58:05.219] TRAIN: iteration 31651 : loss : 0.042912, loss_ce: 0.001183, loss_dice: 0.084641
[12:58:05.426] TRAIN: iteration 31652 : loss : 0.063107, loss_ce: 0.001960, loss_dice: 0.124253
[12:58:05.634] TRAIN: iteration 31653 : loss : 0.032798, loss_ce: 0.001612, loss_dice: 0.063984
[12:58:05.844] TRAIN: iteration 31654 : loss : 0.035599, loss_ce: 0.001681, loss_dice: 0.069517
[12:58:06.054] TRAIN: iteration 31655 : loss : 0.036990, loss_ce: 0.003551, loss_dice: 0.070429
[12:58:06.263] TRAIN: iteration 31656 : loss : 0.099045, loss_ce: 0.004233, loss_dice: 0.193856
[12:58:06.471] TRAIN: iteration 31657 : loss : 0.030933, loss_ce: 0.004463, loss_dice: 0.057402
[12:58:06.680] TRAIN: iteration 31658 : loss : 0.123512, loss_ce: 0.001133, loss_dice: 0.245891
[12:58:06.889] TRAIN: iteration 31659 : loss : 0.250526, loss_ce: 0.001028, loss_dice: 0.500023
[12:58:07.098] TRAIN: iteration 31660 : loss : 0.029273, loss_ce: 0.000626, loss_dice: 0.057920
[12:58:07.335] TRAIN: iteration 31661 : loss : 0.108636, loss_ce: 0.014706, loss_dice: 0.202566
[12:58:07.543] TRAIN: iteration 31662 : loss : 0.038647, loss_ce: 0.000917, loss_dice: 0.076376
[12:58:07.758] TRAIN: iteration 31663 : loss : 0.027775, loss_ce: 0.000963, loss_dice: 0.054586
[12:58:07.969] TRAIN: iteration 31664 : loss : 0.254221, loss_ce: 0.007852, loss_dice: 0.500590
[12:58:08.188] TRAIN: iteration 31665 : loss : 0.034362, loss_ce: 0.002012, loss_dice: 0.066711
[12:58:08.397] TRAIN: iteration 31666 : loss : 0.041462, loss_ce: 0.002138, loss_dice: 0.080787
[12:58:08.604] TRAIN: iteration 31667 : loss : 0.090370, loss_ce: 0.005979, loss_dice: 0.174761
[12:58:08.812] TRAIN: iteration 31668 : loss : 0.089152, loss_ce: 0.001664, loss_dice: 0.176640
[12:58:09.022] TRAIN: iteration 31669 : loss : 0.091316, loss_ce: 0.003338, loss_dice: 0.179294
[12:58:09.232] TRAIN: iteration 31670 : loss : 0.096318, loss_ce: 0.000719, loss_dice: 0.191916
[12:58:09.440] TRAIN: iteration 31671 : loss : 0.058537, loss_ce: 0.002139, loss_dice: 0.114934
[12:58:09.647] TRAIN: iteration 31672 : loss : 0.250522, loss_ce: 0.001004, loss_dice: 0.500040
[12:58:09.855] TRAIN: iteration 31673 : loss : 0.091847, loss_ce: 0.000684, loss_dice: 0.183011
[12:58:10.068] TRAIN: iteration 31674 : loss : 0.031683, loss_ce: 0.002137, loss_dice: 0.061230
[12:58:10.279] TRAIN: iteration 31675 : loss : 0.100267, loss_ce: 0.005655, loss_dice: 0.194880
[12:58:10.488] TRAIN: iteration 31676 : loss : 0.122953, loss_ce: 0.001435, loss_dice: 0.244470
[12:58:10.698] TRAIN: iteration 31677 : loss : 0.068727, loss_ce: 0.000913, loss_dice: 0.136541
[12:58:10.908] TRAIN: iteration 31678 : loss : 0.055431, loss_ce: 0.002340, loss_dice: 0.108521
[12:58:11.120] TRAIN: iteration 31679 : loss : 0.099727, loss_ce: 0.001024, loss_dice: 0.198430
[12:58:11.335] TRAIN: iteration 31680 : loss : 0.100552, loss_ce: 0.005056, loss_dice: 0.196047
[12:58:11.568] TRAIN: iteration 31681 : loss : 0.022224, loss_ce: 0.001971, loss_dice: 0.042476
[12:58:11.778] TRAIN: iteration 31682 : loss : 0.250234, loss_ce: 0.000461, loss_dice: 0.500007
[12:58:11.989] TRAIN: iteration 31683 : loss : 0.051245, loss_ce: 0.005700, loss_dice: 0.096789
[12:58:12.254] TRAIN: iteration 31684 : loss : 0.092208, loss_ce: 0.000663, loss_dice: 0.183753
[12:58:12.462] TRAIN: iteration 31685 : loss : 0.245399, loss_ce: 0.002100, loss_dice: 0.488699
[12:58:12.669] TRAIN: iteration 31686 : loss : 0.043886, loss_ce: 0.004118, loss_dice: 0.083654
[12:58:12.877] TRAIN: iteration 31687 : loss : 0.086039, loss_ce: 0.002505, loss_dice: 0.169574
[12:58:13.085] TRAIN: iteration 31688 : loss : 0.055057, loss_ce: 0.001101, loss_dice: 0.109013
[12:58:13.295] TRAIN: iteration 31689 : loss : 0.122819, loss_ce: 0.007682, loss_dice: 0.237956
[12:58:13.505] TRAIN: iteration 31690 : loss : 0.199304, loss_ce: 0.000729, loss_dice: 0.397878
[12:58:13.713] TRAIN: iteration 31691 : loss : 0.218552, loss_ce: 0.003541, loss_dice: 0.433564
[12:58:13.927] TRAIN: iteration 31692 : loss : 0.027541, loss_ce: 0.002385, loss_dice: 0.052698
[12:58:14.136] TRAIN: iteration 31693 : loss : 0.052911, loss_ce: 0.005146, loss_dice: 0.100676
[12:58:14.344] TRAIN: iteration 31694 : loss : 0.034028, loss_ce: 0.002646, loss_dice: 0.065411
[12:58:14.554] TRAIN: iteration 31695 : loss : 0.197867, loss_ce: 0.032932, loss_dice: 0.362801
[12:58:15.109] TRAIN: iteration 31696 : loss : 0.030585, loss_ce: 0.001973, loss_dice: 0.059197
[12:58:15.318] TRAIN: iteration 31697 : loss : 0.250691, loss_ce: 0.001297, loss_dice: 0.500085
[12:58:15.526] TRAIN: iteration 31698 : loss : 0.108415, loss_ce: 0.003333, loss_dice: 0.213497
[12:58:15.734] TRAIN: iteration 31699 : loss : 0.137938, loss_ce: 0.028102, loss_dice: 0.247774
[12:58:15.942] TRAIN: iteration 31700 : loss : 0.071752, loss_ce: 0.003762, loss_dice: 0.139742
[12:58:16.177] TRAIN: iteration 31701 : loss : 0.072740, loss_ce: 0.003716, loss_dice: 0.141763
[12:58:16.389] TRAIN: iteration 31702 : loss : 0.116287, loss_ce: 0.001208, loss_dice: 0.231367
[12:58:16.596] TRAIN: iteration 31703 : loss : 0.073492, loss_ce: 0.003120, loss_dice: 0.143864
[12:58:16.804] TRAIN: iteration 31704 : loss : 0.087449, loss_ce: 0.004045, loss_dice: 0.170853
[12:58:17.012] TRAIN: iteration 31705 : loss : 0.242786, loss_ce: 0.000871, loss_dice: 0.484700
[12:58:17.220] TRAIN: iteration 31706 : loss : 0.108967, loss_ce: 0.005455, loss_dice: 0.212480
[12:58:17.435] TRAIN: iteration 31707 : loss : 0.086101, loss_ce: 0.004682, loss_dice: 0.167520
[12:58:17.649] TRAIN: iteration 31708 : loss : 0.027423, loss_ce: 0.001389, loss_dice: 0.053456
[12:58:17.857] TRAIN: iteration 31709 : loss : 0.075715, loss_ce: 0.005959, loss_dice: 0.145471
[12:58:18.065] TRAIN: iteration 31710 : loss : 0.029775, loss_ce: 0.002451, loss_dice: 0.057099
[12:58:18.273] TRAIN: iteration 31711 : loss : 0.106841, loss_ce: 0.001667, loss_dice: 0.212016
[12:58:18.481] TRAIN: iteration 31712 : loss : 0.041946, loss_ce: 0.001505, loss_dice: 0.082387
[12:58:18.689] TRAIN: iteration 31713 : loss : 0.250330, loss_ce: 0.000635, loss_dice: 0.500026
[12:58:18.897] TRAIN: iteration 31714 : loss : 0.109214, loss_ce: 0.001984, loss_dice: 0.216443
[12:58:19.105] TRAIN: iteration 31715 : loss : 0.107210, loss_ce: 0.009441, loss_dice: 0.204979
[12:58:19.316] TRAIN: iteration 31716 : loss : 0.196455, loss_ce: 0.002423, loss_dice: 0.390486
[12:58:19.525] TRAIN: iteration 31717 : loss : 0.024382, loss_ce: 0.000610, loss_dice: 0.048155
[12:58:19.735] TRAIN: iteration 31718 : loss : 0.049135, loss_ce: 0.005347, loss_dice: 0.092922
[12:58:19.943] TRAIN: iteration 31719 : loss : 0.107778, loss_ce: 0.004603, loss_dice: 0.210954
[12:58:20.150] TRAIN: iteration 31720 : loss : 0.202473, loss_ce: 0.001130, loss_dice: 0.403815
[12:58:20.389] TRAIN: iteration 31721 : loss : 0.019944, loss_ce: 0.001015, loss_dice: 0.038874
[12:58:20.596] TRAIN: iteration 31722 : loss : 0.035359, loss_ce: 0.000680, loss_dice: 0.070039
[12:58:20.837] TRAIN: iteration 31723 : loss : 0.113033, loss_ce: 0.002020, loss_dice: 0.224046
[12:58:21.046] TRAIN: iteration 31724 : loss : 0.150606, loss_ce: 0.002755, loss_dice: 0.298457
[12:58:21.254] TRAIN: iteration 31725 : loss : 0.037721, loss_ce: 0.001771, loss_dice: 0.073672
[12:58:21.461] TRAIN: iteration 31726 : loss : 0.066196, loss_ce: 0.000774, loss_dice: 0.131618
[12:58:21.670] TRAIN: iteration 31727 : loss : 0.260610, loss_ce: 0.021279, loss_dice: 0.499940
[12:58:21.877] TRAIN: iteration 31728 : loss : 0.047993, loss_ce: 0.006269, loss_dice: 0.089718
[12:58:22.084] TRAIN: iteration 31729 : loss : 0.053411, loss_ce: 0.000896, loss_dice: 0.105925
[12:58:22.295] TRAIN: iteration 31730 : loss : 0.088193, loss_ce: 0.003322, loss_dice: 0.173064
[12:58:22.504] TRAIN: iteration 31731 : loss : 0.057863, loss_ce: 0.002143, loss_dice: 0.113584
[12:58:22.712] TRAIN: iteration 31732 : loss : 0.032189, loss_ce: 0.005779, loss_dice: 0.058599
[12:58:22.920] TRAIN: iteration 31733 : loss : 0.060074, loss_ce: 0.001744, loss_dice: 0.118404
[12:58:23.138] TRAIN: iteration 31734 : loss : 0.017793, loss_ce: 0.000497, loss_dice: 0.035088
[12:58:23.354] TRAIN: iteration 31735 : loss : 0.157253, loss_ce: 0.003362, loss_dice: 0.311144
[12:58:23.570] TRAIN: iteration 31736 : loss : 0.162072, loss_ce: 0.020569, loss_dice: 0.303576
[12:58:23.779] TRAIN: iteration 31737 : loss : 0.060503, loss_ce: 0.002075, loss_dice: 0.118931
[12:58:23.991] TRAIN: iteration 31738 : loss : 0.118933, loss_ce: 0.001566, loss_dice: 0.236300
[12:58:24.223] TRAIN: iteration 31739 : loss : 0.250963, loss_ce: 0.001822, loss_dice: 0.500103
[12:58:24.434] TRAIN: iteration 31740 : loss : 0.114530, loss_ce: 0.003821, loss_dice: 0.225239
[12:58:24.675] TRAIN: iteration 31741 : loss : 0.040449, loss_ce: 0.005617, loss_dice: 0.075281
[12:58:24.883] TRAIN: iteration 31742 : loss : 0.012586, loss_ce: 0.000717, loss_dice: 0.024456
[12:58:25.092] TRAIN: iteration 31743 : loss : 0.051935, loss_ce: 0.002795, loss_dice: 0.101075
[12:58:25.301] TRAIN: iteration 31744 : loss : 0.250446, loss_ce: 0.000859, loss_dice: 0.500034
[12:58:25.511] TRAIN: iteration 31745 : loss : 0.137826, loss_ce: 0.003301, loss_dice: 0.272352
[12:58:25.722] TRAIN: iteration 31746 : loss : 0.047683, loss_ce: 0.004523, loss_dice: 0.090843
[12:58:25.932] TRAIN: iteration 31747 : loss : 0.018447, loss_ce: 0.001012, loss_dice: 0.035883
[12:58:26.140] TRAIN: iteration 31748 : loss : 0.148471, loss_ce: 0.004297, loss_dice: 0.292645
[12:58:26.348] TRAIN: iteration 31749 : loss : 0.250398, loss_ce: 0.000779, loss_dice: 0.500018
[12:58:26.556] TRAIN: iteration 31750 : loss : 0.072254, loss_ce: 0.002606, loss_dice: 0.141902
[12:58:26.764] TRAIN: iteration 31751 : loss : 0.026018, loss_ce: 0.001445, loss_dice: 0.050592
[12:58:26.972] TRAIN: iteration 31752 : loss : 0.041855, loss_ce: 0.002861, loss_dice: 0.080849
[12:58:27.185] TRAIN: iteration 31753 : loss : 0.027554, loss_ce: 0.000451, loss_dice: 0.054657
[12:58:27.395] TRAIN: iteration 31754 : loss : 0.041153, loss_ce: 0.003395, loss_dice: 0.078912
[12:58:27.604] TRAIN: iteration 31755 : loss : 0.029568, loss_ce: 0.000734, loss_dice: 0.058403
[12:58:27.813] TRAIN: iteration 31756 : loss : 0.061655, loss_ce: 0.002409, loss_dice: 0.120902
[12:58:28.020] TRAIN: iteration 31757 : loss : 0.060788, loss_ce: 0.001398, loss_dice: 0.120178
[12:58:28.233] TRAIN: iteration 31758 : loss : 0.040446, loss_ce: 0.000678, loss_dice: 0.080214
[12:58:28.439] TRAIN: iteration 31759 : loss : 0.065448, loss_ce: 0.002275, loss_dice: 0.128622
[12:58:28.648] TRAIN: iteration 31760 : loss : 0.025704, loss_ce: 0.001314, loss_dice: 0.050094
[12:58:29.010] TRAIN: iteration 31761 : loss : 0.047449, loss_ce: 0.005032, loss_dice: 0.089866
[12:58:29.218] TRAIN: iteration 31762 : loss : 0.059495, loss_ce: 0.003028, loss_dice: 0.115961
[12:58:29.426] TRAIN: iteration 31763 : loss : 0.061753, loss_ce: 0.003737, loss_dice: 0.119769
[12:58:29.634] TRAIN: iteration 31764 : loss : 0.063285, loss_ce: 0.001257, loss_dice: 0.125313
[12:58:29.841] TRAIN: iteration 31765 : loss : 0.077131, loss_ce: 0.000698, loss_dice: 0.153564
[12:58:30.048] TRAIN: iteration 31766 : loss : 0.049340, loss_ce: 0.001410, loss_dice: 0.097271
[12:58:30.258] TRAIN: iteration 31767 : loss : 0.137998, loss_ce: 0.001252, loss_dice: 0.274744
[12:58:30.465] TRAIN: iteration 31768 : loss : 0.051028, loss_ce: 0.010032, loss_dice: 0.092024
[12:58:30.672] TRAIN: iteration 31769 : loss : 0.161397, loss_ce: 0.003734, loss_dice: 0.319060
[12:58:30.879] TRAIN: iteration 31770 : loss : 0.015838, loss_ce: 0.000420, loss_dice: 0.031256
[12:58:31.087] TRAIN: iteration 31771 : loss : 0.084472, loss_ce: 0.001784, loss_dice: 0.167159
[12:58:31.382] TRAIN: iteration 31772 : loss : 0.069909, loss_ce: 0.003649, loss_dice: 0.136169
[12:58:31.592] TRAIN: iteration 31773 : loss : 0.045600, loss_ce: 0.001713, loss_dice: 0.089487
[12:58:31.800] TRAIN: iteration 31774 : loss : 0.056744, loss_ce: 0.001596, loss_dice: 0.111893
[12:58:32.007] TRAIN: iteration 31775 : loss : 0.034667, loss_ce: 0.001014, loss_dice: 0.068320
[12:58:32.216] TRAIN: iteration 31776 : loss : 0.101070, loss_ce: 0.002500, loss_dice: 0.199639
[12:58:32.598] TRAIN: iteration 31777 : loss : 0.229848, loss_ce: 0.000981, loss_dice: 0.458716
[12:58:33.279] TRAIN: iteration 31778 : loss : 0.119276, loss_ce: 0.004926, loss_dice: 0.233626
[12:58:33.489] TRAIN: iteration 31779 : loss : 0.233380, loss_ce: 0.002151, loss_dice: 0.464609
[12:58:33.701] TRAIN: iteration 31780 : loss : 0.142387, loss_ce: 0.002874, loss_dice: 0.281900
[12:58:33.932] TRAIN: iteration 31781 : loss : 0.206064, loss_ce: 0.019181, loss_dice: 0.392947
[12:58:34.141] TRAIN: iteration 31782 : loss : 0.103817, loss_ce: 0.012792, loss_dice: 0.194843
[12:58:34.356] TRAIN: iteration 31783 : loss : 0.250430, loss_ce: 0.000830, loss_dice: 0.500029
[12:58:34.566] TRAIN: iteration 31784 : loss : 0.250404, loss_ce: 0.000774, loss_dice: 0.500033
[12:58:34.774] TRAIN: iteration 31785 : loss : 0.015881, loss_ce: 0.001880, loss_dice: 0.029882
[12:58:35.507] TRAIN: iteration 31786 : loss : 0.094758, loss_ce: 0.003785, loss_dice: 0.185731
[12:58:35.715] TRAIN: iteration 31787 : loss : 0.088440, loss_ce: 0.006195, loss_dice: 0.170685
[12:58:35.941] TRAIN: iteration 31788 : loss : 0.053419, loss_ce: 0.003863, loss_dice: 0.102976
[12:58:36.149] TRAIN: iteration 31789 : loss : 0.250543, loss_ce: 0.001044, loss_dice: 0.500042
[12:58:36.357] TRAIN: iteration 31790 : loss : 0.037386, loss_ce: 0.003040, loss_dice: 0.071731
[12:58:36.565] TRAIN: iteration 31791 : loss : 0.171995, loss_ce: 0.001921, loss_dice: 0.342069
[12:58:36.774] TRAIN: iteration 31792 : loss : 0.093570, loss_ce: 0.005182, loss_dice: 0.181957
[12:58:36.983] TRAIN: iteration 31793 : loss : 0.025174, loss_ce: 0.003646, loss_dice: 0.046701
[12:58:37.191] TRAIN: iteration 31794 : loss : 0.131062, loss_ce: 0.009234, loss_dice: 0.252890
[12:58:37.400] TRAIN: iteration 31795 : loss : 0.211357, loss_ce: 0.000916, loss_dice: 0.421798
[12:58:37.617] TRAIN: iteration 31796 : loss : 0.064249, loss_ce: 0.001005, loss_dice: 0.127494
[12:58:37.828] TRAIN: iteration 31797 : loss : 0.250493, loss_ce: 0.000953, loss_dice: 0.500034
[12:58:38.042] TRAIN: iteration 31798 : loss : 0.185273, loss_ce: 0.000918, loss_dice: 0.369629
[12:58:38.249] TRAIN: iteration 31799 : loss : 0.040607, loss_ce: 0.005474, loss_dice: 0.075741
[12:58:38.466] TRAIN: iteration 31800 : loss : 0.048678, loss_ce: 0.001667, loss_dice: 0.095689
[12:58:38.701] TRAIN: iteration 31801 : loss : 0.189083, loss_ce: 0.001859, loss_dice: 0.376306
[12:58:38.908] TRAIN: iteration 31802 : loss : 0.053827, loss_ce: 0.003758, loss_dice: 0.103896
[12:58:39.120] TRAIN: iteration 31803 : loss : 0.050607, loss_ce: 0.002803, loss_dice: 0.098411
[12:58:39.328] TRAIN: iteration 31804 : loss : 0.116227, loss_ce: 0.004350, loss_dice: 0.228104
[12:58:39.545] TRAIN: iteration 31805 : loss : 0.151371, loss_ce: 0.001280, loss_dice: 0.301463
[12:58:39.753] TRAIN: iteration 31806 : loss : 0.153271, loss_ce: 0.001723, loss_dice: 0.304820
[12:58:39.962] TRAIN: iteration 31807 : loss : 0.233752, loss_ce: 0.000969, loss_dice: 0.466534
[12:58:40.170] TRAIN: iteration 31808 : loss : 0.200019, loss_ce: 0.000781, loss_dice: 0.399257
[12:58:40.377] TRAIN: iteration 31809 : loss : 0.075448, loss_ce: 0.008836, loss_dice: 0.142061
[12:58:40.588] TRAIN: iteration 31810 : loss : 0.104055, loss_ce: 0.005004, loss_dice: 0.203107
[12:58:40.797] TRAIN: iteration 31811 : loss : 0.029838, loss_ce: 0.000989, loss_dice: 0.058688
[12:58:41.005] TRAIN: iteration 31812 : loss : 0.128884, loss_ce: 0.003420, loss_dice: 0.254347
[12:58:41.214] TRAIN: iteration 31813 : loss : 0.178854, loss_ce: 0.003351, loss_dice: 0.354357
[12:58:41.423] TRAIN: iteration 31814 : loss : 0.065835, loss_ce: 0.001603, loss_dice: 0.130067
[12:58:41.631] TRAIN: iteration 31815 : loss : 0.250488, loss_ce: 0.000919, loss_dice: 0.500057
[12:58:41.839] TRAIN: iteration 31816 : loss : 0.215241, loss_ce: 0.000777, loss_dice: 0.429706
[12:58:42.045] TRAIN: iteration 31817 : loss : 0.077919, loss_ce: 0.002301, loss_dice: 0.153536
[12:58:42.253] TRAIN: iteration 31818 : loss : 0.033458, loss_ce: 0.001210, loss_dice: 0.065707
[12:58:42.461] TRAIN: iteration 31819 : loss : 0.053937, loss_ce: 0.002380, loss_dice: 0.105494
[12:58:42.671] TRAIN: iteration 31820 : loss : 0.107971, loss_ce: 0.000957, loss_dice: 0.214986
[12:58:42.672] NaN or Inf found in input tensor.
[12:58:42.886] TRAIN: iteration 31821 : loss : 0.031005, loss_ce: 0.000961, loss_dice: 0.061049
[12:58:43.095] TRAIN: iteration 31822 : loss : 0.017498, loss_ce: 0.003858, loss_dice: 0.031137
[12:58:43.302] TRAIN: iteration 31823 : loss : 0.249915, loss_ce: 0.002541, loss_dice: 0.497290
[12:58:43.510] TRAIN: iteration 31824 : loss : 0.072110, loss_ce: 0.002631, loss_dice: 0.141589
[12:58:43.717] TRAIN: iteration 31825 : loss : 0.178741, loss_ce: 0.011445, loss_dice: 0.346038
[12:58:43.924] TRAIN: iteration 31826 : loss : 0.046288, loss_ce: 0.001780, loss_dice: 0.090796
[12:58:44.141] TRAIN: iteration 31827 : loss : 0.062469, loss_ce: 0.003430, loss_dice: 0.121508
[12:58:44.349] TRAIN: iteration 31828 : loss : 0.250129, loss_ce: 0.001952, loss_dice: 0.498306
[12:58:44.557] TRAIN: iteration 31829 : loss : 0.029530, loss_ce: 0.001221, loss_dice: 0.057839
[12:58:45.827] TRAIN: iteration 31830 : loss : 0.035940, loss_ce: 0.002433, loss_dice: 0.069447
[12:58:46.034] TRAIN: iteration 31831 : loss : 0.112284, loss_ce: 0.006426, loss_dice: 0.218143
[12:58:46.242] TRAIN: iteration 31832 : loss : 0.151586, loss_ce: 0.002059, loss_dice: 0.301112
[12:58:46.449] TRAIN: iteration 31833 : loss : 0.042506, loss_ce: 0.004583, loss_dice: 0.080430
[12:58:46.658] TRAIN: iteration 31834 : loss : 0.084956, loss_ce: 0.004208, loss_dice: 0.165703
[12:58:46.866] TRAIN: iteration 31835 : loss : 0.047338, loss_ce: 0.001815, loss_dice: 0.092861
[12:58:47.079] TRAIN: iteration 31836 : loss : 0.019043, loss_ce: 0.001692, loss_dice: 0.036393
[12:58:47.324] TRAIN: iteration 31837 : loss : 0.250466, loss_ce: 0.000887, loss_dice: 0.500046
[12:58:47.535] TRAIN: iteration 31838 : loss : 0.054087, loss_ce: 0.002639, loss_dice: 0.105535
[12:58:47.743] TRAIN: iteration 31839 : loss : 0.046713, loss_ce: 0.002499, loss_dice: 0.090928
[12:58:47.952] TRAIN: iteration 31840 : loss : 0.075984, loss_ce: 0.010994, loss_dice: 0.140974
[12:58:48.192] TRAIN: iteration 31841 : loss : 0.039281, loss_ce: 0.002299, loss_dice: 0.076262
[12:58:48.405] TRAIN: iteration 31842 : loss : 0.033631, loss_ce: 0.001371, loss_dice: 0.065891
[12:58:48.615] TRAIN: iteration 31843 : loss : 0.162724, loss_ce: 0.002069, loss_dice: 0.323380
[12:58:48.824] TRAIN: iteration 31844 : loss : 0.082231, loss_ce: 0.006216, loss_dice: 0.158246
[12:58:49.032] TRAIN: iteration 31845 : loss : 0.251401, loss_ce: 0.002626, loss_dice: 0.500176
[12:58:49.240] TRAIN: iteration 31846 : loss : 0.076715, loss_ce: 0.001217, loss_dice: 0.152214
[12:58:49.448] TRAIN: iteration 31847 : loss : 0.029399, loss_ce: 0.002164, loss_dice: 0.056633
[12:58:49.660] TRAIN: iteration 31848 : loss : 0.250445, loss_ce: 0.000854, loss_dice: 0.500036
[12:58:49.874] TRAIN: iteration 31849 : loss : 0.094367, loss_ce: 0.009803, loss_dice: 0.178931
[12:58:50.085] TRAIN: iteration 31850 : loss : 0.019326, loss_ce: 0.001148, loss_dice: 0.037505
[12:58:50.293] TRAIN: iteration 31851 : loss : 0.062796, loss_ce: 0.002756, loss_dice: 0.122835
[12:58:50.505] TRAIN: iteration 31852 : loss : 0.018356, loss_ce: 0.001186, loss_dice: 0.035526
[12:58:50.712] TRAIN: iteration 31853 : loss : 0.198500, loss_ce: 0.007037, loss_dice: 0.389962
[12:58:50.924] TRAIN: iteration 31854 : loss : 0.057431, loss_ce: 0.001278, loss_dice: 0.113583
[12:58:51.134] TRAIN: iteration 31855 : loss : 0.106287, loss_ce: 0.006813, loss_dice: 0.205761
[12:58:51.343] TRAIN: iteration 31856 : loss : 0.031205, loss_ce: 0.002716, loss_dice: 0.059694
[12:58:51.555] TRAIN: iteration 31857 : loss : 0.067256, loss_ce: 0.003412, loss_dice: 0.131101
[12:58:51.766] TRAIN: iteration 31858 : loss : 0.123544, loss_ce: 0.002147, loss_dice: 0.244941
[12:58:51.974] TRAIN: iteration 31859 : loss : 0.058717, loss_ce: 0.001214, loss_dice: 0.116220
[12:58:52.186] TRAIN: iteration 31860 : loss : 0.138809, loss_ce: 0.003513, loss_dice: 0.274106
[12:58:52.428] TRAIN: iteration 31861 : loss : 0.129933, loss_ce: 0.005525, loss_dice: 0.254340
[12:58:52.635] TRAIN: iteration 31862 : loss : 0.079962, loss_ce: 0.003180, loss_dice: 0.156744
[12:58:52.849] TRAIN: iteration 31863 : loss : 0.048476, loss_ce: 0.001756, loss_dice: 0.095196
[12:58:53.060] TRAIN: iteration 31864 : loss : 0.067819, loss_ce: 0.001448, loss_dice: 0.134191
[12:58:53.270] TRAIN: iteration 31865 : loss : 0.037124, loss_ce: 0.005037, loss_dice: 0.069212
[12:58:53.486] TRAIN: iteration 31866 : loss : 0.033024, loss_ce: 0.001482, loss_dice: 0.064567
[12:58:53.693] TRAIN: iteration 31867 : loss : 0.096845, loss_ce: 0.009057, loss_dice: 0.184633
[12:58:54.004] TRAIN: iteration 31868 : loss : 0.069545, loss_ce: 0.007214, loss_dice: 0.131876
[12:58:54.218] TRAIN: iteration 31869 : loss : 0.070477, loss_ce: 0.002604, loss_dice: 0.138351
[12:58:54.428] TRAIN: iteration 31870 : loss : 0.099339, loss_ce: 0.001466, loss_dice: 0.197212
[12:58:54.635] TRAIN: iteration 31871 : loss : 0.081370, loss_ce: 0.009559, loss_dice: 0.153180
[12:58:54.847] TRAIN: iteration 31872 : loss : 0.250478, loss_ce: 0.000909, loss_dice: 0.500047
[12:58:55.057] TRAIN: iteration 31873 : loss : 0.073804, loss_ce: 0.003539, loss_dice: 0.144068
[12:58:55.264] TRAIN: iteration 31874 : loss : 0.147823, loss_ce: 0.002223, loss_dice: 0.293423
[12:58:55.472] TRAIN: iteration 31875 : loss : 0.083490, loss_ce: 0.001258, loss_dice: 0.165722
[12:58:55.681] TRAIN: iteration 31876 : loss : 0.047466, loss_ce: 0.002610, loss_dice: 0.092321
[12:58:55.889] TRAIN: iteration 31877 : loss : 0.249704, loss_ce: 0.002980, loss_dice: 0.496428
[12:58:56.097] TRAIN: iteration 31878 : loss : 0.251130, loss_ce: 0.002106, loss_dice: 0.500154
[12:58:56.305] TRAIN: iteration 31879 : loss : 0.040672, loss_ce: 0.007029, loss_dice: 0.074316
[12:58:56.520] TRAIN: iteration 31880 : loss : 0.094773, loss_ce: 0.004940, loss_dice: 0.184607
[12:58:56.760] TRAIN: iteration 31881 : loss : 0.049107, loss_ce: 0.002749, loss_dice: 0.095465
[12:58:56.969] TRAIN: iteration 31882 : loss : 0.046490, loss_ce: 0.004082, loss_dice: 0.088899
[12:58:57.189] TRAIN: iteration 31883 : loss : 0.161024, loss_ce: 0.003520, loss_dice: 0.318529
[12:58:57.402] TRAIN: iteration 31884 : loss : 0.044998, loss_ce: 0.001725, loss_dice: 0.088270
[12:58:57.611] TRAIN: iteration 31885 : loss : 0.123492, loss_ce: 0.004244, loss_dice: 0.242740
[12:58:57.821] TRAIN: iteration 31886 : loss : 0.071936, loss_ce: 0.001392, loss_dice: 0.142479
[12:58:58.028] TRAIN: iteration 31887 : loss : 0.041489, loss_ce: 0.003566, loss_dice: 0.079411
[12:58:58.240] TRAIN: iteration 31888 : loss : 0.250300, loss_ce: 0.000590, loss_dice: 0.500009
[12:58:58.456] TRAIN: iteration 31889 : loss : 0.061617, loss_ce: 0.002196, loss_dice: 0.121038
[12:58:58.666] TRAIN: iteration 31890 : loss : 0.032163, loss_ce: 0.001479, loss_dice: 0.062847
[12:58:58.881] TRAIN: iteration 31891 : loss : 0.250494, loss_ce: 0.000950, loss_dice: 0.500037
[12:58:59.091] TRAIN: iteration 31892 : loss : 0.023200, loss_ce: 0.002031, loss_dice: 0.044369
[12:58:59.300] TRAIN: iteration 31893 : loss : 0.078560, loss_ce: 0.002374, loss_dice: 0.154746
[12:58:59.508] TRAIN: iteration 31894 : loss : 0.069650, loss_ce: 0.001865, loss_dice: 0.137435
[12:58:59.717] TRAIN: iteration 31895 : loss : 0.025451, loss_ce: 0.000663, loss_dice: 0.050239
[12:59:00.995] TRAIN: iteration 31896 : loss : 0.030800, loss_ce: 0.003059, loss_dice: 0.058541
[12:59:02.626] TRAIN: iteration 31897 : loss : 0.121877, loss_ce: 0.003161, loss_dice: 0.240593
[12:59:02.836] TRAIN: iteration 31898 : loss : 0.065843, loss_ce: 0.001574, loss_dice: 0.130111
[12:59:03.043] TRAIN: iteration 31899 : loss : 0.145772, loss_ce: 0.002121, loss_dice: 0.289424
[12:59:03.251] TRAIN: iteration 31900 : loss : 0.013660, loss_ce: 0.000594, loss_dice: 0.026726
[12:59:03.488] TRAIN: iteration 31901 : loss : 0.033100, loss_ce: 0.000916, loss_dice: 0.065284
[12:59:03.698] TRAIN: iteration 31902 : loss : 0.044763, loss_ce: 0.002956, loss_dice: 0.086570
[12:59:03.906] TRAIN: iteration 31903 : loss : 0.097580, loss_ce: 0.000774, loss_dice: 0.194386
[12:59:04.117] TRAIN: iteration 31904 : loss : 0.148691, loss_ce: 0.005550, loss_dice: 0.291832
[12:59:05.881] TRAIN: iteration 31905 : loss : 0.029570, loss_ce: 0.001894, loss_dice: 0.057247
[12:59:06.089] TRAIN: iteration 31906 : loss : 0.249698, loss_ce: 0.001330, loss_dice: 0.498065
[12:59:06.302] TRAIN: iteration 31907 : loss : 0.031858, loss_ce: 0.003820, loss_dice: 0.059895
[12:59:06.511] TRAIN: iteration 31908 : loss : 0.250277, loss_ce: 0.000541, loss_dice: 0.500013
[12:59:06.720] TRAIN: iteration 31909 : loss : 0.075301, loss_ce: 0.004498, loss_dice: 0.146104
[12:59:06.934] TRAIN: iteration 31910 : loss : 0.050768, loss_ce: 0.009364, loss_dice: 0.092172
[12:59:07.142] TRAIN: iteration 31911 : loss : 0.057983, loss_ce: 0.001985, loss_dice: 0.113981
[12:59:07.349] TRAIN: iteration 31912 : loss : 0.057348, loss_ce: 0.008606, loss_dice: 0.106090
[12:59:07.557] TRAIN: iteration 31913 : loss : 0.065295, loss_ce: 0.003129, loss_dice: 0.127461
[12:59:07.842] TRAIN: iteration 31914 : loss : 0.096806, loss_ce: 0.002610, loss_dice: 0.191002
[12:59:08.061] TRAIN: iteration 31915 : loss : 0.092269, loss_ce: 0.004275, loss_dice: 0.180262
[12:59:08.268] TRAIN: iteration 31916 : loss : 0.142223, loss_ce: 0.002476, loss_dice: 0.281970
[12:59:08.483] TRAIN: iteration 31917 : loss : 0.054213, loss_ce: 0.001033, loss_dice: 0.107393
[12:59:08.698] TRAIN: iteration 31918 : loss : 0.064404, loss_ce: 0.002290, loss_dice: 0.126517
[12:59:08.906] TRAIN: iteration 31919 : loss : 0.091808, loss_ce: 0.002629, loss_dice: 0.180988
[12:59:09.114] TRAIN: iteration 31920 : loss : 0.038313, loss_ce: 0.002980, loss_dice: 0.073646
[12:59:09.361] TRAIN: iteration 31921 : loss : 0.090998, loss_ce: 0.001854, loss_dice: 0.180142
[12:59:09.569] TRAIN: iteration 31922 : loss : 0.061769, loss_ce: 0.003682, loss_dice: 0.119855
[12:59:09.777] TRAIN: iteration 31923 : loss : 0.060058, loss_ce: 0.005008, loss_dice: 0.115109
[12:59:09.991] TRAIN: iteration 31924 : loss : 0.079726, loss_ce: 0.003497, loss_dice: 0.155956
[12:59:10.209] TRAIN: iteration 31925 : loss : 0.105361, loss_ce: 0.001803, loss_dice: 0.208920
[12:59:10.416] TRAIN: iteration 31926 : loss : 0.229823, loss_ce: 0.001632, loss_dice: 0.458014
[12:59:10.623] TRAIN: iteration 31927 : loss : 0.028349, loss_ce: 0.005023, loss_dice: 0.051676
[12:59:10.831] TRAIN: iteration 31928 : loss : 0.052459, loss_ce: 0.000971, loss_dice: 0.103946
[12:59:11.039] TRAIN: iteration 31929 : loss : 0.077225, loss_ce: 0.012628, loss_dice: 0.141823
[12:59:11.248] TRAIN: iteration 31930 : loss : 0.047462, loss_ce: 0.002490, loss_dice: 0.092435
[12:59:11.458] TRAIN: iteration 31931 : loss : 0.103550, loss_ce: 0.006958, loss_dice: 0.200142
[12:59:11.665] TRAIN: iteration 31932 : loss : 0.043063, loss_ce: 0.003803, loss_dice: 0.082323
[12:59:11.882] TRAIN: iteration 31933 : loss : 0.083473, loss_ce: 0.001791, loss_dice: 0.165155
[12:59:12.091] TRAIN: iteration 31934 : loss : 0.061435, loss_ce: 0.004263, loss_dice: 0.118606
[12:59:12.299] TRAIN: iteration 31935 : loss : 0.110088, loss_ce: 0.001412, loss_dice: 0.218763
[12:59:12.506] TRAIN: iteration 31936 : loss : 0.026289, loss_ce: 0.001549, loss_dice: 0.051028
[12:59:12.715] TRAIN: iteration 31937 : loss : 0.250469, loss_ce: 0.000894, loss_dice: 0.500045
[12:59:12.923] TRAIN: iteration 31938 : loss : 0.250859, loss_ce: 0.001607, loss_dice: 0.500111
[12:59:13.137] TRAIN: iteration 31939 : loss : 0.034474, loss_ce: 0.000884, loss_dice: 0.068063
[12:59:13.352] TRAIN: iteration 31940 : loss : 0.057812, loss_ce: 0.001619, loss_dice: 0.114005
[12:59:13.588] TRAIN: iteration 31941 : loss : 0.051665, loss_ce: 0.007226, loss_dice: 0.096103
[12:59:13.796] TRAIN: iteration 31942 : loss : 0.250530, loss_ce: 0.001010, loss_dice: 0.500050
[12:59:14.005] TRAIN: iteration 31943 : loss : 0.235457, loss_ce: 0.001942, loss_dice: 0.468971
[12:59:14.212] TRAIN: iteration 31944 : loss : 0.065508, loss_ce: 0.002202, loss_dice: 0.128814
[12:59:14.420] TRAIN: iteration 31945 : loss : 0.041341, loss_ce: 0.005105, loss_dice: 0.077576
[12:59:14.628] TRAIN: iteration 31946 : loss : 0.035854, loss_ce: 0.004233, loss_dice: 0.067475
[12:59:14.837] TRAIN: iteration 31947 : loss : 0.089274, loss_ce: 0.013145, loss_dice: 0.165402
[12:59:15.045] TRAIN: iteration 31948 : loss : 0.114116, loss_ce: 0.006910, loss_dice: 0.221322
[12:59:15.253] TRAIN: iteration 31949 : loss : 0.025048, loss_ce: 0.002544, loss_dice: 0.047551
[12:59:15.461] TRAIN: iteration 31950 : loss : 0.250359, loss_ce: 0.000695, loss_dice: 0.500024
[12:59:15.669] TRAIN: iteration 31951 : loss : 0.250288, loss_ce: 0.000561, loss_dice: 0.500015
[12:59:15.877] TRAIN: iteration 31952 : loss : 0.037469, loss_ce: 0.000713, loss_dice: 0.074226
[12:59:16.085] TRAIN: iteration 31953 : loss : 0.054417, loss_ce: 0.001370, loss_dice: 0.107464
[12:59:16.293] TRAIN: iteration 31954 : loss : 0.250387, loss_ce: 0.000737, loss_dice: 0.500036
[12:59:16.503] TRAIN: iteration 31955 : loss : 0.238923, loss_ce: 0.000672, loss_dice: 0.477173
[12:59:16.710] TRAIN: iteration 31956 : loss : 0.042398, loss_ce: 0.002337, loss_dice: 0.082459
[12:59:18.254] TRAIN: iteration 31957 : loss : 0.036829, loss_ce: 0.001153, loss_dice: 0.072505
[12:59:18.462] TRAIN: iteration 31958 : loss : 0.008282, loss_ce: 0.000752, loss_dice: 0.015813
[12:59:18.668] TRAIN: iteration 31959 : loss : 0.038082, loss_ce: 0.002398, loss_dice: 0.073765
[12:59:18.879] TRAIN: iteration 31960 : loss : 0.073225, loss_ce: 0.002185, loss_dice: 0.144265
[12:59:19.122] TRAIN: iteration 31961 : loss : 0.065299, loss_ce: 0.001201, loss_dice: 0.129397
[12:59:19.331] TRAIN: iteration 31962 : loss : 0.043608, loss_ce: 0.004836, loss_dice: 0.082380
[12:59:19.539] TRAIN: iteration 31963 : loss : 0.045481, loss_ce: 0.000693, loss_dice: 0.090268
[12:59:19.749] TRAIN: iteration 31964 : loss : 0.075020, loss_ce: 0.002891, loss_dice: 0.147150
[12:59:19.956] TRAIN: iteration 31965 : loss : 0.046048, loss_ce: 0.007457, loss_dice: 0.084640
[12:59:20.164] TRAIN: iteration 31966 : loss : 0.091340, loss_ce: 0.005015, loss_dice: 0.177665
[12:59:20.371] TRAIN: iteration 31967 : loss : 0.025085, loss_ce: 0.002242, loss_dice: 0.047929
[12:59:20.588] TRAIN: iteration 31968 : loss : 0.086582, loss_ce: 0.003235, loss_dice: 0.169930
[12:59:20.796] TRAIN: iteration 31969 : loss : 0.032396, loss_ce: 0.005326, loss_dice: 0.059466
[12:59:21.005] TRAIN: iteration 31970 : loss : 0.074009, loss_ce: 0.003379, loss_dice: 0.144639
[12:59:21.214] TRAIN: iteration 31971 : loss : 0.166815, loss_ce: 0.001415, loss_dice: 0.332215
[12:59:21.423] TRAIN: iteration 31972 : loss : 0.252531, loss_ce: 0.006907, loss_dice: 0.498155
[12:59:21.631] TRAIN: iteration 31973 : loss : 0.033050, loss_ce: 0.002371, loss_dice: 0.063729
[12:59:21.895] TRAIN: iteration 31974 : loss : 0.051898, loss_ce: 0.002877, loss_dice: 0.100920
[12:59:22.104] TRAIN: iteration 31975 : loss : 0.086945, loss_ce: 0.001929, loss_dice: 0.171960
[12:59:22.311] TRAIN: iteration 31976 : loss : 0.250306, loss_ce: 0.000596, loss_dice: 0.500015
[12:59:22.519] TRAIN: iteration 31977 : loss : 0.201710, loss_ce: 0.003075, loss_dice: 0.400345
[12:59:22.733] TRAIN: iteration 31978 : loss : 0.026095, loss_ce: 0.000852, loss_dice: 0.051338
[12:59:22.941] TRAIN: iteration 31979 : loss : 0.033693, loss_ce: 0.004245, loss_dice: 0.063141
[12:59:23.153] TRAIN: iteration 31980 : loss : 0.058536, loss_ce: 0.001525, loss_dice: 0.115547
[12:59:23.400] TRAIN: iteration 31981 : loss : 0.152531, loss_ce: 0.002436, loss_dice: 0.302625
[12:59:23.608] TRAIN: iteration 31982 : loss : 0.052025, loss_ce: 0.002881, loss_dice: 0.101169
[12:59:23.817] TRAIN: iteration 31983 : loss : 0.052387, loss_ce: 0.001195, loss_dice: 0.103579
[12:59:24.026] TRAIN: iteration 31984 : loss : 0.058322, loss_ce: 0.000943, loss_dice: 0.115701
[12:59:24.233] TRAIN: iteration 31985 : loss : 0.180242, loss_ce: 0.002959, loss_dice: 0.357524
[12:59:24.445] TRAIN: iteration 31986 : loss : 0.112236, loss_ce: 0.005231, loss_dice: 0.219242
[12:59:24.653] TRAIN: iteration 31987 : loss : 0.250439, loss_ce: 0.000839, loss_dice: 0.500040
[12:59:24.862] TRAIN: iteration 31988 : loss : 0.250313, loss_ce: 0.000605, loss_dice: 0.500021
[12:59:25.070] TRAIN: iteration 31989 : loss : 0.046965, loss_ce: 0.001185, loss_dice: 0.092744
[12:59:25.279] TRAIN: iteration 31990 : loss : 0.179666, loss_ce: 0.002034, loss_dice: 0.357297
[12:59:25.489] TRAIN: iteration 31991 : loss : 0.063017, loss_ce: 0.001063, loss_dice: 0.124972
[12:59:25.698] TRAIN: iteration 31992 : loss : 0.113083, loss_ce: 0.002044, loss_dice: 0.224121
[12:59:25.906] TRAIN: iteration 31993 : loss : 0.014616, loss_ce: 0.000811, loss_dice: 0.028422
[12:59:26.118] TRAIN: iteration 31994 : loss : 0.250240, loss_ce: 0.000690, loss_dice: 0.499790
[12:59:26.325] TRAIN: iteration 31995 : loss : 0.029774, loss_ce: 0.003115, loss_dice: 0.056433
[12:59:26.536] TRAIN: iteration 31996 : loss : 0.033788, loss_ce: 0.003498, loss_dice: 0.064077
[12:59:26.747] TRAIN: iteration 31997 : loss : 0.017806, loss_ce: 0.002343, loss_dice: 0.033269
[12:59:27.147] TRAIN: iteration 31998 : loss : 0.070499, loss_ce: 0.001374, loss_dice: 0.139623
[12:59:27.355] TRAIN: iteration 31999 : loss : 0.212242, loss_ce: 0.001285, loss_dice: 0.423198
[12:59:27.564] TRAIN: iteration 32000 : loss : 0.250509, loss_ce: 0.000970, loss_dice: 0.500049
[12:59:27.805] TRAIN: iteration 32001 : loss : 0.068122, loss_ce: 0.002067, loss_dice: 0.134177
[12:59:28.013] TRAIN: iteration 32002 : loss : 0.108198, loss_ce: 0.003021, loss_dice: 0.213375
[12:59:28.221] TRAIN: iteration 32003 : loss : 0.250494, loss_ce: 0.000948, loss_dice: 0.500039
[12:59:28.429] TRAIN: iteration 32004 : loss : 0.067190, loss_ce: 0.003792, loss_dice: 0.130588
[12:59:28.647] TRAIN: iteration 32005 : loss : 0.074587, loss_ce: 0.004494, loss_dice: 0.144679
[12:59:28.858] TRAIN: iteration 32006 : loss : 0.094272, loss_ce: 0.002271, loss_dice: 0.186274
[12:59:29.065] TRAIN: iteration 32007 : loss : 0.055162, loss_ce: 0.002654, loss_dice: 0.107669
[12:59:29.273] TRAIN: iteration 32008 : loss : 0.125397, loss_ce: 0.001777, loss_dice: 0.249016
[12:59:29.486] TRAIN: iteration 32009 : loss : 0.081111, loss_ce: 0.002112, loss_dice: 0.160111
[12:59:29.704] TRAIN: iteration 32010 : loss : 0.022187, loss_ce: 0.002139, loss_dice: 0.042235
[12:59:30.689] TRAIN: iteration 32011 : loss : 0.101478, loss_ce: 0.002970, loss_dice: 0.199986
[12:59:30.896] TRAIN: iteration 32012 : loss : 0.108137, loss_ce: 0.001049, loss_dice: 0.215226
[12:59:31.105] TRAIN: iteration 32013 : loss : 0.013330, loss_ce: 0.001303, loss_dice: 0.025357
[12:59:31.315] TRAIN: iteration 32014 : loss : 0.075092, loss_ce: 0.004633, loss_dice: 0.145551
[12:59:31.523] TRAIN: iteration 32015 : loss : 0.053385, loss_ce: 0.004701, loss_dice: 0.102069
[12:59:31.739] TRAIN: iteration 32016 : loss : 0.157477, loss_ce: 0.001290, loss_dice: 0.313664
[12:59:31.947] TRAIN: iteration 32017 : loss : 0.024152, loss_ce: 0.001970, loss_dice: 0.046334
[12:59:32.155] TRAIN: iteration 32018 : loss : 0.038277, loss_ce: 0.001360, loss_dice: 0.075194
[12:59:33.423] TRAIN: iteration 32019 : loss : 0.047966, loss_ce: 0.001300, loss_dice: 0.094633
[12:59:33.635] TRAIN: iteration 32020 : loss : 0.199362, loss_ce: 0.001625, loss_dice: 0.397100
[12:59:33.879] TRAIN: iteration 32021 : loss : 0.065636, loss_ce: 0.003961, loss_dice: 0.127311
[12:59:34.090] TRAIN: iteration 32022 : loss : 0.093911, loss_ce: 0.001803, loss_dice: 0.186019
[12:59:34.297] TRAIN: iteration 32023 : loss : 0.120864, loss_ce: 0.023507, loss_dice: 0.218222
[12:59:34.507] TRAIN: iteration 32024 : loss : 0.157118, loss_ce: 0.002912, loss_dice: 0.311324
[12:59:34.723] TRAIN: iteration 32025 : loss : 0.207496, loss_ce: 0.003116, loss_dice: 0.411877
[12:59:34.930] TRAIN: iteration 32026 : loss : 0.058037, loss_ce: 0.001646, loss_dice: 0.114428
[12:59:35.138] TRAIN: iteration 32027 : loss : 0.033909, loss_ce: 0.001651, loss_dice: 0.066167
[12:59:35.346] TRAIN: iteration 32028 : loss : 0.047124, loss_ce: 0.004786, loss_dice: 0.089461
[12:59:35.553] TRAIN: iteration 32029 : loss : 0.118190, loss_ce: 0.003808, loss_dice: 0.232571
[12:59:35.760] TRAIN: iteration 32030 : loss : 0.031221, loss_ce: 0.004286, loss_dice: 0.058156
[12:59:35.972] TRAIN: iteration 32031 : loss : 0.036920, loss_ce: 0.002973, loss_dice: 0.070867
[12:59:36.182] TRAIN: iteration 32032 : loss : 0.030601, loss_ce: 0.002378, loss_dice: 0.058824
[12:59:36.389] TRAIN: iteration 32033 : loss : 0.250511, loss_ce: 0.001001, loss_dice: 0.500021
[12:59:36.598] TRAIN: iteration 32034 : loss : 0.064116, loss_ce: 0.001613, loss_dice: 0.126620
[12:59:36.805] TRAIN: iteration 32035 : loss : 0.009319, loss_ce: 0.000983, loss_dice: 0.017656
[12:59:37.015] TRAIN: iteration 32036 : loss : 0.180890, loss_ce: 0.003333, loss_dice: 0.358447
[12:59:37.225] TRAIN: iteration 32037 : loss : 0.021520, loss_ce: 0.000923, loss_dice: 0.042116
[12:59:37.434] TRAIN: iteration 32038 : loss : 0.066224, loss_ce: 0.003764, loss_dice: 0.128683
[12:59:37.643] TRAIN: iteration 32039 : loss : 0.027813, loss_ce: 0.002937, loss_dice: 0.052688
[12:59:37.850] TRAIN: iteration 32040 : loss : 0.010857, loss_ce: 0.000916, loss_dice: 0.020798
[12:59:38.112] TRAIN: iteration 32041 : loss : 0.060249, loss_ce: 0.005887, loss_dice: 0.114612
[12:59:38.320] TRAIN: iteration 32042 : loss : 0.035271, loss_ce: 0.000745, loss_dice: 0.069798
[12:59:38.528] TRAIN: iteration 32043 : loss : 0.065837, loss_ce: 0.001530, loss_dice: 0.130144
[12:59:38.738] TRAIN: iteration 32044 : loss : 0.250933, loss_ce: 0.001765, loss_dice: 0.500101
[12:59:38.945] TRAIN: iteration 32045 : loss : 0.060750, loss_ce: 0.005717, loss_dice: 0.115783
[12:59:39.154] TRAIN: iteration 32046 : loss : 0.059351, loss_ce: 0.002302, loss_dice: 0.116401
[12:59:39.361] TRAIN: iteration 32047 : loss : 0.141055, loss_ce: 0.004557, loss_dice: 0.277552
[12:59:39.569] TRAIN: iteration 32048 : loss : 0.035302, loss_ce: 0.005519, loss_dice: 0.065084
[12:59:39.777] TRAIN: iteration 32049 : loss : 0.074383, loss_ce: 0.004178, loss_dice: 0.144588
[12:59:39.984] TRAIN: iteration 32050 : loss : 0.066285, loss_ce: 0.002854, loss_dice: 0.129716
[12:59:40.193] TRAIN: iteration 32051 : loss : 0.250930, loss_ce: 0.001756, loss_dice: 0.500104
[12:59:40.404] TRAIN: iteration 32052 : loss : 0.034108, loss_ce: 0.002676, loss_dice: 0.065540
[12:59:41.227] TRAIN: iteration 32053 : loss : 0.018848, loss_ce: 0.001157, loss_dice: 0.036540
[12:59:41.434] TRAIN: iteration 32054 : loss : 0.009459, loss_ce: 0.000927, loss_dice: 0.017991
[12:59:41.642] TRAIN: iteration 32055 : loss : 0.034219, loss_ce: 0.001064, loss_dice: 0.067375
[12:59:41.848] TRAIN: iteration 32056 : loss : 0.119014, loss_ce: 0.004362, loss_dice: 0.233666
[12:59:43.490] TRAIN: iteration 32057 : loss : 0.095126, loss_ce: 0.007292, loss_dice: 0.182960
[12:59:43.699] TRAIN: iteration 32058 : loss : 0.105224, loss_ce: 0.002988, loss_dice: 0.207460
[12:59:43.909] TRAIN: iteration 32059 : loss : 0.250593, loss_ce: 0.001139, loss_dice: 0.500047
[12:59:44.116] TRAIN: iteration 32060 : loss : 0.095025, loss_ce: 0.010232, loss_dice: 0.179817
[12:59:44.116] NaN or Inf found in input tensor.
[12:59:44.448] TRAIN: iteration 32061 : loss : 0.082018, loss_ce: 0.004778, loss_dice: 0.159258
[12:59:44.655] TRAIN: iteration 32062 : loss : 0.173431, loss_ce: 0.001203, loss_dice: 0.345659
[12:59:44.862] TRAIN: iteration 32063 : loss : 0.036399, loss_ce: 0.000881, loss_dice: 0.071917
[12:59:45.070] TRAIN: iteration 32064 : loss : 0.247786, loss_ce: 0.000900, loss_dice: 0.494673
[12:59:45.277] TRAIN: iteration 32065 : loss : 0.138949, loss_ce: 0.001839, loss_dice: 0.276060
[12:59:45.488] TRAIN: iteration 32066 : loss : 0.250452, loss_ce: 0.000878, loss_dice: 0.500026
[12:59:45.699] TRAIN: iteration 32067 : loss : 0.011341, loss_ce: 0.000829, loss_dice: 0.021852
[12:59:45.909] TRAIN: iteration 32068 : loss : 0.097597, loss_ce: 0.000944, loss_dice: 0.194251
[12:59:46.118] TRAIN: iteration 32069 : loss : 0.063025, loss_ce: 0.003578, loss_dice: 0.122472
[12:59:46.340] TRAIN: iteration 32070 : loss : 0.100371, loss_ce: 0.006309, loss_dice: 0.194433
[12:59:46.549] TRAIN: iteration 32071 : loss : 0.068632, loss_ce: 0.001609, loss_dice: 0.135654
[12:59:46.757] TRAIN: iteration 32072 : loss : 0.032902, loss_ce: 0.000781, loss_dice: 0.065023
[12:59:46.968] TRAIN: iteration 32073 : loss : 0.201069, loss_ce: 0.001895, loss_dice: 0.400243
[12:59:47.176] TRAIN: iteration 32074 : loss : 0.056915, loss_ce: 0.001287, loss_dice: 0.112543
[12:59:47.384] TRAIN: iteration 32075 : loss : 0.114103, loss_ce: 0.004975, loss_dice: 0.223232
[12:59:47.598] TRAIN: iteration 32076 : loss : 0.032810, loss_ce: 0.002663, loss_dice: 0.062958
[12:59:48.152] TRAIN: iteration 32077 : loss : 0.048212, loss_ce: 0.000977, loss_dice: 0.095447
[12:59:48.364] TRAIN: iteration 32078 : loss : 0.070271, loss_ce: 0.002184, loss_dice: 0.138357
[12:59:48.573] TRAIN: iteration 32079 : loss : 0.042515, loss_ce: 0.001299, loss_dice: 0.083731
[12:59:48.828] TRAIN: iteration 32080 : loss : 0.083748, loss_ce: 0.002792, loss_dice: 0.164705
[12:59:49.302] TRAIN: iteration 32081 : loss : 0.240860, loss_ce: 0.005291, loss_dice: 0.476428
[12:59:49.512] TRAIN: iteration 32082 : loss : 0.250494, loss_ce: 0.000955, loss_dice: 0.500033
[12:59:49.722] TRAIN: iteration 32083 : loss : 0.035695, loss_ce: 0.000918, loss_dice: 0.070471
[12:59:49.931] TRAIN: iteration 32084 : loss : 0.038709, loss_ce: 0.005890, loss_dice: 0.071527
[12:59:50.140] TRAIN: iteration 32085 : loss : 0.157399, loss_ce: 0.005407, loss_dice: 0.309391
[12:59:50.352] TRAIN: iteration 32086 : loss : 0.061782, loss_ce: 0.002588, loss_dice: 0.120976
[12:59:50.566] TRAIN: iteration 32087 : loss : 0.119504, loss_ce: 0.001970, loss_dice: 0.237038
[12:59:50.774] TRAIN: iteration 32088 : loss : 0.056583, loss_ce: 0.001298, loss_dice: 0.111868
[12:59:50.982] TRAIN: iteration 32089 : loss : 0.052542, loss_ce: 0.004436, loss_dice: 0.100648
[12:59:51.193] TRAIN: iteration 32090 : loss : 0.091884, loss_ce: 0.001930, loss_dice: 0.181838
[12:59:51.408] TRAIN: iteration 32091 : loss : 0.250584, loss_ce: 0.001118, loss_dice: 0.500050
[12:59:51.617] TRAIN: iteration 32092 : loss : 0.035944, loss_ce: 0.003038, loss_dice: 0.068851
[12:59:51.824] TRAIN: iteration 32093 : loss : 0.020963, loss_ce: 0.001324, loss_dice: 0.040602
[12:59:53.524] TRAIN: iteration 32094 : loss : 0.065805, loss_ce: 0.001067, loss_dice: 0.130544
[12:59:53.738] TRAIN: iteration 32095 : loss : 0.250695, loss_ce: 0.001313, loss_dice: 0.500077
[12:59:53.949] TRAIN: iteration 32096 : loss : 0.046850, loss_ce: 0.001447, loss_dice: 0.092253
[12:59:54.161] TRAIN: iteration 32097 : loss : 0.074123, loss_ce: 0.003153, loss_dice: 0.145094
[12:59:54.371] TRAIN: iteration 32098 : loss : 0.141255, loss_ce: 0.003364, loss_dice: 0.279146
[12:59:54.579] TRAIN: iteration 32099 : loss : 0.126325, loss_ce: 0.005096, loss_dice: 0.247553
[12:59:54.786] TRAIN: iteration 32100 : loss : 0.046801, loss_ce: 0.004428, loss_dice: 0.089173
[12:59:55.024] TRAIN: iteration 32101 : loss : 0.080884, loss_ce: 0.002988, loss_dice: 0.158779
[12:59:55.232] TRAIN: iteration 32102 : loss : 0.079532, loss_ce: 0.001590, loss_dice: 0.157473
[12:59:55.443] TRAIN: iteration 32103 : loss : 0.034840, loss_ce: 0.001723, loss_dice: 0.067956
[12:59:55.650] TRAIN: iteration 32104 : loss : 0.058231, loss_ce: 0.005001, loss_dice: 0.111461
[12:59:55.857] TRAIN: iteration 32105 : loss : 0.055810, loss_ce: 0.001520, loss_dice: 0.110101
[12:59:56.072] TRAIN: iteration 32106 : loss : 0.045984, loss_ce: 0.000941, loss_dice: 0.091027
[12:59:56.286] TRAIN: iteration 32107 : loss : 0.017952, loss_ce: 0.000886, loss_dice: 0.035018
[12:59:56.493] TRAIN: iteration 32108 : loss : 0.046292, loss_ce: 0.003443, loss_dice: 0.089142
[12:59:56.730] TRAIN: iteration 32109 : loss : 0.155297, loss_ce: 0.006366, loss_dice: 0.304228
[12:59:56.938] TRAIN: iteration 32110 : loss : 0.021276, loss_ce: 0.004231, loss_dice: 0.038321
[12:59:57.147] TRAIN: iteration 32111 : loss : 0.086473, loss_ce: 0.004569, loss_dice: 0.168377
[12:59:57.355] TRAIN: iteration 32112 : loss : 0.250352, loss_ce: 0.000689, loss_dice: 0.500016
[12:59:57.563] TRAIN: iteration 32113 : loss : 0.250617, loss_ce: 0.001170, loss_dice: 0.500063
[12:59:57.773] TRAIN: iteration 32114 : loss : 0.250882, loss_ce: 0.001667, loss_dice: 0.500097
[12:59:57.984] TRAIN: iteration 32115 : loss : 0.038851, loss_ce: 0.000832, loss_dice: 0.076871
[12:59:58.193] TRAIN: iteration 32116 : loss : 0.250346, loss_ce: 0.000678, loss_dice: 0.500014
[12:59:58.403] TRAIN: iteration 32117 : loss : 0.020488, loss_ce: 0.002056, loss_dice: 0.038921
[12:59:58.616] TRAIN: iteration 32118 : loss : 0.233696, loss_ce: 0.000991, loss_dice: 0.466400
[12:59:58.928] TRAIN: iteration 32119 : loss : 0.081530, loss_ce: 0.002279, loss_dice: 0.160780
[12:59:59.140] TRAIN: iteration 32120 : loss : 0.109489, loss_ce: 0.002222, loss_dice: 0.216756
[12:59:59.387] TRAIN: iteration 32121 : loss : 0.068131, loss_ce: 0.006537, loss_dice: 0.129724
[12:59:59.596] TRAIN: iteration 32122 : loss : 0.040670, loss_ce: 0.003335, loss_dice: 0.078005
[13:00:00.383] TRAIN: iteration 32123 : loss : 0.050017, loss_ce: 0.002138, loss_dice: 0.097896
[13:00:00.589] TRAIN: iteration 32124 : loss : 0.040199, loss_ce: 0.001332, loss_dice: 0.079065
[13:00:00.796] TRAIN: iteration 32125 : loss : 0.247030, loss_ce: 0.003205, loss_dice: 0.490856
[13:00:01.406] TRAIN: iteration 32126 : loss : 0.020199, loss_ce: 0.000390, loss_dice: 0.040008
[13:00:01.615] TRAIN: iteration 32127 : loss : 0.050013, loss_ce: 0.002207, loss_dice: 0.097819
[13:00:01.828] TRAIN: iteration 32128 : loss : 0.250688, loss_ce: 0.001293, loss_dice: 0.500084
[13:00:02.041] TRAIN: iteration 32129 : loss : 0.091026, loss_ce: 0.004137, loss_dice: 0.177916
[13:00:02.250] TRAIN: iteration 32130 : loss : 0.068410, loss_ce: 0.002133, loss_dice: 0.134687
[13:00:02.460] TRAIN: iteration 32131 : loss : 0.051074, loss_ce: 0.003356, loss_dice: 0.098791
[13:00:02.672] TRAIN: iteration 32132 : loss : 0.246439, loss_ce: 0.004270, loss_dice: 0.488607
[13:00:02.880] TRAIN: iteration 32133 : loss : 0.113900, loss_ce: 0.001976, loss_dice: 0.225823
[13:00:03.097] TRAIN: iteration 32134 : loss : 0.147918, loss_ce: 0.002373, loss_dice: 0.293463
[13:00:03.309] TRAIN: iteration 32135 : loss : 0.050589, loss_ce: 0.001018, loss_dice: 0.100161
[13:00:03.516] TRAIN: iteration 32136 : loss : 0.102950, loss_ce: 0.001770, loss_dice: 0.204130
[13:00:03.729] TRAIN: iteration 32137 : loss : 0.146416, loss_ce: 0.015778, loss_dice: 0.277055
[13:00:03.943] TRAIN: iteration 32138 : loss : 0.048053, loss_ce: 0.001221, loss_dice: 0.094885
[13:00:06.211] TRAIN: iteration 32139 : loss : 0.075951, loss_ce: 0.001326, loss_dice: 0.150576
[13:00:06.420] TRAIN: iteration 32140 : loss : 0.063961, loss_ce: 0.003224, loss_dice: 0.124697
[13:00:06.644] TRAIN: iteration 32141 : loss : 0.243479, loss_ce: 0.002124, loss_dice: 0.484835
[13:00:06.852] TRAIN: iteration 32142 : loss : 0.250396, loss_ce: 0.004532, loss_dice: 0.496261
[13:00:07.058] TRAIN: iteration 32143 : loss : 0.176937, loss_ce: 0.006657, loss_dice: 0.347216
[13:00:07.265] TRAIN: iteration 32144 : loss : 0.237055, loss_ce: 0.001856, loss_dice: 0.472255
[13:00:07.474] TRAIN: iteration 32145 : loss : 0.250428, loss_ce: 0.000834, loss_dice: 0.500022
[13:00:07.683] TRAIN: iteration 32146 : loss : 0.164186, loss_ce: 0.002802, loss_dice: 0.325569
[13:00:08.573] TRAIN: iteration 32147 : loss : 0.091919, loss_ce: 0.005751, loss_dice: 0.178087
[13:00:08.780] TRAIN: iteration 32148 : loss : 0.236408, loss_ce: 0.001904, loss_dice: 0.470912
[13:00:08.988] TRAIN: iteration 32149 : loss : 0.250512, loss_ce: 0.000990, loss_dice: 0.500034
[13:00:09.195] TRAIN: iteration 32150 : loss : 0.034708, loss_ce: 0.002095, loss_dice: 0.067320
[13:00:09.403] TRAIN: iteration 32151 : loss : 0.026076, loss_ce: 0.003964, loss_dice: 0.048189
[13:00:09.618] TRAIN: iteration 32152 : loss : 0.034596, loss_ce: 0.001016, loss_dice: 0.068177
[13:00:09.826] TRAIN: iteration 32153 : loss : 0.121002, loss_ce: 0.001614, loss_dice: 0.240389
[13:00:10.034] TRAIN: iteration 32154 : loss : 0.021250, loss_ce: 0.003835, loss_dice: 0.038664
[13:00:12.482] TRAIN: iteration 32155 : loss : 0.046743, loss_ce: 0.002297, loss_dice: 0.091190
[13:00:12.691] TRAIN: iteration 32156 : loss : 0.062848, loss_ce: 0.002290, loss_dice: 0.123406
[13:00:12.901] TRAIN: iteration 32157 : loss : 0.042919, loss_ce: 0.007247, loss_dice: 0.078592
[13:00:13.109] TRAIN: iteration 32158 : loss : 0.149886, loss_ce: 0.003529, loss_dice: 0.296243
[13:00:13.321] TRAIN: iteration 32159 : loss : 0.103455, loss_ce: 0.002566, loss_dice: 0.204345
[13:00:13.534] TRAIN: iteration 32160 : loss : 0.072391, loss_ce: 0.001696, loss_dice: 0.143086
[13:00:13.771] TRAIN: iteration 32161 : loss : 0.087539, loss_ce: 0.003334, loss_dice: 0.171743
[13:00:13.980] TRAIN: iteration 32162 : loss : 0.081656, loss_ce: 0.003764, loss_dice: 0.159548
[13:00:16.372] TRAIN: iteration 32163 : loss : 0.064984, loss_ce: 0.002281, loss_dice: 0.127687
[13:00:16.586] TRAIN: iteration 32164 : loss : 0.044445, loss_ce: 0.003590, loss_dice: 0.085301
[13:00:16.798] TRAIN: iteration 32165 : loss : 0.053140, loss_ce: 0.003871, loss_dice: 0.102409
[13:00:17.006] TRAIN: iteration 32166 : loss : 0.026507, loss_ce: 0.002084, loss_dice: 0.050930
[13:00:17.213] TRAIN: iteration 32167 : loss : 0.020671, loss_ce: 0.000767, loss_dice: 0.040575
[13:00:17.421] TRAIN: iteration 32168 : loss : 0.040663, loss_ce: 0.000632, loss_dice: 0.080695
[13:00:17.629] TRAIN: iteration 32169 : loss : 0.091297, loss_ce: 0.001067, loss_dice: 0.181527
[13:00:17.842] TRAIN: iteration 32170 : loss : 0.250558, loss_ce: 0.001062, loss_dice: 0.500054
[13:00:18.366] TRAIN: iteration 32171 : loss : 0.022457, loss_ce: 0.004063, loss_dice: 0.040850
[13:00:18.577] TRAIN: iteration 32172 : loss : 0.133936, loss_ce: 0.001930, loss_dice: 0.265943
[13:00:18.784] TRAIN: iteration 32173 : loss : 0.027824, loss_ce: 0.000767, loss_dice: 0.054881
[13:00:18.995] TRAIN: iteration 32174 : loss : 0.165160, loss_ce: 0.003724, loss_dice: 0.326596
[13:00:19.205] TRAIN: iteration 32175 : loss : 0.075014, loss_ce: 0.004809, loss_dice: 0.145218
[13:00:19.414] TRAIN: iteration 32176 : loss : 0.172922, loss_ce: 0.001654, loss_dice: 0.344190
[13:00:19.628] TRAIN: iteration 32177 : loss : 0.124549, loss_ce: 0.006890, loss_dice: 0.242208
[13:00:19.836] TRAIN: iteration 32178 : loss : 0.070789, loss_ce: 0.001302, loss_dice: 0.140275
[13:00:21.341] TRAIN: iteration 32179 : loss : 0.028123, loss_ce: 0.002162, loss_dice: 0.054084
[13:00:21.596] TRAIN: iteration 32180 : loss : 0.218685, loss_ce: 0.002183, loss_dice: 0.435187
[13:00:21.834] TRAIN: iteration 32181 : loss : 0.062336, loss_ce: 0.000826, loss_dice: 0.123847
[13:00:22.045] TRAIN: iteration 32182 : loss : 0.066713, loss_ce: 0.001734, loss_dice: 0.131691
[13:00:22.255] TRAIN: iteration 32183 : loss : 0.096389, loss_ce: 0.000702, loss_dice: 0.192075
[13:00:22.462] TRAIN: iteration 32184 : loss : 0.245919, loss_ce: 0.001329, loss_dice: 0.490509
[13:00:22.671] TRAIN: iteration 32185 : loss : 0.055405, loss_ce: 0.000900, loss_dice: 0.109910
[13:00:22.879] TRAIN: iteration 32186 : loss : 0.034054, loss_ce: 0.006143, loss_dice: 0.061964
[13:00:23.875] TRAIN: iteration 32187 : loss : 0.251073, loss_ce: 0.002221, loss_dice: 0.499925
[13:00:24.082] TRAIN: iteration 32188 : loss : 0.053777, loss_ce: 0.002509, loss_dice: 0.105046
[13:00:24.289] TRAIN: iteration 32189 : loss : 0.037525, loss_ce: 0.001632, loss_dice: 0.073418
[13:00:24.504] TRAIN: iteration 32190 : loss : 0.032126, loss_ce: 0.001762, loss_dice: 0.062491
[13:00:24.714] TRAIN: iteration 32191 : loss : 0.141889, loss_ce: 0.001324, loss_dice: 0.282455
[13:00:24.929] TRAIN: iteration 32192 : loss : 0.056629, loss_ce: 0.002418, loss_dice: 0.110840
[13:00:25.142] TRAIN: iteration 32193 : loss : 0.027672, loss_ce: 0.001489, loss_dice: 0.053854
[13:00:25.358] TRAIN: iteration 32194 : loss : 0.057688, loss_ce: 0.000709, loss_dice: 0.114666
[13:00:25.567] TRAIN: iteration 32195 : loss : 0.116596, loss_ce: 0.006237, loss_dice: 0.226955
[13:00:25.774] TRAIN: iteration 32196 : loss : 0.039836, loss_ce: 0.004921, loss_dice: 0.074751
[13:00:25.983] TRAIN: iteration 32197 : loss : 0.054868, loss_ce: 0.006668, loss_dice: 0.103068
[13:00:26.192] TRAIN: iteration 32198 : loss : 0.251153, loss_ce: 0.002279, loss_dice: 0.500026
[13:00:26.402] TRAIN: iteration 32199 : loss : 0.035712, loss_ce: 0.000662, loss_dice: 0.070761
[13:00:26.611] TRAIN: iteration 32200 : loss : 0.094735, loss_ce: 0.004134, loss_dice: 0.185337
[13:00:26.854] TRAIN: iteration 32201 : loss : 0.048166, loss_ce: 0.001281, loss_dice: 0.095051
[13:00:27.063] TRAIN: iteration 32202 : loss : 0.247753, loss_ce: 0.003210, loss_dice: 0.492295
[13:00:27.271] TRAIN: iteration 32203 : loss : 0.026442, loss_ce: 0.000586, loss_dice: 0.052298
[13:00:28.489] TRAIN: iteration 32204 : loss : 0.140446, loss_ce: 0.002490, loss_dice: 0.278401
[13:00:28.702] TRAIN: iteration 32205 : loss : 0.070152, loss_ce: 0.001922, loss_dice: 0.138381
[13:00:28.910] TRAIN: iteration 32206 : loss : 0.053833, loss_ce: 0.001231, loss_dice: 0.106434
[13:00:29.119] TRAIN: iteration 32207 : loss : 0.250221, loss_ce: 0.000434, loss_dice: 0.500008
[13:00:29.327] TRAIN: iteration 32208 : loss : 0.250330, loss_ce: 0.000623, loss_dice: 0.500037
[13:00:29.534] TRAIN: iteration 32209 : loss : 0.250316, loss_ce: 0.000616, loss_dice: 0.500016
[13:00:29.742] TRAIN: iteration 32210 : loss : 0.194095, loss_ce: 0.000911, loss_dice: 0.387279
[13:00:29.951] TRAIN: iteration 32211 : loss : 0.075625, loss_ce: 0.001002, loss_dice: 0.150248
[13:00:30.159] TRAIN: iteration 32212 : loss : 0.037704, loss_ce: 0.000888, loss_dice: 0.074520
[13:00:30.367] TRAIN: iteration 32213 : loss : 0.045561, loss_ce: 0.001316, loss_dice: 0.089806
[13:00:30.579] TRAIN: iteration 32214 : loss : 0.085431, loss_ce: 0.003718, loss_dice: 0.167143
[13:00:30.790] TRAIN: iteration 32215 : loss : 0.070092, loss_ce: 0.006500, loss_dice: 0.133684
[13:00:31.328] TRAIN: iteration 32216 : loss : 0.084059, loss_ce: 0.002963, loss_dice: 0.165154
[13:00:31.536] TRAIN: iteration 32217 : loss : 0.122053, loss_ce: 0.001388, loss_dice: 0.242718
[13:00:31.743] TRAIN: iteration 32218 : loss : 0.250201, loss_ce: 0.000393, loss_dice: 0.500009
[13:00:31.950] TRAIN: iteration 32219 : loss : 0.050731, loss_ce: 0.000845, loss_dice: 0.100618
[13:00:32.798] TRAIN: iteration 32220 : loss : 0.250219, loss_ce: 0.000431, loss_dice: 0.500007
[13:00:33.047] TRAIN: iteration 32221 : loss : 0.250146, loss_ce: 0.000290, loss_dice: 0.500002
[13:00:33.254] TRAIN: iteration 32222 : loss : 0.036270, loss_ce: 0.001057, loss_dice: 0.071483
[13:00:33.463] TRAIN: iteration 32223 : loss : 0.207475, loss_ce: 0.000878, loss_dice: 0.414071
[13:00:33.670] TRAIN: iteration 32224 : loss : 0.024792, loss_ce: 0.003200, loss_dice: 0.046384
[13:00:33.883] TRAIN: iteration 32225 : loss : 0.250567, loss_ce: 0.001078, loss_dice: 0.500057
[13:00:34.090] TRAIN: iteration 32226 : loss : 0.043152, loss_ce: 0.001235, loss_dice: 0.085069
[13:00:34.299] TRAIN: iteration 32227 : loss : 0.035097, loss_ce: 0.001134, loss_dice: 0.069060
[13:00:35.716] TRAIN: iteration 32228 : loss : 0.117629, loss_ce: 0.016060, loss_dice: 0.219198
[13:00:35.928] TRAIN: iteration 32229 : loss : 0.041205, loss_ce: 0.003022, loss_dice: 0.079387
[13:00:36.141] TRAIN: iteration 32230 : loss : 0.065357, loss_ce: 0.009127, loss_dice: 0.121588
[13:00:36.350] TRAIN: iteration 32231 : loss : 0.092640, loss_ce: 0.007384, loss_dice: 0.177897
[13:00:36.559] TRAIN: iteration 32232 : loss : 0.050963, loss_ce: 0.004319, loss_dice: 0.097606
[13:00:36.768] TRAIN: iteration 32233 : loss : 0.073779, loss_ce: 0.002671, loss_dice: 0.144886
[13:00:36.978] TRAIN: iteration 32234 : loss : 0.019220, loss_ce: 0.000775, loss_dice: 0.037665
[13:00:37.188] TRAIN: iteration 32235 : loss : 0.026967, loss_ce: 0.004575, loss_dice: 0.049358
[13:00:37.398] TRAIN: iteration 32236 : loss : 0.228960, loss_ce: 0.001264, loss_dice: 0.456656
[13:00:37.606] TRAIN: iteration 32237 : loss : 0.026786, loss_ce: 0.003427, loss_dice: 0.050144
[13:00:37.813] TRAIN: iteration 32238 : loss : 0.089938, loss_ce: 0.001793, loss_dice: 0.178083
[13:00:38.020] TRAIN: iteration 32239 : loss : 0.070374, loss_ce: 0.002858, loss_dice: 0.137890
[13:00:38.479] TRAIN: iteration 32240 : loss : 0.043007, loss_ce: 0.002429, loss_dice: 0.083585
[13:00:38.748] TRAIN: iteration 32241 : loss : 0.094993, loss_ce: 0.003562, loss_dice: 0.186424
[13:00:39.432] TRAIN: iteration 32242 : loss : 0.250686, loss_ce: 0.001322, loss_dice: 0.500051
[13:00:39.639] TRAIN: iteration 32243 : loss : 0.077110, loss_ce: 0.011972, loss_dice: 0.142248
[13:00:39.848] TRAIN: iteration 32244 : loss : 0.075321, loss_ce: 0.001047, loss_dice: 0.149595
[13:00:40.060] TRAIN: iteration 32245 : loss : 0.129256, loss_ce: 0.003748, loss_dice: 0.254763
[13:00:40.268] TRAIN: iteration 32246 : loss : 0.040473, loss_ce: 0.002179, loss_dice: 0.078767
[13:00:40.476] TRAIN: iteration 32247 : loss : 0.036193, loss_ce: 0.003709, loss_dice: 0.068677
[13:00:41.600] TRAIN: iteration 32248 : loss : 0.081829, loss_ce: 0.006185, loss_dice: 0.157473
[13:00:44.445] TRAIN: iteration 32249 : loss : 0.250392, loss_ce: 0.000752, loss_dice: 0.500031
[13:00:44.656] TRAIN: iteration 32250 : loss : 0.168217, loss_ce: 0.000432, loss_dice: 0.336002
[13:00:44.863] TRAIN: iteration 32251 : loss : 0.135753, loss_ce: 0.001092, loss_dice: 0.270414
[13:00:45.071] TRAIN: iteration 32252 : loss : 0.021132, loss_ce: 0.002191, loss_dice: 0.040073
[13:00:45.280] TRAIN: iteration 32253 : loss : 0.056747, loss_ce: 0.002766, loss_dice: 0.110728
[13:00:45.492] TRAIN: iteration 32254 : loss : 0.077008, loss_ce: 0.002456, loss_dice: 0.151560
[13:00:45.700] TRAIN: iteration 32255 : loss : 0.083605, loss_ce: 0.001322, loss_dice: 0.165888
[13:00:45.908] TRAIN: iteration 32256 : loss : 0.109127, loss_ce: 0.005564, loss_dice: 0.212689
[13:00:46.801] TRAIN: iteration 32257 : loss : 0.071856, loss_ce: 0.005066, loss_dice: 0.138646
[13:00:47.009] TRAIN: iteration 32258 : loss : 0.099241, loss_ce: 0.005284, loss_dice: 0.193198
[13:00:47.217] TRAIN: iteration 32259 : loss : 0.085450, loss_ce: 0.003598, loss_dice: 0.167303
[13:00:47.424] TRAIN: iteration 32260 : loss : 0.039010, loss_ce: 0.005251, loss_dice: 0.072770
[13:00:47.666] TRAIN: iteration 32261 : loss : 0.092168, loss_ce: 0.003964, loss_dice: 0.180372
[13:00:47.878] TRAIN: iteration 32262 : loss : 0.067295, loss_ce: 0.007390, loss_dice: 0.127200
[13:00:48.086] TRAIN: iteration 32263 : loss : 0.043960, loss_ce: 0.001053, loss_dice: 0.086866
[13:00:48.297] TRAIN: iteration 32264 : loss : 0.250384, loss_ce: 0.000742, loss_dice: 0.500025
[13:00:49.210] TRAIN: iteration 32265 : loss : 0.250716, loss_ce: 0.001342, loss_dice: 0.500091
[13:00:49.424] TRAIN: iteration 32266 : loss : 0.053816, loss_ce: 0.002574, loss_dice: 0.105057
[13:00:49.631] TRAIN: iteration 32267 : loss : 0.050896, loss_ce: 0.001895, loss_dice: 0.099896
[13:00:49.839] TRAIN: iteration 32268 : loss : 0.080027, loss_ce: 0.008624, loss_dice: 0.151430
[13:00:50.049] TRAIN: iteration 32269 : loss : 0.032025, loss_ce: 0.003502, loss_dice: 0.060548
[13:00:50.258] TRAIN: iteration 32270 : loss : 0.037074, loss_ce: 0.004003, loss_dice: 0.070146
[13:00:50.467] TRAIN: iteration 32271 : loss : 0.051774, loss_ce: 0.005354, loss_dice: 0.098195
[13:00:50.678] TRAIN: iteration 32272 : loss : 0.250460, loss_ce: 0.000881, loss_dice: 0.500039
[13:00:51.265] TRAIN: iteration 32273 : loss : 0.021455, loss_ce: 0.001088, loss_dice: 0.041822
[13:00:51.475] TRAIN: iteration 32274 : loss : 0.250251, loss_ce: 0.000495, loss_dice: 0.500008
[13:00:51.684] TRAIN: iteration 32275 : loss : 0.045250, loss_ce: 0.003482, loss_dice: 0.087019
[13:00:51.891] TRAIN: iteration 32276 : loss : 0.056424, loss_ce: 0.001273, loss_dice: 0.111574
[13:00:53.049] TRAIN: iteration 32277 : loss : 0.017775, loss_ce: 0.001538, loss_dice: 0.034011
[13:00:53.256] TRAIN: iteration 32278 : loss : 0.038794, loss_ce: 0.003132, loss_dice: 0.074455
[13:00:53.463] TRAIN: iteration 32279 : loss : 0.250714, loss_ce: 0.001338, loss_dice: 0.500090
[13:00:53.673] TRAIN: iteration 32280 : loss : 0.250517, loss_ce: 0.000981, loss_dice: 0.500053
[13:00:53.914] TRAIN: iteration 32281 : loss : 0.250442, loss_ce: 0.000850, loss_dice: 0.500034
[13:00:54.123] TRAIN: iteration 32282 : loss : 0.226034, loss_ce: 0.003134, loss_dice: 0.448934
[13:00:54.411] TRAIN: iteration 32283 : loss : 0.191760, loss_ce: 0.007846, loss_dice: 0.375675
[13:00:54.618] TRAIN: iteration 32284 : loss : 0.055838, loss_ce: 0.004542, loss_dice: 0.107133
[13:00:54.825] TRAIN: iteration 32285 : loss : 0.037837, loss_ce: 0.001162, loss_dice: 0.074511
[13:00:55.032] TRAIN: iteration 32286 : loss : 0.085657, loss_ce: 0.002591, loss_dice: 0.168723
[13:00:55.239] TRAIN: iteration 32287 : loss : 0.037245, loss_ce: 0.002681, loss_dice: 0.071809
[13:00:55.458] TRAIN: iteration 32288 : loss : 0.042980, loss_ce: 0.011529, loss_dice: 0.074430
[13:00:55.665] TRAIN: iteration 32289 : loss : 0.253051, loss_ce: 0.005658, loss_dice: 0.500445
[13:00:55.873] TRAIN: iteration 32290 : loss : 0.200777, loss_ce: 0.003239, loss_dice: 0.398316
[13:00:56.080] TRAIN: iteration 32291 : loss : 0.154751, loss_ce: 0.000925, loss_dice: 0.308576
[13:00:56.287] TRAIN: iteration 32292 : loss : 0.079386, loss_ce: 0.013494, loss_dice: 0.145278
[13:00:56.496] TRAIN: iteration 32293 : loss : 0.250679, loss_ce: 0.001293, loss_dice: 0.500066
[13:00:56.705] TRAIN: iteration 32294 : loss : 0.042131, loss_ce: 0.003334, loss_dice: 0.080929
[13:00:56.912] TRAIN: iteration 32295 : loss : 0.043466, loss_ce: 0.000487, loss_dice: 0.086446
[13:00:57.120] TRAIN: iteration 32296 : loss : 0.091868, loss_ce: 0.006453, loss_dice: 0.177282
[13:00:57.332] TRAIN: iteration 32297 : loss : 0.116164, loss_ce: 0.002607, loss_dice: 0.229721
[13:00:57.539] TRAIN: iteration 32298 : loss : 0.119433, loss_ce: 0.002198, loss_dice: 0.236668
[13:00:57.746] TRAIN: iteration 32299 : loss : 0.039188, loss_ce: 0.001560, loss_dice: 0.076815
[13:00:57.954] TRAIN: iteration 32300 : loss : 0.056722, loss_ce: 0.001910, loss_dice: 0.111534
[13:00:58.192] TRAIN: iteration 32301 : loss : 0.042452, loss_ce: 0.002809, loss_dice: 0.082095
[13:00:59.247] TRAIN: iteration 32302 : loss : 0.051111, loss_ce: 0.004731, loss_dice: 0.097490
[13:00:59.459] TRAIN: iteration 32303 : loss : 0.027229, loss_ce: 0.001407, loss_dice: 0.053051
[13:00:59.668] TRAIN: iteration 32304 : loss : 0.012762, loss_ce: 0.000596, loss_dice: 0.024928
[13:00:59.884] TRAIN: iteration 32305 : loss : 0.208586, loss_ce: 0.008769, loss_dice: 0.408404
[13:01:00.094] TRAIN: iteration 32306 : loss : 0.250916, loss_ce: 0.001713, loss_dice: 0.500120
[13:01:00.303] TRAIN: iteration 32307 : loss : 0.100240, loss_ce: 0.002017, loss_dice: 0.198464
[13:01:00.784] TRAIN: iteration 32308 : loss : 0.023221, loss_ce: 0.002872, loss_dice: 0.043570
[13:01:00.992] TRAIN: iteration 32309 : loss : 0.060334, loss_ce: 0.002729, loss_dice: 0.117939
[13:01:01.206] TRAIN: iteration 32310 : loss : 0.073733, loss_ce: 0.004990, loss_dice: 0.142476
[13:01:01.421] TRAIN: iteration 32311 : loss : 0.249408, loss_ce: 0.000686, loss_dice: 0.498130
[13:01:01.634] TRAIN: iteration 32312 : loss : 0.241193, loss_ce: 0.001127, loss_dice: 0.481258
[13:01:01.843] TRAIN: iteration 32313 : loss : 0.249967, loss_ce: 0.001126, loss_dice: 0.498808
[13:01:02.060] TRAIN: iteration 32314 : loss : 0.243741, loss_ce: 0.005860, loss_dice: 0.481622
[13:01:02.268] TRAIN: iteration 32315 : loss : 0.250397, loss_ce: 0.000762, loss_dice: 0.500032
[13:01:03.775] TRAIN: iteration 32316 : loss : 0.069927, loss_ce: 0.003257, loss_dice: 0.136598
[13:01:03.985] TRAIN: iteration 32317 : loss : 0.127652, loss_ce: 0.003871, loss_dice: 0.251433
[13:01:04.281] TRAIN: iteration 32318 : loss : 0.034887, loss_ce: 0.001853, loss_dice: 0.067921
[13:01:04.491] TRAIN: iteration 32319 : loss : 0.092439, loss_ce: 0.002394, loss_dice: 0.182484
[13:01:04.701] TRAIN: iteration 32320 : loss : 0.026032, loss_ce: 0.002191, loss_dice: 0.049872
[13:01:04.939] TRAIN: iteration 32321 : loss : 0.239491, loss_ce: 0.001102, loss_dice: 0.477879
[13:01:05.151] TRAIN: iteration 32322 : loss : 0.109100, loss_ce: 0.001502, loss_dice: 0.216699
[13:01:05.359] TRAIN: iteration 32323 : loss : 0.019732, loss_ce: 0.001237, loss_dice: 0.038227
[13:01:05.610] TRAIN: iteration 32324 : loss : 0.107082, loss_ce: 0.002257, loss_dice: 0.211907
[13:01:05.820] TRAIN: iteration 32325 : loss : 0.038859, loss_ce: 0.004270, loss_dice: 0.073449
[13:01:06.874] TRAIN: iteration 32326 : loss : 0.225696, loss_ce: 0.001737, loss_dice: 0.449655
[13:01:07.087] TRAIN: iteration 32327 : loss : 0.071359, loss_ce: 0.002283, loss_dice: 0.140435
[13:01:07.295] TRAIN: iteration 32328 : loss : 0.130929, loss_ce: 0.004371, loss_dice: 0.257487
[13:01:07.504] TRAIN: iteration 32329 : loss : 0.117333, loss_ce: 0.002059, loss_dice: 0.232607
[13:01:07.721] TRAIN: iteration 32330 : loss : 0.026490, loss_ce: 0.000940, loss_dice: 0.052040
[13:01:07.929] TRAIN: iteration 32331 : loss : 0.079728, loss_ce: 0.001600, loss_dice: 0.157857
[13:01:08.193] TRAIN: iteration 32332 : loss : 0.038455, loss_ce: 0.001889, loss_dice: 0.075020
[13:01:08.401] TRAIN: iteration 32333 : loss : 0.048077, loss_ce: 0.000670, loss_dice: 0.095485
[13:01:09.459] TRAIN: iteration 32334 : loss : 0.036203, loss_ce: 0.003307, loss_dice: 0.069100
[13:01:09.674] TRAIN: iteration 32335 : loss : 0.100078, loss_ce: 0.002331, loss_dice: 0.197825
[13:01:09.881] TRAIN: iteration 32336 : loss : 0.083713, loss_ce: 0.001054, loss_dice: 0.166372
[13:01:10.764] TRAIN: iteration 32337 : loss : 0.019130, loss_ce: 0.000970, loss_dice: 0.037290
[13:01:10.974] TRAIN: iteration 32338 : loss : 0.250224, loss_ce: 0.000442, loss_dice: 0.500005
[13:01:11.198] TRAIN: iteration 32339 : loss : 0.069946, loss_ce: 0.004708, loss_dice: 0.135185
[13:01:11.407] TRAIN: iteration 32340 : loss : 0.178814, loss_ce: 0.001020, loss_dice: 0.356607
[13:01:11.645] TRAIN: iteration 32341 : loss : 0.024120, loss_ce: 0.002680, loss_dice: 0.045561
[13:01:11.853] TRAIN: iteration 32342 : loss : 0.039920, loss_ce: 0.002258, loss_dice: 0.077583
[13:01:12.065] TRAIN: iteration 32343 : loss : 0.028687, loss_ce: 0.001224, loss_dice: 0.056149
[13:01:12.272] TRAIN: iteration 32344 : loss : 0.250541, loss_ce: 0.001040, loss_dice: 0.500042
[13:01:12.839] TRAIN: iteration 32345 : loss : 0.032146, loss_ce: 0.002236, loss_dice: 0.062057
[13:01:13.046] TRAIN: iteration 32346 : loss : 0.027887, loss_ce: 0.002673, loss_dice: 0.053101
[13:01:13.256] TRAIN: iteration 32347 : loss : 0.043154, loss_ce: 0.002710, loss_dice: 0.083598
[13:01:13.464] TRAIN: iteration 32348 : loss : 0.168121, loss_ce: 0.028860, loss_dice: 0.307383
[13:01:13.672] TRAIN: iteration 32349 : loss : 0.033353, loss_ce: 0.001332, loss_dice: 0.065374
[13:01:13.883] TRAIN: iteration 32350 : loss : 0.080432, loss_ce: 0.002460, loss_dice: 0.158403
[13:01:14.093] TRAIN: iteration 32351 : loss : 0.119172, loss_ce: 0.024032, loss_dice: 0.214311
[13:01:14.303] TRAIN: iteration 32352 : loss : 0.065303, loss_ce: 0.005556, loss_dice: 0.125051
[13:01:14.512] TRAIN: iteration 32353 : loss : 0.090965, loss_ce: 0.005396, loss_dice: 0.176535
[13:01:14.721] TRAIN: iteration 32354 : loss : 0.028460, loss_ce: 0.001100, loss_dice: 0.055821
[13:01:14.932] TRAIN: iteration 32355 : loss : 0.229758, loss_ce: 0.001003, loss_dice: 0.458512
[13:01:15.556] TRAIN: iteration 32356 : loss : 0.063552, loss_ce: 0.004588, loss_dice: 0.122517
[13:01:15.763] TRAIN: iteration 32357 : loss : 0.250403, loss_ce: 0.000770, loss_dice: 0.500037
[13:01:15.970] TRAIN: iteration 32358 : loss : 0.047795, loss_ce: 0.008638, loss_dice: 0.086952
[13:01:16.178] TRAIN: iteration 32359 : loss : 0.021716, loss_ce: 0.003808, loss_dice: 0.039624
[13:01:16.385] TRAIN: iteration 32360 : loss : 0.081262, loss_ce: 0.002737, loss_dice: 0.159786
[13:01:16.622] TRAIN: iteration 32361 : loss : 0.106763, loss_ce: 0.002452, loss_dice: 0.211074
[13:01:16.832] TRAIN: iteration 32362 : loss : 0.039704, loss_ce: 0.003313, loss_dice: 0.076095
[13:01:19.307] TRAIN: iteration 32363 : loss : 0.035637, loss_ce: 0.003215, loss_dice: 0.068060
[13:01:19.516] TRAIN: iteration 32364 : loss : 0.048099, loss_ce: 0.002568, loss_dice: 0.093630
[13:01:19.727] TRAIN: iteration 32365 : loss : 0.041927, loss_ce: 0.001192, loss_dice: 0.082662
[13:01:19.934] TRAIN: iteration 32366 : loss : 0.131587, loss_ce: 0.006787, loss_dice: 0.256387
[13:01:20.143] TRAIN: iteration 32367 : loss : 0.088257, loss_ce: 0.002350, loss_dice: 0.174164
[13:01:20.354] TRAIN: iteration 32368 : loss : 0.056677, loss_ce: 0.006050, loss_dice: 0.107304
[13:01:20.562] TRAIN: iteration 32369 : loss : 0.027385, loss_ce: 0.001266, loss_dice: 0.053505
[13:01:20.770] TRAIN: iteration 32370 : loss : 0.052463, loss_ce: 0.001639, loss_dice: 0.103286
[13:01:22.351] TRAIN: iteration 32371 : loss : 0.055179, loss_ce: 0.001763, loss_dice: 0.108594
[13:01:22.559] TRAIN: iteration 32372 : loss : 0.032146, loss_ce: 0.002691, loss_dice: 0.061601
[13:01:22.767] TRAIN: iteration 32373 : loss : 0.022739, loss_ce: 0.000902, loss_dice: 0.044577
[13:01:22.977] TRAIN: iteration 32374 : loss : 0.033290, loss_ce: 0.002593, loss_dice: 0.063986
[13:01:23.188] TRAIN: iteration 32375 : loss : 0.025469, loss_ce: 0.004203, loss_dice: 0.046736
[13:01:23.396] TRAIN: iteration 32376 : loss : 0.155086, loss_ce: 0.006249, loss_dice: 0.303922
[13:01:23.604] TRAIN: iteration 32377 : loss : 0.030093, loss_ce: 0.003092, loss_dice: 0.057094
[13:01:23.817] TRAIN: iteration 32378 : loss : 0.040010, loss_ce: 0.004739, loss_dice: 0.075282
[13:01:24.026] TRAIN: iteration 32379 : loss : 0.155585, loss_ce: 0.001360, loss_dice: 0.309811
[13:01:24.241] TRAIN: iteration 32380 : loss : 0.040097, loss_ce: 0.001962, loss_dice: 0.078233
[13:01:24.481] TRAIN: iteration 32381 : loss : 0.029050, loss_ce: 0.003398, loss_dice: 0.054701
[13:01:24.712] TRAIN: iteration 32382 : loss : 0.069107, loss_ce: 0.001287, loss_dice: 0.136927
[13:01:24.925] TRAIN: iteration 32383 : loss : 0.244790, loss_ce: 0.001642, loss_dice: 0.487938
[13:01:25.134] TRAIN: iteration 32384 : loss : 0.049311, loss_ce: 0.003721, loss_dice: 0.094902
[13:01:25.342] TRAIN: iteration 32385 : loss : 0.043742, loss_ce: 0.002846, loss_dice: 0.084639
[13:01:25.551] TRAIN: iteration 32386 : loss : 0.075519, loss_ce: 0.003395, loss_dice: 0.147642
[13:01:25.766] TRAIN: iteration 32387 : loss : 0.108012, loss_ce: 0.001166, loss_dice: 0.214858
[13:01:25.975] TRAIN: iteration 32388 : loss : 0.251312, loss_ce: 0.002452, loss_dice: 0.500173
[13:01:26.338] TRAIN: iteration 32389 : loss : 0.033484, loss_ce: 0.001028, loss_dice: 0.065940
[13:01:27.771] TRAIN: iteration 32390 : loss : 0.062447, loss_ce: 0.006624, loss_dice: 0.118270
[13:01:27.980] TRAIN: iteration 32391 : loss : 0.111020, loss_ce: 0.002069, loss_dice: 0.219970
[13:01:28.188] TRAIN: iteration 32392 : loss : 0.250376, loss_ce: 0.000733, loss_dice: 0.500018
[13:01:28.395] TRAIN: iteration 32393 : loss : 0.140023, loss_ce: 0.001180, loss_dice: 0.278865
[13:01:28.602] TRAIN: iteration 32394 : loss : 0.038023, loss_ce: 0.001563, loss_dice: 0.074483
[13:01:28.810] TRAIN: iteration 32395 : loss : 0.251375, loss_ce: 0.002562, loss_dice: 0.500188
[13:01:29.017] TRAIN: iteration 32396 : loss : 0.059853, loss_ce: 0.002563, loss_dice: 0.117143
[13:01:29.226] TRAIN: iteration 32397 : loss : 0.042237, loss_ce: 0.001583, loss_dice: 0.082890
[13:01:33.775] TRAIN: iteration 32398 : loss : 0.065370, loss_ce: 0.002155, loss_dice: 0.128584
[13:01:33.987] TRAIN: iteration 32399 : loss : 0.224552, loss_ce: 0.001681, loss_dice: 0.447423
[13:01:34.197] TRAIN: iteration 32400 : loss : 0.112285, loss_ce: 0.001172, loss_dice: 0.223398
[13:01:34.438] TRAIN: iteration 32401 : loss : 0.082455, loss_ce: 0.004066, loss_dice: 0.160845
[13:01:34.647] TRAIN: iteration 32402 : loss : 0.035184, loss_ce: 0.002694, loss_dice: 0.067674
[13:01:34.859] TRAIN: iteration 32403 : loss : 0.025272, loss_ce: 0.001035, loss_dice: 0.049509
[13:01:35.074] TRAIN: iteration 32404 : loss : 0.093541, loss_ce: 0.006834, loss_dice: 0.180247
[13:01:35.284] TRAIN: iteration 32405 : loss : 0.127885, loss_ce: 0.002237, loss_dice: 0.253533
[13:01:35.492] TRAIN: iteration 32406 : loss : 0.159569, loss_ce: 0.002274, loss_dice: 0.316864
[13:01:35.704] TRAIN: iteration 32407 : loss : 0.049730, loss_ce: 0.000469, loss_dice: 0.098991
[13:01:35.912] TRAIN: iteration 32408 : loss : 0.250631, loss_ce: 0.001183, loss_dice: 0.500079
[13:01:36.120] TRAIN: iteration 32409 : loss : 0.031468, loss_ce: 0.001838, loss_dice: 0.061098
[13:01:36.329] TRAIN: iteration 32410 : loss : 0.026353, loss_ce: 0.000338, loss_dice: 0.052368
[13:01:36.537] TRAIN: iteration 32411 : loss : 0.028751, loss_ce: 0.002276, loss_dice: 0.055227
[13:01:36.745] TRAIN: iteration 32412 : loss : 0.028320, loss_ce: 0.001688, loss_dice: 0.054953
[13:01:36.952] TRAIN: iteration 32413 : loss : 0.025942, loss_ce: 0.001182, loss_dice: 0.050702
[13:01:38.130] TRAIN: iteration 32414 : loss : 0.052313, loss_ce: 0.002533, loss_dice: 0.102093
[13:01:38.337] TRAIN: iteration 32415 : loss : 0.086031, loss_ce: 0.001443, loss_dice: 0.170620
[13:01:38.545] TRAIN: iteration 32416 : loss : 0.048902, loss_ce: 0.005940, loss_dice: 0.091865
[13:01:38.752] TRAIN: iteration 32417 : loss : 0.183338, loss_ce: 0.001343, loss_dice: 0.365334
[13:01:38.963] TRAIN: iteration 32418 : loss : 0.015767, loss_ce: 0.000526, loss_dice: 0.031008
[13:01:39.175] TRAIN: iteration 32419 : loss : 0.143146, loss_ce: 0.007420, loss_dice: 0.278871
[13:01:39.382] TRAIN: iteration 32420 : loss : 0.028854, loss_ce: 0.000841, loss_dice: 0.056867
[13:01:39.383] NaN or Inf found in input tensor.
[13:01:39.606] TRAIN: iteration 32421 : loss : 0.048536, loss_ce: 0.006103, loss_dice: 0.090969
[13:01:43.447] TRAIN: iteration 32422 : loss : 0.115666, loss_ce: 0.005939, loss_dice: 0.225392
[13:01:43.654] TRAIN: iteration 32423 : loss : 0.047283, loss_ce: 0.005127, loss_dice: 0.089439
[13:01:43.862] TRAIN: iteration 32424 : loss : 0.048919, loss_ce: 0.003489, loss_dice: 0.094349
[13:01:44.069] TRAIN: iteration 32425 : loss : 0.040959, loss_ce: 0.001163, loss_dice: 0.080755
[13:01:44.277] TRAIN: iteration 32426 : loss : 0.247979, loss_ce: 0.001019, loss_dice: 0.494939
[13:01:44.491] TRAIN: iteration 32427 : loss : 0.112882, loss_ce: 0.003036, loss_dice: 0.222728
[13:01:44.699] TRAIN: iteration 32428 : loss : 0.120530, loss_ce: 0.007182, loss_dice: 0.233879
[13:01:44.906] TRAIN: iteration 32429 : loss : 0.064590, loss_ce: 0.006390, loss_dice: 0.122791
[13:01:46.788] TRAIN: iteration 32430 : loss : 0.043486, loss_ce: 0.006555, loss_dice: 0.080418
[13:01:46.998] TRAIN: iteration 32431 : loss : 0.193670, loss_ce: 0.001953, loss_dice: 0.385387
[13:01:47.206] TRAIN: iteration 32432 : loss : 0.200956, loss_ce: 0.005312, loss_dice: 0.396601
[13:01:47.414] TRAIN: iteration 32433 : loss : 0.142070, loss_ce: 0.005582, loss_dice: 0.278558
[13:01:47.623] TRAIN: iteration 32434 : loss : 0.042450, loss_ce: 0.001189, loss_dice: 0.083712
[13:01:47.836] TRAIN: iteration 32435 : loss : 0.040677, loss_ce: 0.000491, loss_dice: 0.080864
[13:01:48.045] TRAIN: iteration 32436 : loss : 0.113087, loss_ce: 0.002972, loss_dice: 0.223203
[13:01:48.253] TRAIN: iteration 32437 : loss : 0.088673, loss_ce: 0.001494, loss_dice: 0.175853
[13:01:49.289] TRAIN: iteration 32438 : loss : 0.231600, loss_ce: 0.001233, loss_dice: 0.461966
[13:01:49.496] TRAIN: iteration 32439 : loss : 0.050866, loss_ce: 0.001701, loss_dice: 0.100031
[13:01:49.704] TRAIN: iteration 32440 : loss : 0.178106, loss_ce: 0.002338, loss_dice: 0.353873
[13:01:49.944] TRAIN: iteration 32441 : loss : 0.058660, loss_ce: 0.001964, loss_dice: 0.115356
[13:01:50.153] TRAIN: iteration 32442 : loss : 0.170889, loss_ce: 0.003171, loss_dice: 0.338607
[13:01:50.362] TRAIN: iteration 32443 : loss : 0.054903, loss_ce: 0.002425, loss_dice: 0.107382
[13:01:50.570] TRAIN: iteration 32444 : loss : 0.175884, loss_ce: 0.001348, loss_dice: 0.350420
[13:01:50.776] TRAIN: iteration 32445 : loss : 0.018420, loss_ce: 0.000980, loss_dice: 0.035860
[13:01:52.562] TRAIN: iteration 32446 : loss : 0.022170, loss_ce: 0.002353, loss_dice: 0.041986
[13:01:52.769] TRAIN: iteration 32447 : loss : 0.034048, loss_ce: 0.004341, loss_dice: 0.063755
[13:01:52.978] TRAIN: iteration 32448 : loss : 0.114565, loss_ce: 0.003947, loss_dice: 0.225183
[13:01:53.188] TRAIN: iteration 32449 : loss : 0.050992, loss_ce: 0.003937, loss_dice: 0.098047
[13:01:54.149] TRAIN: iteration 32450 : loss : 0.062792, loss_ce: 0.002396, loss_dice: 0.123188
[13:01:54.357] TRAIN: iteration 32451 : loss : 0.078976, loss_ce: 0.005153, loss_dice: 0.152800
[13:01:54.569] TRAIN: iteration 32452 : loss : 0.076937, loss_ce: 0.007255, loss_dice: 0.146620
[13:01:54.776] TRAIN: iteration 32453 : loss : 0.250377, loss_ce: 0.000730, loss_dice: 0.500023
[13:01:56.299] TRAIN: iteration 32454 : loss : 0.036532, loss_ce: 0.000982, loss_dice: 0.072082
[13:01:56.507] TRAIN: iteration 32455 : loss : 0.130444, loss_ce: 0.001302, loss_dice: 0.259586
[13:01:56.716] TRAIN: iteration 32456 : loss : 0.064988, loss_ce: 0.001349, loss_dice: 0.128628
[13:01:56.925] TRAIN: iteration 32457 : loss : 0.031338, loss_ce: 0.001008, loss_dice: 0.061668
[13:01:58.141] TRAIN: iteration 32458 : loss : 0.075812, loss_ce: 0.003884, loss_dice: 0.147740
[13:01:58.349] TRAIN: iteration 32459 : loss : 0.218468, loss_ce: 0.001572, loss_dice: 0.435365
[13:01:58.558] TRAIN: iteration 32460 : loss : 0.075466, loss_ce: 0.001879, loss_dice: 0.149053
[13:01:58.792] TRAIN: iteration 32461 : loss : 0.040802, loss_ce: 0.002333, loss_dice: 0.079271
[13:02:00.451] TRAIN: iteration 32462 : loss : 0.027196, loss_ce: 0.004033, loss_dice: 0.050359
[13:02:00.659] TRAIN: iteration 32463 : loss : 0.050825, loss_ce: 0.004176, loss_dice: 0.097474
[13:02:00.866] TRAIN: iteration 32464 : loss : 0.181126, loss_ce: 0.001835, loss_dice: 0.360418
[13:02:01.076] TRAIN: iteration 32465 : loss : 0.027570, loss_ce: 0.003872, loss_dice: 0.051268
[13:02:01.286] TRAIN: iteration 32466 : loss : 0.250848, loss_ce: 0.001586, loss_dice: 0.500111
[13:02:01.496] TRAIN: iteration 32467 : loss : 0.250449, loss_ce: 0.000871, loss_dice: 0.500027
[13:02:01.805] TRAIN: iteration 32468 : loss : 0.250336, loss_ce: 0.000657, loss_dice: 0.500016
[13:02:02.013] TRAIN: iteration 32469 : loss : 0.251947, loss_ce: 0.003617, loss_dice: 0.500278
[13:02:02.784] TRAIN: iteration 32470 : loss : 0.037092, loss_ce: 0.002469, loss_dice: 0.071716
[13:02:02.992] TRAIN: iteration 32471 : loss : 0.058943, loss_ce: 0.003169, loss_dice: 0.114717
[13:02:04.369] TRAIN: iteration 32472 : loss : 0.162707, loss_ce: 0.004500, loss_dice: 0.320914
[13:02:04.578] TRAIN: iteration 32473 : loss : 0.250390, loss_ce: 0.000756, loss_dice: 0.500024
[13:02:04.786] TRAIN: iteration 32474 : loss : 0.250289, loss_ce: 0.000566, loss_dice: 0.500013
[13:02:04.993] TRAIN: iteration 32475 : loss : 0.042888, loss_ce: 0.002533, loss_dice: 0.083243
[13:02:05.200] TRAIN: iteration 32476 : loss : 0.087570, loss_ce: 0.001059, loss_dice: 0.174081
[13:02:05.407] TRAIN: iteration 32477 : loss : 0.046188, loss_ce: 0.003189, loss_dice: 0.089188
[13:02:05.614] TRAIN: iteration 32478 : loss : 0.048413, loss_ce: 0.000713, loss_dice: 0.096113
[13:02:05.822] TRAIN: iteration 32479 : loss : 0.013884, loss_ce: 0.001294, loss_dice: 0.026473
[13:02:08.429] TRAIN: iteration 32480 : loss : 0.135140, loss_ce: 0.002291, loss_dice: 0.267988
[13:02:08.663] TRAIN: iteration 32481 : loss : 0.146657, loss_ce: 0.004368, loss_dice: 0.288945
[13:02:08.871] TRAIN: iteration 32482 : loss : 0.097787, loss_ce: 0.003108, loss_dice: 0.192466
[13:02:09.079] TRAIN: iteration 32483 : loss : 0.073582, loss_ce: 0.001686, loss_dice: 0.145478
[13:02:09.288] TRAIN: iteration 32484 : loss : 0.152749, loss_ce: 0.001750, loss_dice: 0.303749
[13:02:09.495] TRAIN: iteration 32485 : loss : 0.084472, loss_ce: 0.000883, loss_dice: 0.168062
[13:02:09.701] TRAIN: iteration 32486 : loss : 0.056400, loss_ce: 0.002391, loss_dice: 0.110409
[13:02:09.912] TRAIN: iteration 32487 : loss : 0.077586, loss_ce: 0.004112, loss_dice: 0.151060
[13:02:11.778] TRAIN: iteration 32488 : loss : 0.074925, loss_ce: 0.005468, loss_dice: 0.144381
[13:02:11.987] TRAIN: iteration 32489 : loss : 0.043485, loss_ce: 0.003896, loss_dice: 0.083075
[13:02:12.196] TRAIN: iteration 32490 : loss : 0.018822, loss_ce: 0.002085, loss_dice: 0.035558
[13:02:12.407] TRAIN: iteration 32491 : loss : 0.116328, loss_ce: 0.003379, loss_dice: 0.229277
[13:02:12.622] TRAIN: iteration 32492 : loss : 0.060201, loss_ce: 0.005460, loss_dice: 0.114942
[13:02:12.830] TRAIN: iteration 32493 : loss : 0.187841, loss_ce: 0.006173, loss_dice: 0.369509
[13:02:13.039] TRAIN: iteration 32494 : loss : 0.052677, loss_ce: 0.003638, loss_dice: 0.101717
[13:02:13.248] TRAIN: iteration 32495 : loss : 0.251282, loss_ce: 0.002386, loss_dice: 0.500177
[13:02:13.915] TRAIN: iteration 32496 : loss : 0.016921, loss_ce: 0.000975, loss_dice: 0.032868
[13:02:14.122] TRAIN: iteration 32497 : loss : 0.239483, loss_ce: 0.000918, loss_dice: 0.478049
[13:02:14.333] TRAIN: iteration 32498 : loss : 0.250182, loss_ce: 0.000805, loss_dice: 0.499560
[13:02:14.540] TRAIN: iteration 32499 : loss : 0.071111, loss_ce: 0.006623, loss_dice: 0.135600
[13:02:14.748] TRAIN: iteration 32500 : loss : 0.025366, loss_ce: 0.000650, loss_dice: 0.050081
[13:02:14.994] TRAIN: iteration 32501 : loss : 0.059001, loss_ce: 0.001250, loss_dice: 0.116753
[13:02:16.435] TRAIN: iteration 32502 : loss : 0.250357, loss_ce: 0.000686, loss_dice: 0.500027
[13:02:16.649] TRAIN: iteration 32503 : loss : 0.123172, loss_ce: 0.002398, loss_dice: 0.243945
[13:02:16.859] TRAIN: iteration 32504 : loss : 0.134574, loss_ce: 0.004784, loss_dice: 0.264365
[13:02:17.072] TRAIN: iteration 32505 : loss : 0.243590, loss_ce: 0.000856, loss_dice: 0.486325
[13:02:20.661] TRAIN: iteration 32506 : loss : 0.086237, loss_ce: 0.000709, loss_dice: 0.171766
[13:02:20.871] TRAIN: iteration 32507 : loss : 0.250490, loss_ce: 0.000923, loss_dice: 0.500056
[13:02:21.079] TRAIN: iteration 32508 : loss : 0.126152, loss_ce: 0.003777, loss_dice: 0.248526
[13:02:21.287] TRAIN: iteration 32509 : loss : 0.120336, loss_ce: 0.004265, loss_dice: 0.236406
[13:02:21.494] TRAIN: iteration 32510 : loss : 0.025383, loss_ce: 0.002251, loss_dice: 0.048514
[13:02:21.701] TRAIN: iteration 32511 : loss : 0.177049, loss_ce: 0.002344, loss_dice: 0.351754
[13:02:21.910] TRAIN: iteration 32512 : loss : 0.085265, loss_ce: 0.003403, loss_dice: 0.167127
[13:02:22.119] TRAIN: iteration 32513 : loss : 0.066830, loss_ce: 0.005210, loss_dice: 0.128449
[13:02:24.771] TRAIN: iteration 32514 : loss : 0.244504, loss_ce: 0.000832, loss_dice: 0.488175
[13:02:24.977] TRAIN: iteration 32515 : loss : 0.054313, loss_ce: 0.000863, loss_dice: 0.107764
[13:02:25.187] TRAIN: iteration 32516 : loss : 0.013720, loss_ce: 0.001860, loss_dice: 0.025580
[13:02:25.394] TRAIN: iteration 32517 : loss : 0.251523, loss_ce: 0.002855, loss_dice: 0.500191
[13:02:25.604] TRAIN: iteration 32518 : loss : 0.029152, loss_ce: 0.002727, loss_dice: 0.055577
[13:02:25.813] TRAIN: iteration 32519 : loss : 0.133381, loss_ce: 0.002194, loss_dice: 0.264567
[13:02:26.019] TRAIN: iteration 32520 : loss : 0.183721, loss_ce: 0.001608, loss_dice: 0.365833
[13:02:26.253] TRAIN: iteration 32521 : loss : 0.148483, loss_ce: 0.003264, loss_dice: 0.293702
[13:02:26.460] TRAIN: iteration 32522 : loss : 0.250525, loss_ce: 0.001004, loss_dice: 0.500045
[13:02:26.667] TRAIN: iteration 32523 : loss : 0.053818, loss_ce: 0.005081, loss_dice: 0.102556
[13:02:26.876] TRAIN: iteration 32524 : loss : 0.040498, loss_ce: 0.001028, loss_dice: 0.079968
[13:02:27.084] TRAIN: iteration 32525 : loss : 0.037145, loss_ce: 0.001872, loss_dice: 0.072419
[13:02:27.294] TRAIN: iteration 32526 : loss : 0.100201, loss_ce: 0.000941, loss_dice: 0.199461
[13:02:27.502] TRAIN: iteration 32527 : loss : 0.095588, loss_ce: 0.005035, loss_dice: 0.186141
[13:02:27.710] TRAIN: iteration 32528 : loss : 0.076656, loss_ce: 0.001931, loss_dice: 0.151381
[13:02:27.916] TRAIN: iteration 32529 : loss : 0.075647, loss_ce: 0.002830, loss_dice: 0.148465
[13:02:28.537] TRAIN: iteration 32530 : loss : 0.250720, loss_ce: 0.001361, loss_dice: 0.500078
[13:02:28.751] TRAIN: iteration 32531 : loss : 0.187801, loss_ce: 0.001808, loss_dice: 0.373793
[13:02:28.959] TRAIN: iteration 32532 : loss : 0.251206, loss_ce: 0.002260, loss_dice: 0.500152
[13:02:29.739] TRAIN: iteration 32533 : loss : 0.029336, loss_ce: 0.001151, loss_dice: 0.057520
[13:02:30.514] TRAIN: iteration 32534 : loss : 0.247531, loss_ce: 0.002602, loss_dice: 0.492459
[13:02:30.732] TRAIN: iteration 32535 : loss : 0.044441, loss_ce: 0.003374, loss_dice: 0.085507
[13:02:31.091] TRAIN: iteration 32536 : loss : 0.058699, loss_ce: 0.005654, loss_dice: 0.111744
[13:02:31.300] TRAIN: iteration 32537 : loss : 0.097616, loss_ce: 0.001718, loss_dice: 0.193513
[13:02:31.507] TRAIN: iteration 32538 : loss : 0.250725, loss_ce: 0.001379, loss_dice: 0.500072
[13:02:31.715] TRAIN: iteration 32539 : loss : 0.104641, loss_ce: 0.003241, loss_dice: 0.206042
[13:02:31.924] TRAIN: iteration 32540 : loss : 0.031326, loss_ce: 0.002896, loss_dice: 0.059755
[13:02:32.581] TRAIN: iteration 32541 : loss : 0.041937, loss_ce: 0.006278, loss_dice: 0.077595
[13:02:33.907] TRAIN: iteration 32542 : loss : 0.078375, loss_ce: 0.001929, loss_dice: 0.154822
[13:02:34.121] TRAIN: iteration 32543 : loss : 0.065755, loss_ce: 0.001603, loss_dice: 0.129906
[13:02:34.330] TRAIN: iteration 32544 : loss : 0.049179, loss_ce: 0.007690, loss_dice: 0.090668
[13:02:34.545] TRAIN: iteration 32545 : loss : 0.085455, loss_ce: 0.005289, loss_dice: 0.165620
[13:02:34.755] TRAIN: iteration 32546 : loss : 0.045796, loss_ce: 0.002646, loss_dice: 0.088946
[13:02:35.257] TRAIN: iteration 32547 : loss : 0.100972, loss_ce: 0.004709, loss_dice: 0.197235
[13:02:35.464] TRAIN: iteration 32548 : loss : 0.029703, loss_ce: 0.001821, loss_dice: 0.057585
[13:02:36.685] TRAIN: iteration 32549 : loss : 0.065868, loss_ce: 0.000803, loss_dice: 0.130932
[13:02:39.451] TRAIN: iteration 32550 : loss : 0.162222, loss_ce: 0.002883, loss_dice: 0.321561
[13:02:39.659] TRAIN: iteration 32551 : loss : 0.250325, loss_ce: 0.000630, loss_dice: 0.500019
[13:02:39.871] TRAIN: iteration 32552 : loss : 0.058823, loss_ce: 0.003487, loss_dice: 0.114159
[13:02:40.078] TRAIN: iteration 32553 : loss : 0.029832, loss_ce: 0.002020, loss_dice: 0.057644
[13:02:40.288] TRAIN: iteration 32554 : loss : 0.037749, loss_ce: 0.001039, loss_dice: 0.074459
[13:02:40.527] TRAIN: iteration 32555 : loss : 0.033722, loss_ce: 0.002322, loss_dice: 0.065122
[13:02:40.737] TRAIN: iteration 32556 : loss : 0.039075, loss_ce: 0.002708, loss_dice: 0.075441
[13:02:40.945] TRAIN: iteration 32557 : loss : 0.085181, loss_ce: 0.002469, loss_dice: 0.167893
[13:02:43.016] TRAIN: iteration 32558 : loss : 0.058211, loss_ce: 0.014188, loss_dice: 0.102234
[13:02:43.224] TRAIN: iteration 32559 : loss : 0.222730, loss_ce: 0.002905, loss_dice: 0.442555
[13:02:43.431] TRAIN: iteration 32560 : loss : 0.044667, loss_ce: 0.003164, loss_dice: 0.086169
[13:02:43.667] TRAIN: iteration 32561 : loss : 0.036963, loss_ce: 0.002806, loss_dice: 0.071121
[13:02:43.877] TRAIN: iteration 32562 : loss : 0.040427, loss_ce: 0.000821, loss_dice: 0.080032
[13:02:44.085] TRAIN: iteration 32563 : loss : 0.056666, loss_ce: 0.006565, loss_dice: 0.106767
[13:02:44.292] TRAIN: iteration 32564 : loss : 0.250333, loss_ce: 0.000645, loss_dice: 0.500021
[13:02:44.508] TRAIN: iteration 32565 : loss : 0.042428, loss_ce: 0.000828, loss_dice: 0.084028
[13:02:47.121] TRAIN: iteration 32566 : loss : 0.016837, loss_ce: 0.000908, loss_dice: 0.032766
[13:02:47.328] TRAIN: iteration 32567 : loss : 0.038047, loss_ce: 0.000531, loss_dice: 0.075564
[13:02:47.537] TRAIN: iteration 32568 : loss : 0.020343, loss_ce: 0.000937, loss_dice: 0.039749
[13:02:47.744] TRAIN: iteration 32569 : loss : 0.251428, loss_ce: 0.003207, loss_dice: 0.499648
[13:02:47.951] TRAIN: iteration 32570 : loss : 0.221010, loss_ce: 0.001197, loss_dice: 0.440823
[13:02:48.158] TRAIN: iteration 32571 : loss : 0.250460, loss_ce: 0.000874, loss_dice: 0.500046
[13:02:48.365] TRAIN: iteration 32572 : loss : 0.214795, loss_ce: 0.001354, loss_dice: 0.428236
[13:02:48.573] TRAIN: iteration 32573 : loss : 0.027848, loss_ce: 0.005113, loss_dice: 0.050583
[13:02:51.158] TRAIN: iteration 32574 : loss : 0.031791, loss_ce: 0.000415, loss_dice: 0.063166
[13:02:51.368] TRAIN: iteration 32575 : loss : 0.052120, loss_ce: 0.002034, loss_dice: 0.102206
[13:02:51.579] TRAIN: iteration 32576 : loss : 0.041270, loss_ce: 0.003902, loss_dice: 0.078638
[13:02:51.789] TRAIN: iteration 32577 : loss : 0.141442, loss_ce: 0.003038, loss_dice: 0.279846
[13:02:51.997] TRAIN: iteration 32578 : loss : 0.057441, loss_ce: 0.000645, loss_dice: 0.114237
[13:02:52.206] TRAIN: iteration 32579 : loss : 0.250149, loss_ce: 0.000294, loss_dice: 0.500005
[13:02:52.412] TRAIN: iteration 32580 : loss : 0.144606, loss_ce: 0.001187, loss_dice: 0.288024
[13:02:52.649] TRAIN: iteration 32581 : loss : 0.047391, loss_ce: 0.001687, loss_dice: 0.093094
[13:02:54.752] TRAIN: iteration 32582 : loss : 0.035223, loss_ce: 0.000531, loss_dice: 0.069916
[13:02:54.959] TRAIN: iteration 32583 : loss : 0.038154, loss_ce: 0.002076, loss_dice: 0.074233
[13:02:55.166] TRAIN: iteration 32584 : loss : 0.250247, loss_ce: 0.000478, loss_dice: 0.500015
[13:02:55.374] TRAIN: iteration 32585 : loss : 0.085485, loss_ce: 0.003389, loss_dice: 0.167582
[13:02:55.582] TRAIN: iteration 32586 : loss : 0.173371, loss_ce: 0.002510, loss_dice: 0.344233
[13:02:55.789] TRAIN: iteration 32587 : loss : 0.044200, loss_ce: 0.002696, loss_dice: 0.085704
[13:02:55.996] TRAIN: iteration 32588 : loss : 0.081796, loss_ce: 0.003826, loss_dice: 0.159766
[13:02:56.207] TRAIN: iteration 32589 : loss : 0.036705, loss_ce: 0.002353, loss_dice: 0.071057
[13:02:56.970] TRAIN: iteration 32590 : loss : 0.059811, loss_ce: 0.001883, loss_dice: 0.117740
[13:02:57.177] TRAIN: iteration 32591 : loss : 0.161511, loss_ce: 0.006281, loss_dice: 0.316741
[13:02:57.387] TRAIN: iteration 32592 : loss : 0.171189, loss_ce: 0.001796, loss_dice: 0.340582
[13:02:57.594] TRAIN: iteration 32593 : loss : 0.250262, loss_ce: 0.000516, loss_dice: 0.500008
[13:02:57.802] TRAIN: iteration 32594 : loss : 0.031232, loss_ce: 0.004286, loss_dice: 0.058178
[13:02:58.009] TRAIN: iteration 32595 : loss : 0.131013, loss_ce: 0.003895, loss_dice: 0.258131
[13:02:58.219] TRAIN: iteration 32596 : loss : 0.139277, loss_ce: 0.001477, loss_dice: 0.277076
[13:02:58.428] TRAIN: iteration 32597 : loss : 0.247978, loss_ce: 0.001733, loss_dice: 0.494224
[13:03:02.625] TRAIN: iteration 32598 : loss : 0.022634, loss_ce: 0.000957, loss_dice: 0.044311
[13:03:02.838] TRAIN: iteration 32599 : loss : 0.051455, loss_ce: 0.001881, loss_dice: 0.101029
[13:03:03.103] TRAIN: iteration 32600 : loss : 0.050205, loss_ce: 0.002535, loss_dice: 0.097876
[13:03:03.334] TRAIN: iteration 32601 : loss : 0.149123, loss_ce: 0.000980, loss_dice: 0.297267
[13:03:03.543] TRAIN: iteration 32602 : loss : 0.160200, loss_ce: 0.001403, loss_dice: 0.318998
[13:03:03.750] TRAIN: iteration 32603 : loss : 0.049643, loss_ce: 0.001245, loss_dice: 0.098041
[13:03:03.958] TRAIN: iteration 32604 : loss : 0.188937, loss_ce: 0.008612, loss_dice: 0.369262
[13:03:04.166] TRAIN: iteration 32605 : loss : 0.026677, loss_ce: 0.000612, loss_dice: 0.052741
[13:03:04.373] TRAIN: iteration 32606 : loss : 0.085176, loss_ce: 0.003246, loss_dice: 0.167106
[13:03:04.580] TRAIN: iteration 32607 : loss : 0.153023, loss_ce: 0.000802, loss_dice: 0.305244
[13:03:04.789] TRAIN: iteration 32608 : loss : 0.028092, loss_ce: 0.002599, loss_dice: 0.053585
[13:03:06.225] TRAIN: iteration 32609 : loss : 0.102840, loss_ce: 0.008161, loss_dice: 0.197519
[13:03:06.433] TRAIN: iteration 32610 : loss : 0.186509, loss_ce: 0.026342, loss_dice: 0.346675
[13:03:06.648] TRAIN: iteration 32611 : loss : 0.221845, loss_ce: 0.001679, loss_dice: 0.442011
[13:03:06.855] TRAIN: iteration 32612 : loss : 0.037516, loss_ce: 0.001420, loss_dice: 0.073612
[13:03:07.067] TRAIN: iteration 32613 : loss : 0.022920, loss_ce: 0.002043, loss_dice: 0.043798
[13:03:11.966] TRAIN: iteration 32614 : loss : 0.042809, loss_ce: 0.002462, loss_dice: 0.083157
[13:03:12.180] TRAIN: iteration 32615 : loss : 0.028631, loss_ce: 0.001432, loss_dice: 0.055831
[13:03:12.390] TRAIN: iteration 32616 : loss : 0.037532, loss_ce: 0.000540, loss_dice: 0.074525
[13:03:12.598] TRAIN: iteration 32617 : loss : 0.042873, loss_ce: 0.004739, loss_dice: 0.081008
[13:03:12.813] TRAIN: iteration 32618 : loss : 0.066528, loss_ce: 0.003745, loss_dice: 0.129312
[13:03:13.022] TRAIN: iteration 32619 : loss : 0.069843, loss_ce: 0.011095, loss_dice: 0.128590
[13:03:13.229] TRAIN: iteration 32620 : loss : 0.111569, loss_ce: 0.001806, loss_dice: 0.221332
[13:03:13.467] TRAIN: iteration 32621 : loss : 0.094463, loss_ce: 0.000931, loss_dice: 0.187995
[13:03:14.284] TRAIN: iteration 32622 : loss : 0.116660, loss_ce: 0.018159, loss_dice: 0.215161
[13:03:14.499] TRAIN: iteration 32623 : loss : 0.087688, loss_ce: 0.003819, loss_dice: 0.171558
[13:03:14.715] TRAIN: iteration 32624 : loss : 0.092607, loss_ce: 0.001960, loss_dice: 0.183253
[13:03:14.922] TRAIN: iteration 32625 : loss : 0.106402, loss_ce: 0.001856, loss_dice: 0.210948
[13:03:15.238] TRAIN: iteration 32626 : loss : 0.011210, loss_ce: 0.001191, loss_dice: 0.021230
[13:03:15.447] TRAIN: iteration 32627 : loss : 0.086089, loss_ce: 0.003629, loss_dice: 0.168549
[13:03:15.655] TRAIN: iteration 32628 : loss : 0.221079, loss_ce: 0.002149, loss_dice: 0.440010
[13:03:15.864] TRAIN: iteration 32629 : loss : 0.043054, loss_ce: 0.006995, loss_dice: 0.079113
[13:03:19.499] TRAIN: iteration 32630 : loss : 0.089667, loss_ce: 0.002146, loss_dice: 0.177188
[13:03:19.707] TRAIN: iteration 32631 : loss : 0.056091, loss_ce: 0.004621, loss_dice: 0.107562
[13:03:19.916] TRAIN: iteration 32632 : loss : 0.075021, loss_ce: 0.001121, loss_dice: 0.148922
[13:03:20.124] TRAIN: iteration 32633 : loss : 0.046004, loss_ce: 0.003050, loss_dice: 0.088958
[13:03:20.331] TRAIN: iteration 32634 : loss : 0.121684, loss_ce: 0.002406, loss_dice: 0.240962
[13:03:20.538] TRAIN: iteration 32635 : loss : 0.120066, loss_ce: 0.001752, loss_dice: 0.238380
[13:03:20.746] TRAIN: iteration 32636 : loss : 0.159415, loss_ce: 0.001951, loss_dice: 0.316879
[13:03:20.954] TRAIN: iteration 32637 : loss : 0.080977, loss_ce: 0.002357, loss_dice: 0.159596
[13:03:22.724] TRAIN: iteration 32638 : loss : 0.216585, loss_ce: 0.001391, loss_dice: 0.431780
[13:03:22.931] TRAIN: iteration 32639 : loss : 0.064465, loss_ce: 0.001772, loss_dice: 0.127159
[13:03:23.999] TRAIN: iteration 32640 : loss : 0.050749, loss_ce: 0.001632, loss_dice: 0.099866
[13:03:24.236] TRAIN: iteration 32641 : loss : 0.071292, loss_ce: 0.002829, loss_dice: 0.139756
[13:03:24.454] TRAIN: iteration 32642 : loss : 0.028451, loss_ce: 0.001270, loss_dice: 0.055631
[13:03:24.665] TRAIN: iteration 32643 : loss : 0.250221, loss_ce: 0.000429, loss_dice: 0.500012
[13:03:24.872] TRAIN: iteration 32644 : loss : 0.171357, loss_ce: 0.005560, loss_dice: 0.337154
[13:03:25.081] TRAIN: iteration 32645 : loss : 0.087903, loss_ce: 0.000588, loss_dice: 0.175218
[13:03:25.427] TRAIN: iteration 32646 : loss : 0.077594, loss_ce: 0.001808, loss_dice: 0.153381
[13:03:25.634] TRAIN: iteration 32647 : loss : 0.053007, loss_ce: 0.002085, loss_dice: 0.103928
[13:03:29.692] TRAIN: iteration 32648 : loss : 0.035536, loss_ce: 0.004522, loss_dice: 0.066550
[13:03:29.993] TRAIN: iteration 32649 : loss : 0.082013, loss_ce: 0.001343, loss_dice: 0.162683
[13:03:30.201] TRAIN: iteration 32650 : loss : 0.067353, loss_ce: 0.003851, loss_dice: 0.130856
[13:03:30.411] TRAIN: iteration 32651 : loss : 0.027314, loss_ce: 0.003025, loss_dice: 0.051604
[13:03:30.619] TRAIN: iteration 32652 : loss : 0.044290, loss_ce: 0.000413, loss_dice: 0.088168
[13:03:30.828] TRAIN: iteration 32653 : loss : 0.250386, loss_ce: 0.000742, loss_dice: 0.500029
[13:03:31.754] TRAIN: iteration 32654 : loss : 0.065587, loss_ce: 0.007633, loss_dice: 0.123541
[13:03:31.962] TRAIN: iteration 32655 : loss : 0.039499, loss_ce: 0.001343, loss_dice: 0.077655
[13:03:32.174] TRAIN: iteration 32656 : loss : 0.095943, loss_ce: 0.002987, loss_dice: 0.188898
[13:03:32.381] TRAIN: iteration 32657 : loss : 0.096285, loss_ce: 0.012536, loss_dice: 0.180035
[13:03:32.588] TRAIN: iteration 32658 : loss : 0.081158, loss_ce: 0.000882, loss_dice: 0.161435
[13:03:32.799] TRAIN: iteration 32659 : loss : 0.033471, loss_ce: 0.003693, loss_dice: 0.063249
[13:03:33.014] TRAIN: iteration 32660 : loss : 0.060795, loss_ce: 0.003759, loss_dice: 0.117830
[13:03:33.247] TRAIN: iteration 32661 : loss : 0.058996, loss_ce: 0.001496, loss_dice: 0.116496
[13:03:37.634] TRAIN: iteration 32662 : loss : 0.058837, loss_ce: 0.003917, loss_dice: 0.113757
[13:03:37.879] TRAIN: iteration 32663 : loss : 0.116640, loss_ce: 0.002121, loss_dice: 0.231158
[13:03:39.454] TRAIN: iteration 32664 : loss : 0.060134, loss_ce: 0.006286, loss_dice: 0.113982
[13:03:39.664] TRAIN: iteration 32665 : loss : 0.125144, loss_ce: 0.002060, loss_dice: 0.248228
[13:03:39.876] TRAIN: iteration 32666 : loss : 0.072202, loss_ce: 0.001356, loss_dice: 0.143047
[13:03:40.085] TRAIN: iteration 32667 : loss : 0.083458, loss_ce: 0.002077, loss_dice: 0.164840
[13:03:40.295] TRAIN: iteration 32668 : loss : 0.068293, loss_ce: 0.001025, loss_dice: 0.135561
[13:03:40.505] TRAIN: iteration 32669 : loss : 0.024164, loss_ce: 0.000406, loss_dice: 0.047921
[13:03:43.054] TRAIN: iteration 32670 : loss : 0.029391, loss_ce: 0.004472, loss_dice: 0.054310
[13:03:43.261] TRAIN: iteration 32671 : loss : 0.086112, loss_ce: 0.005024, loss_dice: 0.167200
[13:03:44.573] TRAIN: iteration 32672 : loss : 0.037433, loss_ce: 0.002044, loss_dice: 0.072823
[13:03:44.780] TRAIN: iteration 32673 : loss : 0.035844, loss_ce: 0.001932, loss_dice: 0.069756
[13:03:44.988] TRAIN: iteration 32674 : loss : 0.250573, loss_ce: 0.001099, loss_dice: 0.500046
[13:03:45.196] TRAIN: iteration 32675 : loss : 0.076523, loss_ce: 0.002854, loss_dice: 0.150192
[13:03:45.404] TRAIN: iteration 32676 : loss : 0.129595, loss_ce: 0.001119, loss_dice: 0.258071
[13:03:45.614] TRAIN: iteration 32677 : loss : 0.051327, loss_ce: 0.000844, loss_dice: 0.101810
[13:03:47.666] TRAIN: iteration 32678 : loss : 0.078209, loss_ce: 0.004459, loss_dice: 0.151959
[13:03:47.875] TRAIN: iteration 32679 : loss : 0.214323, loss_ce: 0.001287, loss_dice: 0.427358
[13:03:48.927] TRAIN: iteration 32680 : loss : 0.072252, loss_ce: 0.001732, loss_dice: 0.142772
[13:03:49.166] TRAIN: iteration 32681 : loss : 0.250437, loss_ce: 0.000841, loss_dice: 0.500032
[13:03:49.375] TRAIN: iteration 32682 : loss : 0.250365, loss_ce: 0.000711, loss_dice: 0.500019
[13:03:49.585] TRAIN: iteration 32683 : loss : 0.141896, loss_ce: 0.001899, loss_dice: 0.281893
[13:03:49.792] TRAIN: iteration 32684 : loss : 0.029280, loss_ce: 0.005460, loss_dice: 0.053100
[13:03:50.000] TRAIN: iteration 32685 : loss : 0.060597, loss_ce: 0.001422, loss_dice: 0.119771
[13:03:51.149] TRAIN: iteration 32686 : loss : 0.085918, loss_ce: 0.005576, loss_dice: 0.166259
[13:03:51.357] TRAIN: iteration 32687 : loss : 0.029247, loss_ce: 0.000710, loss_dice: 0.057783
[13:03:55.958] TRAIN: iteration 32688 : loss : 0.088550, loss_ce: 0.001409, loss_dice: 0.175691
[13:03:56.166] TRAIN: iteration 32689 : loss : 0.069903, loss_ce: 0.005229, loss_dice: 0.134577
[13:03:56.373] TRAIN: iteration 32690 : loss : 0.041702, loss_ce: 0.002434, loss_dice: 0.080969
[13:03:56.581] TRAIN: iteration 32691 : loss : 0.149675, loss_ce: 0.005906, loss_dice: 0.293444
[13:03:56.788] TRAIN: iteration 32692 : loss : 0.169974, loss_ce: 0.001348, loss_dice: 0.338600
[13:03:56.994] TRAIN: iteration 32693 : loss : 0.054331, loss_ce: 0.013312, loss_dice: 0.095350
[13:03:57.202] TRAIN: iteration 32694 : loss : 0.104453, loss_ce: 0.000765, loss_dice: 0.208142
[13:03:57.410] TRAIN: iteration 32695 : loss : 0.132357, loss_ce: 0.001635, loss_dice: 0.263079
[13:04:00.486] TRAIN: iteration 32696 : loss : 0.035974, loss_ce: 0.000845, loss_dice: 0.071104
[13:04:00.693] TRAIN: iteration 32697 : loss : 0.039236, loss_ce: 0.001278, loss_dice: 0.077193
[13:04:00.900] TRAIN: iteration 32698 : loss : 0.160077, loss_ce: 0.002317, loss_dice: 0.317838
[13:04:01.107] TRAIN: iteration 32699 : loss : 0.075822, loss_ce: 0.005907, loss_dice: 0.145737
[13:04:01.314] TRAIN: iteration 32700 : loss : 0.159664, loss_ce: 0.004762, loss_dice: 0.314567
[13:04:01.548] TRAIN: iteration 32701 : loss : 0.071706, loss_ce: 0.002615, loss_dice: 0.140797
[13:04:01.755] TRAIN: iteration 32702 : loss : 0.163646, loss_ce: 0.000985, loss_dice: 0.326306
[13:04:01.962] TRAIN: iteration 32703 : loss : 0.067695, loss_ce: 0.002305, loss_dice: 0.133085
[13:04:07.793] TRAIN: iteration 32704 : loss : 0.251301, loss_ce: 0.002425, loss_dice: 0.500176
[13:04:08.002] TRAIN: iteration 32705 : loss : 0.071212, loss_ce: 0.005098, loss_dice: 0.137327
[13:04:08.210] TRAIN: iteration 32706 : loss : 0.042738, loss_ce: 0.002802, loss_dice: 0.082673
[13:04:08.418] TRAIN: iteration 32707 : loss : 0.250078, loss_ce: 0.001339, loss_dice: 0.498817
[13:04:08.625] TRAIN: iteration 32708 : loss : 0.047041, loss_ce: 0.005851, loss_dice: 0.088231
[13:04:08.835] TRAIN: iteration 32709 : loss : 0.074608, loss_ce: 0.001723, loss_dice: 0.147494
[13:04:09.041] TRAIN: iteration 32710 : loss : 0.023685, loss_ce: 0.002677, loss_dice: 0.044694
[13:04:09.248] TRAIN: iteration 32711 : loss : 0.070662, loss_ce: 0.001048, loss_dice: 0.140275
[13:04:10.556] TRAIN: iteration 32712 : loss : 0.155379, loss_ce: 0.003430, loss_dice: 0.307328
[13:04:10.762] TRAIN: iteration 32713 : loss : 0.055203, loss_ce: 0.002929, loss_dice: 0.107478
[13:04:10.969] TRAIN: iteration 32714 : loss : 0.102271, loss_ce: 0.002853, loss_dice: 0.201688
[13:04:11.177] TRAIN: iteration 32715 : loss : 0.092748, loss_ce: 0.001319, loss_dice: 0.184178
[13:04:11.384] TRAIN: iteration 32716 : loss : 0.053713, loss_ce: 0.001082, loss_dice: 0.106343
[13:04:11.591] TRAIN: iteration 32717 : loss : 0.123954, loss_ce: 0.002435, loss_dice: 0.245474
[13:04:12.447] TRAIN: iteration 32718 : loss : 0.098215, loss_ce: 0.000936, loss_dice: 0.195493
[13:04:12.654] TRAIN: iteration 32719 : loss : 0.127167, loss_ce: 0.007457, loss_dice: 0.246877
[13:04:14.957] TRAIN: iteration 32720 : loss : 0.073168, loss_ce: 0.005770, loss_dice: 0.140566
[13:04:15.195] TRAIN: iteration 32721 : loss : 0.053772, loss_ce: 0.001048, loss_dice: 0.106496
[13:04:15.402] TRAIN: iteration 32722 : loss : 0.250236, loss_ce: 0.000461, loss_dice: 0.500010
[13:04:15.610] TRAIN: iteration 32723 : loss : 0.036256, loss_ce: 0.002668, loss_dice: 0.069844
[13:04:15.817] TRAIN: iteration 32724 : loss : 0.054106, loss_ce: 0.007354, loss_dice: 0.100858
[13:04:16.024] TRAIN: iteration 32725 : loss : 0.062414, loss_ce: 0.003799, loss_dice: 0.121030
[13:04:18.105] TRAIN: iteration 32726 : loss : 0.251028, loss_ce: 0.001926, loss_dice: 0.500129
[13:04:19.116] TRAIN: iteration 32727 : loss : 0.250988, loss_ce: 0.001847, loss_dice: 0.500129
[13:04:21.617] TRAIN: iteration 32728 : loss : 0.067268, loss_ce: 0.006408, loss_dice: 0.128129
[13:04:21.825] TRAIN: iteration 32729 : loss : 0.150165, loss_ce: 0.002540, loss_dice: 0.297791
[13:04:22.032] TRAIN: iteration 32730 : loss : 0.078091, loss_ce: 0.003308, loss_dice: 0.152874
[13:04:22.240] TRAIN: iteration 32731 : loss : 0.243664, loss_ce: 0.001225, loss_dice: 0.486104
[13:04:22.448] TRAIN: iteration 32732 : loss : 0.049475, loss_ce: 0.005692, loss_dice: 0.093258
[13:04:22.656] TRAIN: iteration 32733 : loss : 0.071794, loss_ce: 0.007418, loss_dice: 0.136169
[13:04:23.275] TRAIN: iteration 32734 : loss : 0.060738, loss_ce: 0.002399, loss_dice: 0.119077
[13:04:24.046] TRAIN: iteration 32735 : loss : 0.093728, loss_ce: 0.000678, loss_dice: 0.186778
[13:04:25.467] TRAIN: iteration 32736 : loss : 0.040624, loss_ce: 0.007107, loss_dice: 0.074141
[13:04:25.674] TRAIN: iteration 32737 : loss : 0.065333, loss_ce: 0.004520, loss_dice: 0.126146
[13:04:25.884] TRAIN: iteration 32738 : loss : 0.114012, loss_ce: 0.003259, loss_dice: 0.224765
[13:04:26.095] TRAIN: iteration 32739 : loss : 0.250139, loss_ce: 0.000275, loss_dice: 0.500003
[13:04:26.302] TRAIN: iteration 32740 : loss : 0.250222, loss_ce: 0.000642, loss_dice: 0.499802
[13:04:26.536] TRAIN: iteration 32741 : loss : 0.233912, loss_ce: 0.003770, loss_dice: 0.464055
[13:04:28.191] TRAIN: iteration 32742 : loss : 0.134388, loss_ce: 0.000816, loss_dice: 0.267960
[13:04:28.399] TRAIN: iteration 32743 : loss : 0.029293, loss_ce: 0.000924, loss_dice: 0.057663
[13:04:31.954] TRAIN: iteration 32744 : loss : 0.029706, loss_ce: 0.002884, loss_dice: 0.056527
[13:04:32.161] TRAIN: iteration 32745 : loss : 0.113399, loss_ce: 0.005541, loss_dice: 0.221257
[13:04:32.370] TRAIN: iteration 32746 : loss : 0.015932, loss_ce: 0.001142, loss_dice: 0.030723
[13:04:32.577] TRAIN: iteration 32747 : loss : 0.059223, loss_ce: 0.013626, loss_dice: 0.104819
[13:04:32.787] TRAIN: iteration 32748 : loss : 0.040202, loss_ce: 0.002455, loss_dice: 0.077948
[13:04:32.996] TRAIN: iteration 32749 : loss : 0.050486, loss_ce: 0.003710, loss_dice: 0.097261
[13:04:35.123] TRAIN: iteration 32750 : loss : 0.087871, loss_ce: 0.001145, loss_dice: 0.174598
[13:04:36.184] TRAIN: iteration 32751 : loss : 0.052522, loss_ce: 0.002562, loss_dice: 0.102482
[13:04:37.942] TRAIN: iteration 32752 : loss : 0.093404, loss_ce: 0.001449, loss_dice: 0.185359
[13:04:38.149] TRAIN: iteration 32753 : loss : 0.083068, loss_ce: 0.005522, loss_dice: 0.160615
[13:04:38.358] TRAIN: iteration 32754 : loss : 0.035948, loss_ce: 0.001032, loss_dice: 0.070865
[13:04:38.938] TRAIN: iteration 32755 : loss : 0.080511, loss_ce: 0.001482, loss_dice: 0.159541
[13:04:39.146] TRAIN: iteration 32756 : loss : 0.018007, loss_ce: 0.000394, loss_dice: 0.035621
[13:04:39.354] TRAIN: iteration 32757 : loss : 0.024966, loss_ce: 0.003010, loss_dice: 0.046923
[13:04:40.797] TRAIN: iteration 32758 : loss : 0.217244, loss_ce: 0.000520, loss_dice: 0.433968
[13:04:43.687] TRAIN: iteration 32759 : loss : 0.250172, loss_ce: 0.000339, loss_dice: 0.500005
[13:04:44.408] TRAIN: iteration 32760 : loss : 0.045517, loss_ce: 0.001039, loss_dice: 0.089994
[13:04:44.643] TRAIN: iteration 32761 : loss : 0.022716, loss_ce: 0.001507, loss_dice: 0.043926
[13:04:44.851] TRAIN: iteration 32762 : loss : 0.051373, loss_ce: 0.003591, loss_dice: 0.099156
[13:04:45.059] TRAIN: iteration 32763 : loss : 0.045876, loss_ce: 0.001592, loss_dice: 0.090159
[13:04:45.267] TRAIN: iteration 32764 : loss : 0.250465, loss_ce: 0.000874, loss_dice: 0.500057
[13:04:45.474] TRAIN: iteration 32765 : loss : 0.250628, loss_ce: 0.001186, loss_dice: 0.500069
[13:04:45.681] TRAIN: iteration 32766 : loss : 0.084023, loss_ce: 0.001325, loss_dice: 0.166722
[13:04:49.441] TRAIN: iteration 32767 : loss : 0.033726, loss_ce: 0.001166, loss_dice: 0.066287
[13:04:50.614] TRAIN: iteration 32768 : loss : 0.051126, loss_ce: 0.005522, loss_dice: 0.096730
[13:04:50.821] TRAIN: iteration 32769 : loss : 0.055022, loss_ce: 0.003011, loss_dice: 0.107033
[13:04:51.029] TRAIN: iteration 32770 : loss : 0.038088, loss_ce: 0.003350, loss_dice: 0.072826
[13:04:51.240] TRAIN: iteration 32771 : loss : 0.052588, loss_ce: 0.004733, loss_dice: 0.100443
[13:04:51.448] TRAIN: iteration 32772 : loss : 0.026141, loss_ce: 0.001207, loss_dice: 0.051075
[13:04:51.678] TRAIN: iteration 32773 : loss : 0.092357, loss_ce: 0.000710, loss_dice: 0.184004
[13:04:51.885] TRAIN: iteration 32774 : loss : 0.233044, loss_ce: 0.001317, loss_dice: 0.464772
[13:04:54.730] TRAIN: iteration 32775 : loss : 0.059748, loss_ce: 0.002468, loss_dice: 0.117028
[13:04:58.241] TRAIN: iteration 32776 : loss : 0.132244, loss_ce: 0.003181, loss_dice: 0.261307
[13:04:58.449] TRAIN: iteration 32777 : loss : 0.137542, loss_ce: 0.003295, loss_dice: 0.271789
[13:04:58.657] TRAIN: iteration 32778 : loss : 0.233335, loss_ce: 0.000509, loss_dice: 0.466161
[13:04:58.865] TRAIN: iteration 32779 : loss : 0.089465, loss_ce: 0.011928, loss_dice: 0.167002
[13:04:59.073] TRAIN: iteration 32780 : loss : 0.035725, loss_ce: 0.000616, loss_dice: 0.070834
[13:04:59.310] TRAIN: iteration 32781 : loss : 0.105760, loss_ce: 0.014703, loss_dice: 0.196818
[13:04:59.518] TRAIN: iteration 32782 : loss : 0.027772, loss_ce: 0.000679, loss_dice: 0.054865
[13:04:59.727] TRAIN: iteration 32783 : loss : 0.250338, loss_ce: 0.000653, loss_dice: 0.500022
[13:05:06.251] TRAIN: iteration 32784 : loss : 0.063297, loss_ce: 0.006483, loss_dice: 0.120111
[13:05:06.465] TRAIN: iteration 32785 : loss : 0.080680, loss_ce: 0.010193, loss_dice: 0.151168
[13:05:06.675] TRAIN: iteration 32786 : loss : 0.036510, loss_ce: 0.001107, loss_dice: 0.071913
[13:05:06.883] TRAIN: iteration 32787 : loss : 0.046882, loss_ce: 0.000871, loss_dice: 0.092894
[13:05:07.091] TRAIN: iteration 32788 : loss : 0.022120, loss_ce: 0.001597, loss_dice: 0.042643
[13:05:07.301] TRAIN: iteration 32789 : loss : 0.037061, loss_ce: 0.007354, loss_dice: 0.066768
[13:05:07.509] TRAIN: iteration 32790 : loss : 0.106249, loss_ce: 0.001673, loss_dice: 0.210826
[13:05:07.716] TRAIN: iteration 32791 : loss : 0.029318, loss_ce: 0.001869, loss_dice: 0.056767
[13:05:14.819] TRAIN: iteration 32792 : loss : 0.033795, loss_ce: 0.008716, loss_dice: 0.058873
[13:05:15.026] TRAIN: iteration 32793 : loss : 0.089394, loss_ce: 0.005365, loss_dice: 0.173423
[13:05:15.236] TRAIN: iteration 32794 : loss : 0.027599, loss_ce: 0.000841, loss_dice: 0.054356
[13:05:15.445] TRAIN: iteration 32795 : loss : 0.074836, loss_ce: 0.002390, loss_dice: 0.147283
[13:05:15.653] TRAIN: iteration 32796 : loss : 0.233431, loss_ce: 0.001458, loss_dice: 0.465405
[13:05:15.862] TRAIN: iteration 32797 : loss : 0.078021, loss_ce: 0.006133, loss_dice: 0.149910
[13:05:16.070] TRAIN: iteration 32798 : loss : 0.099022, loss_ce: 0.004070, loss_dice: 0.193975
[13:05:16.279] TRAIN: iteration 32799 : loss : 0.063328, loss_ce: 0.004906, loss_dice: 0.121750
[13:05:21.801] TRAIN: iteration 32800 : loss : 0.251061, loss_ce: 0.001980, loss_dice: 0.500142
[13:05:22.039] TRAIN: iteration 32801 : loss : 0.087730, loss_ce: 0.004524, loss_dice: 0.170935
[13:05:22.250] TRAIN: iteration 32802 : loss : 0.066736, loss_ce: 0.000625, loss_dice: 0.132848
[13:05:22.458] TRAIN: iteration 32803 : loss : 0.035932, loss_ce: 0.003774, loss_dice: 0.068090
[13:05:22.665] TRAIN: iteration 32804 : loss : 0.106862, loss_ce: 0.002364, loss_dice: 0.211360
[13:05:22.873] TRAIN: iteration 32805 : loss : 0.047706, loss_ce: 0.005676, loss_dice: 0.089735
[13:05:23.081] TRAIN: iteration 32806 : loss : 0.030921, loss_ce: 0.001108, loss_dice: 0.060734
[13:05:23.288] TRAIN: iteration 32807 : loss : 0.166697, loss_ce: 0.001390, loss_dice: 0.332005
[13:05:27.101] TRAIN: iteration 32808 : loss : 0.041679, loss_ce: 0.000964, loss_dice: 0.082395
[13:05:27.307] TRAIN: iteration 32809 : loss : 0.154991, loss_ce: 0.004485, loss_dice: 0.305497
[13:05:27.514] TRAIN: iteration 32810 : loss : 0.250281, loss_ce: 0.000550, loss_dice: 0.500011
[13:05:27.720] TRAIN: iteration 32811 : loss : 0.034629, loss_ce: 0.000508, loss_dice: 0.068750
[13:05:28.685] TRAIN: iteration 32812 : loss : 0.250302, loss_ce: 0.000592, loss_dice: 0.500013
[13:05:28.893] TRAIN: iteration 32813 : loss : 0.049617, loss_ce: 0.002337, loss_dice: 0.096898
[13:05:29.103] TRAIN: iteration 32814 : loss : 0.094440, loss_ce: 0.002220, loss_dice: 0.186659
[13:05:29.314] TRAIN: iteration 32815 : loss : 0.252648, loss_ce: 0.006809, loss_dice: 0.498487
[13:05:34.341] TRAIN: iteration 32816 : loss : 0.080066, loss_ce: 0.002930, loss_dice: 0.157203
[13:05:34.548] TRAIN: iteration 32817 : loss : 0.071410, loss_ce: 0.009757, loss_dice: 0.133063
[13:05:34.758] TRAIN: iteration 32818 : loss : 0.081253, loss_ce: 0.001672, loss_dice: 0.160834
[13:05:34.966] TRAIN: iteration 32819 : loss : 0.070338, loss_ce: 0.001729, loss_dice: 0.138947
[13:05:37.682] TRAIN: iteration 32820 : loss : 0.049487, loss_ce: 0.002017, loss_dice: 0.096958
[13:05:37.919] TRAIN: iteration 32821 : loss : 0.250400, loss_ce: 0.000758, loss_dice: 0.500041
[13:05:38.127] TRAIN: iteration 32822 : loss : 0.066251, loss_ce: 0.001182, loss_dice: 0.131319
[13:05:38.334] TRAIN: iteration 32823 : loss : 0.063970, loss_ce: 0.004470, loss_dice: 0.123469
[13:05:41.906] TRAIN: iteration 32824 : loss : 0.079530, loss_ce: 0.001618, loss_dice: 0.157442
[13:05:42.116] TRAIN: iteration 32825 : loss : 0.127074, loss_ce: 0.003579, loss_dice: 0.250569
[13:05:42.325] TRAIN: iteration 32826 : loss : 0.049935, loss_ce: 0.000741, loss_dice: 0.099130
[13:05:42.532] TRAIN: iteration 32827 : loss : 0.055203, loss_ce: 0.003802, loss_dice: 0.106604
[13:05:44.953] TRAIN: iteration 32828 : loss : 0.214833, loss_ce: 0.000796, loss_dice: 0.428870
[13:05:45.167] TRAIN: iteration 32829 : loss : 0.028670, loss_ce: 0.003389, loss_dice: 0.053951
[13:05:45.375] TRAIN: iteration 32830 : loss : 0.018763, loss_ce: 0.002082, loss_dice: 0.035444
[13:05:45.582] TRAIN: iteration 32831 : loss : 0.067617, loss_ce: 0.003153, loss_dice: 0.132082
[13:05:49.751] TRAIN: iteration 32832 : loss : 0.032568, loss_ce: 0.001979, loss_dice: 0.063157
[13:05:49.958] TRAIN: iteration 32833 : loss : 0.059025, loss_ce: 0.001144, loss_dice: 0.116905
[13:05:50.167] TRAIN: iteration 32834 : loss : 0.117725, loss_ce: 0.002002, loss_dice: 0.233448
[13:05:50.374] TRAIN: iteration 32835 : loss : 0.039432, loss_ce: 0.005587, loss_dice: 0.073277
[13:05:53.410] TRAIN: iteration 32836 : loss : 0.071325, loss_ce: 0.001678, loss_dice: 0.140972
[13:05:53.619] TRAIN: iteration 32837 : loss : 0.069692, loss_ce: 0.003317, loss_dice: 0.136067
[13:05:53.827] TRAIN: iteration 32838 : loss : 0.052579, loss_ce: 0.003469, loss_dice: 0.101688
[13:05:54.037] TRAIN: iteration 32839 : loss : 0.039032, loss_ce: 0.002575, loss_dice: 0.075489
[13:05:58.226] TRAIN: iteration 32840 : loss : 0.070975, loss_ce: 0.001014, loss_dice: 0.140936
[13:05:58.458] TRAIN: iteration 32841 : loss : 0.250972, loss_ce: 0.001818, loss_dice: 0.500125
[13:05:58.664] TRAIN: iteration 32842 : loss : 0.250669, loss_ce: 0.001261, loss_dice: 0.500077
[13:05:58.871] TRAIN: iteration 32843 : loss : 0.140354, loss_ce: 0.002477, loss_dice: 0.278230
[13:05:59.172] TRAIN: iteration 32844 : loss : 0.066924, loss_ce: 0.003531, loss_dice: 0.130318
[13:05:59.385] TRAIN: iteration 32845 : loss : 0.134201, loss_ce: 0.001362, loss_dice: 0.267039
[13:05:59.592] TRAIN: iteration 32846 : loss : 0.250409, loss_ce: 0.000786, loss_dice: 0.500031
[13:06:00.335] TRAIN: iteration 32847 : loss : 0.122565, loss_ce: 0.003755, loss_dice: 0.241376
[13:06:08.020] TRAIN: iteration 32848 : loss : 0.151105, loss_ce: 0.008348, loss_dice: 0.293862
[13:06:08.228] TRAIN: iteration 32849 : loss : 0.250546, loss_ce: 0.001043, loss_dice: 0.500049
[13:06:08.435] TRAIN: iteration 32850 : loss : 0.250328, loss_ce: 0.000643, loss_dice: 0.500012
[13:06:08.642] TRAIN: iteration 32851 : loss : 0.161182, loss_ce: 0.001362, loss_dice: 0.321003
[13:06:08.848] TRAIN: iteration 32852 : loss : 0.251131, loss_ce: 0.002115, loss_dice: 0.500148
[13:06:09.055] TRAIN: iteration 32853 : loss : 0.216787, loss_ce: 0.003782, loss_dice: 0.429793
[13:06:09.264] TRAIN: iteration 32854 : loss : 0.057587, loss_ce: 0.001095, loss_dice: 0.114078
[13:06:09.471] TRAIN: iteration 32855 : loss : 0.223362, loss_ce: 0.001906, loss_dice: 0.444818
[13:06:13.122] TRAIN: iteration 32856 : loss : 0.204237, loss_ce: 0.002833, loss_dice: 0.405642
[13:06:15.446] TRAIN: iteration 32857 : loss : 0.031239, loss_ce: 0.000604, loss_dice: 0.061873
[13:06:15.658] TRAIN: iteration 32858 : loss : 0.068644, loss_ce: 0.001063, loss_dice: 0.136224
[13:06:15.865] TRAIN: iteration 32859 : loss : 0.062799, loss_ce: 0.001392, loss_dice: 0.124207
[13:06:16.074] TRAIN: iteration 32860 : loss : 0.031361, loss_ce: 0.007806, loss_dice: 0.054916
[13:06:16.310] TRAIN: iteration 32861 : loss : 0.045318, loss_ce: 0.000695, loss_dice: 0.089940
[13:06:16.519] TRAIN: iteration 32862 : loss : 0.061180, loss_ce: 0.006258, loss_dice: 0.116102
[13:06:19.066] TRAIN: iteration 32863 : loss : 0.154956, loss_ce: 0.004400, loss_dice: 0.305513
[13:06:20.785] TRAIN: iteration 32864 : loss : 0.072091, loss_ce: 0.007348, loss_dice: 0.136833
[13:06:22.929] TRAIN: iteration 32865 : loss : 0.165749, loss_ce: 0.001615, loss_dice: 0.329883
[13:06:23.138] TRAIN: iteration 32866 : loss : 0.229515, loss_ce: 0.003008, loss_dice: 0.456022
[13:06:23.344] TRAIN: iteration 32867 : loss : 0.021024, loss_ce: 0.004233, loss_dice: 0.037815
[13:06:23.551] TRAIN: iteration 32868 : loss : 0.033054, loss_ce: 0.002687, loss_dice: 0.063421
[13:06:23.757] TRAIN: iteration 32869 : loss : 0.153181, loss_ce: 0.008245, loss_dice: 0.298118
[13:06:23.964] TRAIN: iteration 32870 : loss : 0.131956, loss_ce: 0.011930, loss_dice: 0.251982
[13:06:28.057] TRAIN: iteration 32871 : loss : 0.051571, loss_ce: 0.000725, loss_dice: 0.102416
[13:06:29.027] TRAIN: iteration 32872 : loss : 0.250353, loss_ce: 0.000689, loss_dice: 0.500017
[13:06:33.282] TRAIN: iteration 32873 : loss : 0.126040, loss_ce: 0.001548, loss_dice: 0.250531
[13:06:33.491] TRAIN: iteration 32874 : loss : 0.105269, loss_ce: 0.002245, loss_dice: 0.208293
[13:06:33.699] TRAIN: iteration 32875 : loss : 0.097716, loss_ce: 0.007379, loss_dice: 0.188053
[13:06:33.906] TRAIN: iteration 32876 : loss : 0.053126, loss_ce: 0.000819, loss_dice: 0.105433
[13:06:34.116] TRAIN: iteration 32877 : loss : 0.250467, loss_ce: 0.000906, loss_dice: 0.500029
[13:06:34.355] TRAIN: iteration 32878 : loss : 0.074504, loss_ce: 0.002375, loss_dice: 0.146632
[13:06:37.631] TRAIN: iteration 32879 : loss : 0.250568, loss_ce: 0.002449, loss_dice: 0.498686
[13:06:37.840] TRAIN: iteration 32880 : loss : 0.070289, loss_ce: 0.001653, loss_dice: 0.138924
[13:06:41.666] TRAIN: iteration 32881 : loss : 0.042914, loss_ce: 0.001184, loss_dice: 0.084644
[13:06:41.874] TRAIN: iteration 32882 : loss : 0.134406, loss_ce: 0.002985, loss_dice: 0.265826
[13:06:42.082] TRAIN: iteration 32883 : loss : 0.048837, loss_ce: 0.008699, loss_dice: 0.088974
[13:06:42.290] TRAIN: iteration 32884 : loss : 0.048525, loss_ce: 0.003443, loss_dice: 0.093607
[13:06:42.501] TRAIN: iteration 32885 : loss : 0.104643, loss_ce: 0.003143, loss_dice: 0.206144
[13:06:42.709] TRAIN: iteration 32886 : loss : 0.082093, loss_ce: 0.002917, loss_dice: 0.161269
[13:06:45.019] TRAIN: iteration 32887 : loss : 0.088782, loss_ce: 0.001220, loss_dice: 0.176344
[13:06:45.229] TRAIN: iteration 32888 : loss : 0.052939, loss_ce: 0.003951, loss_dice: 0.101928
[13:06:50.888] TRAIN: iteration 32889 : loss : 0.036335, loss_ce: 0.001900, loss_dice: 0.070770
[13:06:51.096] TRAIN: iteration 32890 : loss : 0.080765, loss_ce: 0.002586, loss_dice: 0.158945
[13:06:51.304] TRAIN: iteration 32891 : loss : 0.094502, loss_ce: 0.004937, loss_dice: 0.184066
[13:06:51.511] TRAIN: iteration 32892 : loss : 0.250507, loss_ce: 0.000960, loss_dice: 0.500053
[13:06:51.719] TRAIN: iteration 32893 : loss : 0.080537, loss_ce: 0.003274, loss_dice: 0.157801
[13:06:51.927] TRAIN: iteration 32894 : loss : 0.144458, loss_ce: 0.004091, loss_dice: 0.284826
[13:06:54.138] TRAIN: iteration 32895 : loss : 0.250701, loss_ce: 0.001326, loss_dice: 0.500077
[13:06:54.348] TRAIN: iteration 32896 : loss : 0.035630, loss_ce: 0.002330, loss_dice: 0.068931
[13:07:01.278] TRAIN: iteration 32897 : loss : 0.096624, loss_ce: 0.005964, loss_dice: 0.187283
[13:07:01.493] TRAIN: iteration 32898 : loss : 0.069429, loss_ce: 0.001363, loss_dice: 0.137495
[13:07:01.702] TRAIN: iteration 32899 : loss : 0.109840, loss_ce: 0.002615, loss_dice: 0.217064
[13:07:01.910] TRAIN: iteration 32900 : loss : 0.031225, loss_ce: 0.002808, loss_dice: 0.059643
[13:07:02.148] TRAIN: iteration 32901 : loss : 0.100696, loss_ce: 0.008265, loss_dice: 0.193127
[13:07:02.355] TRAIN: iteration 32902 : loss : 0.051963, loss_ce: 0.005067, loss_dice: 0.098859
[13:07:03.342] TRAIN: iteration 32903 : loss : 0.250475, loss_ce: 0.000923, loss_dice: 0.500028
[13:07:03.549] TRAIN: iteration 32904 : loss : 0.055768, loss_ce: 0.003541, loss_dice: 0.107994
[13:07:09.801] TRAIN: iteration 32905 : loss : 0.050654, loss_ce: 0.002225, loss_dice: 0.099083
[13:07:10.010] TRAIN: iteration 32906 : loss : 0.133929, loss_ce: 0.001258, loss_dice: 0.266601
[13:07:10.218] TRAIN: iteration 32907 : loss : 0.030593, loss_ce: 0.001230, loss_dice: 0.059957
[13:07:10.427] TRAIN: iteration 32908 : loss : 0.030596, loss_ce: 0.007458, loss_dice: 0.053735
[13:07:10.634] TRAIN: iteration 32909 : loss : 0.028566, loss_ce: 0.002024, loss_dice: 0.055107
[13:07:10.842] TRAIN: iteration 32910 : loss : 0.179577, loss_ce: 0.001727, loss_dice: 0.357428
[13:07:11.563] TRAIN: iteration 32911 : loss : 0.068610, loss_ce: 0.004278, loss_dice: 0.132943
[13:07:11.772] TRAIN: iteration 32912 : loss : 0.050063, loss_ce: 0.002585, loss_dice: 0.097541
[13:07:21.110] TRAIN: iteration 32913 : loss : 0.071245, loss_ce: 0.005719, loss_dice: 0.136771
[13:07:21.318] TRAIN: iteration 32914 : loss : 0.074280, loss_ce: 0.001300, loss_dice: 0.147261
[13:07:21.527] TRAIN: iteration 32915 : loss : 0.045864, loss_ce: 0.003693, loss_dice: 0.088034
[13:07:21.735] TRAIN: iteration 32916 : loss : 0.249422, loss_ce: 0.002065, loss_dice: 0.496778
[13:07:21.942] TRAIN: iteration 32917 : loss : 0.046266, loss_ce: 0.002230, loss_dice: 0.090302
[13:07:22.150] TRAIN: iteration 32918 : loss : 0.092402, loss_ce: 0.016525, loss_dice: 0.168279
[13:07:22.357] TRAIN: iteration 32919 : loss : 0.056665, loss_ce: 0.002797, loss_dice: 0.110533
[13:07:22.564] TRAIN: iteration 32920 : loss : 0.040330, loss_ce: 0.002935, loss_dice: 0.077725
[13:07:30.837] TRAIN: iteration 32921 : loss : 0.060919, loss_ce: 0.003538, loss_dice: 0.118300
[13:07:31.051] TRAIN: iteration 32922 : loss : 0.111064, loss_ce: 0.002194, loss_dice: 0.219934
[13:07:31.259] TRAIN: iteration 32923 : loss : 0.141669, loss_ce: 0.002342, loss_dice: 0.280996
[13:07:31.472] TRAIN: iteration 32924 : loss : 0.082567, loss_ce: 0.004393, loss_dice: 0.160741
[13:07:31.681] TRAIN: iteration 32925 : loss : 0.052936, loss_ce: 0.005339, loss_dice: 0.100533
[13:07:31.891] TRAIN: iteration 32926 : loss : 0.089061, loss_ce: 0.010042, loss_dice: 0.168079
[13:07:32.101] TRAIN: iteration 32927 : loss : 0.139437, loss_ce: 0.003737, loss_dice: 0.275137
[13:07:32.308] TRAIN: iteration 32928 : loss : 0.050641, loss_ce: 0.004220, loss_dice: 0.097061
[13:07:41.258] TRAIN: iteration 32929 : loss : 0.181974, loss_ce: 0.003611, loss_dice: 0.360336
[13:07:41.465] TRAIN: iteration 32930 : loss : 0.245493, loss_ce: 0.004254, loss_dice: 0.486731
[13:07:41.681] TRAIN: iteration 32931 : loss : 0.042656, loss_ce: 0.002461, loss_dice: 0.082852
[13:07:41.887] TRAIN: iteration 32932 : loss : 0.250519, loss_ce: 0.000992, loss_dice: 0.500045
[13:07:42.095] TRAIN: iteration 32933 : loss : 0.028608, loss_ce: 0.004103, loss_dice: 0.053113
[13:07:42.302] TRAIN: iteration 32934 : loss : 0.079976, loss_ce: 0.002693, loss_dice: 0.157258
[13:07:42.509] TRAIN: iteration 32935 : loss : 0.060509, loss_ce: 0.004023, loss_dice: 0.116995
[13:07:42.724] TRAIN: iteration 32936 : loss : 0.251228, loss_ce: 0.002291, loss_dice: 0.500165
[13:07:50.071] TRAIN: iteration 32937 : loss : 0.250743, loss_ce: 0.001421, loss_dice: 0.500065
[13:07:50.278] TRAIN: iteration 32938 : loss : 0.191870, loss_ce: 0.010538, loss_dice: 0.373202
[13:07:50.486] TRAIN: iteration 32939 : loss : 0.099138, loss_ce: 0.004074, loss_dice: 0.194202
[13:07:50.695] TRAIN: iteration 32940 : loss : 0.042562, loss_ce: 0.000593, loss_dice: 0.084532
[13:07:50.930] TRAIN: iteration 32941 : loss : 0.071433, loss_ce: 0.006040, loss_dice: 0.136826
[13:07:51.138] TRAIN: iteration 32942 : loss : 0.244094, loss_ce: 0.003352, loss_dice: 0.484835
[13:07:51.348] TRAIN: iteration 32943 : loss : 0.051370, loss_ce: 0.003479, loss_dice: 0.099260
[13:07:51.557] TRAIN: iteration 32944 : loss : 0.035025, loss_ce: 0.001850, loss_dice: 0.068199
[13:08:00.480] TRAIN: iteration 32945 : loss : 0.026787, loss_ce: 0.006347, loss_dice: 0.047226
[13:08:00.688] TRAIN: iteration 32946 : loss : 0.250524, loss_ce: 0.000997, loss_dice: 0.500051
[13:08:00.895] TRAIN: iteration 32947 : loss : 0.248083, loss_ce: 0.001197, loss_dice: 0.494970
[13:08:01.103] TRAIN: iteration 32948 : loss : 0.096163, loss_ce: 0.004687, loss_dice: 0.187639
[13:08:01.312] TRAIN: iteration 32949 : loss : 0.066622, loss_ce: 0.008891, loss_dice: 0.124353
[13:08:01.521] TRAIN: iteration 32950 : loss : 0.223031, loss_ce: 0.025116, loss_dice: 0.420946
[13:08:01.729] TRAIN: iteration 32951 : loss : 0.194853, loss_ce: 0.002379, loss_dice: 0.387328
[13:08:01.939] TRAIN: iteration 32952 : loss : 0.178929, loss_ce: 0.002417, loss_dice: 0.355442
[13:08:09.790] TRAIN: iteration 32953 : loss : 0.250262, loss_ce: 0.000514, loss_dice: 0.500011
[13:08:09.998] TRAIN: iteration 32954 : loss : 0.048099, loss_ce: 0.001828, loss_dice: 0.094370
[13:08:10.205] TRAIN: iteration 32955 : loss : 0.082749, loss_ce: 0.003083, loss_dice: 0.162415
[13:08:10.411] TRAIN: iteration 32956 : loss : 0.043742, loss_ce: 0.001214, loss_dice: 0.086271
[13:08:10.620] TRAIN: iteration 32957 : loss : 0.014234, loss_ce: 0.001200, loss_dice: 0.027268
[13:08:10.826] TRAIN: iteration 32958 : loss : 0.023211, loss_ce: 0.001339, loss_dice: 0.045082
[13:08:11.032] TRAIN: iteration 32959 : loss : 0.110451, loss_ce: 0.004252, loss_dice: 0.216650
[13:08:11.239] TRAIN: iteration 32960 : loss : 0.195923, loss_ce: 0.023075, loss_dice: 0.368771
[13:08:19.575] TRAIN: iteration 32961 : loss : 0.239311, loss_ce: 0.008878, loss_dice: 0.469743
[13:08:19.782] TRAIN: iteration 32962 : loss : 0.021263, loss_ce: 0.000454, loss_dice: 0.042073
[13:08:19.991] TRAIN: iteration 32963 : loss : 0.106925, loss_ce: 0.002921, loss_dice: 0.210929
[13:08:20.200] TRAIN: iteration 32964 : loss : 0.025709, loss_ce: 0.003386, loss_dice: 0.048032
[13:08:20.296] TRAIN: iteration 32965 : loss : 0.043746, loss_ce: 0.004727, loss_dice: 0.082765
[13:13:49.629] VALIDATION: iteration 18 : loss : 0.102355, loss_ce: 0.003469, loss_dice: 0.201242
[13:13:50.323] TRAIN: iteration 32966 : loss : 0.053490, loss_ce: 0.007315, loss_dice: 0.099665
[13:13:50.856] TRAIN: iteration 32967 : loss : 0.058606, loss_ce: 0.004073, loss_dice: 0.113140
[13:13:51.082] TRAIN: iteration 32968 : loss : 0.087993, loss_ce: 0.001746, loss_dice: 0.174239
[13:13:51.310] TRAIN: iteration 32969 : loss : 0.250624, loss_ce: 0.001197, loss_dice: 0.500051
[13:13:51.527] TRAIN: iteration 32970 : loss : 0.065378, loss_ce: 0.000962, loss_dice: 0.129795
[13:13:51.735] TRAIN: iteration 32971 : loss : 0.070431, loss_ce: 0.001133, loss_dice: 0.139730
[13:13:51.953] TRAIN: iteration 32972 : loss : 0.250893, loss_ce: 0.001688, loss_dice: 0.500097
[13:13:52.162] TRAIN: iteration 32973 : loss : 0.026037, loss_ce: 0.004813, loss_dice: 0.047261
[13:13:52.373] TRAIN: iteration 32974 : loss : 0.056034, loss_ce: 0.002435, loss_dice: 0.109632
[13:13:52.583] TRAIN: iteration 32975 : loss : 0.250501, loss_ce: 0.000954, loss_dice: 0.500048
[13:13:52.793] TRAIN: iteration 32976 : loss : 0.055193, loss_ce: 0.002724, loss_dice: 0.107663
[13:13:53.005] TRAIN: iteration 32977 : loss : 0.087581, loss_ce: 0.007728, loss_dice: 0.167434
[13:13:53.218] TRAIN: iteration 32978 : loss : 0.250772, loss_ce: 0.001466, loss_dice: 0.500077
[13:13:53.426] TRAIN: iteration 32979 : loss : 0.251177, loss_ce: 0.002207, loss_dice: 0.500146
[13:13:53.637] TRAIN: iteration 32980 : loss : 0.067175, loss_ce: 0.002296, loss_dice: 0.132053
[13:13:53.886] TRAIN: iteration 32981 : loss : 0.040488, loss_ce: 0.001670, loss_dice: 0.079306
[13:13:54.097] TRAIN: iteration 32982 : loss : 0.167775, loss_ce: 0.001389, loss_dice: 0.334161
[13:13:54.305] TRAIN: iteration 32983 : loss : 0.034758, loss_ce: 0.002100, loss_dice: 0.067415
[13:13:54.516] TRAIN: iteration 32984 : loss : 0.133179, loss_ce: 0.002638, loss_dice: 0.263721
[13:13:54.724] TRAIN: iteration 32985 : loss : 0.057600, loss_ce: 0.001745, loss_dice: 0.113456
[13:13:54.931] TRAIN: iteration 32986 : loss : 0.250334, loss_ce: 0.000646, loss_dice: 0.500022
[13:13:55.146] TRAIN: iteration 32987 : loss : 0.109728, loss_ce: 0.002945, loss_dice: 0.216511
[13:13:55.354] TRAIN: iteration 32988 : loss : 0.110747, loss_ce: 0.001521, loss_dice: 0.219974
[13:13:55.564] TRAIN: iteration 32989 : loss : 0.040542, loss_ce: 0.002028, loss_dice: 0.079057
[13:13:55.777] TRAIN: iteration 32990 : loss : 0.052604, loss_ce: 0.001943, loss_dice: 0.103265
[13:13:55.985] TRAIN: iteration 32991 : loss : 0.118416, loss_ce: 0.001285, loss_dice: 0.235547
[13:13:56.193] TRAIN: iteration 32992 : loss : 0.062740, loss_ce: 0.001837, loss_dice: 0.123642
[13:13:56.406] TRAIN: iteration 32993 : loss : 0.017269, loss_ce: 0.002560, loss_dice: 0.031979
[13:13:56.613] TRAIN: iteration 32994 : loss : 0.022027, loss_ce: 0.000801, loss_dice: 0.043252
[13:13:56.824] TRAIN: iteration 32995 : loss : 0.024254, loss_ce: 0.000488, loss_dice: 0.048020
[13:13:57.034] TRAIN: iteration 32996 : loss : 0.063626, loss_ce: 0.002631, loss_dice: 0.124622
[13:13:57.241] TRAIN: iteration 32997 : loss : 0.058924, loss_ce: 0.002180, loss_dice: 0.115667
[13:13:57.452] TRAIN: iteration 32998 : loss : 0.250565, loss_ce: 0.002770, loss_dice: 0.498359
[13:13:57.660] TRAIN: iteration 32999 : loss : 0.033872, loss_ce: 0.000856, loss_dice: 0.066889
[13:13:57.867] TRAIN: iteration 33000 : loss : 0.057594, loss_ce: 0.001298, loss_dice: 0.113890
[13:13:58.105] TRAIN: iteration 33001 : loss : 0.053394, loss_ce: 0.003114, loss_dice: 0.103674
[13:13:58.314] TRAIN: iteration 33002 : loss : 0.044892, loss_ce: 0.002111, loss_dice: 0.087674
[13:13:58.523] TRAIN: iteration 33003 : loss : 0.033219, loss_ce: 0.001177, loss_dice: 0.065261
[13:13:58.732] TRAIN: iteration 33004 : loss : 0.250871, loss_ce: 0.001641, loss_dice: 0.500100
[13:13:58.942] TRAIN: iteration 33005 : loss : 0.164640, loss_ce: 0.024056, loss_dice: 0.305224
[13:13:59.155] TRAIN: iteration 33006 : loss : 0.214009, loss_ce: 0.001555, loss_dice: 0.426463
[13:13:59.367] TRAIN: iteration 33007 : loss : 0.249734, loss_ce: 0.001091, loss_dice: 0.498377
[13:13:59.581] TRAIN: iteration 33008 : loss : 0.034946, loss_ce: 0.002125, loss_dice: 0.067767
[13:13:59.790] TRAIN: iteration 33009 : loss : 0.057122, loss_ce: 0.001221, loss_dice: 0.113022
[13:14:00.000] TRAIN: iteration 33010 : loss : 0.073820, loss_ce: 0.003443, loss_dice: 0.144198
[13:14:00.209] TRAIN: iteration 33011 : loss : 0.077666, loss_ce: 0.001761, loss_dice: 0.153571
[13:14:00.419] TRAIN: iteration 33012 : loss : 0.041589, loss_ce: 0.005235, loss_dice: 0.077943
[13:14:00.627] TRAIN: iteration 33013 : loss : 0.014799, loss_ce: 0.001067, loss_dice: 0.028532
[13:14:00.836] TRAIN: iteration 33014 : loss : 0.142689, loss_ce: 0.001145, loss_dice: 0.284233
[13:14:01.046] TRAIN: iteration 33015 : loss : 0.223982, loss_ce: 0.004125, loss_dice: 0.443839
[13:14:01.262] TRAIN: iteration 33016 : loss : 0.059800, loss_ce: 0.002275, loss_dice: 0.117325
[13:14:01.472] TRAIN: iteration 33017 : loss : 0.050116, loss_ce: 0.001740, loss_dice: 0.098492
[13:14:01.688] TRAIN: iteration 33018 : loss : 0.041381, loss_ce: 0.003061, loss_dice: 0.079702
[13:14:01.895] TRAIN: iteration 33019 : loss : 0.250504, loss_ce: 0.000969, loss_dice: 0.500040
[13:14:02.103] TRAIN: iteration 33020 : loss : 0.095726, loss_ce: 0.003479, loss_dice: 0.187972
[13:14:02.337] TRAIN: iteration 33021 : loss : 0.022393, loss_ce: 0.002999, loss_dice: 0.041788
[13:14:02.554] TRAIN: iteration 33022 : loss : 0.092305, loss_ce: 0.006284, loss_dice: 0.178326
[13:14:02.766] TRAIN: iteration 33023 : loss : 0.040477, loss_ce: 0.004950, loss_dice: 0.076004
[13:14:02.975] TRAIN: iteration 33024 : loss : 0.260785, loss_ce: 0.021485, loss_dice: 0.500085
[13:14:03.191] TRAIN: iteration 33025 : loss : 0.074303, loss_ce: 0.001430, loss_dice: 0.147176
[13:14:03.399] TRAIN: iteration 33026 : loss : 0.038396, loss_ce: 0.001587, loss_dice: 0.075205
[13:14:03.607] TRAIN: iteration 33027 : loss : 0.060763, loss_ce: 0.002977, loss_dice: 0.118549
[13:14:03.818] TRAIN: iteration 33028 : loss : 0.128173, loss_ce: 0.005958, loss_dice: 0.250387
[13:14:04.027] TRAIN: iteration 33029 : loss : 0.023163, loss_ce: 0.001110, loss_dice: 0.045216
[13:14:04.234] TRAIN: iteration 33030 : loss : 0.061622, loss_ce: 0.001906, loss_dice: 0.121338
[13:14:04.441] TRAIN: iteration 33031 : loss : 0.080864, loss_ce: 0.001417, loss_dice: 0.160311
[13:14:04.654] TRAIN: iteration 33032 : loss : 0.041015, loss_ce: 0.001822, loss_dice: 0.080208
[13:14:04.863] TRAIN: iteration 33033 : loss : 0.250329, loss_ce: 0.000635, loss_dice: 0.500023
[13:14:05.071] TRAIN: iteration 33034 : loss : 0.211190, loss_ce: 0.001091, loss_dice: 0.421288
[13:14:05.279] TRAIN: iteration 33035 : loss : 0.168212, loss_ce: 0.002372, loss_dice: 0.334052
[13:14:05.487] TRAIN: iteration 33036 : loss : 0.250508, loss_ce: 0.000972, loss_dice: 0.500044
[13:14:05.695] TRAIN: iteration 33037 : loss : 0.238887, loss_ce: 0.008829, loss_dice: 0.468944
[13:14:05.901] TRAIN: iteration 33038 : loss : 0.076300, loss_ce: 0.012927, loss_dice: 0.139672
[13:14:06.109] TRAIN: iteration 33039 : loss : 0.063476, loss_ce: 0.004834, loss_dice: 0.122118
[13:14:06.318] TRAIN: iteration 33040 : loss : 0.061362, loss_ce: 0.003727, loss_dice: 0.118997
[13:14:06.566] TRAIN: iteration 33041 : loss : 0.023200, loss_ce: 0.000730, loss_dice: 0.045671
[13:14:06.775] TRAIN: iteration 33042 : loss : 0.055488, loss_ce: 0.001958, loss_dice: 0.109018
[13:14:06.984] TRAIN: iteration 33043 : loss : 0.147846, loss_ce: 0.001649, loss_dice: 0.294043
[13:14:07.197] TRAIN: iteration 33044 : loss : 0.030997, loss_ce: 0.001148, loss_dice: 0.060846
[13:14:07.404] TRAIN: iteration 33045 : loss : 0.244886, loss_ce: 0.001184, loss_dice: 0.488589
[13:14:07.614] TRAIN: iteration 33046 : loss : 0.057444, loss_ce: 0.001923, loss_dice: 0.112965
[13:14:07.822] TRAIN: iteration 33047 : loss : 0.076343, loss_ce: 0.004503, loss_dice: 0.148184
[13:14:08.030] TRAIN: iteration 33048 : loss : 0.076376, loss_ce: 0.004536, loss_dice: 0.148216
[13:14:08.240] TRAIN: iteration 33049 : loss : 0.019445, loss_ce: 0.001865, loss_dice: 0.037026
[13:14:08.476] TRAIN: iteration 33050 : loss : 0.054485, loss_ce: 0.001196, loss_dice: 0.107774
[13:14:08.684] TRAIN: iteration 33051 : loss : 0.207876, loss_ce: 0.004916, loss_dice: 0.410836
[13:14:08.892] TRAIN: iteration 33052 : loss : 0.250570, loss_ce: 0.001088, loss_dice: 0.500052
[13:14:09.101] TRAIN: iteration 33053 : loss : 0.075339, loss_ce: 0.003196, loss_dice: 0.147481
[13:14:09.309] TRAIN: iteration 33054 : loss : 0.048820, loss_ce: 0.001101, loss_dice: 0.096539
[13:14:09.653] TRAIN: iteration 33055 : loss : 0.032398, loss_ce: 0.001175, loss_dice: 0.063621
[13:14:09.861] TRAIN: iteration 33056 : loss : 0.064584, loss_ce: 0.001301, loss_dice: 0.127867
[13:14:10.069] TRAIN: iteration 33057 : loss : 0.085805, loss_ce: 0.003335, loss_dice: 0.168276
[13:14:10.277] TRAIN: iteration 33058 : loss : 0.039371, loss_ce: 0.002957, loss_dice: 0.075785
[13:14:10.485] TRAIN: iteration 33059 : loss : 0.058177, loss_ce: 0.003823, loss_dice: 0.112531
[13:14:10.693] TRAIN: iteration 33060 : loss : 0.055638, loss_ce: 0.001835, loss_dice: 0.109442
[13:14:10.694] NaN or Inf found in input tensor.
[13:14:10.911] TRAIN: iteration 33061 : loss : 0.179574, loss_ce: 0.001637, loss_dice: 0.357512
[13:14:11.125] TRAIN: iteration 33062 : loss : 0.238230, loss_ce: 0.001483, loss_dice: 0.474977
[13:14:11.334] TRAIN: iteration 33063 : loss : 0.025482, loss_ce: 0.001700, loss_dice: 0.049264
[13:14:11.542] TRAIN: iteration 33064 : loss : 0.043996, loss_ce: 0.000751, loss_dice: 0.087242
[13:14:11.756] TRAIN: iteration 33065 : loss : 0.228675, loss_ce: 0.001049, loss_dice: 0.456301
[13:14:11.969] TRAIN: iteration 33066 : loss : 0.148050, loss_ce: 0.003470, loss_dice: 0.292630
[13:14:12.178] TRAIN: iteration 33067 : loss : 0.058966, loss_ce: 0.001006, loss_dice: 0.116926
[13:14:12.385] TRAIN: iteration 33068 : loss : 0.048567, loss_ce: 0.002736, loss_dice: 0.094397
[13:14:12.596] TRAIN: iteration 33069 : loss : 0.026249, loss_ce: 0.001746, loss_dice: 0.050751
[13:14:12.810] TRAIN: iteration 33070 : loss : 0.032176, loss_ce: 0.002574, loss_dice: 0.061777
[13:14:13.026] TRAIN: iteration 33071 : loss : 0.063699, loss_ce: 0.003344, loss_dice: 0.124055
[13:14:13.236] TRAIN: iteration 33072 : loss : 0.242770, loss_ce: 0.005296, loss_dice: 0.480245
[13:14:13.445] TRAIN: iteration 33073 : loss : 0.061795, loss_ce: 0.004343, loss_dice: 0.119247
[13:14:13.654] TRAIN: iteration 33074 : loss : 0.091308, loss_ce: 0.007570, loss_dice: 0.175046
[13:14:13.868] TRAIN: iteration 33075 : loss : 0.064840, loss_ce: 0.001628, loss_dice: 0.128052
[13:14:14.077] TRAIN: iteration 33076 : loss : 0.021485, loss_ce: 0.001958, loss_dice: 0.041012
[13:14:14.287] TRAIN: iteration 33077 : loss : 0.250333, loss_ce: 0.000650, loss_dice: 0.500016
[13:14:14.494] TRAIN: iteration 33078 : loss : 0.078592, loss_ce: 0.001328, loss_dice: 0.155856
[13:14:14.702] TRAIN: iteration 33079 : loss : 0.086065, loss_ce: 0.006373, loss_dice: 0.165757
[13:14:14.911] TRAIN: iteration 33080 : loss : 0.074635, loss_ce: 0.003916, loss_dice: 0.145355
[13:14:15.156] TRAIN: iteration 33081 : loss : 0.070355, loss_ce: 0.007523, loss_dice: 0.133187
[13:14:15.364] TRAIN: iteration 33082 : loss : 0.250352, loss_ce: 0.000690, loss_dice: 0.500014
[13:14:15.571] TRAIN: iteration 33083 : loss : 0.246236, loss_ce: 0.004348, loss_dice: 0.488124
[13:14:15.779] TRAIN: iteration 33084 : loss : 0.023065, loss_ce: 0.000641, loss_dice: 0.045488
[13:14:15.988] TRAIN: iteration 33085 : loss : 0.026469, loss_ce: 0.004326, loss_dice: 0.048613
[13:14:16.202] TRAIN: iteration 33086 : loss : 0.084810, loss_ce: 0.004109, loss_dice: 0.165510
[13:14:16.409] TRAIN: iteration 33087 : loss : 0.169656, loss_ce: 0.004860, loss_dice: 0.334453
[13:14:16.617] TRAIN: iteration 33088 : loss : 0.078998, loss_ce: 0.001732, loss_dice: 0.156265
[13:14:16.824] TRAIN: iteration 33089 : loss : 0.155831, loss_ce: 0.010173, loss_dice: 0.301490
[13:14:17.032] TRAIN: iteration 33090 : loss : 0.101640, loss_ce: 0.003039, loss_dice: 0.200241
[13:14:17.239] TRAIN: iteration 33091 : loss : 0.109729, loss_ce: 0.002767, loss_dice: 0.216692
[13:14:17.449] TRAIN: iteration 33092 : loss : 0.101968, loss_ce: 0.007371, loss_dice: 0.196566
[13:14:17.663] TRAIN: iteration 33093 : loss : 0.055362, loss_ce: 0.000955, loss_dice: 0.109768
[13:14:17.876] TRAIN: iteration 33094 : loss : 0.045576, loss_ce: 0.002785, loss_dice: 0.088367
[13:14:18.091] TRAIN: iteration 33095 : loss : 0.068120, loss_ce: 0.003146, loss_dice: 0.133094
[13:14:18.302] TRAIN: iteration 33096 : loss : 0.136017, loss_ce: 0.004504, loss_dice: 0.267529
[13:14:18.509] TRAIN: iteration 33097 : loss : 0.101720, loss_ce: 0.001205, loss_dice: 0.202235
[13:14:18.718] TRAIN: iteration 33098 : loss : 0.019068, loss_ce: 0.002428, loss_dice: 0.035708
[13:14:18.926] TRAIN: iteration 33099 : loss : 0.104725, loss_ce: 0.004057, loss_dice: 0.205392
[13:14:19.139] TRAIN: iteration 33100 : loss : 0.087449, loss_ce: 0.002837, loss_dice: 0.172062
[13:14:19.380] TRAIN: iteration 33101 : loss : 0.126515, loss_ce: 0.005578, loss_dice: 0.247452
[13:14:19.588] TRAIN: iteration 33102 : loss : 0.084344, loss_ce: 0.002651, loss_dice: 0.166036
[13:14:19.796] TRAIN: iteration 33103 : loss : 0.247883, loss_ce: 0.001295, loss_dice: 0.494471
[13:14:20.004] TRAIN: iteration 33104 : loss : 0.019073, loss_ce: 0.000683, loss_dice: 0.037462
[13:14:20.213] TRAIN: iteration 33105 : loss : 0.050633, loss_ce: 0.001404, loss_dice: 0.099862
[13:14:20.422] TRAIN: iteration 33106 : loss : 0.238162, loss_ce: 0.001596, loss_dice: 0.474728
[13:14:20.630] TRAIN: iteration 33107 : loss : 0.066833, loss_ce: 0.002184, loss_dice: 0.131481
[13:14:20.839] TRAIN: iteration 33108 : loss : 0.074408, loss_ce: 0.003820, loss_dice: 0.144997
[13:14:21.048] TRAIN: iteration 33109 : loss : 0.089762, loss_ce: 0.007682, loss_dice: 0.171842
[13:14:21.257] TRAIN: iteration 33110 : loss : 0.053188, loss_ce: 0.004266, loss_dice: 0.102109
[13:14:21.467] TRAIN: iteration 33111 : loss : 0.175995, loss_ce: 0.006537, loss_dice: 0.345452
[13:14:21.676] TRAIN: iteration 33112 : loss : 0.251248, loss_ce: 0.002338, loss_dice: 0.500158
[13:14:22.374] TRAIN: iteration 33113 : loss : 0.167700, loss_ce: 0.004952, loss_dice: 0.330447
[13:14:22.581] TRAIN: iteration 33114 : loss : 0.042161, loss_ce: 0.002926, loss_dice: 0.081395
[13:14:22.792] TRAIN: iteration 33115 : loss : 0.035757, loss_ce: 0.001042, loss_dice: 0.070472
[13:14:23.004] TRAIN: iteration 33116 : loss : 0.045310, loss_ce: 0.009239, loss_dice: 0.081381
[13:14:23.221] TRAIN: iteration 33117 : loss : 0.057436, loss_ce: 0.005547, loss_dice: 0.109326
[13:14:23.428] TRAIN: iteration 33118 : loss : 0.240567, loss_ce: 0.001141, loss_dice: 0.479993
[13:14:23.642] TRAIN: iteration 33119 : loss : 0.129578, loss_ce: 0.001287, loss_dice: 0.257869
[13:14:23.858] TRAIN: iteration 33120 : loss : 0.056977, loss_ce: 0.000981, loss_dice: 0.112972
[13:14:24.111] TRAIN: iteration 33121 : loss : 0.206604, loss_ce: 0.001338, loss_dice: 0.411869
[13:14:24.318] TRAIN: iteration 33122 : loss : 0.071937, loss_ce: 0.006266, loss_dice: 0.137607
[13:14:24.526] TRAIN: iteration 33123 : loss : 0.184255, loss_ce: 0.001215, loss_dice: 0.367295
[13:14:24.735] TRAIN: iteration 33124 : loss : 0.250710, loss_ce: 0.001351, loss_dice: 0.500069
[13:14:24.942] TRAIN: iteration 33125 : loss : 0.173469, loss_ce: 0.009602, loss_dice: 0.337337
[13:14:25.190] TRAIN: iteration 33126 : loss : 0.117190, loss_ce: 0.004564, loss_dice: 0.229817
[13:14:25.399] TRAIN: iteration 33127 : loss : 0.039269, loss_ce: 0.001598, loss_dice: 0.076941
[13:14:25.610] TRAIN: iteration 33128 : loss : 0.119098, loss_ce: 0.010513, loss_dice: 0.227682
[13:14:25.819] TRAIN: iteration 33129 : loss : 0.053744, loss_ce: 0.001846, loss_dice: 0.105643
[13:14:26.028] TRAIN: iteration 33130 : loss : 0.224757, loss_ce: 0.001924, loss_dice: 0.447590
[13:14:26.238] TRAIN: iteration 33131 : loss : 0.014187, loss_ce: 0.000502, loss_dice: 0.027872
[13:14:26.449] TRAIN: iteration 33132 : loss : 0.093925, loss_ce: 0.002818, loss_dice: 0.185032
[13:14:26.658] TRAIN: iteration 33133 : loss : 0.025026, loss_ce: 0.001773, loss_dice: 0.048280
[13:14:26.866] TRAIN: iteration 33134 : loss : 0.047532, loss_ce: 0.001419, loss_dice: 0.093645
[13:14:27.075] TRAIN: iteration 33135 : loss : 0.091100, loss_ce: 0.002051, loss_dice: 0.180149
[13:14:27.283] TRAIN: iteration 33136 : loss : 0.189503, loss_ce: 0.001035, loss_dice: 0.377972
[13:14:27.490] TRAIN: iteration 33137 : loss : 0.250287, loss_ce: 0.000563, loss_dice: 0.500011
[13:14:27.698] TRAIN: iteration 33138 : loss : 0.036788, loss_ce: 0.003100, loss_dice: 0.070476
[13:14:27.907] TRAIN: iteration 33139 : loss : 0.250367, loss_ce: 0.000714, loss_dice: 0.500020
[13:14:28.120] TRAIN: iteration 33140 : loss : 0.156866, loss_ce: 0.018976, loss_dice: 0.294756
[13:14:28.358] TRAIN: iteration 33141 : loss : 0.168014, loss_ce: 0.003072, loss_dice: 0.332955
[13:14:28.565] TRAIN: iteration 33142 : loss : 0.048660, loss_ce: 0.000603, loss_dice: 0.096716
[13:14:28.778] TRAIN: iteration 33143 : loss : 0.163790, loss_ce: 0.001606, loss_dice: 0.325974
[13:14:28.993] TRAIN: iteration 33144 : loss : 0.126065, loss_ce: 0.009128, loss_dice: 0.243002
[13:14:29.208] TRAIN: iteration 33145 : loss : 0.183114, loss_ce: 0.009046, loss_dice: 0.357182
[13:14:29.423] TRAIN: iteration 33146 : loss : 0.042300, loss_ce: 0.001827, loss_dice: 0.082773
[13:14:29.630] TRAIN: iteration 33147 : loss : 0.074623, loss_ce: 0.003984, loss_dice: 0.145261
[13:14:29.838] TRAIN: iteration 33148 : loss : 0.058526, loss_ce: 0.008402, loss_dice: 0.108650
[13:14:30.048] TRAIN: iteration 33149 : loss : 0.052187, loss_ce: 0.001612, loss_dice: 0.102762
[13:14:30.477] TRAIN: iteration 33150 : loss : 0.250306, loss_ce: 0.000597, loss_dice: 0.500016
[13:14:30.687] TRAIN: iteration 33151 : loss : 0.048680, loss_ce: 0.001470, loss_dice: 0.095889
[13:14:30.894] TRAIN: iteration 33152 : loss : 0.102031, loss_ce: 0.004683, loss_dice: 0.199378
[13:14:31.104] TRAIN: iteration 33153 : loss : 0.053364, loss_ce: 0.001451, loss_dice: 0.105276
[13:14:31.332] TRAIN: iteration 33154 : loss : 0.065864, loss_ce: 0.003020, loss_dice: 0.128707
[13:14:31.543] TRAIN: iteration 33155 : loss : 0.051840, loss_ce: 0.001572, loss_dice: 0.102108
[13:14:31.753] TRAIN: iteration 33156 : loss : 0.054426, loss_ce: 0.002350, loss_dice: 0.106503
[13:14:31.967] TRAIN: iteration 33157 : loss : 0.250521, loss_ce: 0.001014, loss_dice: 0.500028
[13:14:32.181] TRAIN: iteration 33158 : loss : 0.064190, loss_ce: 0.003758, loss_dice: 0.124622
[13:14:32.388] TRAIN: iteration 33159 : loss : 0.053852, loss_ce: 0.003498, loss_dice: 0.104206
[13:14:32.596] TRAIN: iteration 33160 : loss : 0.084142, loss_ce: 0.001745, loss_dice: 0.166538
[13:14:32.836] TRAIN: iteration 33161 : loss : 0.165064, loss_ce: 0.004187, loss_dice: 0.325942
[13:14:33.045] TRAIN: iteration 33162 : loss : 0.164810, loss_ce: 0.001101, loss_dice: 0.328519
[13:14:33.257] TRAIN: iteration 33163 : loss : 0.058072, loss_ce: 0.003660, loss_dice: 0.112484
[13:14:33.471] TRAIN: iteration 33164 : loss : 0.012916, loss_ce: 0.000747, loss_dice: 0.025085
[13:14:33.678] TRAIN: iteration 33165 : loss : 0.054614, loss_ce: 0.004805, loss_dice: 0.104422
[13:14:33.887] TRAIN: iteration 33166 : loss : 0.030669, loss_ce: 0.000568, loss_dice: 0.060770
[13:14:34.095] TRAIN: iteration 33167 : loss : 0.095598, loss_ce: 0.003700, loss_dice: 0.187496
[13:14:34.303] TRAIN: iteration 33168 : loss : 0.241852, loss_ce: 0.001161, loss_dice: 0.482543
[13:14:34.512] TRAIN: iteration 33169 : loss : 0.065715, loss_ce: 0.002298, loss_dice: 0.129132
[13:14:34.721] TRAIN: iteration 33170 : loss : 0.186438, loss_ce: 0.002314, loss_dice: 0.370562
[13:14:34.929] TRAIN: iteration 33171 : loss : 0.065571, loss_ce: 0.004172, loss_dice: 0.126970
[13:14:35.139] TRAIN: iteration 33172 : loss : 0.012946, loss_ce: 0.000747, loss_dice: 0.025146
[13:14:35.347] TRAIN: iteration 33173 : loss : 0.062724, loss_ce: 0.002481, loss_dice: 0.122967
[13:14:35.557] TRAIN: iteration 33174 : loss : 0.251615, loss_ce: 0.004338, loss_dice: 0.498891
[13:14:35.767] TRAIN: iteration 33175 : loss : 0.038358, loss_ce: 0.005496, loss_dice: 0.071221
[13:14:35.984] TRAIN: iteration 33176 : loss : 0.040633, loss_ce: 0.004518, loss_dice: 0.076749
[13:14:36.193] TRAIN: iteration 33177 : loss : 0.052153, loss_ce: 0.003846, loss_dice: 0.100459
[13:14:36.401] TRAIN: iteration 33178 : loss : 0.047805, loss_ce: 0.000870, loss_dice: 0.094739
[13:14:36.612] TRAIN: iteration 33179 : loss : 0.110175, loss_ce: 0.002245, loss_dice: 0.218106
[13:14:36.824] TRAIN: iteration 33180 : loss : 0.077087, loss_ce: 0.003206, loss_dice: 0.150968
[13:14:37.070] TRAIN: iteration 33181 : loss : 0.040788, loss_ce: 0.004980, loss_dice: 0.076596
[13:14:37.285] TRAIN: iteration 33182 : loss : 0.055895, loss_ce: 0.007935, loss_dice: 0.103855
[13:14:37.496] TRAIN: iteration 33183 : loss : 0.054138, loss_ce: 0.004487, loss_dice: 0.103789
[13:14:37.705] TRAIN: iteration 33184 : loss : 0.076602, loss_ce: 0.001761, loss_dice: 0.151444
[13:14:37.915] TRAIN: iteration 33185 : loss : 0.079096, loss_ce: 0.001701, loss_dice: 0.156491
[13:14:38.131] TRAIN: iteration 33186 : loss : 0.206246, loss_ce: 0.001500, loss_dice: 0.410992
[13:14:38.340] TRAIN: iteration 33187 : loss : 0.124532, loss_ce: 0.000737, loss_dice: 0.248326
[13:14:38.547] TRAIN: iteration 33188 : loss : 0.025079, loss_ce: 0.000894, loss_dice: 0.049264
[13:14:38.755] TRAIN: iteration 33189 : loss : 0.160553, loss_ce: 0.005572, loss_dice: 0.315534
[13:14:38.963] TRAIN: iteration 33190 : loss : 0.005093, loss_ce: 0.000588, loss_dice: 0.009598
[13:14:39.174] TRAIN: iteration 33191 : loss : 0.021421, loss_ce: 0.003911, loss_dice: 0.038930
[13:14:39.384] TRAIN: iteration 33192 : loss : 0.103598, loss_ce: 0.003581, loss_dice: 0.203614
[13:14:39.593] TRAIN: iteration 33193 : loss : 0.046574, loss_ce: 0.000873, loss_dice: 0.092274
[13:14:39.808] TRAIN: iteration 33194 : loss : 0.202365, loss_ce: 0.001416, loss_dice: 0.403315
[13:14:40.018] TRAIN: iteration 33195 : loss : 0.137375, loss_ce: 0.005324, loss_dice: 0.269427
[13:14:40.226] TRAIN: iteration 33196 : loss : 0.019485, loss_ce: 0.000403, loss_dice: 0.038566
[13:14:40.435] TRAIN: iteration 33197 : loss : 0.065308, loss_ce: 0.001791, loss_dice: 0.128826
[13:14:40.644] TRAIN: iteration 33198 : loss : 0.101048, loss_ce: 0.011879, loss_dice: 0.190217
[13:14:40.852] TRAIN: iteration 33199 : loss : 0.250927, loss_ce: 0.001739, loss_dice: 0.500115
[13:14:41.060] TRAIN: iteration 33200 : loss : 0.153995, loss_ce: 0.004673, loss_dice: 0.303316
[13:14:41.297] TRAIN: iteration 33201 : loss : 0.061668, loss_ce: 0.001084, loss_dice: 0.122252
[13:14:41.513] TRAIN: iteration 33202 : loss : 0.042240, loss_ce: 0.001446, loss_dice: 0.083034
[13:14:41.722] TRAIN: iteration 33203 : loss : 0.056342, loss_ce: 0.002358, loss_dice: 0.110325
[13:14:41.930] TRAIN: iteration 33204 : loss : 0.056380, loss_ce: 0.003284, loss_dice: 0.109476
[13:14:42.137] TRAIN: iteration 33205 : loss : 0.155523, loss_ce: 0.006837, loss_dice: 0.304208
[13:14:42.345] TRAIN: iteration 33206 : loss : 0.042261, loss_ce: 0.000985, loss_dice: 0.083537
[13:14:42.554] TRAIN: iteration 33207 : loss : 0.048759, loss_ce: 0.004042, loss_dice: 0.093476
[13:14:42.763] TRAIN: iteration 33208 : loss : 0.036945, loss_ce: 0.001741, loss_dice: 0.072150
[13:14:42.969] TRAIN: iteration 33209 : loss : 0.113304, loss_ce: 0.004418, loss_dice: 0.222189
[13:14:43.185] TRAIN: iteration 33210 : loss : 0.056148, loss_ce: 0.008528, loss_dice: 0.103768
[13:14:43.401] TRAIN: iteration 33211 : loss : 0.069246, loss_ce: 0.000638, loss_dice: 0.137855
[13:14:43.621] TRAIN: iteration 33212 : loss : 0.051806, loss_ce: 0.001326, loss_dice: 0.102287
[13:14:43.828] TRAIN: iteration 33213 : loss : 0.109348, loss_ce: 0.003177, loss_dice: 0.215518
[13:14:44.043] TRAIN: iteration 33214 : loss : 0.048911, loss_ce: 0.001348, loss_dice: 0.096475
[13:14:44.253] TRAIN: iteration 33215 : loss : 0.051726, loss_ce: 0.003314, loss_dice: 0.100137
[13:14:44.467] TRAIN: iteration 33216 : loss : 0.250191, loss_ce: 0.000375, loss_dice: 0.500007
[13:14:44.675] TRAIN: iteration 33217 : loss : 0.082750, loss_ce: 0.001378, loss_dice: 0.164122
[13:14:44.884] TRAIN: iteration 33218 : loss : 0.250511, loss_ce: 0.000964, loss_dice: 0.500058
[13:14:45.091] TRAIN: iteration 33219 : loss : 0.096087, loss_ce: 0.004349, loss_dice: 0.187825
[13:14:45.302] TRAIN: iteration 33220 : loss : 0.049575, loss_ce: 0.004158, loss_dice: 0.094992
[13:14:45.538] TRAIN: iteration 33221 : loss : 0.033298, loss_ce: 0.003883, loss_dice: 0.062713
[13:14:45.748] TRAIN: iteration 33222 : loss : 0.055714, loss_ce: 0.000794, loss_dice: 0.110635
[13:14:45.956] TRAIN: iteration 33223 : loss : 0.113881, loss_ce: 0.000822, loss_dice: 0.226940
[13:14:46.164] TRAIN: iteration 33224 : loss : 0.111562, loss_ce: 0.001680, loss_dice: 0.221444
[13:14:46.382] TRAIN: iteration 33225 : loss : 0.025787, loss_ce: 0.001230, loss_dice: 0.050345
[13:14:46.589] TRAIN: iteration 33226 : loss : 0.059238, loss_ce: 0.001587, loss_dice: 0.116888
[13:14:46.797] TRAIN: iteration 33227 : loss : 0.250352, loss_ce: 0.000675, loss_dice: 0.500030
[13:14:47.012] TRAIN: iteration 33228 : loss : 0.057372, loss_ce: 0.003746, loss_dice: 0.110997
[13:14:47.224] TRAIN: iteration 33229 : loss : 0.027586, loss_ce: 0.000872, loss_dice: 0.054300
[13:14:47.439] TRAIN: iteration 33230 : loss : 0.038782, loss_ce: 0.000749, loss_dice: 0.076816
[13:14:47.653] TRAIN: iteration 33231 : loss : 0.130328, loss_ce: 0.000497, loss_dice: 0.260159
[13:14:47.863] TRAIN: iteration 33232 : loss : 0.184876, loss_ce: 0.001494, loss_dice: 0.368258
[13:14:48.079] TRAIN: iteration 33233 : loss : 0.104736, loss_ce: 0.003043, loss_dice: 0.206430
[13:14:48.289] TRAIN: iteration 33234 : loss : 0.026304, loss_ce: 0.000627, loss_dice: 0.051981
[13:14:48.502] TRAIN: iteration 33235 : loss : 0.034881, loss_ce: 0.002327, loss_dice: 0.067435
[13:14:48.714] TRAIN: iteration 33236 : loss : 0.045161, loss_ce: 0.008845, loss_dice: 0.081477
[13:14:48.926] TRAIN: iteration 33237 : loss : 0.160204, loss_ce: 0.010754, loss_dice: 0.309654
[13:14:49.138] TRAIN: iteration 33238 : loss : 0.198745, loss_ce: 0.018190, loss_dice: 0.379300
[13:14:49.345] TRAIN: iteration 33239 : loss : 0.223621, loss_ce: 0.001479, loss_dice: 0.445762
[13:14:49.556] TRAIN: iteration 33240 : loss : 0.034993, loss_ce: 0.002562, loss_dice: 0.067424
[13:14:49.814] TRAIN: iteration 33241 : loss : 0.048365, loss_ce: 0.002586, loss_dice: 0.094143
[13:14:50.027] TRAIN: iteration 33242 : loss : 0.076333, loss_ce: 0.002066, loss_dice: 0.150600
[13:14:50.236] TRAIN: iteration 33243 : loss : 0.052878, loss_ce: 0.002152, loss_dice: 0.103604
[13:14:50.445] TRAIN: iteration 33244 : loss : 0.068095, loss_ce: 0.003526, loss_dice: 0.132663
[13:14:50.654] TRAIN: iteration 33245 : loss : 0.124166, loss_ce: 0.001044, loss_dice: 0.247288
[13:14:50.863] TRAIN: iteration 33246 : loss : 0.087401, loss_ce: 0.000715, loss_dice: 0.174087
[13:14:51.070] TRAIN: iteration 33247 : loss : 0.018537, loss_ce: 0.002098, loss_dice: 0.034976
[13:14:51.279] TRAIN: iteration 33248 : loss : 0.250733, loss_ce: 0.001384, loss_dice: 0.500082
[13:14:51.487] TRAIN: iteration 33249 : loss : 0.032221, loss_ce: 0.002502, loss_dice: 0.061940
[13:14:51.695] TRAIN: iteration 33250 : loss : 0.056281, loss_ce: 0.001329, loss_dice: 0.111234
[13:14:51.903] TRAIN: iteration 33251 : loss : 0.056059, loss_ce: 0.001722, loss_dice: 0.110396
[13:14:52.116] TRAIN: iteration 33252 : loss : 0.078248, loss_ce: 0.004864, loss_dice: 0.151632
[13:14:52.327] TRAIN: iteration 33253 : loss : 0.060941, loss_ce: 0.004118, loss_dice: 0.117763
[13:14:52.537] TRAIN: iteration 33254 : loss : 0.164824, loss_ce: 0.000841, loss_dice: 0.328808
[13:14:52.744] TRAIN: iteration 33255 : loss : 0.133379, loss_ce: 0.002392, loss_dice: 0.264366
[13:14:52.962] TRAIN: iteration 33256 : loss : 0.160514, loss_ce: 0.000991, loss_dice: 0.320037
[13:14:53.179] TRAIN: iteration 33257 : loss : 0.135602, loss_ce: 0.009666, loss_dice: 0.261539
[13:14:53.389] TRAIN: iteration 33258 : loss : 0.022231, loss_ce: 0.000515, loss_dice: 0.043948
[13:14:53.597] TRAIN: iteration 33259 : loss : 0.016003, loss_ce: 0.000676, loss_dice: 0.031329
[13:14:53.809] TRAIN: iteration 33260 : loss : 0.031831, loss_ce: 0.000881, loss_dice: 0.062782
[13:14:54.046] TRAIN: iteration 33261 : loss : 0.087135, loss_ce: 0.001481, loss_dice: 0.172789
[13:14:54.255] TRAIN: iteration 33262 : loss : 0.037089, loss_ce: 0.006262, loss_dice: 0.067915
[13:14:54.463] TRAIN: iteration 33263 : loss : 0.250580, loss_ce: 0.001094, loss_dice: 0.500065
[13:14:54.673] TRAIN: iteration 33264 : loss : 0.250308, loss_ce: 0.000597, loss_dice: 0.500019
[13:14:54.882] TRAIN: iteration 33265 : loss : 0.021262, loss_ce: 0.001877, loss_dice: 0.040647
[13:14:55.096] TRAIN: iteration 33266 : loss : 0.020803, loss_ce: 0.001536, loss_dice: 0.040070
[13:14:55.317] TRAIN: iteration 33267 : loss : 0.053256, loss_ce: 0.003136, loss_dice: 0.103376
[13:14:55.526] TRAIN: iteration 33268 : loss : 0.044275, loss_ce: 0.005706, loss_dice: 0.082845
[13:14:55.734] TRAIN: iteration 33269 : loss : 0.064599, loss_ce: 0.000494, loss_dice: 0.128704
[13:14:55.942] TRAIN: iteration 33270 : loss : 0.023189, loss_ce: 0.000894, loss_dice: 0.045484
[13:14:56.151] TRAIN: iteration 33271 : loss : 0.025495, loss_ce: 0.001872, loss_dice: 0.049117
[13:14:56.361] TRAIN: iteration 33272 : loss : 0.037876, loss_ce: 0.004997, loss_dice: 0.070756
[13:14:56.575] TRAIN: iteration 33273 : loss : 0.086974, loss_ce: 0.003062, loss_dice: 0.170887
[13:14:56.783] TRAIN: iteration 33274 : loss : 0.253104, loss_ce: 0.006905, loss_dice: 0.499303
[13:14:56.992] TRAIN: iteration 33275 : loss : 0.048369, loss_ce: 0.007418, loss_dice: 0.089319
[13:14:57.199] TRAIN: iteration 33276 : loss : 0.071215, loss_ce: 0.009258, loss_dice: 0.133172
[13:14:57.407] TRAIN: iteration 33277 : loss : 0.043420, loss_ce: 0.003667, loss_dice: 0.083173
[13:14:57.615] TRAIN: iteration 33278 : loss : 0.017977, loss_ce: 0.000452, loss_dice: 0.035503
[13:14:57.827] TRAIN: iteration 33279 : loss : 0.061220, loss_ce: 0.002541, loss_dice: 0.119899
[13:14:58.043] TRAIN: iteration 33280 : loss : 0.103894, loss_ce: 0.001130, loss_dice: 0.206658
[13:14:58.286] TRAIN: iteration 33281 : loss : 0.180645, loss_ce: 0.001401, loss_dice: 0.359890
[13:14:58.494] TRAIN: iteration 33282 : loss : 0.041145, loss_ce: 0.001143, loss_dice: 0.081146
[13:14:58.710] TRAIN: iteration 33283 : loss : 0.144469, loss_ce: 0.002464, loss_dice: 0.286473
[13:14:58.920] TRAIN: iteration 33284 : loss : 0.061006, loss_ce: 0.001890, loss_dice: 0.120121
[13:14:59.129] TRAIN: iteration 33285 : loss : 0.163783, loss_ce: 0.001803, loss_dice: 0.325763
[13:14:59.341] TRAIN: iteration 33286 : loss : 0.086469, loss_ce: 0.000954, loss_dice: 0.171984
[13:14:59.553] TRAIN: iteration 33287 : loss : 0.146072, loss_ce: 0.002407, loss_dice: 0.289737
[13:14:59.767] TRAIN: iteration 33288 : loss : 0.057526, loss_ce: 0.003813, loss_dice: 0.111238
[13:14:59.976] TRAIN: iteration 33289 : loss : 0.063207, loss_ce: 0.003355, loss_dice: 0.123059
[13:15:00.183] TRAIN: iteration 33290 : loss : 0.067152, loss_ce: 0.001231, loss_dice: 0.133073
[13:15:00.392] TRAIN: iteration 33291 : loss : 0.041543, loss_ce: 0.002469, loss_dice: 0.080617
[13:15:00.600] TRAIN: iteration 33292 : loss : 0.055175, loss_ce: 0.003250, loss_dice: 0.107100
[13:15:00.811] TRAIN: iteration 33293 : loss : 0.135182, loss_ce: 0.002433, loss_dice: 0.267930
[13:15:01.019] TRAIN: iteration 33294 : loss : 0.101282, loss_ce: 0.003616, loss_dice: 0.198948
[13:15:01.229] TRAIN: iteration 33295 : loss : 0.102453, loss_ce: 0.005573, loss_dice: 0.199332
[13:15:01.436] TRAIN: iteration 33296 : loss : 0.077791, loss_ce: 0.007776, loss_dice: 0.147805
[13:15:01.642] TRAIN: iteration 33297 : loss : 0.068921, loss_ce: 0.005771, loss_dice: 0.132070
[13:15:01.853] TRAIN: iteration 33298 : loss : 0.040726, loss_ce: 0.001006, loss_dice: 0.080446
[13:15:02.062] TRAIN: iteration 33299 : loss : 0.075568, loss_ce: 0.004916, loss_dice: 0.146220
[13:15:02.268] TRAIN: iteration 33300 : loss : 0.031150, loss_ce: 0.005558, loss_dice: 0.056742
[13:15:02.516] TRAIN: iteration 33301 : loss : 0.250286, loss_ce: 0.000549, loss_dice: 0.500023
[13:15:02.728] TRAIN: iteration 33302 : loss : 0.251430, loss_ce: 0.002905, loss_dice: 0.499954
[13:15:02.940] TRAIN: iteration 33303 : loss : 0.259476, loss_ce: 0.019923, loss_dice: 0.499029
[13:15:03.150] TRAIN: iteration 33304 : loss : 0.036576, loss_ce: 0.005581, loss_dice: 0.067572
[13:15:03.357] TRAIN: iteration 33305 : loss : 0.094782, loss_ce: 0.004744, loss_dice: 0.184821
[13:15:03.565] TRAIN: iteration 33306 : loss : 0.086608, loss_ce: 0.001556, loss_dice: 0.171660
[13:15:03.774] TRAIN: iteration 33307 : loss : 0.088313, loss_ce: 0.002775, loss_dice: 0.173852
[13:15:03.984] TRAIN: iteration 33308 : loss : 0.080654, loss_ce: 0.003320, loss_dice: 0.157987
[13:15:04.198] TRAIN: iteration 33309 : loss : 0.033385, loss_ce: 0.004255, loss_dice: 0.062514
[13:15:04.405] TRAIN: iteration 33310 : loss : 0.062999, loss_ce: 0.002326, loss_dice: 0.123672
[13:15:04.611] TRAIN: iteration 33311 : loss : 0.016883, loss_ce: 0.003352, loss_dice: 0.030415
[13:15:05.095] TRAIN: iteration 33312 : loss : 0.060699, loss_ce: 0.003545, loss_dice: 0.117852
[13:15:05.304] TRAIN: iteration 33313 : loss : 0.086056, loss_ce: 0.002592, loss_dice: 0.169519
[13:15:05.511] TRAIN: iteration 33314 : loss : 0.055421, loss_ce: 0.001871, loss_dice: 0.108970
[13:15:05.728] TRAIN: iteration 33315 : loss : 0.078064, loss_ce: 0.001241, loss_dice: 0.154887
[13:15:05.938] TRAIN: iteration 33316 : loss : 0.050922, loss_ce: 0.002445, loss_dice: 0.099399
[13:15:06.152] TRAIN: iteration 33317 : loss : 0.112717, loss_ce: 0.004510, loss_dice: 0.220923
[13:15:06.359] TRAIN: iteration 33318 : loss : 0.161204, loss_ce: 0.001349, loss_dice: 0.321058
[13:15:06.567] TRAIN: iteration 33319 : loss : 0.030702, loss_ce: 0.001720, loss_dice: 0.059685
[13:15:07.181] TRAIN: iteration 33320 : loss : 0.081944, loss_ce: 0.003323, loss_dice: 0.160566
[13:15:07.418] TRAIN: iteration 33321 : loss : 0.250491, loss_ce: 0.000931, loss_dice: 0.500051
[13:15:07.634] TRAIN: iteration 33322 : loss : 0.063516, loss_ce: 0.001097, loss_dice: 0.125936
[13:15:07.842] TRAIN: iteration 33323 : loss : 0.183916, loss_ce: 0.001914, loss_dice: 0.365918
[13:15:08.052] TRAIN: iteration 33324 : loss : 0.123561, loss_ce: 0.005630, loss_dice: 0.241492
[13:15:08.260] TRAIN: iteration 33325 : loss : 0.054061, loss_ce: 0.010444, loss_dice: 0.097677
[13:15:08.469] TRAIN: iteration 33326 : loss : 0.250593, loss_ce: 0.001116, loss_dice: 0.500070
[13:15:08.676] TRAIN: iteration 33327 : loss : 0.089742, loss_ce: 0.003373, loss_dice: 0.176111
[13:15:10.142] TRAIN: iteration 33328 : loss : 0.249939, loss_ce: 0.001504, loss_dice: 0.498375
[13:15:10.350] TRAIN: iteration 33329 : loss : 0.248053, loss_ce: 0.001352, loss_dice: 0.494754
[13:15:10.559] TRAIN: iteration 33330 : loss : 0.176160, loss_ce: 0.001758, loss_dice: 0.350562
[13:15:10.768] TRAIN: iteration 33331 : loss : 0.058689, loss_ce: 0.004993, loss_dice: 0.112385
[13:15:10.978] TRAIN: iteration 33332 : loss : 0.044703, loss_ce: 0.003626, loss_dice: 0.085781
[13:15:11.189] TRAIN: iteration 33333 : loss : 0.050575, loss_ce: 0.004164, loss_dice: 0.096986
[13:15:11.398] TRAIN: iteration 33334 : loss : 0.089665, loss_ce: 0.003363, loss_dice: 0.175968
[13:15:11.608] TRAIN: iteration 33335 : loss : 0.115376, loss_ce: 0.001829, loss_dice: 0.228923
[13:15:11.818] TRAIN: iteration 33336 : loss : 0.222982, loss_ce: 0.001051, loss_dice: 0.444913
[13:15:12.026] TRAIN: iteration 33337 : loss : 0.057253, loss_ce: 0.001728, loss_dice: 0.112779
[13:15:12.233] TRAIN: iteration 33338 : loss : 0.080090, loss_ce: 0.003477, loss_dice: 0.156702
[13:15:12.443] TRAIN: iteration 33339 : loss : 0.089596, loss_ce: 0.001281, loss_dice: 0.177911
[13:15:12.652] TRAIN: iteration 33340 : loss : 0.025313, loss_ce: 0.000378, loss_dice: 0.050249
[13:15:12.901] TRAIN: iteration 33341 : loss : 0.074707, loss_ce: 0.001605, loss_dice: 0.147809
[13:15:13.113] TRAIN: iteration 33342 : loss : 0.050472, loss_ce: 0.002673, loss_dice: 0.098271
[13:15:13.321] TRAIN: iteration 33343 : loss : 0.250750, loss_ce: 0.001407, loss_dice: 0.500094
[13:15:13.579] TRAIN: iteration 33344 : loss : 0.156028, loss_ce: 0.003191, loss_dice: 0.308864
[13:15:13.790] TRAIN: iteration 33345 : loss : 0.039817, loss_ce: 0.004292, loss_dice: 0.075342
[13:15:13.999] TRAIN: iteration 33346 : loss : 0.075047, loss_ce: 0.004598, loss_dice: 0.145496
[13:15:14.231] TRAIN: iteration 33347 : loss : 0.075567, loss_ce: 0.005178, loss_dice: 0.145957
[13:15:14.442] TRAIN: iteration 33348 : loss : 0.071990, loss_ce: 0.002650, loss_dice: 0.141330
[13:15:14.649] TRAIN: iteration 33349 : loss : 0.049254, loss_ce: 0.004918, loss_dice: 0.093590
[13:15:14.857] TRAIN: iteration 33350 : loss : 0.075206, loss_ce: 0.002673, loss_dice: 0.147738
[13:15:15.063] TRAIN: iteration 33351 : loss : 0.034503, loss_ce: 0.001698, loss_dice: 0.067308
[13:15:15.807] TRAIN: iteration 33352 : loss : 0.075712, loss_ce: 0.002039, loss_dice: 0.149385
[13:15:16.024] TRAIN: iteration 33353 : loss : 0.081239, loss_ce: 0.003842, loss_dice: 0.158635
[13:15:16.231] TRAIN: iteration 33354 : loss : 0.082802, loss_ce: 0.003140, loss_dice: 0.162465
[13:15:16.442] TRAIN: iteration 33355 : loss : 0.009687, loss_ce: 0.000932, loss_dice: 0.018442
[13:15:16.650] TRAIN: iteration 33356 : loss : 0.250357, loss_ce: 0.000696, loss_dice: 0.500017
[13:15:16.857] TRAIN: iteration 33357 : loss : 0.250157, loss_ce: 0.000311, loss_dice: 0.500003
[13:15:17.066] TRAIN: iteration 33358 : loss : 0.022386, loss_ce: 0.000386, loss_dice: 0.044386
[13:15:18.086] TRAIN: iteration 33359 : loss : 0.250509, loss_ce: 0.000963, loss_dice: 0.500054
[13:15:18.294] TRAIN: iteration 33360 : loss : 0.093778, loss_ce: 0.001822, loss_dice: 0.185733
[13:15:18.532] TRAIN: iteration 33361 : loss : 0.233132, loss_ce: 0.000358, loss_dice: 0.465906
[13:15:18.739] TRAIN: iteration 33362 : loss : 0.058585, loss_ce: 0.004998, loss_dice: 0.112172
[13:15:18.946] TRAIN: iteration 33363 : loss : 0.051089, loss_ce: 0.001471, loss_dice: 0.100706
[13:15:19.169] TRAIN: iteration 33364 : loss : 0.250122, loss_ce: 0.000246, loss_dice: 0.499999
[13:15:19.376] TRAIN: iteration 33365 : loss : 0.084620, loss_ce: 0.002433, loss_dice: 0.166807
[13:15:19.585] TRAIN: iteration 33366 : loss : 0.057155, loss_ce: 0.001419, loss_dice: 0.112891
[13:15:20.526] TRAIN: iteration 33367 : loss : 0.036422, loss_ce: 0.003461, loss_dice: 0.069382
[13:15:20.733] TRAIN: iteration 33368 : loss : 0.058040, loss_ce: 0.005100, loss_dice: 0.110980
[13:15:20.945] TRAIN: iteration 33369 : loss : 0.181992, loss_ce: 0.000819, loss_dice: 0.363164
[13:15:21.156] TRAIN: iteration 33370 : loss : 0.117896, loss_ce: 0.003240, loss_dice: 0.232551
[13:15:21.363] TRAIN: iteration 33371 : loss : 0.051421, loss_ce: 0.003160, loss_dice: 0.099681
[13:15:21.570] TRAIN: iteration 33372 : loss : 0.035242, loss_ce: 0.006638, loss_dice: 0.063846
[13:15:21.777] TRAIN: iteration 33373 : loss : 0.035513, loss_ce: 0.002626, loss_dice: 0.068401
[13:15:21.983] TRAIN: iteration 33374 : loss : 0.089684, loss_ce: 0.002436, loss_dice: 0.176932
[13:15:22.190] TRAIN: iteration 33375 : loss : 0.043082, loss_ce: 0.001781, loss_dice: 0.084382
[13:15:22.398] TRAIN: iteration 33376 : loss : 0.203365, loss_ce: 0.001253, loss_dice: 0.405477
[13:15:22.606] TRAIN: iteration 33377 : loss : 0.033024, loss_ce: 0.001573, loss_dice: 0.064474
[13:15:22.813] TRAIN: iteration 33378 : loss : 0.035978, loss_ce: 0.003747, loss_dice: 0.068209
[13:15:23.022] TRAIN: iteration 33379 : loss : 0.052497, loss_ce: 0.001435, loss_dice: 0.103559
[13:15:23.238] TRAIN: iteration 33380 : loss : 0.046872, loss_ce: 0.002766, loss_dice: 0.090979
[13:15:23.472] TRAIN: iteration 33381 : loss : 0.041661, loss_ce: 0.004252, loss_dice: 0.079070
[13:15:23.681] TRAIN: iteration 33382 : loss : 0.029714, loss_ce: 0.000622, loss_dice: 0.058807
[13:15:23.889] TRAIN: iteration 33383 : loss : 0.022887, loss_ce: 0.000856, loss_dice: 0.044918
[13:15:24.096] TRAIN: iteration 33384 : loss : 0.039799, loss_ce: 0.003887, loss_dice: 0.075711
[13:15:24.304] TRAIN: iteration 33385 : loss : 0.029571, loss_ce: 0.002031, loss_dice: 0.057110
[13:15:24.517] TRAIN: iteration 33386 : loss : 0.027234, loss_ce: 0.002448, loss_dice: 0.052020
[13:15:24.723] TRAIN: iteration 33387 : loss : 0.021257, loss_ce: 0.001866, loss_dice: 0.040648
[13:15:24.933] TRAIN: iteration 33388 : loss : 0.250576, loss_ce: 0.001083, loss_dice: 0.500068
[13:15:25.143] TRAIN: iteration 33389 : loss : 0.242840, loss_ce: 0.001973, loss_dice: 0.483708
[13:15:25.353] TRAIN: iteration 33390 : loss : 0.054089, loss_ce: 0.001017, loss_dice: 0.107161
[13:15:25.563] TRAIN: iteration 33391 : loss : 0.152429, loss_ce: 0.001946, loss_dice: 0.302912
[13:15:25.773] TRAIN: iteration 33392 : loss : 0.246820, loss_ce: 0.003848, loss_dice: 0.489792
[13:15:25.986] TRAIN: iteration 33393 : loss : 0.104488, loss_ce: 0.000968, loss_dice: 0.208008
[13:15:26.197] TRAIN: iteration 33394 : loss : 0.045245, loss_ce: 0.002243, loss_dice: 0.088246
[13:15:26.406] TRAIN: iteration 33395 : loss : 0.122053, loss_ce: 0.002213, loss_dice: 0.241893
[13:15:26.614] TRAIN: iteration 33396 : loss : 0.159856, loss_ce: 0.015868, loss_dice: 0.303844
[13:15:26.824] TRAIN: iteration 33397 : loss : 0.049197, loss_ce: 0.005049, loss_dice: 0.093346
[13:15:27.038] TRAIN: iteration 33398 : loss : 0.210052, loss_ce: 0.002602, loss_dice: 0.417501
[13:15:27.251] TRAIN: iteration 33399 : loss : 0.042753, loss_ce: 0.002120, loss_dice: 0.083387
[13:15:27.463] TRAIN: iteration 33400 : loss : 0.092990, loss_ce: 0.002054, loss_dice: 0.183926
[13:15:27.701] TRAIN: iteration 33401 : loss : 0.040986, loss_ce: 0.002450, loss_dice: 0.079521
[13:15:27.908] TRAIN: iteration 33402 : loss : 0.026666, loss_ce: 0.006185, loss_dice: 0.047148
[13:15:28.116] TRAIN: iteration 33403 : loss : 0.027032, loss_ce: 0.002286, loss_dice: 0.051777
[13:15:28.326] TRAIN: iteration 33404 : loss : 0.251058, loss_ce: 0.001976, loss_dice: 0.500139
[13:15:28.534] TRAIN: iteration 33405 : loss : 0.030826, loss_ce: 0.002671, loss_dice: 0.058980
[13:15:28.747] TRAIN: iteration 33406 : loss : 0.058622, loss_ce: 0.000870, loss_dice: 0.116375
[13:15:28.967] TRAIN: iteration 33407 : loss : 0.189056, loss_ce: 0.002771, loss_dice: 0.375341
[13:15:29.178] TRAIN: iteration 33408 : loss : 0.060105, loss_ce: 0.013710, loss_dice: 0.106501
[13:15:29.590] TRAIN: iteration 33409 : loss : 0.097428, loss_ce: 0.001274, loss_dice: 0.193582
[13:15:29.799] TRAIN: iteration 33410 : loss : 0.224459, loss_ce: 0.004880, loss_dice: 0.444039
[13:15:30.010] TRAIN: iteration 33411 : loss : 0.051007, loss_ce: 0.002873, loss_dice: 0.099141
[13:15:30.217] TRAIN: iteration 33412 : loss : 0.079141, loss_ce: 0.001486, loss_dice: 0.156797
[13:15:30.427] TRAIN: iteration 33413 : loss : 0.047663, loss_ce: 0.001709, loss_dice: 0.093617
[13:15:30.638] TRAIN: iteration 33414 : loss : 0.099030, loss_ce: 0.002398, loss_dice: 0.195662
[13:15:30.849] TRAIN: iteration 33415 : loss : 0.027203, loss_ce: 0.002778, loss_dice: 0.051627
[13:15:31.060] TRAIN: iteration 33416 : loss : 0.225821, loss_ce: 0.000823, loss_dice: 0.450820
[13:15:31.277] TRAIN: iteration 33417 : loss : 0.250463, loss_ce: 0.000896, loss_dice: 0.500031
[13:15:31.727] TRAIN: iteration 33418 : loss : 0.043778, loss_ce: 0.006002, loss_dice: 0.081554
[13:15:31.938] TRAIN: iteration 33419 : loss : 0.250261, loss_ce: 0.000512, loss_dice: 0.500010
[13:15:32.147] TRAIN: iteration 33420 : loss : 0.243663, loss_ce: 0.003091, loss_dice: 0.484234
[13:15:32.382] TRAIN: iteration 33421 : loss : 0.024755, loss_ce: 0.001119, loss_dice: 0.048390
[13:15:32.607] TRAIN: iteration 33422 : loss : 0.250373, loss_ce: 0.000716, loss_dice: 0.500031
[13:15:32.823] TRAIN: iteration 33423 : loss : 0.082427, loss_ce: 0.002581, loss_dice: 0.162274
[13:15:33.031] TRAIN: iteration 33424 : loss : 0.026187, loss_ce: 0.001283, loss_dice: 0.051090
[13:15:33.238] TRAIN: iteration 33425 : loss : 0.028267, loss_ce: 0.002202, loss_dice: 0.054333
[13:15:33.445] TRAIN: iteration 33426 : loss : 0.088354, loss_ce: 0.002075, loss_dice: 0.174633
[13:15:33.654] TRAIN: iteration 33427 : loss : 0.103080, loss_ce: 0.005979, loss_dice: 0.200180
[13:15:33.861] TRAIN: iteration 33428 : loss : 0.247271, loss_ce: 0.009790, loss_dice: 0.484752
[13:15:34.069] TRAIN: iteration 33429 : loss : 0.087631, loss_ce: 0.002283, loss_dice: 0.172980
[13:15:34.276] TRAIN: iteration 33430 : loss : 0.057565, loss_ce: 0.001271, loss_dice: 0.113858
[13:15:34.483] TRAIN: iteration 33431 : loss : 0.101668, loss_ce: 0.004263, loss_dice: 0.199073
[13:15:34.690] TRAIN: iteration 33432 : loss : 0.063148, loss_ce: 0.003121, loss_dice: 0.123175
[13:15:34.905] TRAIN: iteration 33433 : loss : 0.235222, loss_ce: 0.001885, loss_dice: 0.468558
[13:15:35.133] TRAIN: iteration 33434 : loss : 0.019197, loss_ce: 0.000401, loss_dice: 0.037992
[13:15:35.341] TRAIN: iteration 33435 : loss : 0.098499, loss_ce: 0.005707, loss_dice: 0.191290
[13:15:35.551] TRAIN: iteration 33436 : loss : 0.046063, loss_ce: 0.001130, loss_dice: 0.090996
[13:15:35.758] TRAIN: iteration 33437 : loss : 0.250882, loss_ce: 0.001833, loss_dice: 0.499930
[13:15:35.967] TRAIN: iteration 33438 : loss : 0.250269, loss_ce: 0.000521, loss_dice: 0.500016
[13:15:36.203] TRAIN: iteration 33439 : loss : 0.021518, loss_ce: 0.003522, loss_dice: 0.039513
[13:15:36.419] TRAIN: iteration 33440 : loss : 0.032002, loss_ce: 0.003786, loss_dice: 0.060217
[13:15:36.658] TRAIN: iteration 33441 : loss : 0.089353, loss_ce: 0.002487, loss_dice: 0.176219
[13:15:36.872] TRAIN: iteration 33442 : loss : 0.052864, loss_ce: 0.003389, loss_dice: 0.102339
[13:15:37.080] TRAIN: iteration 33443 : loss : 0.072186, loss_ce: 0.003648, loss_dice: 0.140725
[13:15:37.290] TRAIN: iteration 33444 : loss : 0.250472, loss_ce: 0.000895, loss_dice: 0.500048
[13:15:37.498] TRAIN: iteration 33445 : loss : 0.151370, loss_ce: 0.000943, loss_dice: 0.301797
[13:15:37.711] TRAIN: iteration 33446 : loss : 0.154302, loss_ce: 0.002641, loss_dice: 0.305963
[13:15:37.921] TRAIN: iteration 33447 : loss : 0.129028, loss_ce: 0.006238, loss_dice: 0.251817
[13:15:38.138] TRAIN: iteration 33448 : loss : 0.250332, loss_ce: 0.000649, loss_dice: 0.500016
[13:15:38.346] TRAIN: iteration 33449 : loss : 0.070415, loss_ce: 0.000643, loss_dice: 0.140187
[13:15:38.556] TRAIN: iteration 33450 : loss : 0.111143, loss_ce: 0.002292, loss_dice: 0.219993
[13:15:38.771] TRAIN: iteration 33451 : loss : 0.087837, loss_ce: 0.002703, loss_dice: 0.172971
[13:15:38.980] TRAIN: iteration 33452 : loss : 0.144542, loss_ce: 0.003520, loss_dice: 0.285564
[13:15:39.191] TRAIN: iteration 33453 : loss : 0.047153, loss_ce: 0.004856, loss_dice: 0.089450
[13:15:39.401] TRAIN: iteration 33454 : loss : 0.079699, loss_ce: 0.000695, loss_dice: 0.158704
[13:15:39.610] TRAIN: iteration 33455 : loss : 0.250350, loss_ce: 0.000677, loss_dice: 0.500023
[13:15:39.820] TRAIN: iteration 33456 : loss : 0.065150, loss_ce: 0.003802, loss_dice: 0.126498
[13:15:40.029] TRAIN: iteration 33457 : loss : 0.053113, loss_ce: 0.003116, loss_dice: 0.103110
[13:15:40.238] TRAIN: iteration 33458 : loss : 0.250375, loss_ce: 0.000723, loss_dice: 0.500028
[13:15:40.445] TRAIN: iteration 33459 : loss : 0.223591, loss_ce: 0.000891, loss_dice: 0.446291
[13:15:40.652] TRAIN: iteration 33460 : loss : 0.099698, loss_ce: 0.001900, loss_dice: 0.197495
[13:15:40.893] TRAIN: iteration 33461 : loss : 0.079032, loss_ce: 0.001600, loss_dice: 0.156464
[13:15:41.101] TRAIN: iteration 33462 : loss : 0.040676, loss_ce: 0.007936, loss_dice: 0.073416
[13:15:41.310] TRAIN: iteration 33463 : loss : 0.065879, loss_ce: 0.016030, loss_dice: 0.115727
[13:15:41.522] TRAIN: iteration 33464 : loss : 0.015895, loss_ce: 0.003468, loss_dice: 0.028321
[13:15:41.776] TRAIN: iteration 33465 : loss : 0.080088, loss_ce: 0.003258, loss_dice: 0.156918
[13:15:41.984] TRAIN: iteration 33466 : loss : 0.087616, loss_ce: 0.002632, loss_dice: 0.172600
[13:15:42.193] TRAIN: iteration 33467 : loss : 0.250840, loss_ce: 0.001592, loss_dice: 0.500087
[13:15:42.401] TRAIN: iteration 33468 : loss : 0.246236, loss_ce: 0.002851, loss_dice: 0.489620
[13:15:42.613] TRAIN: iteration 33469 : loss : 0.024643, loss_ce: 0.002512, loss_dice: 0.046774
[13:15:42.821] TRAIN: iteration 33470 : loss : 0.030024, loss_ce: 0.000987, loss_dice: 0.059062
[13:15:43.028] TRAIN: iteration 33471 : loss : 0.064715, loss_ce: 0.000706, loss_dice: 0.128723
[13:15:43.248] TRAIN: iteration 33472 : loss : 0.063708, loss_ce: 0.000595, loss_dice: 0.126820
[13:15:43.456] TRAIN: iteration 33473 : loss : 0.146313, loss_ce: 0.004322, loss_dice: 0.288304
[13:15:43.664] TRAIN: iteration 33474 : loss : 0.050790, loss_ce: 0.001178, loss_dice: 0.100403
[13:15:43.875] TRAIN: iteration 33475 : loss : 0.250234, loss_ce: 0.000459, loss_dice: 0.500008
[13:15:44.088] TRAIN: iteration 33476 : loss : 0.156465, loss_ce: 0.000905, loss_dice: 0.312024
[13:15:44.295] TRAIN: iteration 33477 : loss : 0.061350, loss_ce: 0.004993, loss_dice: 0.117707
[13:15:44.502] TRAIN: iteration 33478 : loss : 0.073749, loss_ce: 0.000771, loss_dice: 0.146727
[13:15:44.717] TRAIN: iteration 33479 : loss : 0.045021, loss_ce: 0.004301, loss_dice: 0.085741
[13:15:44.932] TRAIN: iteration 33480 : loss : 0.025424, loss_ce: 0.004957, loss_dice: 0.045891
[13:15:45.175] TRAIN: iteration 33481 : loss : 0.250352, loss_ce: 0.000677, loss_dice: 0.500026
[13:15:45.384] TRAIN: iteration 33482 : loss : 0.167312, loss_ce: 0.005088, loss_dice: 0.329536
[13:15:45.591] TRAIN: iteration 33483 : loss : 0.037772, loss_ce: 0.001896, loss_dice: 0.073647
[13:15:45.806] TRAIN: iteration 33484 : loss : 0.040085, loss_ce: 0.002781, loss_dice: 0.077390
[13:15:46.014] TRAIN: iteration 33485 : loss : 0.074447, loss_ce: 0.010996, loss_dice: 0.137899
[13:15:46.225] TRAIN: iteration 33486 : loss : 0.064084, loss_ce: 0.005519, loss_dice: 0.122649
[13:15:46.437] TRAIN: iteration 33487 : loss : 0.125722, loss_ce: 0.001634, loss_dice: 0.249810
[13:15:46.652] TRAIN: iteration 33488 : loss : 0.105411, loss_ce: 0.001537, loss_dice: 0.209285
[13:15:46.862] TRAIN: iteration 33489 : loss : 0.025588, loss_ce: 0.003548, loss_dice: 0.047627
[13:15:47.080] TRAIN: iteration 33490 : loss : 0.218125, loss_ce: 0.006274, loss_dice: 0.429976
[13:15:47.290] TRAIN: iteration 33491 : loss : 0.147883, loss_ce: 0.000982, loss_dice: 0.294784
[13:15:47.498] TRAIN: iteration 33492 : loss : 0.032651, loss_ce: 0.001477, loss_dice: 0.063824
[13:15:47.705] TRAIN: iteration 33493 : loss : 0.032241, loss_ce: 0.000865, loss_dice: 0.063617
[13:15:47.912] TRAIN: iteration 33494 : loss : 0.014563, loss_ce: 0.000528, loss_dice: 0.028598
[13:15:48.120] TRAIN: iteration 33495 : loss : 0.091858, loss_ce: 0.010509, loss_dice: 0.173208
[13:15:48.335] TRAIN: iteration 33496 : loss : 0.051116, loss_ce: 0.001297, loss_dice: 0.100934
[13:15:48.542] TRAIN: iteration 33497 : loss : 0.033881, loss_ce: 0.000862, loss_dice: 0.066900
[13:15:48.751] TRAIN: iteration 33498 : loss : 0.121945, loss_ce: 0.001358, loss_dice: 0.242531
[13:15:48.965] TRAIN: iteration 33499 : loss : 0.041743, loss_ce: 0.007410, loss_dice: 0.076076
[13:15:49.175] TRAIN: iteration 33500 : loss : 0.060036, loss_ce: 0.001153, loss_dice: 0.118920
[13:15:49.423] TRAIN: iteration 33501 : loss : 0.126505, loss_ce: 0.011781, loss_dice: 0.241229
[13:15:49.630] TRAIN: iteration 33502 : loss : 0.106683, loss_ce: 0.003541, loss_dice: 0.209826
[13:15:49.841] TRAIN: iteration 33503 : loss : 0.034847, loss_ce: 0.001024, loss_dice: 0.068670
[13:15:50.050] TRAIN: iteration 33504 : loss : 0.057930, loss_ce: 0.002349, loss_dice: 0.113511
[13:15:50.263] TRAIN: iteration 33505 : loss : 0.065116, loss_ce: 0.001166, loss_dice: 0.129066
[13:15:50.477] TRAIN: iteration 33506 : loss : 0.251358, loss_ce: 0.002541, loss_dice: 0.500175
[13:15:50.686] TRAIN: iteration 33507 : loss : 0.053429, loss_ce: 0.002095, loss_dice: 0.104763
[13:15:50.900] TRAIN: iteration 33508 : loss : 0.057455, loss_ce: 0.003042, loss_dice: 0.111868
[13:15:51.113] TRAIN: iteration 33509 : loss : 0.250397, loss_ce: 0.000758, loss_dice: 0.500036
[13:15:51.329] TRAIN: iteration 33510 : loss : 0.068289, loss_ce: 0.002185, loss_dice: 0.134393
[13:15:51.538] TRAIN: iteration 33511 : loss : 0.082596, loss_ce: 0.001323, loss_dice: 0.163869
[13:15:51.754] TRAIN: iteration 33512 : loss : 0.040637, loss_ce: 0.002606, loss_dice: 0.078668
[13:15:51.961] TRAIN: iteration 33513 : loss : 0.250483, loss_ce: 0.000930, loss_dice: 0.500037
[13:15:52.173] TRAIN: iteration 33514 : loss : 0.184300, loss_ce: 0.002771, loss_dice: 0.365830
[13:15:52.398] TRAIN: iteration 33515 : loss : 0.147474, loss_ce: 0.002731, loss_dice: 0.292217
[13:15:52.605] TRAIN: iteration 33516 : loss : 0.250306, loss_ce: 0.000596, loss_dice: 0.500016
[13:15:52.813] TRAIN: iteration 33517 : loss : 0.019665, loss_ce: 0.001201, loss_dice: 0.038130
[13:15:53.029] TRAIN: iteration 33518 : loss : 0.042051, loss_ce: 0.003966, loss_dice: 0.080136
[13:15:53.239] TRAIN: iteration 33519 : loss : 0.100594, loss_ce: 0.006283, loss_dice: 0.194904
[13:15:53.447] TRAIN: iteration 33520 : loss : 0.043547, loss_ce: 0.002254, loss_dice: 0.084840
[13:15:53.448] NaN or Inf found in input tensor.
[13:15:53.665] TRAIN: iteration 33521 : loss : 0.034346, loss_ce: 0.000903, loss_dice: 0.067790
[13:15:53.874] TRAIN: iteration 33522 : loss : 0.250311, loss_ce: 0.000607, loss_dice: 0.500015
[13:15:54.086] TRAIN: iteration 33523 : loss : 0.250242, loss_ce: 0.000477, loss_dice: 0.500007
[13:15:54.295] TRAIN: iteration 33524 : loss : 0.132711, loss_ce: 0.001101, loss_dice: 0.264321
[13:15:54.502] TRAIN: iteration 33525 : loss : 0.252186, loss_ce: 0.004082, loss_dice: 0.500290
[13:15:54.709] TRAIN: iteration 33526 : loss : 0.184295, loss_ce: 0.001154, loss_dice: 0.367435
[13:15:54.916] TRAIN: iteration 33527 : loss : 0.045713, loss_ce: 0.001608, loss_dice: 0.089817
[13:15:55.125] TRAIN: iteration 33528 : loss : 0.123556, loss_ce: 0.001908, loss_dice: 0.245204
[13:15:55.335] TRAIN: iteration 33529 : loss : 0.035977, loss_ce: 0.003247, loss_dice: 0.068706
[13:15:55.543] TRAIN: iteration 33530 : loss : 0.061673, loss_ce: 0.001289, loss_dice: 0.122058
[13:15:55.750] TRAIN: iteration 33531 : loss : 0.128033, loss_ce: 0.003601, loss_dice: 0.252465
[13:15:55.958] TRAIN: iteration 33532 : loss : 0.041263, loss_ce: 0.003064, loss_dice: 0.079463
[13:15:56.168] TRAIN: iteration 33533 : loss : 0.034998, loss_ce: 0.002122, loss_dice: 0.067874
[13:15:56.382] TRAIN: iteration 33534 : loss : 0.053308, loss_ce: 0.006609, loss_dice: 0.100006
[13:15:56.998] TRAIN: iteration 33535 : loss : 0.250319, loss_ce: 0.000628, loss_dice: 0.500009
[13:15:57.581] TRAIN: iteration 33536 : loss : 0.060132, loss_ce: 0.002184, loss_dice: 0.118080
[13:15:57.790] TRAIN: iteration 33537 : loss : 0.250414, loss_ce: 0.000800, loss_dice: 0.500028
[13:15:57.999] TRAIN: iteration 33538 : loss : 0.034897, loss_ce: 0.000639, loss_dice: 0.069155
[13:15:58.207] TRAIN: iteration 33539 : loss : 0.029286, loss_ce: 0.000849, loss_dice: 0.057723
[13:15:58.415] TRAIN: iteration 33540 : loss : 0.024018, loss_ce: 0.001956, loss_dice: 0.046080
[13:15:58.649] TRAIN: iteration 33541 : loss : 0.052613, loss_ce: 0.007398, loss_dice: 0.097828
[13:15:58.864] TRAIN: iteration 33542 : loss : 0.245729, loss_ce: 0.002606, loss_dice: 0.488852
[13:16:00.331] TRAIN: iteration 33543 : loss : 0.029573, loss_ce: 0.000541, loss_dice: 0.058605
[13:16:00.541] TRAIN: iteration 33544 : loss : 0.052576, loss_ce: 0.005255, loss_dice: 0.099896
[13:16:00.753] TRAIN: iteration 33545 : loss : 0.096718, loss_ce: 0.001364, loss_dice: 0.192071
[13:16:00.963] TRAIN: iteration 33546 : loss : 0.251186, loss_ce: 0.002225, loss_dice: 0.500147
[13:16:01.174] TRAIN: iteration 33547 : loss : 0.250466, loss_ce: 0.000901, loss_dice: 0.500031
[13:16:01.383] TRAIN: iteration 33548 : loss : 0.234927, loss_ce: 0.000602, loss_dice: 0.469252
[13:16:01.595] TRAIN: iteration 33549 : loss : 0.072733, loss_ce: 0.002509, loss_dice: 0.142957
[13:16:01.812] TRAIN: iteration 33550 : loss : 0.046928, loss_ce: 0.002676, loss_dice: 0.091179
[13:16:02.545] TRAIN: iteration 33551 : loss : 0.199993, loss_ce: 0.004005, loss_dice: 0.395981
[13:16:02.753] TRAIN: iteration 33552 : loss : 0.068637, loss_ce: 0.006382, loss_dice: 0.130891
[13:16:02.961] TRAIN: iteration 33553 : loss : 0.099653, loss_ce: 0.002371, loss_dice: 0.196936
[13:16:03.171] TRAIN: iteration 33554 : loss : 0.043699, loss_ce: 0.004331, loss_dice: 0.083066
[13:16:03.382] TRAIN: iteration 33555 : loss : 0.042745, loss_ce: 0.001201, loss_dice: 0.084289
[13:16:03.593] TRAIN: iteration 33556 : loss : 0.025146, loss_ce: 0.005033, loss_dice: 0.045259
[13:16:03.804] TRAIN: iteration 33557 : loss : 0.035495, loss_ce: 0.003270, loss_dice: 0.067721
[13:16:04.011] TRAIN: iteration 33558 : loss : 0.021045, loss_ce: 0.003324, loss_dice: 0.038767
[13:16:05.103] TRAIN: iteration 33559 : loss : 0.250313, loss_ce: 0.000613, loss_dice: 0.500014
[13:16:05.314] TRAIN: iteration 33560 : loss : 0.034904, loss_ce: 0.004340, loss_dice: 0.065467
[13:16:05.555] TRAIN: iteration 33561 : loss : 0.041069, loss_ce: 0.001337, loss_dice: 0.080801
[13:16:05.763] TRAIN: iteration 33562 : loss : 0.040433, loss_ce: 0.004586, loss_dice: 0.076280
[13:16:05.971] TRAIN: iteration 33563 : loss : 0.224282, loss_ce: 0.000934, loss_dice: 0.447631
[13:16:06.181] TRAIN: iteration 33564 : loss : 0.250211, loss_ce: 0.000416, loss_dice: 0.500007
[13:16:06.389] TRAIN: iteration 33565 : loss : 0.242227, loss_ce: 0.001285, loss_dice: 0.483169
[13:16:06.626] TRAIN: iteration 33566 : loss : 0.076124, loss_ce: 0.002068, loss_dice: 0.150180
[13:16:06.836] TRAIN: iteration 33567 : loss : 0.058442, loss_ce: 0.002643, loss_dice: 0.114242
[13:16:07.044] TRAIN: iteration 33568 : loss : 0.144378, loss_ce: 0.001739, loss_dice: 0.287017
[13:16:07.251] TRAIN: iteration 33569 : loss : 0.033805, loss_ce: 0.002399, loss_dice: 0.065211
[13:16:07.461] TRAIN: iteration 33570 : loss : 0.035866, loss_ce: 0.003983, loss_dice: 0.067748
[13:16:07.678] TRAIN: iteration 33571 : loss : 0.033858, loss_ce: 0.002862, loss_dice: 0.064853
[13:16:07.886] TRAIN: iteration 33572 : loss : 0.063245, loss_ce: 0.001975, loss_dice: 0.124516
[13:16:08.099] TRAIN: iteration 33573 : loss : 0.068745, loss_ce: 0.000697, loss_dice: 0.136793
[13:16:08.309] TRAIN: iteration 33574 : loss : 0.051371, loss_ce: 0.003135, loss_dice: 0.099607
[13:16:08.519] TRAIN: iteration 33575 : loss : 0.081436, loss_ce: 0.002352, loss_dice: 0.160520
[13:16:08.728] TRAIN: iteration 33576 : loss : 0.250394, loss_ce: 0.000761, loss_dice: 0.500027
[13:16:08.937] TRAIN: iteration 33577 : loss : 0.047139, loss_ce: 0.001607, loss_dice: 0.092672
[13:16:09.145] TRAIN: iteration 33578 : loss : 0.250315, loss_ce: 0.000612, loss_dice: 0.500018
[13:16:09.353] TRAIN: iteration 33579 : loss : 0.075473, loss_ce: 0.002970, loss_dice: 0.147976
[13:16:09.562] TRAIN: iteration 33580 : loss : 0.047711, loss_ce: 0.003182, loss_dice: 0.092239
[13:16:09.803] TRAIN: iteration 33581 : loss : 0.048885, loss_ce: 0.002156, loss_dice: 0.095614
[13:16:10.012] TRAIN: iteration 33582 : loss : 0.033459, loss_ce: 0.001027, loss_dice: 0.065891
[13:16:10.223] TRAIN: iteration 33583 : loss : 0.092305, loss_ce: 0.002444, loss_dice: 0.182166
[13:16:10.431] TRAIN: iteration 33584 : loss : 0.032725, loss_ce: 0.002303, loss_dice: 0.063146
[13:16:10.641] TRAIN: iteration 33585 : loss : 0.067583, loss_ce: 0.002633, loss_dice: 0.132533
[13:16:10.850] TRAIN: iteration 33586 : loss : 0.049462, loss_ce: 0.002173, loss_dice: 0.096750
[13:16:11.070] TRAIN: iteration 33587 : loss : 0.220349, loss_ce: 0.001585, loss_dice: 0.439114
[13:16:11.280] TRAIN: iteration 33588 : loss : 0.054301, loss_ce: 0.003125, loss_dice: 0.105476
[13:16:11.487] TRAIN: iteration 33589 : loss : 0.076965, loss_ce: 0.002527, loss_dice: 0.151402
[13:16:11.695] TRAIN: iteration 33590 : loss : 0.022903, loss_ce: 0.001706, loss_dice: 0.044100
[13:16:11.903] TRAIN: iteration 33591 : loss : 0.250642, loss_ce: 0.001213, loss_dice: 0.500071
[13:16:12.112] TRAIN: iteration 33592 : loss : 0.084803, loss_ce: 0.002320, loss_dice: 0.167287
[13:16:12.321] TRAIN: iteration 33593 : loss : 0.041000, loss_ce: 0.000847, loss_dice: 0.081152
[13:16:12.530] TRAIN: iteration 33594 : loss : 0.046421, loss_ce: 0.001472, loss_dice: 0.091370
[13:16:12.738] TRAIN: iteration 33595 : loss : 0.130047, loss_ce: 0.009697, loss_dice: 0.250397
[13:16:12.948] TRAIN: iteration 33596 : loss : 0.034253, loss_ce: 0.002736, loss_dice: 0.065770
[13:16:13.156] TRAIN: iteration 33597 : loss : 0.031391, loss_ce: 0.002488, loss_dice: 0.060294
[13:16:13.364] TRAIN: iteration 33598 : loss : 0.251612, loss_ce: 0.003003, loss_dice: 0.500220
[13:16:13.575] TRAIN: iteration 33599 : loss : 0.052311, loss_ce: 0.001806, loss_dice: 0.102815
[13:16:13.785] TRAIN: iteration 33600 : loss : 0.022385, loss_ce: 0.004145, loss_dice: 0.040625
[13:16:14.022] TRAIN: iteration 33601 : loss : 0.068587, loss_ce: 0.001177, loss_dice: 0.135996
[13:16:14.230] TRAIN: iteration 33602 : loss : 0.046941, loss_ce: 0.003166, loss_dice: 0.090716
[13:16:14.439] TRAIN: iteration 33603 : loss : 0.029851, loss_ce: 0.001523, loss_dice: 0.058179
[13:16:14.647] TRAIN: iteration 33604 : loss : 0.034419, loss_ce: 0.002652, loss_dice: 0.066187
[13:16:15.224] TRAIN: iteration 33605 : loss : 0.030231, loss_ce: 0.002277, loss_dice: 0.058186
[13:16:15.431] TRAIN: iteration 33606 : loss : 0.031291, loss_ce: 0.003832, loss_dice: 0.058750
[13:16:15.642] TRAIN: iteration 33607 : loss : 0.059546, loss_ce: 0.006175, loss_dice: 0.112917
[13:16:15.852] TRAIN: iteration 33608 : loss : 0.130215, loss_ce: 0.002894, loss_dice: 0.257536
[13:16:16.060] TRAIN: iteration 33609 : loss : 0.059257, loss_ce: 0.006438, loss_dice: 0.112076
[13:16:16.274] TRAIN: iteration 33610 : loss : 0.094947, loss_ce: 0.006535, loss_dice: 0.183359
[13:16:16.486] TRAIN: iteration 33611 : loss : 0.154835, loss_ce: 0.001129, loss_dice: 0.308541
[13:16:16.693] TRAIN: iteration 33612 : loss : 0.039829, loss_ce: 0.000872, loss_dice: 0.078786
[13:16:16.901] TRAIN: iteration 33613 : loss : 0.250542, loss_ce: 0.001028, loss_dice: 0.500057
[13:16:17.434] TRAIN: iteration 33614 : loss : 0.036562, loss_ce: 0.002012, loss_dice: 0.071113
[13:16:17.640] TRAIN: iteration 33615 : loss : 0.201647, loss_ce: 0.000850, loss_dice: 0.402444
[13:16:17.847] TRAIN: iteration 33616 : loss : 0.136812, loss_ce: 0.003652, loss_dice: 0.269972
[13:16:18.058] TRAIN: iteration 33617 : loss : 0.125798, loss_ce: 0.007079, loss_dice: 0.244517
[13:16:18.266] TRAIN: iteration 33618 : loss : 0.048799, loss_ce: 0.001745, loss_dice: 0.095854
[13:16:18.475] TRAIN: iteration 33619 : loss : 0.052886, loss_ce: 0.001681, loss_dice: 0.104091
[13:16:18.681] TRAIN: iteration 33620 : loss : 0.178158, loss_ce: 0.001261, loss_dice: 0.355055
[13:16:18.928] TRAIN: iteration 33621 : loss : 0.034445, loss_ce: 0.000938, loss_dice: 0.067952
[13:16:19.139] TRAIN: iteration 33622 : loss : 0.123623, loss_ce: 0.002656, loss_dice: 0.244591
[13:16:19.347] TRAIN: iteration 33623 : loss : 0.166494, loss_ce: 0.008896, loss_dice: 0.324091
[13:16:19.554] TRAIN: iteration 33624 : loss : 0.078913, loss_ce: 0.002900, loss_dice: 0.154925
[13:16:19.763] TRAIN: iteration 33625 : loss : 0.063757, loss_ce: 0.007519, loss_dice: 0.119996
[13:16:19.970] TRAIN: iteration 33626 : loss : 0.250437, loss_ce: 0.000851, loss_dice: 0.500023
[13:16:20.187] TRAIN: iteration 33627 : loss : 0.038726, loss_ce: 0.003488, loss_dice: 0.073964
[13:16:20.597] TRAIN: iteration 33628 : loss : 0.205806, loss_ce: 0.001753, loss_dice: 0.409858
[13:16:20.806] TRAIN: iteration 33629 : loss : 0.088203, loss_ce: 0.003919, loss_dice: 0.172488
[13:16:21.019] TRAIN: iteration 33630 : loss : 0.199484, loss_ce: 0.001564, loss_dice: 0.397405
[13:16:21.231] TRAIN: iteration 33631 : loss : 0.048866, loss_ce: 0.003361, loss_dice: 0.094371
[13:16:21.438] TRAIN: iteration 33632 : loss : 0.049018, loss_ce: 0.003256, loss_dice: 0.094779
[13:16:21.646] TRAIN: iteration 33633 : loss : 0.038365, loss_ce: 0.001209, loss_dice: 0.075521
[13:16:21.854] TRAIN: iteration 33634 : loss : 0.064886, loss_ce: 0.000855, loss_dice: 0.128917
[13:16:22.062] TRAIN: iteration 33635 : loss : 0.021752, loss_ce: 0.002280, loss_dice: 0.041223
[13:16:22.271] TRAIN: iteration 33636 : loss : 0.095057, loss_ce: 0.001016, loss_dice: 0.189097
[13:16:22.482] TRAIN: iteration 33637 : loss : 0.119210, loss_ce: 0.004353, loss_dice: 0.234067
[13:16:22.690] TRAIN: iteration 33638 : loss : 0.081974, loss_ce: 0.009228, loss_dice: 0.154720
[13:16:22.900] TRAIN: iteration 33639 : loss : 0.097703, loss_ce: 0.004621, loss_dice: 0.190785
[13:16:23.112] TRAIN: iteration 33640 : loss : 0.061713, loss_ce: 0.001687, loss_dice: 0.121738
[13:16:23.348] TRAIN: iteration 33641 : loss : 0.041756, loss_ce: 0.001115, loss_dice: 0.082396
[13:16:23.556] TRAIN: iteration 33642 : loss : 0.245829, loss_ce: 0.001290, loss_dice: 0.490369
[13:16:23.764] TRAIN: iteration 33643 : loss : 0.158501, loss_ce: 0.005280, loss_dice: 0.311721
[13:16:23.971] TRAIN: iteration 33644 : loss : 0.087839, loss_ce: 0.001079, loss_dice: 0.174599
[13:16:25.088] TRAIN: iteration 33645 : loss : 0.088405, loss_ce: 0.003462, loss_dice: 0.173348
[13:16:25.301] TRAIN: iteration 33646 : loss : 0.050682, loss_ce: 0.003654, loss_dice: 0.097710
[13:16:25.509] TRAIN: iteration 33647 : loss : 0.029544, loss_ce: 0.003386, loss_dice: 0.055703
[13:16:25.715] TRAIN: iteration 33648 : loss : 0.079521, loss_ce: 0.005840, loss_dice: 0.153202
[13:16:25.929] TRAIN: iteration 33649 : loss : 0.058573, loss_ce: 0.001341, loss_dice: 0.115805
[13:16:26.143] TRAIN: iteration 33650 : loss : 0.095791, loss_ce: 0.005242, loss_dice: 0.186340
[13:16:26.350] TRAIN: iteration 33651 : loss : 0.038365, loss_ce: 0.002964, loss_dice: 0.073766
[13:16:26.565] TRAIN: iteration 33652 : loss : 0.095600, loss_ce: 0.001024, loss_dice: 0.190176
[13:16:26.806] TRAIN: iteration 33653 : loss : 0.163793, loss_ce: 0.001498, loss_dice: 0.326087
[13:16:27.016] TRAIN: iteration 33654 : loss : 0.046603, loss_ce: 0.002198, loss_dice: 0.091009
[13:16:27.232] TRAIN: iteration 33655 : loss : 0.126209, loss_ce: 0.002256, loss_dice: 0.250162
[13:16:27.439] TRAIN: iteration 33656 : loss : 0.041259, loss_ce: 0.003116, loss_dice: 0.079402
[13:16:27.647] TRAIN: iteration 33657 : loss : 0.037330, loss_ce: 0.003333, loss_dice: 0.071328
[13:16:27.855] TRAIN: iteration 33658 : loss : 0.082484, loss_ce: 0.007124, loss_dice: 0.157843
[13:16:28.070] TRAIN: iteration 33659 : loss : 0.069471, loss_ce: 0.002623, loss_dice: 0.136319
[13:16:28.285] TRAIN: iteration 33660 : loss : 0.055837, loss_ce: 0.004027, loss_dice: 0.107648
[13:16:28.522] TRAIN: iteration 33661 : loss : 0.250176, loss_ce: 0.000348, loss_dice: 0.500004
[13:16:28.740] TRAIN: iteration 33662 : loss : 0.028854, loss_ce: 0.000559, loss_dice: 0.057149
[13:16:28.954] TRAIN: iteration 33663 : loss : 0.180121, loss_ce: 0.009930, loss_dice: 0.350312
[13:16:29.164] TRAIN: iteration 33664 : loss : 0.216991, loss_ce: 0.004946, loss_dice: 0.429037
[13:16:29.380] TRAIN: iteration 33665 : loss : 0.024214, loss_ce: 0.002923, loss_dice: 0.045505
[13:16:29.598] TRAIN: iteration 33666 : loss : 0.240701, loss_ce: 0.001194, loss_dice: 0.480208
[13:16:29.806] TRAIN: iteration 33667 : loss : 0.052598, loss_ce: 0.001617, loss_dice: 0.103578
[13:16:30.013] TRAIN: iteration 33668 : loss : 0.099398, loss_ce: 0.001750, loss_dice: 0.197047
[13:16:30.220] TRAIN: iteration 33669 : loss : 0.048623, loss_ce: 0.001321, loss_dice: 0.095924
[13:16:30.435] TRAIN: iteration 33670 : loss : 0.044253, loss_ce: 0.002637, loss_dice: 0.085868
[13:16:30.643] TRAIN: iteration 33671 : loss : 0.249954, loss_ce: 0.001563, loss_dice: 0.498345
[13:16:30.859] TRAIN: iteration 33672 : loss : 0.053841, loss_ce: 0.002640, loss_dice: 0.105042
[13:16:31.069] TRAIN: iteration 33673 : loss : 0.116209, loss_ce: 0.006369, loss_dice: 0.226048
[13:16:31.279] TRAIN: iteration 33674 : loss : 0.108540, loss_ce: 0.003514, loss_dice: 0.213567
[13:16:31.487] TRAIN: iteration 33675 : loss : 0.051551, loss_ce: 0.005054, loss_dice: 0.098047
[13:16:31.696] TRAIN: iteration 33676 : loss : 0.250278, loss_ce: 0.000544, loss_dice: 0.500011
[13:16:31.905] TRAIN: iteration 33677 : loss : 0.043053, loss_ce: 0.001065, loss_dice: 0.085041
[13:16:32.113] TRAIN: iteration 33678 : loss : 0.023925, loss_ce: 0.000387, loss_dice: 0.047462
[13:16:32.320] TRAIN: iteration 33679 : loss : 0.054981, loss_ce: 0.002028, loss_dice: 0.107934
[13:16:32.529] TRAIN: iteration 33680 : loss : 0.060194, loss_ce: 0.009791, loss_dice: 0.110598
[13:16:32.776] TRAIN: iteration 33681 : loss : 0.113978, loss_ce: 0.007342, loss_dice: 0.220614
[13:16:32.984] TRAIN: iteration 33682 : loss : 0.128990, loss_ce: 0.003402, loss_dice: 0.254579
[13:16:33.195] TRAIN: iteration 33683 : loss : 0.067673, loss_ce: 0.002054, loss_dice: 0.133292
[13:16:33.409] TRAIN: iteration 33684 : loss : 0.091632, loss_ce: 0.002909, loss_dice: 0.180356
[13:16:33.616] TRAIN: iteration 33685 : loss : 0.066105, loss_ce: 0.000965, loss_dice: 0.131244
[13:16:33.823] TRAIN: iteration 33686 : loss : 0.250511, loss_ce: 0.000990, loss_dice: 0.500032
[13:16:34.033] TRAIN: iteration 33687 : loss : 0.037584, loss_ce: 0.001922, loss_dice: 0.073247
[13:16:34.244] TRAIN: iteration 33688 : loss : 0.047764, loss_ce: 0.001038, loss_dice: 0.094489
[13:16:34.454] TRAIN: iteration 33689 : loss : 0.118303, loss_ce: 0.001575, loss_dice: 0.235031
[13:16:34.663] TRAIN: iteration 33690 : loss : 0.081688, loss_ce: 0.004785, loss_dice: 0.158591
[13:16:34.871] TRAIN: iteration 33691 : loss : 0.158472, loss_ce: 0.002854, loss_dice: 0.314091
[13:16:35.079] TRAIN: iteration 33692 : loss : 0.250282, loss_ce: 0.000554, loss_dice: 0.500009
[13:16:35.286] TRAIN: iteration 33693 : loss : 0.048438, loss_ce: 0.000657, loss_dice: 0.096218
[13:16:35.678] TRAIN: iteration 33694 : loss : 0.036190, loss_ce: 0.006992, loss_dice: 0.065388
[13:16:36.463] TRAIN: iteration 33695 : loss : 0.050739, loss_ce: 0.002150, loss_dice: 0.099329
[13:16:36.670] TRAIN: iteration 33696 : loss : 0.056823, loss_ce: 0.005752, loss_dice: 0.107895
[13:16:36.877] TRAIN: iteration 33697 : loss : 0.051498, loss_ce: 0.003554, loss_dice: 0.099443
[13:16:37.085] TRAIN: iteration 33698 : loss : 0.073148, loss_ce: 0.001489, loss_dice: 0.144807
[13:16:37.300] TRAIN: iteration 33699 : loss : 0.105942, loss_ce: 0.002075, loss_dice: 0.209809
[13:16:37.508] TRAIN: iteration 33700 : loss : 0.244495, loss_ce: 0.004359, loss_dice: 0.484632
[13:16:37.744] TRAIN: iteration 33701 : loss : 0.033879, loss_ce: 0.000463, loss_dice: 0.067295
[13:16:37.963] TRAIN: iteration 33702 : loss : 0.250750, loss_ce: 0.001412, loss_dice: 0.500088
[13:16:38.172] TRAIN: iteration 33703 : loss : 0.044365, loss_ce: 0.003077, loss_dice: 0.085653
[13:16:38.420] TRAIN: iteration 33704 : loss : 0.072222, loss_ce: 0.002490, loss_dice: 0.141954
[13:16:38.627] TRAIN: iteration 33705 : loss : 0.204639, loss_ce: 0.001040, loss_dice: 0.408238
[13:16:38.844] TRAIN: iteration 33706 : loss : 0.150252, loss_ce: 0.021932, loss_dice: 0.278572
[13:16:39.057] TRAIN: iteration 33707 : loss : 0.049634, loss_ce: 0.006471, loss_dice: 0.092796
[13:16:39.275] TRAIN: iteration 33708 : loss : 0.079967, loss_ce: 0.001773, loss_dice: 0.158160
[13:16:39.485] TRAIN: iteration 33709 : loss : 0.050881, loss_ce: 0.001158, loss_dice: 0.100604
[13:16:39.705] TRAIN: iteration 33710 : loss : 0.104837, loss_ce: 0.002444, loss_dice: 0.207229
[13:16:39.914] TRAIN: iteration 33711 : loss : 0.124171, loss_ce: 0.000980, loss_dice: 0.247362
[13:16:40.125] TRAIN: iteration 33712 : loss : 0.073194, loss_ce: 0.006508, loss_dice: 0.139880
[13:16:40.333] TRAIN: iteration 33713 : loss : 0.042678, loss_ce: 0.001735, loss_dice: 0.083621
[13:16:40.543] TRAIN: iteration 33714 : loss : 0.077328, loss_ce: 0.001051, loss_dice: 0.153605
[13:16:40.758] TRAIN: iteration 33715 : loss : 0.044718, loss_ce: 0.004190, loss_dice: 0.085247
[13:16:40.969] TRAIN: iteration 33716 : loss : 0.201156, loss_ce: 0.001492, loss_dice: 0.400820
[13:16:41.177] TRAIN: iteration 33717 : loss : 0.241415, loss_ce: 0.000674, loss_dice: 0.482157
[13:16:41.384] TRAIN: iteration 33718 : loss : 0.230132, loss_ce: 0.010756, loss_dice: 0.449508
[13:16:41.596] TRAIN: iteration 33719 : loss : 0.250295, loss_ce: 0.000577, loss_dice: 0.500012
[13:16:41.808] TRAIN: iteration 33720 : loss : 0.250537, loss_ce: 0.001026, loss_dice: 0.500048
[13:16:42.054] TRAIN: iteration 33721 : loss : 0.057943, loss_ce: 0.002639, loss_dice: 0.113247
[13:16:42.266] TRAIN: iteration 33722 : loss : 0.095636, loss_ce: 0.002316, loss_dice: 0.188957
[13:16:42.475] TRAIN: iteration 33723 : loss : 0.250248, loss_ce: 0.000487, loss_dice: 0.500009
[13:16:42.724] TRAIN: iteration 33724 : loss : 0.111726, loss_ce: 0.004652, loss_dice: 0.218801
[13:16:42.933] TRAIN: iteration 33725 : loss : 0.235270, loss_ce: 0.003764, loss_dice: 0.466775
[13:16:43.141] TRAIN: iteration 33726 : loss : 0.055194, loss_ce: 0.001978, loss_dice: 0.108410
[13:16:43.350] TRAIN: iteration 33727 : loss : 0.250273, loss_ce: 0.000538, loss_dice: 0.500008
[13:16:43.559] TRAIN: iteration 33728 : loss : 0.033632, loss_ce: 0.000529, loss_dice: 0.066735
[13:16:43.770] TRAIN: iteration 33729 : loss : 0.192443, loss_ce: 0.002143, loss_dice: 0.382742
[13:16:43.977] TRAIN: iteration 33730 : loss : 0.250356, loss_ce: 0.000679, loss_dice: 0.500033
[13:16:44.192] TRAIN: iteration 33731 : loss : 0.067347, loss_ce: 0.001802, loss_dice: 0.132892
[13:16:44.403] TRAIN: iteration 33732 : loss : 0.130697, loss_ce: 0.002404, loss_dice: 0.258990
[13:16:44.614] TRAIN: iteration 33733 : loss : 0.054322, loss_ce: 0.002006, loss_dice: 0.106638
[13:16:44.822] TRAIN: iteration 33734 : loss : 0.086824, loss_ce: 0.003062, loss_dice: 0.170585
[13:16:45.031] TRAIN: iteration 33735 : loss : 0.114804, loss_ce: 0.003535, loss_dice: 0.226073
[13:16:45.238] TRAIN: iteration 33736 : loss : 0.064153, loss_ce: 0.005419, loss_dice: 0.122888
[13:16:45.445] TRAIN: iteration 33737 : loss : 0.107543, loss_ce: 0.007926, loss_dice: 0.207160
[13:16:45.653] TRAIN: iteration 33738 : loss : 0.151949, loss_ce: 0.002979, loss_dice: 0.300919
[13:16:45.860] TRAIN: iteration 33739 : loss : 0.061407, loss_ce: 0.002223, loss_dice: 0.120590
[13:16:46.068] TRAIN: iteration 33740 : loss : 0.075415, loss_ce: 0.000852, loss_dice: 0.149978
[13:16:46.306] TRAIN: iteration 33741 : loss : 0.043351, loss_ce: 0.000659, loss_dice: 0.086042
[13:16:46.518] TRAIN: iteration 33742 : loss : 0.133380, loss_ce: 0.002987, loss_dice: 0.263773
[13:16:46.734] TRAIN: iteration 33743 : loss : 0.036067, loss_ce: 0.002302, loss_dice: 0.069832
[13:16:46.949] TRAIN: iteration 33744 : loss : 0.114514, loss_ce: 0.004383, loss_dice: 0.224646
[13:16:47.168] TRAIN: iteration 33745 : loss : 0.035704, loss_ce: 0.004177, loss_dice: 0.067230
[13:16:47.377] TRAIN: iteration 33746 : loss : 0.078823, loss_ce: 0.000943, loss_dice: 0.156702
[13:16:47.585] TRAIN: iteration 33747 : loss : 0.031752, loss_ce: 0.000739, loss_dice: 0.062766
[13:16:48.447] TRAIN: iteration 33748 : loss : 0.062039, loss_ce: 0.003191, loss_dice: 0.120887
[13:16:48.653] TRAIN: iteration 33749 : loss : 0.137953, loss_ce: 0.001586, loss_dice: 0.274320
[13:16:48.861] TRAIN: iteration 33750 : loss : 0.213951, loss_ce: 0.008473, loss_dice: 0.419430
[13:16:49.069] TRAIN: iteration 33751 : loss : 0.098646, loss_ce: 0.003153, loss_dice: 0.194139
[13:16:49.277] TRAIN: iteration 33752 : loss : 0.250290, loss_ce: 0.000564, loss_dice: 0.500015
[13:16:49.486] TRAIN: iteration 33753 : loss : 0.090546, loss_ce: 0.003219, loss_dice: 0.177873
[13:16:49.694] TRAIN: iteration 33754 : loss : 0.250343, loss_ce: 0.000655, loss_dice: 0.500030
[13:16:49.910] TRAIN: iteration 33755 : loss : 0.224885, loss_ce: 0.000991, loss_dice: 0.448780
[13:16:50.138] TRAIN: iteration 33756 : loss : 0.080072, loss_ce: 0.001569, loss_dice: 0.158575
[13:16:50.345] TRAIN: iteration 33757 : loss : 0.119183, loss_ce: 0.000539, loss_dice: 0.237827
[13:16:50.553] TRAIN: iteration 33758 : loss : 0.119163, loss_ce: 0.003356, loss_dice: 0.234969
[13:16:50.762] TRAIN: iteration 33759 : loss : 0.031354, loss_ce: 0.002480, loss_dice: 0.060228
[13:16:50.970] TRAIN: iteration 33760 : loss : 0.023665, loss_ce: 0.004305, loss_dice: 0.043025
[13:16:51.207] TRAIN: iteration 33761 : loss : 0.094676, loss_ce: 0.013843, loss_dice: 0.175508
[13:16:51.421] TRAIN: iteration 33762 : loss : 0.184439, loss_ce: 0.002973, loss_dice: 0.365905
[13:16:51.628] TRAIN: iteration 33763 : loss : 0.250628, loss_ce: 0.001183, loss_dice: 0.500073
[13:16:51.835] TRAIN: iteration 33764 : loss : 0.024478, loss_ce: 0.002526, loss_dice: 0.046431
[13:16:52.043] TRAIN: iteration 33765 : loss : 0.096228, loss_ce: 0.001922, loss_dice: 0.190534
[13:16:52.251] TRAIN: iteration 33766 : loss : 0.034920, loss_ce: 0.001100, loss_dice: 0.068741
[13:16:52.461] TRAIN: iteration 33767 : loss : 0.070364, loss_ce: 0.000839, loss_dice: 0.139889
[13:16:52.675] TRAIN: iteration 33768 : loss : 0.064107, loss_ce: 0.002325, loss_dice: 0.125888
[13:16:52.885] TRAIN: iteration 33769 : loss : 0.025269, loss_ce: 0.001085, loss_dice: 0.049453
[13:16:53.101] TRAIN: iteration 33770 : loss : 0.042492, loss_ce: 0.001939, loss_dice: 0.083044
[13:16:53.309] TRAIN: iteration 33771 : loss : 0.065250, loss_ce: 0.003441, loss_dice: 0.127059
[13:16:53.519] TRAIN: iteration 33772 : loss : 0.250536, loss_ce: 0.001025, loss_dice: 0.500047
[13:16:53.731] TRAIN: iteration 33773 : loss : 0.068885, loss_ce: 0.002110, loss_dice: 0.135659
[13:16:53.946] TRAIN: iteration 33774 : loss : 0.162087, loss_ce: 0.001852, loss_dice: 0.322322
[13:16:54.154] TRAIN: iteration 33775 : loss : 0.062227, loss_ce: 0.007362, loss_dice: 0.117092
[13:16:54.363] TRAIN: iteration 33776 : loss : 0.042762, loss_ce: 0.005077, loss_dice: 0.080446
[13:16:58.546] TRAIN: iteration 33777 : loss : 0.091360, loss_ce: 0.006170, loss_dice: 0.176550
[13:16:58.754] TRAIN: iteration 33778 : loss : 0.033110, loss_ce: 0.003441, loss_dice: 0.062779
[13:16:58.966] TRAIN: iteration 33779 : loss : 0.250845, loss_ce: 0.001588, loss_dice: 0.500102
[13:16:59.173] TRAIN: iteration 33780 : loss : 0.108234, loss_ce: 0.001887, loss_dice: 0.214582
[13:16:59.411] TRAIN: iteration 33781 : loss : 0.088899, loss_ce: 0.002198, loss_dice: 0.175600
[13:16:59.618] TRAIN: iteration 33782 : loss : 0.034411, loss_ce: 0.003070, loss_dice: 0.065752
[13:16:59.826] TRAIN: iteration 33783 : loss : 0.074825, loss_ce: 0.001351, loss_dice: 0.148299
[13:17:00.034] TRAIN: iteration 33784 : loss : 0.123823, loss_ce: 0.001067, loss_dice: 0.246580
[13:17:00.317] TRAIN: iteration 33785 : loss : 0.240016, loss_ce: 0.001214, loss_dice: 0.478818
[13:17:00.524] TRAIN: iteration 33786 : loss : 0.029880, loss_ce: 0.003255, loss_dice: 0.056504
[13:17:00.735] TRAIN: iteration 33787 : loss : 0.084237, loss_ce: 0.004352, loss_dice: 0.164122
[13:17:00.945] TRAIN: iteration 33788 : loss : 0.098310, loss_ce: 0.003256, loss_dice: 0.193364
[13:17:01.159] TRAIN: iteration 33789 : loss : 0.044046, loss_ce: 0.002643, loss_dice: 0.085450
[13:17:01.367] TRAIN: iteration 33790 : loss : 0.250428, loss_ce: 0.000823, loss_dice: 0.500033
[13:17:01.580] TRAIN: iteration 33791 : loss : 0.249280, loss_ce: 0.001741, loss_dice: 0.496819
[13:17:01.794] TRAIN: iteration 33792 : loss : 0.009103, loss_ce: 0.000855, loss_dice: 0.017352
[13:17:02.003] TRAIN: iteration 33793 : loss : 0.249673, loss_ce: 0.001242, loss_dice: 0.498105
[13:17:02.356] TRAIN: iteration 33794 : loss : 0.110731, loss_ce: 0.007917, loss_dice: 0.213545
[13:17:03.284] TRAIN: iteration 33795 : loss : 0.071042, loss_ce: 0.005575, loss_dice: 0.136510
[13:17:03.491] TRAIN: iteration 33796 : loss : 0.195683, loss_ce: 0.007421, loss_dice: 0.383946
[13:17:03.705] TRAIN: iteration 33797 : loss : 0.037280, loss_ce: 0.006057, loss_dice: 0.068503
[13:17:03.913] TRAIN: iteration 33798 : loss : 0.103671, loss_ce: 0.004735, loss_dice: 0.202608
[13:17:04.187] TRAIN: iteration 33799 : loss : 0.016111, loss_ce: 0.000872, loss_dice: 0.031351
[13:17:04.401] TRAIN: iteration 33800 : loss : 0.016756, loss_ce: 0.000549, loss_dice: 0.032962
[13:17:04.634] TRAIN: iteration 33801 : loss : 0.059996, loss_ce: 0.000992, loss_dice: 0.119000
[13:17:04.842] TRAIN: iteration 33802 : loss : 0.044657, loss_ce: 0.003063, loss_dice: 0.086251
[13:17:05.053] TRAIN: iteration 33803 : loss : 0.060321, loss_ce: 0.002790, loss_dice: 0.117852
[13:17:05.283] TRAIN: iteration 33804 : loss : 0.100637, loss_ce: 0.003239, loss_dice: 0.198036
[13:17:05.495] TRAIN: iteration 33805 : loss : 0.034482, loss_ce: 0.006565, loss_dice: 0.062399
[13:17:05.703] TRAIN: iteration 33806 : loss : 0.189548, loss_ce: 0.001035, loss_dice: 0.378060
[13:17:05.910] TRAIN: iteration 33807 : loss : 0.057513, loss_ce: 0.002348, loss_dice: 0.112677
[13:17:06.120] TRAIN: iteration 33808 : loss : 0.106957, loss_ce: 0.004181, loss_dice: 0.209734
[13:17:06.326] TRAIN: iteration 33809 : loss : 0.071501, loss_ce: 0.003773, loss_dice: 0.139229
[13:17:06.534] TRAIN: iteration 33810 : loss : 0.111023, loss_ce: 0.002737, loss_dice: 0.219309
[13:17:06.748] TRAIN: iteration 33811 : loss : 0.103101, loss_ce: 0.005445, loss_dice: 0.200758
[13:17:06.958] TRAIN: iteration 33812 : loss : 0.060817, loss_ce: 0.001624, loss_dice: 0.120010
[13:17:07.169] TRAIN: iteration 33813 : loss : 0.108361, loss_ce: 0.001042, loss_dice: 0.215679
[13:17:07.377] TRAIN: iteration 33814 : loss : 0.109670, loss_ce: 0.002903, loss_dice: 0.216437
[13:17:07.585] TRAIN: iteration 33815 : loss : 0.144819, loss_ce: 0.030134, loss_dice: 0.259504
[13:17:07.796] TRAIN: iteration 33816 : loss : 0.038201, loss_ce: 0.003878, loss_dice: 0.072524
[13:17:08.007] TRAIN: iteration 33817 : loss : 0.041352, loss_ce: 0.003203, loss_dice: 0.079500
[13:17:08.219] TRAIN: iteration 33818 : loss : 0.049649, loss_ce: 0.002176, loss_dice: 0.097121
[13:17:08.426] TRAIN: iteration 33819 : loss : 0.250199, loss_ce: 0.000392, loss_dice: 0.500006
[13:17:08.640] TRAIN: iteration 33820 : loss : 0.069330, loss_ce: 0.004349, loss_dice: 0.134312
[13:17:08.869] TRAIN: iteration 33821 : loss : 0.061751, loss_ce: 0.001405, loss_dice: 0.122098
[13:17:09.079] TRAIN: iteration 33822 : loss : 0.068222, loss_ce: 0.007431, loss_dice: 0.129013
[13:17:09.301] TRAIN: iteration 33823 : loss : 0.250639, loss_ce: 0.001206, loss_dice: 0.500073
[13:17:09.517] TRAIN: iteration 33824 : loss : 0.062932, loss_ce: 0.006728, loss_dice: 0.119137
[13:17:09.725] TRAIN: iteration 33825 : loss : 0.033456, loss_ce: 0.002501, loss_dice: 0.064411
[13:17:09.933] TRAIN: iteration 33826 : loss : 0.089903, loss_ce: 0.019533, loss_dice: 0.160273
[13:17:10.145] TRAIN: iteration 33827 : loss : 0.031150, loss_ce: 0.004647, loss_dice: 0.057653
[13:17:10.357] TRAIN: iteration 33828 : loss : 0.250342, loss_ce: 0.000652, loss_dice: 0.500033
[13:17:10.571] TRAIN: iteration 33829 : loss : 0.143151, loss_ce: 0.000612, loss_dice: 0.285690
[13:17:10.782] TRAIN: iteration 33830 : loss : 0.039655, loss_ce: 0.001451, loss_dice: 0.077859
[13:17:10.991] TRAIN: iteration 33831 : loss : 0.092631, loss_ce: 0.008324, loss_dice: 0.176939
[13:17:11.214] TRAIN: iteration 33832 : loss : 0.234873, loss_ce: 0.002175, loss_dice: 0.467572
[13:17:11.424] TRAIN: iteration 33833 : loss : 0.045493, loss_ce: 0.002142, loss_dice: 0.088843
[13:17:11.741] TRAIN: iteration 33834 : loss : 0.125717, loss_ce: 0.003501, loss_dice: 0.247933
[13:17:11.948] TRAIN: iteration 33835 : loss : 0.040691, loss_ce: 0.000661, loss_dice: 0.080722
[13:17:12.158] TRAIN: iteration 33836 : loss : 0.072127, loss_ce: 0.000718, loss_dice: 0.143536
[13:17:12.366] TRAIN: iteration 33837 : loss : 0.250520, loss_ce: 0.000990, loss_dice: 0.500051
[13:17:12.573] TRAIN: iteration 33838 : loss : 0.053529, loss_ce: 0.002956, loss_dice: 0.104102
[13:17:12.780] TRAIN: iteration 33839 : loss : 0.039060, loss_ce: 0.000524, loss_dice: 0.077595
[13:17:13.747] TRAIN: iteration 33840 : loss : 0.022661, loss_ce: 0.001728, loss_dice: 0.043593
[13:17:13.983] TRAIN: iteration 33841 : loss : 0.014599, loss_ce: 0.000862, loss_dice: 0.028336
[13:17:14.467] TRAIN: iteration 33842 : loss : 0.051594, loss_ce: 0.002387, loss_dice: 0.100800
[13:17:14.679] TRAIN: iteration 33843 : loss : 0.035976, loss_ce: 0.003114, loss_dice: 0.068839
[13:17:14.889] TRAIN: iteration 33844 : loss : 0.250277, loss_ce: 0.000545, loss_dice: 0.500010
[13:17:15.102] TRAIN: iteration 33845 : loss : 0.033381, loss_ce: 0.003765, loss_dice: 0.062998
[13:17:15.311] TRAIN: iteration 33846 : loss : 0.048855, loss_ce: 0.002382, loss_dice: 0.095329
[13:17:15.519] TRAIN: iteration 33847 : loss : 0.021953, loss_ce: 0.000985, loss_dice: 0.042922
[13:17:16.485] TRAIN: iteration 33848 : loss : 0.250710, loss_ce: 0.001354, loss_dice: 0.500066
[13:17:16.697] TRAIN: iteration 33849 : loss : 0.097774, loss_ce: 0.002568, loss_dice: 0.192980
[13:17:16.905] TRAIN: iteration 33850 : loss : 0.025009, loss_ce: 0.001114, loss_dice: 0.048903
[13:17:17.115] TRAIN: iteration 33851 : loss : 0.035800, loss_ce: 0.001906, loss_dice: 0.069694
[13:17:17.322] TRAIN: iteration 33852 : loss : 0.070746, loss_ce: 0.000711, loss_dice: 0.140782
[13:17:17.530] TRAIN: iteration 33853 : loss : 0.046343, loss_ce: 0.001756, loss_dice: 0.090930
[13:17:17.738] TRAIN: iteration 33854 : loss : 0.082543, loss_ce: 0.003082, loss_dice: 0.162004
[13:17:17.946] TRAIN: iteration 33855 : loss : 0.207819, loss_ce: 0.003196, loss_dice: 0.412443
[13:17:18.154] TRAIN: iteration 33856 : loss : 0.027673, loss_ce: 0.002363, loss_dice: 0.052983
[13:17:18.361] TRAIN: iteration 33857 : loss : 0.250250, loss_ce: 0.000492, loss_dice: 0.500009
[13:17:18.569] TRAIN: iteration 33858 : loss : 0.071863, loss_ce: 0.004918, loss_dice: 0.138807
[13:17:18.776] TRAIN: iteration 33859 : loss : 0.037672, loss_ce: 0.001919, loss_dice: 0.073426
[13:17:18.983] TRAIN: iteration 33860 : loss : 0.049061, loss_ce: 0.001432, loss_dice: 0.096690
[13:17:19.219] TRAIN: iteration 33861 : loss : 0.216473, loss_ce: 0.000484, loss_dice: 0.432463
[13:17:19.521] TRAIN: iteration 33862 : loss : 0.087774, loss_ce: 0.005977, loss_dice: 0.169572
[13:17:19.729] TRAIN: iteration 33863 : loss : 0.250703, loss_ce: 0.001323, loss_dice: 0.500084
[13:17:20.917] TRAIN: iteration 33864 : loss : 0.039647, loss_ce: 0.000951, loss_dice: 0.078343
[13:17:21.126] TRAIN: iteration 33865 : loss : 0.034664, loss_ce: 0.001498, loss_dice: 0.067829
[13:17:21.342] TRAIN: iteration 33866 : loss : 0.057967, loss_ce: 0.000775, loss_dice: 0.115160
[13:17:21.550] TRAIN: iteration 33867 : loss : 0.017265, loss_ce: 0.001027, loss_dice: 0.033504
[13:17:21.758] TRAIN: iteration 33868 : loss : 0.084795, loss_ce: 0.001680, loss_dice: 0.167911
[13:17:21.971] TRAIN: iteration 33869 : loss : 0.250426, loss_ce: 0.000816, loss_dice: 0.500037
[13:17:22.179] TRAIN: iteration 33870 : loss : 0.127876, loss_ce: 0.008333, loss_dice: 0.247419
[13:17:22.386] TRAIN: iteration 33871 : loss : 0.020114, loss_ce: 0.002536, loss_dice: 0.037692
[13:17:23.276] TRAIN: iteration 33872 : loss : 0.054659, loss_ce: 0.008581, loss_dice: 0.100737
[13:17:23.483] TRAIN: iteration 33873 : loss : 0.063443, loss_ce: 0.003209, loss_dice: 0.123677
[13:17:23.691] TRAIN: iteration 33874 : loss : 0.055449, loss_ce: 0.001732, loss_dice: 0.109166
[13:17:23.897] TRAIN: iteration 33875 : loss : 0.099150, loss_ce: 0.002044, loss_dice: 0.196255
[13:17:24.107] TRAIN: iteration 33876 : loss : 0.061877, loss_ce: 0.001998, loss_dice: 0.121756
[13:17:24.318] TRAIN: iteration 33877 : loss : 0.056830, loss_ce: 0.002314, loss_dice: 0.111347
[13:17:24.527] TRAIN: iteration 33878 : loss : 0.019804, loss_ce: 0.000384, loss_dice: 0.039225
[13:17:24.743] TRAIN: iteration 33879 : loss : 0.090648, loss_ce: 0.001871, loss_dice: 0.179425
[13:17:24.951] TRAIN: iteration 33880 : loss : 0.060559, loss_ce: 0.007652, loss_dice: 0.113466
[13:17:25.190] TRAIN: iteration 33881 : loss : 0.245395, loss_ce: 0.003369, loss_dice: 0.487421
[13:17:25.397] TRAIN: iteration 33882 : loss : 0.036376, loss_ce: 0.001367, loss_dice: 0.071385
[13:17:25.605] TRAIN: iteration 33883 : loss : 0.055622, loss_ce: 0.003814, loss_dice: 0.107429
[13:17:25.814] TRAIN: iteration 33884 : loss : 0.048651, loss_ce: 0.000904, loss_dice: 0.096399
[13:17:26.022] TRAIN: iteration 33885 : loss : 0.130462, loss_ce: 0.007941, loss_dice: 0.252983
[13:17:26.237] TRAIN: iteration 33886 : loss : 0.097184, loss_ce: 0.001714, loss_dice: 0.192653
[13:17:26.445] TRAIN: iteration 33887 : loss : 0.050862, loss_ce: 0.003183, loss_dice: 0.098541
[13:17:26.796] TRAIN: iteration 33888 : loss : 0.028653, loss_ce: 0.002737, loss_dice: 0.054569
[13:17:27.004] TRAIN: iteration 33889 : loss : 0.113714, loss_ce: 0.003618, loss_dice: 0.223810
[13:17:27.212] TRAIN: iteration 33890 : loss : 0.032033, loss_ce: 0.002030, loss_dice: 0.062036
[13:17:27.421] TRAIN: iteration 33891 : loss : 0.037689, loss_ce: 0.003586, loss_dice: 0.071793
[13:17:27.628] TRAIN: iteration 33892 : loss : 0.250792, loss_ce: 0.001479, loss_dice: 0.500104
[13:17:27.835] TRAIN: iteration 33893 : loss : 0.127274, loss_ce: 0.004834, loss_dice: 0.249714
[13:17:28.046] TRAIN: iteration 33894 : loss : 0.136662, loss_ce: 0.003248, loss_dice: 0.270076
[13:17:28.255] TRAIN: iteration 33895 : loss : 0.248724, loss_ce: 0.000358, loss_dice: 0.497090
[13:17:28.466] TRAIN: iteration 33896 : loss : 0.048614, loss_ce: 0.004850, loss_dice: 0.092378
[13:17:29.156] TRAIN: iteration 33897 : loss : 0.079433, loss_ce: 0.001358, loss_dice: 0.157509
[13:17:29.364] TRAIN: iteration 33898 : loss : 0.036270, loss_ce: 0.002886, loss_dice: 0.069655
[13:17:29.571] TRAIN: iteration 33899 : loss : 0.170563, loss_ce: 0.002868, loss_dice: 0.338257
[13:17:29.786] TRAIN: iteration 33900 : loss : 0.250424, loss_ce: 0.000812, loss_dice: 0.500036
[13:17:30.035] TRAIN: iteration 33901 : loss : 0.088181, loss_ce: 0.003375, loss_dice: 0.172986
[13:17:30.243] TRAIN: iteration 33902 : loss : 0.030864, loss_ce: 0.002559, loss_dice: 0.059170
[13:17:30.450] TRAIN: iteration 33903 : loss : 0.080090, loss_ce: 0.001150, loss_dice: 0.159030
[13:17:30.659] TRAIN: iteration 33904 : loss : 0.092594, loss_ce: 0.002767, loss_dice: 0.182421
[13:17:30.869] TRAIN: iteration 33905 : loss : 0.239262, loss_ce: 0.000662, loss_dice: 0.477862
[13:17:31.081] TRAIN: iteration 33906 : loss : 0.022562, loss_ce: 0.002336, loss_dice: 0.042789
[13:17:31.287] TRAIN: iteration 33907 : loss : 0.056955, loss_ce: 0.004213, loss_dice: 0.109698
[13:17:31.495] TRAIN: iteration 33908 : loss : 0.250466, loss_ce: 0.000886, loss_dice: 0.500045
[13:17:31.703] TRAIN: iteration 33909 : loss : 0.080036, loss_ce: 0.003359, loss_dice: 0.156714
[13:17:31.913] TRAIN: iteration 33910 : loss : 0.076640, loss_ce: 0.002510, loss_dice: 0.150770
[13:17:32.465] TRAIN: iteration 33911 : loss : 0.101929, loss_ce: 0.003858, loss_dice: 0.200000
[13:17:32.679] TRAIN: iteration 33912 : loss : 0.171495, loss_ce: 0.002395, loss_dice: 0.340594
[13:17:32.887] TRAIN: iteration 33913 : loss : 0.250276, loss_ce: 0.000536, loss_dice: 0.500017
[13:17:33.094] TRAIN: iteration 33914 : loss : 0.088007, loss_ce: 0.001474, loss_dice: 0.174541
[13:17:33.303] TRAIN: iteration 33915 : loss : 0.050871, loss_ce: 0.002882, loss_dice: 0.098861
[13:17:33.510] TRAIN: iteration 33916 : loss : 0.111742, loss_ce: 0.002215, loss_dice: 0.221269
[13:17:33.720] TRAIN: iteration 33917 : loss : 0.157052, loss_ce: 0.007372, loss_dice: 0.306732
[13:17:33.927] TRAIN: iteration 33918 : loss : 0.024537, loss_ce: 0.000726, loss_dice: 0.048347
[13:17:34.766] TRAIN: iteration 33919 : loss : 0.080638, loss_ce: 0.001542, loss_dice: 0.159733
[13:17:34.972] TRAIN: iteration 33920 : loss : 0.145962, loss_ce: 0.001690, loss_dice: 0.290235
[13:17:35.212] TRAIN: iteration 33921 : loss : 0.051606, loss_ce: 0.009451, loss_dice: 0.093761
[13:17:35.423] TRAIN: iteration 33922 : loss : 0.015590, loss_ce: 0.000517, loss_dice: 0.030663
[13:17:35.631] TRAIN: iteration 33923 : loss : 0.069209, loss_ce: 0.001317, loss_dice: 0.137100
[13:17:35.839] TRAIN: iteration 33924 : loss : 0.181309, loss_ce: 0.003680, loss_dice: 0.358938
[13:17:36.047] TRAIN: iteration 33925 : loss : 0.043255, loss_ce: 0.002247, loss_dice: 0.084263
[13:17:36.255] TRAIN: iteration 33926 : loss : 0.070757, loss_ce: 0.006733, loss_dice: 0.134780
[13:17:36.465] TRAIN: iteration 33927 : loss : 0.089347, loss_ce: 0.004320, loss_dice: 0.174373
[13:17:36.673] TRAIN: iteration 33928 : loss : 0.250595, loss_ce: 0.001123, loss_dice: 0.500066
[13:17:37.004] TRAIN: iteration 33929 : loss : 0.033820, loss_ce: 0.004152, loss_dice: 0.063488
[13:17:37.219] TRAIN: iteration 33930 : loss : 0.067730, loss_ce: 0.006212, loss_dice: 0.129248
[13:17:37.427] TRAIN: iteration 33931 : loss : 0.066456, loss_ce: 0.001225, loss_dice: 0.131687
[13:17:37.635] TRAIN: iteration 33932 : loss : 0.250855, loss_ce: 0.001605, loss_dice: 0.500104
[13:17:37.852] TRAIN: iteration 33933 : loss : 0.077859, loss_ce: 0.004028, loss_dice: 0.151691
[13:17:38.060] TRAIN: iteration 33934 : loss : 0.027961, loss_ce: 0.001399, loss_dice: 0.054524
[13:17:38.269] TRAIN: iteration 33935 : loss : 0.062172, loss_ce: 0.007199, loss_dice: 0.117145
[13:17:38.515] TRAIN: iteration 33936 : loss : 0.051310, loss_ce: 0.004161, loss_dice: 0.098460
[13:17:39.716] TRAIN: iteration 33937 : loss : 0.032236, loss_ce: 0.003274, loss_dice: 0.061198
[13:17:39.924] TRAIN: iteration 33938 : loss : 0.022947, loss_ce: 0.000403, loss_dice: 0.045491
[13:17:40.133] TRAIN: iteration 33939 : loss : 0.030680, loss_ce: 0.001018, loss_dice: 0.060342
[13:17:40.343] TRAIN: iteration 33940 : loss : 0.121177, loss_ce: 0.002462, loss_dice: 0.239892
[13:17:40.579] TRAIN: iteration 33941 : loss : 0.122145, loss_ce: 0.000829, loss_dice: 0.243461
[13:17:40.787] TRAIN: iteration 33942 : loss : 0.181318, loss_ce: 0.003495, loss_dice: 0.359141
[13:17:40.997] TRAIN: iteration 33943 : loss : 0.250980, loss_ce: 0.001838, loss_dice: 0.500123
[13:17:41.205] TRAIN: iteration 33944 : loss : 0.184866, loss_ce: 0.001035, loss_dice: 0.368696
[13:17:41.573] TRAIN: iteration 33945 : loss : 0.086231, loss_ce: 0.001310, loss_dice: 0.171153
[13:17:41.781] TRAIN: iteration 33946 : loss : 0.023222, loss_ce: 0.001934, loss_dice: 0.044510
[13:17:41.988] TRAIN: iteration 33947 : loss : 0.129445, loss_ce: 0.002135, loss_dice: 0.256755
[13:17:42.195] TRAIN: iteration 33948 : loss : 0.087485, loss_ce: 0.002948, loss_dice: 0.172022
[13:17:42.412] TRAIN: iteration 33949 : loss : 0.036938, loss_ce: 0.005507, loss_dice: 0.068368
[13:17:42.621] TRAIN: iteration 33950 : loss : 0.025206, loss_ce: 0.000878, loss_dice: 0.049535
[13:17:43.328] TRAIN: iteration 33951 : loss : 0.057855, loss_ce: 0.005410, loss_dice: 0.110300
[13:17:43.537] TRAIN: iteration 33952 : loss : 0.045672, loss_ce: 0.002973, loss_dice: 0.088371
[13:17:47.467] TRAIN: iteration 33953 : loss : 0.025883, loss_ce: 0.001586, loss_dice: 0.050179
[13:17:47.673] TRAIN: iteration 33954 : loss : 0.251620, loss_ce: 0.003025, loss_dice: 0.500215
[13:17:47.887] TRAIN: iteration 33955 : loss : 0.031102, loss_ce: 0.003061, loss_dice: 0.059143
[13:17:48.094] TRAIN: iteration 33956 : loss : 0.150385, loss_ce: 0.004967, loss_dice: 0.295802
[13:17:48.302] TRAIN: iteration 33957 : loss : 0.113034, loss_ce: 0.006660, loss_dice: 0.219409
[13:17:48.509] TRAIN: iteration 33958 : loss : 0.133388, loss_ce: 0.002235, loss_dice: 0.264542
[13:17:48.716] TRAIN: iteration 33959 : loss : 0.250211, loss_ce: 0.000413, loss_dice: 0.500008
[13:17:48.924] TRAIN: iteration 33960 : loss : 0.251027, loss_ce: 0.001919, loss_dice: 0.500136
[13:17:49.721] TRAIN: iteration 33961 : loss : 0.052238, loss_ce: 0.003223, loss_dice: 0.101252
[13:17:49.929] TRAIN: iteration 33962 : loss : 0.027256, loss_ce: 0.005081, loss_dice: 0.049431
[13:17:50.137] TRAIN: iteration 33963 : loss : 0.056151, loss_ce: 0.004322, loss_dice: 0.107980
[13:17:50.344] TRAIN: iteration 33964 : loss : 0.103055, loss_ce: 0.005798, loss_dice: 0.200312
[13:17:50.552] TRAIN: iteration 33965 : loss : 0.044709, loss_ce: 0.002011, loss_dice: 0.087408
[13:17:50.761] TRAIN: iteration 33966 : loss : 0.043638, loss_ce: 0.005028, loss_dice: 0.082249
[13:17:50.968] TRAIN: iteration 33967 : loss : 0.073539, loss_ce: 0.006059, loss_dice: 0.141018
[13:17:51.176] TRAIN: iteration 33968 : loss : 0.041904, loss_ce: 0.006546, loss_dice: 0.077262
[13:17:51.383] TRAIN: iteration 33969 : loss : 0.057553, loss_ce: 0.019065, loss_dice: 0.096040
[13:17:51.590] TRAIN: iteration 33970 : loss : 0.062032, loss_ce: 0.002847, loss_dice: 0.121217
[13:17:51.800] TRAIN: iteration 33971 : loss : 0.039600, loss_ce: 0.002674, loss_dice: 0.076525
[13:17:52.011] TRAIN: iteration 33972 : loss : 0.080851, loss_ce: 0.001282, loss_dice: 0.160421
[13:17:52.219] TRAIN: iteration 33973 : loss : 0.053691, loss_ce: 0.002770, loss_dice: 0.104612
[13:17:52.426] TRAIN: iteration 33974 : loss : 0.250188, loss_ce: 0.000370, loss_dice: 0.500006
[13:17:52.634] TRAIN: iteration 33975 : loss : 0.250255, loss_ce: 0.000500, loss_dice: 0.500009
[13:17:53.230] TRAIN: iteration 33976 : loss : 0.056137, loss_ce: 0.000851, loss_dice: 0.111424
[13:17:54.507] TRAIN: iteration 33977 : loss : 0.041182, loss_ce: 0.000493, loss_dice: 0.081870
[13:17:54.714] TRAIN: iteration 33978 : loss : 0.250195, loss_ce: 0.000387, loss_dice: 0.500003
[13:17:54.922] TRAIN: iteration 33979 : loss : 0.077810, loss_ce: 0.002616, loss_dice: 0.153004
[13:17:56.098] TRAIN: iteration 33980 : loss : 0.046012, loss_ce: 0.001581, loss_dice: 0.090444
[13:17:56.337] TRAIN: iteration 33981 : loss : 0.028307, loss_ce: 0.001248, loss_dice: 0.055366
[13:17:56.549] TRAIN: iteration 33982 : loss : 0.029319, loss_ce: 0.000781, loss_dice: 0.057856
[13:17:56.757] TRAIN: iteration 33983 : loss : 0.036037, loss_ce: 0.003729, loss_dice: 0.068344
[13:17:56.973] TRAIN: iteration 33984 : loss : 0.058995, loss_ce: 0.005308, loss_dice: 0.112683
[13:17:57.186] TRAIN: iteration 33985 : loss : 0.079062, loss_ce: 0.001810, loss_dice: 0.156315
[13:17:57.394] TRAIN: iteration 33986 : loss : 0.031810, loss_ce: 0.004129, loss_dice: 0.059491
[13:17:57.608] TRAIN: iteration 33987 : loss : 0.059632, loss_ce: 0.001176, loss_dice: 0.118087
[13:17:59.202] TRAIN: iteration 33988 : loss : 0.252098, loss_ce: 0.003999, loss_dice: 0.500197
[13:17:59.409] TRAIN: iteration 33989 : loss : 0.220327, loss_ce: 0.002080, loss_dice: 0.438574
[13:17:59.616] TRAIN: iteration 33990 : loss : 0.078758, loss_ce: 0.000835, loss_dice: 0.156681
[13:17:59.824] TRAIN: iteration 33991 : loss : 0.006478, loss_ce: 0.000725, loss_dice: 0.012231
[13:18:00.038] TRAIN: iteration 33992 : loss : 0.084998, loss_ce: 0.002227, loss_dice: 0.167768
[13:18:00.245] TRAIN: iteration 33993 : loss : 0.055938, loss_ce: 0.003247, loss_dice: 0.108630
[13:18:00.452] TRAIN: iteration 33994 : loss : 0.047359, loss_ce: 0.001454, loss_dice: 0.093265
[13:18:00.660] TRAIN: iteration 33995 : loss : 0.144374, loss_ce: 0.003620, loss_dice: 0.285128
[13:18:00.872] TRAIN: iteration 33996 : loss : 0.046029, loss_ce: 0.004527, loss_dice: 0.087531
[13:18:01.080] TRAIN: iteration 33997 : loss : 0.060172, loss_ce: 0.000712, loss_dice: 0.119633
[13:18:01.289] TRAIN: iteration 33998 : loss : 0.144837, loss_ce: 0.000575, loss_dice: 0.289099
[13:18:01.496] TRAIN: iteration 33999 : loss : 0.031236, loss_ce: 0.000688, loss_dice: 0.061785
[13:18:01.704] TRAIN: iteration 34000 : loss : 0.226347, loss_ce: 0.001962, loss_dice: 0.450732
[13:18:01.942] TRAIN: iteration 34001 : loss : 0.092678, loss_ce: 0.001803, loss_dice: 0.183552
[13:18:02.150] TRAIN: iteration 34002 : loss : 0.028648, loss_ce: 0.002246, loss_dice: 0.055051
[13:18:02.358] TRAIN: iteration 34003 : loss : 0.252101, loss_ce: 0.006840, loss_dice: 0.497363
[13:18:04.638] TRAIN: iteration 34004 : loss : 0.072556, loss_ce: 0.001709, loss_dice: 0.143402
[13:18:04.846] TRAIN: iteration 34005 : loss : 0.058958, loss_ce: 0.002959, loss_dice: 0.114957
[13:18:05.054] TRAIN: iteration 34006 : loss : 0.069094, loss_ce: 0.002079, loss_dice: 0.136109
[13:18:05.322] TRAIN: iteration 34007 : loss : 0.006086, loss_ce: 0.000874, loss_dice: 0.011297
[13:18:05.534] TRAIN: iteration 34008 : loss : 0.017871, loss_ce: 0.001925, loss_dice: 0.033818
[13:18:05.743] TRAIN: iteration 34009 : loss : 0.057585, loss_ce: 0.007381, loss_dice: 0.107789
[13:18:05.960] TRAIN: iteration 34010 : loss : 0.047832, loss_ce: 0.003563, loss_dice: 0.092102
[13:18:06.168] TRAIN: iteration 34011 : loss : 0.124533, loss_ce: 0.003887, loss_dice: 0.245179
[13:18:06.376] TRAIN: iteration 34012 : loss : 0.079208, loss_ce: 0.004138, loss_dice: 0.154278
[13:18:06.583] TRAIN: iteration 34013 : loss : 0.102585, loss_ce: 0.001908, loss_dice: 0.203263
[13:18:06.791] TRAIN: iteration 34014 : loss : 0.044442, loss_ce: 0.007659, loss_dice: 0.081225
[13:18:06.998] TRAIN: iteration 34015 : loss : 0.025865, loss_ce: 0.001441, loss_dice: 0.050288
[13:18:07.207] TRAIN: iteration 34016 : loss : 0.040001, loss_ce: 0.004530, loss_dice: 0.075471
[13:18:07.418] TRAIN: iteration 34017 : loss : 0.191729, loss_ce: 0.001318, loss_dice: 0.382141
[13:18:07.637] TRAIN: iteration 34018 : loss : 0.029429, loss_ce: 0.002622, loss_dice: 0.056235
[13:18:07.845] TRAIN: iteration 34019 : loss : 0.053435, loss_ce: 0.006434, loss_dice: 0.100435
[13:18:08.054] TRAIN: iteration 34020 : loss : 0.012645, loss_ce: 0.000755, loss_dice: 0.024535
[13:18:08.295] TRAIN: iteration 34021 : loss : 0.063414, loss_ce: 0.001035, loss_dice: 0.125792
[13:18:08.515] TRAIN: iteration 34022 : loss : 0.240025, loss_ce: 0.002111, loss_dice: 0.477938
[13:18:08.722] TRAIN: iteration 34023 : loss : 0.048251, loss_ce: 0.005251, loss_dice: 0.091250
[13:18:08.932] TRAIN: iteration 34024 : loss : 0.072957, loss_ce: 0.001567, loss_dice: 0.144348
[13:18:09.152] TRAIN: iteration 34025 : loss : 0.250290, loss_ce: 0.001150, loss_dice: 0.499431
[13:18:09.359] TRAIN: iteration 34026 : loss : 0.032325, loss_ce: 0.003272, loss_dice: 0.061379
[13:18:09.575] TRAIN: iteration 34027 : loss : 0.071293, loss_ce: 0.001654, loss_dice: 0.140932
[13:18:09.782] TRAIN: iteration 34028 : loss : 0.248686, loss_ce: 0.002982, loss_dice: 0.494389
[13:18:09.989] TRAIN: iteration 34029 : loss : 0.135805, loss_ce: 0.001422, loss_dice: 0.270187
[13:18:10.198] TRAIN: iteration 34030 : loss : 0.167356, loss_ce: 0.003616, loss_dice: 0.331096
[13:18:10.406] TRAIN: iteration 34031 : loss : 0.250501, loss_ce: 0.004962, loss_dice: 0.496040
[13:18:10.613] TRAIN: iteration 34032 : loss : 0.040785, loss_ce: 0.005213, loss_dice: 0.076356
[13:18:10.820] TRAIN: iteration 34033 : loss : 0.092778, loss_ce: 0.005565, loss_dice: 0.179991
[13:18:11.028] TRAIN: iteration 34034 : loss : 0.228420, loss_ce: 0.001249, loss_dice: 0.455591
[13:18:12.730] TRAIN: iteration 34035 : loss : 0.071545, loss_ce: 0.002960, loss_dice: 0.140129
[13:18:12.937] TRAIN: iteration 34036 : loss : 0.040720, loss_ce: 0.002316, loss_dice: 0.079125
[13:18:13.145] TRAIN: iteration 34037 : loss : 0.211101, loss_ce: 0.001072, loss_dice: 0.421131
[13:18:13.353] TRAIN: iteration 34038 : loss : 0.108862, loss_ce: 0.006722, loss_dice: 0.211001
[13:18:13.578] TRAIN: iteration 34039 : loss : 0.057261, loss_ce: 0.006188, loss_dice: 0.108335
[13:18:13.785] TRAIN: iteration 34040 : loss : 0.083342, loss_ce: 0.002106, loss_dice: 0.164577
[13:18:14.024] TRAIN: iteration 34041 : loss : 0.251050, loss_ce: 0.004189, loss_dice: 0.497911
[13:18:14.232] TRAIN: iteration 34042 : loss : 0.104613, loss_ce: 0.001172, loss_dice: 0.208055
[13:18:14.517] TRAIN: iteration 34043 : loss : 0.250697, loss_ce: 0.001342, loss_dice: 0.500052
[13:18:14.724] TRAIN: iteration 34044 : loss : 0.045667, loss_ce: 0.002372, loss_dice: 0.088961
[13:18:14.932] TRAIN: iteration 34045 : loss : 0.077970, loss_ce: 0.003844, loss_dice: 0.152097
[13:18:15.140] TRAIN: iteration 34046 : loss : 0.147009, loss_ce: 0.001732, loss_dice: 0.292286
[13:18:15.807] TRAIN: iteration 34047 : loss : 0.033691, loss_ce: 0.002788, loss_dice: 0.064594
[13:18:17.173] TRAIN: iteration 34048 : loss : 0.095785, loss_ce: 0.004551, loss_dice: 0.187020
[13:18:17.381] TRAIN: iteration 34049 : loss : 0.222099, loss_ce: 0.001379, loss_dice: 0.442819
[13:18:17.593] TRAIN: iteration 34050 : loss : 0.037087, loss_ce: 0.001998, loss_dice: 0.072176
[13:18:17.802] TRAIN: iteration 34051 : loss : 0.023592, loss_ce: 0.001889, loss_dice: 0.045296
[13:18:18.013] TRAIN: iteration 34052 : loss : 0.037969, loss_ce: 0.003205, loss_dice: 0.072732
[13:18:18.230] TRAIN: iteration 34053 : loss : 0.068248, loss_ce: 0.000761, loss_dice: 0.135735
[13:18:18.438] TRAIN: iteration 34054 : loss : 0.230463, loss_ce: 0.001316, loss_dice: 0.459609
[13:18:18.646] TRAIN: iteration 34055 : loss : 0.062421, loss_ce: 0.004391, loss_dice: 0.120451
[13:18:18.855] TRAIN: iteration 34056 : loss : 0.250327, loss_ce: 0.000642, loss_dice: 0.500013
[13:18:19.062] TRAIN: iteration 34057 : loss : 0.066122, loss_ce: 0.004830, loss_dice: 0.127414
[13:18:19.269] TRAIN: iteration 34058 : loss : 0.052910, loss_ce: 0.001938, loss_dice: 0.103881
[13:18:19.925] TRAIN: iteration 34059 : loss : 0.046013, loss_ce: 0.002816, loss_dice: 0.089210
[13:18:20.132] TRAIN: iteration 34060 : loss : 0.102700, loss_ce: 0.001721, loss_dice: 0.203680
[13:18:20.382] TRAIN: iteration 34061 : loss : 0.034820, loss_ce: 0.000748, loss_dice: 0.068891
[13:18:20.597] TRAIN: iteration 34062 : loss : 0.019688, loss_ce: 0.000376, loss_dice: 0.039001
[13:18:20.808] TRAIN: iteration 34063 : loss : 0.027810, loss_ce: 0.002040, loss_dice: 0.053580
[13:18:21.024] TRAIN: iteration 34064 : loss : 0.245559, loss_ce: 0.000986, loss_dice: 0.490133
[13:18:21.236] TRAIN: iteration 34065 : loss : 0.068256, loss_ce: 0.005686, loss_dice: 0.130826
[13:18:21.446] TRAIN: iteration 34066 : loss : 0.017424, loss_ce: 0.000997, loss_dice: 0.033851
[13:18:21.654] TRAIN: iteration 34067 : loss : 0.063982, loss_ce: 0.003181, loss_dice: 0.124782
[13:18:21.862] TRAIN: iteration 34068 : loss : 0.054676, loss_ce: 0.001934, loss_dice: 0.107417
[13:18:22.071] TRAIN: iteration 34069 : loss : 0.090470, loss_ce: 0.000770, loss_dice: 0.180169
[13:18:22.862] TRAIN: iteration 34070 : loss : 0.251421, loss_ce: 0.002692, loss_dice: 0.500150
[13:18:23.071] TRAIN: iteration 34071 : loss : 0.050965, loss_ce: 0.001811, loss_dice: 0.100119
[13:18:23.280] TRAIN: iteration 34072 : loss : 0.059368, loss_ce: 0.004472, loss_dice: 0.114263
[13:18:23.488] TRAIN: iteration 34073 : loss : 0.150064, loss_ce: 0.002088, loss_dice: 0.298039
[13:18:23.820] TRAIN: iteration 34074 : loss : 0.038314, loss_ce: 0.005241, loss_dice: 0.071387
[13:18:24.027] TRAIN: iteration 34075 : loss : 0.250679, loss_ce: 0.001293, loss_dice: 0.500065
[13:18:24.623] TRAIN: iteration 34076 : loss : 0.250602, loss_ce: 0.001140, loss_dice: 0.500064
[13:18:24.830] TRAIN: iteration 34077 : loss : 0.063716, loss_ce: 0.005707, loss_dice: 0.121725
[13:18:25.222] TRAIN: iteration 34078 : loss : 0.144876, loss_ce: 0.009933, loss_dice: 0.279820
[13:18:25.429] TRAIN: iteration 34079 : loss : 0.067557, loss_ce: 0.005860, loss_dice: 0.129254
[13:18:25.641] TRAIN: iteration 34080 : loss : 0.029874, loss_ce: 0.002536, loss_dice: 0.057213
[13:18:25.881] TRAIN: iteration 34081 : loss : 0.194395, loss_ce: 0.001421, loss_dice: 0.387369
[13:18:28.978] TRAIN: iteration 34082 : loss : 0.164747, loss_ce: 0.001113, loss_dice: 0.328382
[13:18:29.192] TRAIN: iteration 34083 : loss : 0.031211, loss_ce: 0.000577, loss_dice: 0.061845
[13:18:29.401] TRAIN: iteration 34084 : loss : 0.060528, loss_ce: 0.001804, loss_dice: 0.119251
[13:18:29.613] TRAIN: iteration 34085 : loss : 0.160234, loss_ce: 0.001791, loss_dice: 0.318677
[13:18:29.820] TRAIN: iteration 34086 : loss : 0.080015, loss_ce: 0.001034, loss_dice: 0.158996
[13:18:30.030] TRAIN: iteration 34087 : loss : 0.090192, loss_ce: 0.005714, loss_dice: 0.174669
[13:18:30.238] TRAIN: iteration 34088 : loss : 0.111483, loss_ce: 0.002471, loss_dice: 0.220494
[13:18:30.459] TRAIN: iteration 34089 : loss : 0.147761, loss_ce: 0.006367, loss_dice: 0.289156
[13:18:35.564] TRAIN: iteration 34090 : loss : 0.250254, loss_ce: 0.000498, loss_dice: 0.500010
[13:18:35.772] TRAIN: iteration 34091 : loss : 0.041067, loss_ce: 0.009252, loss_dice: 0.072883
[13:18:35.979] TRAIN: iteration 34092 : loss : 0.130995, loss_ce: 0.003935, loss_dice: 0.258056
[13:18:36.191] TRAIN: iteration 34093 : loss : 0.078214, loss_ce: 0.003486, loss_dice: 0.152941
[13:18:36.399] TRAIN: iteration 34094 : loss : 0.074005, loss_ce: 0.004336, loss_dice: 0.143674
[13:18:36.608] TRAIN: iteration 34095 : loss : 0.250502, loss_ce: 0.000961, loss_dice: 0.500042
[13:18:36.816] TRAIN: iteration 34096 : loss : 0.250322, loss_ce: 0.000629, loss_dice: 0.500014
[13:18:37.023] TRAIN: iteration 34097 : loss : 0.116438, loss_ce: 0.002545, loss_dice: 0.230332
[13:18:37.231] TRAIN: iteration 34098 : loss : 0.100230, loss_ce: 0.001791, loss_dice: 0.198669
[13:18:37.443] TRAIN: iteration 34099 : loss : 0.250464, loss_ce: 0.000896, loss_dice: 0.500031
[13:18:37.650] TRAIN: iteration 34100 : loss : 0.252555, loss_ce: 0.004774, loss_dice: 0.500337
[13:18:37.885] TRAIN: iteration 34101 : loss : 0.044921, loss_ce: 0.004210, loss_dice: 0.085632
[13:18:38.099] TRAIN: iteration 34102 : loss : 0.251084, loss_ce: 0.002027, loss_dice: 0.500140
[13:18:38.306] TRAIN: iteration 34103 : loss : 0.035034, loss_ce: 0.004887, loss_dice: 0.065181
[13:18:38.513] TRAIN: iteration 34104 : loss : 0.253006, loss_ce: 0.005598, loss_dice: 0.500414
[13:18:38.721] TRAIN: iteration 34105 : loss : 0.053708, loss_ce: 0.003770, loss_dice: 0.103646
[13:18:39.900] TRAIN: iteration 34106 : loss : 0.230038, loss_ce: 0.000965, loss_dice: 0.459110
[13:18:40.108] TRAIN: iteration 34107 : loss : 0.015873, loss_ce: 0.001195, loss_dice: 0.030551
[13:18:40.325] TRAIN: iteration 34108 : loss : 0.030708, loss_ce: 0.001632, loss_dice: 0.059784
[13:18:40.533] TRAIN: iteration 34109 : loss : 0.077384, loss_ce: 0.001837, loss_dice: 0.152932
[13:18:40.742] TRAIN: iteration 34110 : loss : 0.061884, loss_ce: 0.009078, loss_dice: 0.114690
[13:18:40.949] TRAIN: iteration 34111 : loss : 0.134327, loss_ce: 0.003095, loss_dice: 0.265559
[13:18:41.158] TRAIN: iteration 34112 : loss : 0.149589, loss_ce: 0.006883, loss_dice: 0.292294
[13:18:41.372] TRAIN: iteration 34113 : loss : 0.063731, loss_ce: 0.001437, loss_dice: 0.126025
[13:18:41.579] TRAIN: iteration 34114 : loss : 0.224868, loss_ce: 0.001646, loss_dice: 0.448090
[13:18:41.915] TRAIN: iteration 34115 : loss : 0.047564, loss_ce: 0.000889, loss_dice: 0.094240
[13:18:42.274] TRAIN: iteration 34116 : loss : 0.033691, loss_ce: 0.001024, loss_dice: 0.066358
[13:18:42.483] TRAIN: iteration 34117 : loss : 0.048524, loss_ce: 0.002137, loss_dice: 0.094912
[13:18:42.691] TRAIN: iteration 34118 : loss : 0.075570, loss_ce: 0.004023, loss_dice: 0.147116
[13:18:42.900] TRAIN: iteration 34119 : loss : 0.107150, loss_ce: 0.005057, loss_dice: 0.209243
[13:18:43.114] TRAIN: iteration 34120 : loss : 0.040196, loss_ce: 0.002160, loss_dice: 0.078232
[13:18:43.351] TRAIN: iteration 34121 : loss : 0.108951, loss_ce: 0.006259, loss_dice: 0.211643
[13:18:43.559] TRAIN: iteration 34122 : loss : 0.058748, loss_ce: 0.002259, loss_dice: 0.115238
[13:18:43.774] TRAIN: iteration 34123 : loss : 0.036571, loss_ce: 0.000581, loss_dice: 0.072561
[13:18:44.920] TRAIN: iteration 34124 : loss : 0.055148, loss_ce: 0.001649, loss_dice: 0.108647
[13:18:45.133] TRAIN: iteration 34125 : loss : 0.017431, loss_ce: 0.002275, loss_dice: 0.032587
[13:18:45.340] TRAIN: iteration 34126 : loss : 0.096030, loss_ce: 0.003148, loss_dice: 0.188913
[13:18:46.929] TRAIN: iteration 34127 : loss : 0.108066, loss_ce: 0.004239, loss_dice: 0.211894
[13:18:47.289] TRAIN: iteration 34128 : loss : 0.034886, loss_ce: 0.001642, loss_dice: 0.068131
[13:18:47.505] TRAIN: iteration 34129 : loss : 0.060046, loss_ce: 0.003977, loss_dice: 0.116115
[13:18:47.715] TRAIN: iteration 34130 : loss : 0.050640, loss_ce: 0.001077, loss_dice: 0.100204
[13:18:47.928] TRAIN: iteration 34131 : loss : 0.039947, loss_ce: 0.002495, loss_dice: 0.077400
[13:18:48.502] TRAIN: iteration 34132 : loss : 0.099260, loss_ce: 0.006337, loss_dice: 0.192184
[13:18:48.715] TRAIN: iteration 34133 : loss : 0.013940, loss_ce: 0.000638, loss_dice: 0.027242
[13:18:48.923] TRAIN: iteration 34134 : loss : 0.101912, loss_ce: 0.005276, loss_dice: 0.198549
[13:18:49.134] TRAIN: iteration 34135 : loss : 0.042568, loss_ce: 0.000906, loss_dice: 0.084230
[13:18:50.693] TRAIN: iteration 34136 : loss : 0.249310, loss_ce: 0.004996, loss_dice: 0.493624
[13:18:50.900] TRAIN: iteration 34137 : loss : 0.196617, loss_ce: 0.000765, loss_dice: 0.392470
[13:18:51.111] TRAIN: iteration 34138 : loss : 0.142015, loss_ce: 0.001793, loss_dice: 0.282236
[13:18:51.320] TRAIN: iteration 34139 : loss : 0.033610, loss_ce: 0.001258, loss_dice: 0.065962
[13:18:51.530] TRAIN: iteration 34140 : loss : 0.181705, loss_ce: 0.003165, loss_dice: 0.360245
[13:18:51.767] TRAIN: iteration 34141 : loss : 0.014013, loss_ce: 0.000688, loss_dice: 0.027338
[13:18:51.974] TRAIN: iteration 34142 : loss : 0.175655, loss_ce: 0.019883, loss_dice: 0.331428
[13:18:52.183] TRAIN: iteration 34143 : loss : 0.073405, loss_ce: 0.002464, loss_dice: 0.144347
[13:18:52.392] TRAIN: iteration 34144 : loss : 0.157539, loss_ce: 0.002810, loss_dice: 0.312269
[13:18:52.599] TRAIN: iteration 34145 : loss : 0.062720, loss_ce: 0.002076, loss_dice: 0.123365
[13:18:52.807] TRAIN: iteration 34146 : loss : 0.018298, loss_ce: 0.001107, loss_dice: 0.035488
[13:18:53.015] TRAIN: iteration 34147 : loss : 0.139986, loss_ce: 0.003654, loss_dice: 0.276318
[13:18:53.226] TRAIN: iteration 34148 : loss : 0.165435, loss_ce: 0.003346, loss_dice: 0.327524
[13:18:53.439] TRAIN: iteration 34149 : loss : 0.049312, loss_ce: 0.002183, loss_dice: 0.096441
[13:18:55.142] TRAIN: iteration 34150 : loss : 0.077187, loss_ce: 0.003268, loss_dice: 0.151106
[13:18:55.350] TRAIN: iteration 34151 : loss : 0.146089, loss_ce: 0.005169, loss_dice: 0.287009
[13:18:55.560] TRAIN: iteration 34152 : loss : 0.177694, loss_ce: 0.001766, loss_dice: 0.353622
[13:18:55.767] TRAIN: iteration 34153 : loss : 0.035482, loss_ce: 0.011917, loss_dice: 0.059047
[13:18:55.974] TRAIN: iteration 34154 : loss : 0.034080, loss_ce: 0.002283, loss_dice: 0.065878
[13:18:56.184] TRAIN: iteration 34155 : loss : 0.251048, loss_ce: 0.001965, loss_dice: 0.500131
[13:18:56.390] TRAIN: iteration 34156 : loss : 0.250170, loss_ce: 0.002206, loss_dice: 0.498134
[13:18:56.598] TRAIN: iteration 34157 : loss : 0.030942, loss_ce: 0.001158, loss_dice: 0.060727
[13:18:57.810] TRAIN: iteration 34158 : loss : 0.030798, loss_ce: 0.002624, loss_dice: 0.058973
[13:18:58.017] TRAIN: iteration 34159 : loss : 0.169583, loss_ce: 0.003914, loss_dice: 0.335252
[13:18:58.225] TRAIN: iteration 34160 : loss : 0.018677, loss_ce: 0.001845, loss_dice: 0.035510
[13:18:58.461] TRAIN: iteration 34161 : loss : 0.075849, loss_ce: 0.007445, loss_dice: 0.144253
[13:18:58.706] TRAIN: iteration 34162 : loss : 0.250690, loss_ce: 0.001309, loss_dice: 0.500071
[13:18:58.915] TRAIN: iteration 34163 : loss : 0.044943, loss_ce: 0.005684, loss_dice: 0.084202
[13:18:59.122] TRAIN: iteration 34164 : loss : 0.044473, loss_ce: 0.001070, loss_dice: 0.087876
[13:18:59.330] TRAIN: iteration 34165 : loss : 0.148500, loss_ce: 0.000890, loss_dice: 0.296110
[13:19:02.770] TRAIN: iteration 34166 : loss : 0.049968, loss_ce: 0.004177, loss_dice: 0.095758
[13:19:02.976] TRAIN: iteration 34167 : loss : 0.250914, loss_ce: 0.001716, loss_dice: 0.500111
[13:19:03.187] TRAIN: iteration 34168 : loss : 0.140727, loss_ce: 0.006072, loss_dice: 0.275382
[13:19:03.393] TRAIN: iteration 34169 : loss : 0.250745, loss_ce: 0.001418, loss_dice: 0.500072
[13:19:03.604] TRAIN: iteration 34170 : loss : 0.250583, loss_ce: 0.001103, loss_dice: 0.500063
[13:19:03.814] TRAIN: iteration 34171 : loss : 0.043039, loss_ce: 0.001417, loss_dice: 0.084661
[13:19:04.022] TRAIN: iteration 34172 : loss : 0.052240, loss_ce: 0.001939, loss_dice: 0.102541
[13:19:04.230] TRAIN: iteration 34173 : loss : 0.250456, loss_ce: 0.000884, loss_dice: 0.500027
[13:19:06.502] TRAIN: iteration 34174 : loss : 0.075182, loss_ce: 0.006865, loss_dice: 0.143498
[13:19:06.714] TRAIN: iteration 34175 : loss : 0.247070, loss_ce: 0.000742, loss_dice: 0.493397
[13:19:06.925] TRAIN: iteration 34176 : loss : 0.248490, loss_ce: 0.003442, loss_dice: 0.493538
[13:19:07.135] TRAIN: iteration 34177 : loss : 0.028636, loss_ce: 0.000715, loss_dice: 0.056556
[13:19:07.343] TRAIN: iteration 34178 : loss : 0.155106, loss_ce: 0.001429, loss_dice: 0.308783
[13:19:07.551] TRAIN: iteration 34179 : loss : 0.017442, loss_ce: 0.000361, loss_dice: 0.034523
[13:19:07.759] TRAIN: iteration 34180 : loss : 0.045757, loss_ce: 0.002207, loss_dice: 0.089307
[13:19:07.996] TRAIN: iteration 34181 : loss : 0.024472, loss_ce: 0.002439, loss_dice: 0.046505
[13:19:09.994] TRAIN: iteration 34182 : loss : 0.131395, loss_ce: 0.001489, loss_dice: 0.261301
[13:19:10.201] TRAIN: iteration 34183 : loss : 0.011414, loss_ce: 0.001246, loss_dice: 0.021582
[13:19:10.408] TRAIN: iteration 34184 : loss : 0.212745, loss_ce: 0.000879, loss_dice: 0.424611
[13:19:10.616] TRAIN: iteration 34185 : loss : 0.250771, loss_ce: 0.001472, loss_dice: 0.500070
[13:19:10.824] TRAIN: iteration 34186 : loss : 0.124870, loss_ce: 0.001596, loss_dice: 0.248144
[13:19:11.038] TRAIN: iteration 34187 : loss : 0.238178, loss_ce: 0.001991, loss_dice: 0.474365
[13:19:11.246] TRAIN: iteration 34188 : loss : 0.049407, loss_ce: 0.001101, loss_dice: 0.097713
[13:19:11.462] TRAIN: iteration 34189 : loss : 0.108443, loss_ce: 0.001913, loss_dice: 0.214973
[13:19:13.649] TRAIN: iteration 34190 : loss : 0.074781, loss_ce: 0.000690, loss_dice: 0.148872
[13:19:13.855] TRAIN: iteration 34191 : loss : 0.034012, loss_ce: 0.005076, loss_dice: 0.062947
[13:19:14.065] TRAIN: iteration 34192 : loss : 0.250679, loss_ce: 0.001277, loss_dice: 0.500081
[13:19:14.280] TRAIN: iteration 34193 : loss : 0.196683, loss_ce: 0.002563, loss_dice: 0.390803
[13:19:14.496] TRAIN: iteration 34194 : loss : 0.034576, loss_ce: 0.001832, loss_dice: 0.067320
[13:19:14.703] TRAIN: iteration 34195 : loss : 0.075332, loss_ce: 0.002882, loss_dice: 0.147782
[13:19:15.091] TRAIN: iteration 34196 : loss : 0.075688, loss_ce: 0.003506, loss_dice: 0.147870
[13:19:15.298] TRAIN: iteration 34197 : loss : 0.187844, loss_ce: 0.003295, loss_dice: 0.372392
[13:19:17.297] TRAIN: iteration 34198 : loss : 0.026506, loss_ce: 0.000611, loss_dice: 0.052401
[13:19:17.506] TRAIN: iteration 34199 : loss : 0.046449, loss_ce: 0.009591, loss_dice: 0.083307
[13:19:17.745] TRAIN: iteration 34200 : loss : 0.034667, loss_ce: 0.002512, loss_dice: 0.066822
[13:19:17.990] TRAIN: iteration 34201 : loss : 0.017934, loss_ce: 0.000498, loss_dice: 0.035370
[13:19:18.198] TRAIN: iteration 34202 : loss : 0.026585, loss_ce: 0.000747, loss_dice: 0.052423
[13:19:18.407] TRAIN: iteration 34203 : loss : 0.021896, loss_ce: 0.000620, loss_dice: 0.043172
[13:19:18.616] TRAIN: iteration 34204 : loss : 0.058861, loss_ce: 0.002903, loss_dice: 0.114819
[13:19:18.824] TRAIN: iteration 34205 : loss : 0.048276, loss_ce: 0.001747, loss_dice: 0.094806
[13:19:19.032] TRAIN: iteration 34206 : loss : 0.022472, loss_ce: 0.001236, loss_dice: 0.043709
[13:19:19.248] TRAIN: iteration 34207 : loss : 0.053779, loss_ce: 0.005276, loss_dice: 0.102282
[13:19:19.456] TRAIN: iteration 34208 : loss : 0.111059, loss_ce: 0.002255, loss_dice: 0.219863
[13:19:19.664] TRAIN: iteration 34209 : loss : 0.042738, loss_ce: 0.003157, loss_dice: 0.082318
[13:19:19.871] TRAIN: iteration 34210 : loss : 0.049999, loss_ce: 0.001426, loss_dice: 0.098571
[13:19:20.079] TRAIN: iteration 34211 : loss : 0.018779, loss_ce: 0.003281, loss_dice: 0.034277
[13:19:21.047] TRAIN: iteration 34212 : loss : 0.049125, loss_ce: 0.002710, loss_dice: 0.095539
[13:19:21.257] TRAIN: iteration 34213 : loss : 0.250449, loss_ce: 0.000867, loss_dice: 0.500031
[13:19:21.464] TRAIN: iteration 34214 : loss : 0.250938, loss_ce: 0.001775, loss_dice: 0.500100
[13:19:21.671] TRAIN: iteration 34215 : loss : 0.076882, loss_ce: 0.002200, loss_dice: 0.151563
[13:19:21.928] TRAIN: iteration 34216 : loss : 0.170529, loss_ce: 0.002000, loss_dice: 0.339057
[13:19:22.135] TRAIN: iteration 34217 : loss : 0.209900, loss_ce: 0.002937, loss_dice: 0.416864
[13:19:22.341] TRAIN: iteration 34218 : loss : 0.114727, loss_ce: 0.005141, loss_dice: 0.224312
[13:19:22.548] TRAIN: iteration 34219 : loss : 0.018574, loss_ce: 0.001342, loss_dice: 0.035806
[13:19:27.085] TRAIN: iteration 34220 : loss : 0.250513, loss_ce: 0.000975, loss_dice: 0.500051
[13:19:27.319] TRAIN: iteration 34221 : loss : 0.067633, loss_ce: 0.003152, loss_dice: 0.132113
[13:19:27.540] TRAIN: iteration 34222 : loss : 0.079724, loss_ce: 0.005592, loss_dice: 0.153857
[13:19:27.747] TRAIN: iteration 34223 : loss : 0.059208, loss_ce: 0.006860, loss_dice: 0.111557
[13:19:27.954] TRAIN: iteration 34224 : loss : 0.248749, loss_ce: 0.002242, loss_dice: 0.495257
[13:19:28.161] TRAIN: iteration 34225 : loss : 0.038645, loss_ce: 0.001485, loss_dice: 0.075805
[13:19:28.369] TRAIN: iteration 34226 : loss : 0.208109, loss_ce: 0.004702, loss_dice: 0.411515
[13:19:28.576] TRAIN: iteration 34227 : loss : 0.055952, loss_ce: 0.008077, loss_dice: 0.103827
[13:19:28.784] TRAIN: iteration 34228 : loss : 0.113952, loss_ce: 0.001954, loss_dice: 0.225949
[13:19:28.992] TRAIN: iteration 34229 : loss : 0.144349, loss_ce: 0.004108, loss_dice: 0.284591
[13:19:29.201] TRAIN: iteration 34230 : loss : 0.106215, loss_ce: 0.001651, loss_dice: 0.210779
[13:19:29.410] TRAIN: iteration 34231 : loss : 0.250906, loss_ce: 0.001692, loss_dice: 0.500119
[13:19:29.627] TRAIN: iteration 34232 : loss : 0.055841, loss_ce: 0.002654, loss_dice: 0.109028
[13:19:29.835] TRAIN: iteration 34233 : loss : 0.154788, loss_ce: 0.009903, loss_dice: 0.299672
[13:19:30.731] TRAIN: iteration 34234 : loss : 0.080998, loss_ce: 0.007800, loss_dice: 0.154196
[13:19:30.943] TRAIN: iteration 34235 : loss : 0.074332, loss_ce: 0.001648, loss_dice: 0.147016
[13:19:31.496] TRAIN: iteration 34236 : loss : 0.036900, loss_ce: 0.005439, loss_dice: 0.068361
[13:19:31.707] TRAIN: iteration 34237 : loss : 0.182641, loss_ce: 0.002397, loss_dice: 0.362885
[13:19:31.916] TRAIN: iteration 34238 : loss : 0.028665, loss_ce: 0.002418, loss_dice: 0.054912
[13:19:32.131] TRAIN: iteration 34239 : loss : 0.035647, loss_ce: 0.000991, loss_dice: 0.070303
[13:19:32.999] TRAIN: iteration 34240 : loss : 0.032634, loss_ce: 0.002096, loss_dice: 0.063172
[13:19:33.240] TRAIN: iteration 34241 : loss : 0.025824, loss_ce: 0.002536, loss_dice: 0.049112
[13:19:33.448] TRAIN: iteration 34242 : loss : 0.023071, loss_ce: 0.000943, loss_dice: 0.045200
[13:19:33.657] TRAIN: iteration 34243 : loss : 0.044519, loss_ce: 0.000686, loss_dice: 0.088352
[13:19:34.587] TRAIN: iteration 34244 : loss : 0.040745, loss_ce: 0.003250, loss_dice: 0.078239
[13:19:34.795] TRAIN: iteration 34245 : loss : 0.049334, loss_ce: 0.007328, loss_dice: 0.091340
[13:19:35.006] TRAIN: iteration 34246 : loss : 0.063148, loss_ce: 0.003261, loss_dice: 0.123035
[13:19:35.374] TRAIN: iteration 34247 : loss : 0.200015, loss_ce: 0.001239, loss_dice: 0.398790
[13:19:36.221] TRAIN: iteration 34248 : loss : 0.027115, loss_ce: 0.001615, loss_dice: 0.052615
[13:19:36.428] TRAIN: iteration 34249 : loss : 0.169938, loss_ce: 0.001678, loss_dice: 0.338198
[13:19:36.637] TRAIN: iteration 34250 : loss : 0.064135, loss_ce: 0.002526, loss_dice: 0.125743
[13:19:36.847] TRAIN: iteration 34251 : loss : 0.075730, loss_ce: 0.000907, loss_dice: 0.150553
[13:19:37.054] TRAIN: iteration 34252 : loss : 0.180591, loss_ce: 0.003336, loss_dice: 0.357846
[13:19:37.265] TRAIN: iteration 34253 : loss : 0.031834, loss_ce: 0.003387, loss_dice: 0.060281
[13:19:38.394] TRAIN: iteration 34254 : loss : 0.020244, loss_ce: 0.001910, loss_dice: 0.038579
[13:19:38.605] TRAIN: iteration 34255 : loss : 0.085170, loss_ce: 0.006007, loss_dice: 0.164332
[13:19:39.743] TRAIN: iteration 34256 : loss : 0.223772, loss_ce: 0.001363, loss_dice: 0.446180
[13:19:39.951] TRAIN: iteration 34257 : loss : 0.096985, loss_ce: 0.001714, loss_dice: 0.192256
[13:19:40.159] TRAIN: iteration 34258 : loss : 0.195185, loss_ce: 0.001217, loss_dice: 0.389154
[13:19:40.366] TRAIN: iteration 34259 : loss : 0.164882, loss_ce: 0.002898, loss_dice: 0.326866
[13:19:40.574] TRAIN: iteration 34260 : loss : 0.247854, loss_ce: 0.000765, loss_dice: 0.494944
[13:19:40.810] TRAIN: iteration 34261 : loss : 0.157580, loss_ce: 0.001851, loss_dice: 0.313308
[13:19:41.778] TRAIN: iteration 34262 : loss : 0.038063, loss_ce: 0.002070, loss_dice: 0.074056
[13:19:41.989] TRAIN: iteration 34263 : loss : 0.094099, loss_ce: 0.001636, loss_dice: 0.186562
[13:19:42.200] TRAIN: iteration 34264 : loss : 0.039273, loss_ce: 0.001465, loss_dice: 0.077081
[13:19:42.407] TRAIN: iteration 34265 : loss : 0.078694, loss_ce: 0.002135, loss_dice: 0.155254
[13:19:42.616] TRAIN: iteration 34266 : loss : 0.053406, loss_ce: 0.004226, loss_dice: 0.102586
[13:19:42.823] TRAIN: iteration 34267 : loss : 0.023186, loss_ce: 0.003581, loss_dice: 0.042790
[13:19:43.037] TRAIN: iteration 34268 : loss : 0.103955, loss_ce: 0.006586, loss_dice: 0.201323
[13:19:43.246] TRAIN: iteration 34269 : loss : 0.104393, loss_ce: 0.001094, loss_dice: 0.207691
[13:19:43.688] TRAIN: iteration 34270 : loss : 0.035910, loss_ce: 0.003676, loss_dice: 0.068144
[13:19:44.048] TRAIN: iteration 34271 : loss : 0.250353, loss_ce: 0.000689, loss_dice: 0.500017
[13:19:45.058] TRAIN: iteration 34272 : loss : 0.037351, loss_ce: 0.001023, loss_dice: 0.073680
[13:19:45.266] TRAIN: iteration 34273 : loss : 0.088372, loss_ce: 0.002105, loss_dice: 0.174639
[13:19:45.473] TRAIN: iteration 34274 : loss : 0.156279, loss_ce: 0.001204, loss_dice: 0.311354
[13:19:45.684] TRAIN: iteration 34275 : loss : 0.049248, loss_ce: 0.001026, loss_dice: 0.097471
[13:19:45.892] TRAIN: iteration 34276 : loss : 0.036671, loss_ce: 0.002039, loss_dice: 0.071303
[13:19:46.100] TRAIN: iteration 34277 : loss : 0.139796, loss_ce: 0.002048, loss_dice: 0.277544
[13:19:47.094] TRAIN: iteration 34278 : loss : 0.139182, loss_ce: 0.011268, loss_dice: 0.267096
[13:19:48.456] TRAIN: iteration 34279 : loss : 0.102807, loss_ce: 0.006614, loss_dice: 0.199001
[13:19:48.663] TRAIN: iteration 34280 : loss : 0.086601, loss_ce: 0.008109, loss_dice: 0.165093
[13:19:51.810] TRAIN: iteration 34281 : loss : 0.048935, loss_ce: 0.011516, loss_dice: 0.086354
[13:19:52.022] TRAIN: iteration 34282 : loss : 0.124525, loss_ce: 0.000779, loss_dice: 0.248272
[13:19:52.228] TRAIN: iteration 34283 : loss : 0.109948, loss_ce: 0.000678, loss_dice: 0.219218
[13:19:52.439] TRAIN: iteration 34284 : loss : 0.033461, loss_ce: 0.002095, loss_dice: 0.064828
[13:19:52.716] TRAIN: iteration 34285 : loss : 0.173212, loss_ce: 0.002966, loss_dice: 0.343457
[13:19:52.923] TRAIN: iteration 34286 : loss : 0.031483, loss_ce: 0.001043, loss_dice: 0.061924
[13:19:53.346] TRAIN: iteration 34287 : loss : 0.022746, loss_ce: 0.003037, loss_dice: 0.042456
[13:19:53.552] TRAIN: iteration 34288 : loss : 0.089350, loss_ce: 0.007655, loss_dice: 0.171044
[13:19:53.938] TRAIN: iteration 34289 : loss : 0.046632, loss_ce: 0.000806, loss_dice: 0.092458
[13:19:54.146] TRAIN: iteration 34290 : loss : 0.018413, loss_ce: 0.002324, loss_dice: 0.034502
[13:19:54.353] TRAIN: iteration 34291 : loss : 0.250603, loss_ce: 0.001134, loss_dice: 0.500072
[13:19:54.566] TRAIN: iteration 34292 : loss : 0.069895, loss_ce: 0.003381, loss_dice: 0.136409
[13:19:54.774] TRAIN: iteration 34293 : loss : 0.063368, loss_ce: 0.005341, loss_dice: 0.121395
[13:19:55.018] TRAIN: iteration 34294 : loss : 0.028241, loss_ce: 0.000610, loss_dice: 0.055871
[13:19:58.676] TRAIN: iteration 34295 : loss : 0.039216, loss_ce: 0.001557, loss_dice: 0.076876
[13:19:58.883] TRAIN: iteration 34296 : loss : 0.138996, loss_ce: 0.004049, loss_dice: 0.273943
[13:19:59.092] TRAIN: iteration 34297 : loss : 0.242490, loss_ce: 0.000492, loss_dice: 0.484488
[13:19:59.301] TRAIN: iteration 34298 : loss : 0.153815, loss_ce: 0.003263, loss_dice: 0.304367
[13:19:59.517] TRAIN: iteration 34299 : loss : 0.116940, loss_ce: 0.002098, loss_dice: 0.231783
[13:19:59.723] TRAIN: iteration 34300 : loss : 0.038531, loss_ce: 0.002786, loss_dice: 0.074275
[13:19:59.961] TRAIN: iteration 34301 : loss : 0.233389, loss_ce: 0.002540, loss_dice: 0.464239
[13:20:00.169] TRAIN: iteration 34302 : loss : 0.039154, loss_ce: 0.001859, loss_dice: 0.076450
[13:20:04.179] TRAIN: iteration 34303 : loss : 0.058543, loss_ce: 0.005105, loss_dice: 0.111980
[13:20:04.386] TRAIN: iteration 34304 : loss : 0.016209, loss_ce: 0.001205, loss_dice: 0.031212
[13:20:04.593] TRAIN: iteration 34305 : loss : 0.073136, loss_ce: 0.002367, loss_dice: 0.143904
[13:20:04.800] TRAIN: iteration 34306 : loss : 0.074975, loss_ce: 0.004168, loss_dice: 0.145783
[13:20:05.008] TRAIN: iteration 34307 : loss : 0.043776, loss_ce: 0.010813, loss_dice: 0.076738
[13:20:05.215] TRAIN: iteration 34308 : loss : 0.031197, loss_ce: 0.000482, loss_dice: 0.061912
[13:20:05.424] TRAIN: iteration 34309 : loss : 0.071792, loss_ce: 0.001144, loss_dice: 0.142440
[13:20:05.631] TRAIN: iteration 34310 : loss : 0.057964, loss_ce: 0.002004, loss_dice: 0.113924
[13:20:08.536] TRAIN: iteration 34311 : loss : 0.076566, loss_ce: 0.001561, loss_dice: 0.151571
[13:20:08.744] TRAIN: iteration 34312 : loss : 0.027857, loss_ce: 0.003430, loss_dice: 0.052285
[13:20:08.951] TRAIN: iteration 34313 : loss : 0.079857, loss_ce: 0.001685, loss_dice: 0.158030
[13:20:09.159] TRAIN: iteration 34314 : loss : 0.132932, loss_ce: 0.001612, loss_dice: 0.264252
[13:20:09.460] TRAIN: iteration 34315 : loss : 0.050791, loss_ce: 0.001922, loss_dice: 0.099660
[13:20:09.668] TRAIN: iteration 34316 : loss : 0.058032, loss_ce: 0.005394, loss_dice: 0.110669
[13:20:09.877] TRAIN: iteration 34317 : loss : 0.048807, loss_ce: 0.006916, loss_dice: 0.090699
[13:20:10.084] TRAIN: iteration 34318 : loss : 0.035139, loss_ce: 0.005377, loss_dice: 0.064900
[13:20:12.184] TRAIN: iteration 34319 : loss : 0.042922, loss_ce: 0.001972, loss_dice: 0.083872
[13:20:12.393] TRAIN: iteration 34320 : loss : 0.082710, loss_ce: 0.007920, loss_dice: 0.157500
[13:20:12.629] TRAIN: iteration 34321 : loss : 0.044695, loss_ce: 0.005469, loss_dice: 0.083922
[13:20:12.839] TRAIN: iteration 34322 : loss : 0.252560, loss_ce: 0.007844, loss_dice: 0.497275
[13:20:13.049] TRAIN: iteration 34323 : loss : 0.048992, loss_ce: 0.003763, loss_dice: 0.094221
[13:20:13.256] TRAIN: iteration 34324 : loss : 0.128469, loss_ce: 0.006915, loss_dice: 0.250023
[13:20:13.464] TRAIN: iteration 34325 : loss : 0.048558, loss_ce: 0.004023, loss_dice: 0.093093
[13:20:13.789] TRAIN: iteration 34326 : loss : 0.025615, loss_ce: 0.002423, loss_dice: 0.048807
[13:20:13.997] TRAIN: iteration 34327 : loss : 0.250420, loss_ce: 0.000806, loss_dice: 0.500034
[13:20:14.205] TRAIN: iteration 34328 : loss : 0.216431, loss_ce: 0.000827, loss_dice: 0.432034
[13:20:14.413] TRAIN: iteration 34329 : loss : 0.126153, loss_ce: 0.001144, loss_dice: 0.251162
[13:20:14.623] TRAIN: iteration 34330 : loss : 0.049820, loss_ce: 0.001927, loss_dice: 0.097713
[13:20:14.831] TRAIN: iteration 34331 : loss : 0.012517, loss_ce: 0.001090, loss_dice: 0.023943
[13:20:15.042] TRAIN: iteration 34332 : loss : 0.250331, loss_ce: 0.000633, loss_dice: 0.500029
[13:20:15.254] TRAIN: iteration 34333 : loss : 0.252333, loss_ce: 0.004340, loss_dice: 0.500326
[13:20:15.463] TRAIN: iteration 34334 : loss : 0.092137, loss_ce: 0.001252, loss_dice: 0.183023
[13:20:17.914] TRAIN: iteration 34335 : loss : 0.021803, loss_ce: 0.001851, loss_dice: 0.041756
[13:20:18.129] TRAIN: iteration 34336 : loss : 0.086276, loss_ce: 0.002854, loss_dice: 0.169698
[13:20:18.336] TRAIN: iteration 34337 : loss : 0.058405, loss_ce: 0.001283, loss_dice: 0.115528
[13:20:18.545] TRAIN: iteration 34338 : loss : 0.154846, loss_ce: 0.003454, loss_dice: 0.306238
[13:20:18.754] TRAIN: iteration 34339 : loss : 0.250255, loss_ce: 0.000494, loss_dice: 0.500017
[13:20:18.962] TRAIN: iteration 34340 : loss : 0.250319, loss_ce: 0.000618, loss_dice: 0.500021
[13:20:19.200] TRAIN: iteration 34341 : loss : 0.048178, loss_ce: 0.002557, loss_dice: 0.093800
[13:20:19.409] TRAIN: iteration 34342 : loss : 0.063045, loss_ce: 0.001240, loss_dice: 0.124851
[13:20:20.206] TRAIN: iteration 34343 : loss : 0.057804, loss_ce: 0.003093, loss_dice: 0.112515
[13:20:21.052] TRAIN: iteration 34344 : loss : 0.047360, loss_ce: 0.004345, loss_dice: 0.090376
[13:20:21.260] TRAIN: iteration 34345 : loss : 0.025984, loss_ce: 0.004012, loss_dice: 0.047955
[13:20:21.468] TRAIN: iteration 34346 : loss : 0.197946, loss_ce: 0.002703, loss_dice: 0.393188
[13:20:21.675] TRAIN: iteration 34347 : loss : 0.070912, loss_ce: 0.008084, loss_dice: 0.133740
[13:20:21.883] TRAIN: iteration 34348 : loss : 0.149247, loss_ce: 0.003966, loss_dice: 0.294528
[13:20:22.096] TRAIN: iteration 34349 : loss : 0.156516, loss_ce: 0.001013, loss_dice: 0.312019
[13:20:22.303] TRAIN: iteration 34350 : loss : 0.090769, loss_ce: 0.011275, loss_dice: 0.170262
[13:20:25.234] TRAIN: iteration 34351 : loss : 0.033617, loss_ce: 0.002908, loss_dice: 0.064326
[13:20:26.032] TRAIN: iteration 34352 : loss : 0.223902, loss_ce: 0.001193, loss_dice: 0.446611
[13:20:26.243] TRAIN: iteration 34353 : loss : 0.127356, loss_ce: 0.006911, loss_dice: 0.247800
[13:20:26.458] TRAIN: iteration 34354 : loss : 0.019523, loss_ce: 0.000453, loss_dice: 0.038592
[13:20:26.665] TRAIN: iteration 34355 : loss : 0.060660, loss_ce: 0.002396, loss_dice: 0.118923
[13:20:26.873] TRAIN: iteration 34356 : loss : 0.056319, loss_ce: 0.000577, loss_dice: 0.112062
[13:20:27.088] TRAIN: iteration 34357 : loss : 0.038945, loss_ce: 0.000839, loss_dice: 0.077052
[13:20:27.294] TRAIN: iteration 34358 : loss : 0.194978, loss_ce: 0.001816, loss_dice: 0.388140
[13:20:29.992] TRAIN: iteration 34359 : loss : 0.055349, loss_ce: 0.002148, loss_dice: 0.108549
[13:20:31.563] TRAIN: iteration 34360 : loss : 0.058024, loss_ce: 0.001855, loss_dice: 0.114193
[13:20:31.801] TRAIN: iteration 34361 : loss : 0.056581, loss_ce: 0.001197, loss_dice: 0.111964
[13:20:32.011] TRAIN: iteration 34362 : loss : 0.045723, loss_ce: 0.004150, loss_dice: 0.087297
[13:20:32.226] TRAIN: iteration 34363 : loss : 0.169333, loss_ce: 0.001328, loss_dice: 0.337339
[13:20:32.435] TRAIN: iteration 34364 : loss : 0.231954, loss_ce: 0.005054, loss_dice: 0.458855
[13:20:32.642] TRAIN: iteration 34365 : loss : 0.250291, loss_ce: 0.000565, loss_dice: 0.500017
[13:20:32.850] TRAIN: iteration 34366 : loss : 0.161146, loss_ce: 0.000922, loss_dice: 0.321369
[13:20:33.059] TRAIN: iteration 34367 : loss : 0.251384, loss_ce: 0.002578, loss_dice: 0.500190
[13:20:35.725] TRAIN: iteration 34368 : loss : 0.053022, loss_ce: 0.007067, loss_dice: 0.098977
[13:20:35.933] TRAIN: iteration 34369 : loss : 0.096256, loss_ce: 0.002063, loss_dice: 0.190449
[13:20:36.141] TRAIN: iteration 34370 : loss : 0.067677, loss_ce: 0.008972, loss_dice: 0.126381
[13:20:36.347] TRAIN: iteration 34371 : loss : 0.250613, loss_ce: 0.001169, loss_dice: 0.500056
[13:20:36.555] TRAIN: iteration 34372 : loss : 0.045244, loss_ce: 0.004148, loss_dice: 0.086340
[13:20:36.762] TRAIN: iteration 34373 : loss : 0.137615, loss_ce: 0.001386, loss_dice: 0.273843
[13:20:36.970] TRAIN: iteration 34374 : loss : 0.164579, loss_ce: 0.003462, loss_dice: 0.325695
[13:20:37.179] TRAIN: iteration 34375 : loss : 0.195201, loss_ce: 0.005722, loss_dice: 0.384680
[13:20:38.217] TRAIN: iteration 34376 : loss : 0.250160, loss_ce: 0.000317, loss_dice: 0.500003
[13:20:38.424] TRAIN: iteration 34377 : loss : 0.060811, loss_ce: 0.001142, loss_dice: 0.120479
[13:20:38.630] TRAIN: iteration 34378 : loss : 0.058044, loss_ce: 0.002821, loss_dice: 0.113266
[13:20:38.837] TRAIN: iteration 34379 : loss : 0.038846, loss_ce: 0.004919, loss_dice: 0.072773
[13:20:39.138] TRAIN: iteration 34380 : loss : 0.097878, loss_ce: 0.002218, loss_dice: 0.193537
[13:20:39.374] TRAIN: iteration 34381 : loss : 0.250207, loss_ce: 0.000408, loss_dice: 0.500006
[13:20:39.582] TRAIN: iteration 34382 : loss : 0.044207, loss_ce: 0.000535, loss_dice: 0.087880
[13:20:43.192] TRAIN: iteration 34383 : loss : 0.050706, loss_ce: 0.001281, loss_dice: 0.100131
[13:20:43.402] TRAIN: iteration 34384 : loss : 0.035550, loss_ce: 0.001333, loss_dice: 0.069767
[13:20:43.609] TRAIN: iteration 34385 : loss : 0.250428, loss_ce: 0.000831, loss_dice: 0.500026
[13:20:43.817] TRAIN: iteration 34386 : loss : 0.074501, loss_ce: 0.001495, loss_dice: 0.147507
[13:20:44.026] TRAIN: iteration 34387 : loss : 0.198924, loss_ce: 0.009301, loss_dice: 0.388547
[13:20:44.235] TRAIN: iteration 34388 : loss : 0.038053, loss_ce: 0.002258, loss_dice: 0.073848
[13:20:44.443] TRAIN: iteration 34389 : loss : 0.086152, loss_ce: 0.002035, loss_dice: 0.170269
[13:20:44.650] TRAIN: iteration 34390 : loss : 0.064177, loss_ce: 0.005870, loss_dice: 0.122484
[13:20:47.053] TRAIN: iteration 34391 : loss : 0.074118, loss_ce: 0.001180, loss_dice: 0.147056
[13:20:47.260] TRAIN: iteration 34392 : loss : 0.103168, loss_ce: 0.002587, loss_dice: 0.203749
[13:20:47.467] TRAIN: iteration 34393 : loss : 0.015539, loss_ce: 0.000947, loss_dice: 0.030131
[13:20:47.678] TRAIN: iteration 34394 : loss : 0.250625, loss_ce: 0.001180, loss_dice: 0.500069
[13:20:47.894] TRAIN: iteration 34395 : loss : 0.080145, loss_ce: 0.002162, loss_dice: 0.158128
[13:20:48.101] TRAIN: iteration 34396 : loss : 0.192759, loss_ce: 0.000660, loss_dice: 0.384858
[13:20:48.311] TRAIN: iteration 34397 : loss : 0.025555, loss_ce: 0.000930, loss_dice: 0.050180
[13:20:48.680] TRAIN: iteration 34398 : loss : 0.026113, loss_ce: 0.002909, loss_dice: 0.049316
[13:20:49.601] TRAIN: iteration 34399 : loss : 0.073152, loss_ce: 0.001182, loss_dice: 0.145122
[13:20:51.759] TRAIN: iteration 34400 : loss : 0.138205, loss_ce: 0.001611, loss_dice: 0.274800
[13:20:51.998] TRAIN: iteration 34401 : loss : 0.019589, loss_ce: 0.003100, loss_dice: 0.036079
[13:20:52.209] TRAIN: iteration 34402 : loss : 0.024704, loss_ce: 0.000902, loss_dice: 0.048506
[13:20:52.416] TRAIN: iteration 34403 : loss : 0.072438, loss_ce: 0.001932, loss_dice: 0.142945
[13:20:52.623] TRAIN: iteration 34404 : loss : 0.067615, loss_ce: 0.001043, loss_dice: 0.134187
[13:20:52.832] TRAIN: iteration 34405 : loss : 0.051631, loss_ce: 0.001495, loss_dice: 0.101766
[13:20:56.237] TRAIN: iteration 34406 : loss : 0.107784, loss_ce: 0.001765, loss_dice: 0.213804
[13:20:56.446] TRAIN: iteration 34407 : loss : 0.077483, loss_ce: 0.001886, loss_dice: 0.153080
[13:20:56.658] TRAIN: iteration 34408 : loss : 0.026534, loss_ce: 0.000588, loss_dice: 0.052481
[13:20:56.866] TRAIN: iteration 34409 : loss : 0.028594, loss_ce: 0.003965, loss_dice: 0.053224
[13:20:57.078] TRAIN: iteration 34410 : loss : 0.242300, loss_ce: 0.001765, loss_dice: 0.482836
[13:20:57.286] TRAIN: iteration 34411 : loss : 0.066646, loss_ce: 0.002429, loss_dice: 0.130863
[13:20:58.706] TRAIN: iteration 34412 : loss : 0.146565, loss_ce: 0.002725, loss_dice: 0.290405
[13:20:58.914] TRAIN: iteration 34413 : loss : 0.064386, loss_ce: 0.000854, loss_dice: 0.127917
[13:21:01.405] TRAIN: iteration 34414 : loss : 0.250450, loss_ce: 0.000869, loss_dice: 0.500032
[13:21:01.617] TRAIN: iteration 34415 : loss : 0.030659, loss_ce: 0.001200, loss_dice: 0.060119
[13:21:01.824] TRAIN: iteration 34416 : loss : 0.071665, loss_ce: 0.006326, loss_dice: 0.137003
[13:21:02.032] TRAIN: iteration 34417 : loss : 0.024842, loss_ce: 0.001289, loss_dice: 0.048395
[13:21:02.240] TRAIN: iteration 34418 : loss : 0.054225, loss_ce: 0.001228, loss_dice: 0.107222
[13:21:02.453] TRAIN: iteration 34419 : loss : 0.094617, loss_ce: 0.006612, loss_dice: 0.182621
[13:21:03.089] TRAIN: iteration 34420 : loss : 0.049094, loss_ce: 0.001116, loss_dice: 0.097072
[13:21:03.090] NaN or Inf found in input tensor.
[13:21:03.304] TRAIN: iteration 34421 : loss : 0.036308, loss_ce: 0.001046, loss_dice: 0.071570
[13:21:06.273] TRAIN: iteration 34422 : loss : 0.069517, loss_ce: 0.000846, loss_dice: 0.138188
[13:21:06.481] TRAIN: iteration 34423 : loss : 0.250289, loss_ce: 0.000564, loss_dice: 0.500013
[13:21:06.688] TRAIN: iteration 34424 : loss : 0.085435, loss_ce: 0.001671, loss_dice: 0.169200
[13:21:06.897] TRAIN: iteration 34425 : loss : 0.037338, loss_ce: 0.003824, loss_dice: 0.070851
[13:21:07.109] TRAIN: iteration 34426 : loss : 0.244704, loss_ce: 0.002420, loss_dice: 0.486987
[13:21:07.317] TRAIN: iteration 34427 : loss : 0.229355, loss_ce: 0.005152, loss_dice: 0.453558
[13:21:07.642] TRAIN: iteration 34428 : loss : 0.250892, loss_ce: 0.001678, loss_dice: 0.500105
[13:21:07.849] TRAIN: iteration 34429 : loss : 0.150262, loss_ce: 0.002785, loss_dice: 0.297739
[13:21:14.921] TRAIN: iteration 34430 : loss : 0.035494, loss_ce: 0.001565, loss_dice: 0.069423
[13:21:15.135] TRAIN: iteration 34431 : loss : 0.080905, loss_ce: 0.002499, loss_dice: 0.159311
[13:21:15.345] TRAIN: iteration 34432 : loss : 0.154990, loss_ce: 0.005783, loss_dice: 0.304196
[13:21:15.591] TRAIN: iteration 34433 : loss : 0.060109, loss_ce: 0.014227, loss_dice: 0.105991
[13:21:15.799] TRAIN: iteration 34434 : loss : 0.047855, loss_ce: 0.000712, loss_dice: 0.094998
[13:21:16.009] TRAIN: iteration 34435 : loss : 0.073436, loss_ce: 0.003797, loss_dice: 0.143074
[13:21:16.222] TRAIN: iteration 34436 : loss : 0.024287, loss_ce: 0.005268, loss_dice: 0.043306
[13:21:16.431] TRAIN: iteration 34437 : loss : 0.042357, loss_ce: 0.002139, loss_dice: 0.082575
[13:21:20.028] TRAIN: iteration 34438 : loss : 0.058268, loss_ce: 0.002385, loss_dice: 0.114150
[13:21:20.239] TRAIN: iteration 34439 : loss : 0.027902, loss_ce: 0.003512, loss_dice: 0.052292
[13:21:20.446] TRAIN: iteration 34440 : loss : 0.094025, loss_ce: 0.001379, loss_dice: 0.186672
[13:21:20.687] TRAIN: iteration 34441 : loss : 0.036044, loss_ce: 0.001941, loss_dice: 0.070148
[13:21:20.896] TRAIN: iteration 34442 : loss : 0.251123, loss_ce: 0.002104, loss_dice: 0.500141
[13:21:21.105] TRAIN: iteration 34443 : loss : 0.020469, loss_ce: 0.001681, loss_dice: 0.039257
[13:21:21.312] TRAIN: iteration 34444 : loss : 0.250748, loss_ce: 0.001410, loss_dice: 0.500086
[13:21:21.519] TRAIN: iteration 34445 : loss : 0.085458, loss_ce: 0.001701, loss_dice: 0.169215
[13:21:24.865] TRAIN: iteration 34446 : loss : 0.228780, loss_ce: 0.001750, loss_dice: 0.455810
[13:21:25.073] TRAIN: iteration 34447 : loss : 0.077889, loss_ce: 0.001118, loss_dice: 0.154660
[13:21:25.279] TRAIN: iteration 34448 : loss : 0.030695, loss_ce: 0.005788, loss_dice: 0.055602
[13:21:25.486] TRAIN: iteration 34449 : loss : 0.011626, loss_ce: 0.001248, loss_dice: 0.022004
[13:21:25.826] TRAIN: iteration 34450 : loss : 0.094262, loss_ce: 0.001605, loss_dice: 0.186918
[13:21:26.033] TRAIN: iteration 34451 : loss : 0.084572, loss_ce: 0.007217, loss_dice: 0.161927
[13:21:26.241] TRAIN: iteration 34452 : loss : 0.043323, loss_ce: 0.001308, loss_dice: 0.085339
[13:21:26.453] TRAIN: iteration 34453 : loss : 0.060397, loss_ce: 0.002629, loss_dice: 0.118164
[13:21:30.827] TRAIN: iteration 34454 : loss : 0.250185, loss_ce: 0.000365, loss_dice: 0.500005
[13:21:31.035] TRAIN: iteration 34455 : loss : 0.098052, loss_ce: 0.002412, loss_dice: 0.193692
[13:21:31.246] TRAIN: iteration 34456 : loss : 0.143114, loss_ce: 0.004687, loss_dice: 0.281541
[13:21:31.461] TRAIN: iteration 34457 : loss : 0.106413, loss_ce: 0.003389, loss_dice: 0.209438
[13:21:31.671] TRAIN: iteration 34458 : loss : 0.043802, loss_ce: 0.005601, loss_dice: 0.082003
[13:21:31.884] TRAIN: iteration 34459 : loss : 0.051006, loss_ce: 0.001678, loss_dice: 0.100335
[13:21:32.093] TRAIN: iteration 34460 : loss : 0.036959, loss_ce: 0.001683, loss_dice: 0.072234
[13:21:32.722] TRAIN: iteration 34461 : loss : 0.068348, loss_ce: 0.002363, loss_dice: 0.134333
[13:21:35.894] TRAIN: iteration 34462 : loss : 0.077398, loss_ce: 0.007860, loss_dice: 0.146935
[13:21:36.105] TRAIN: iteration 34463 : loss : 0.028155, loss_ce: 0.006195, loss_dice: 0.050115
[13:21:36.313] TRAIN: iteration 34464 : loss : 0.143325, loss_ce: 0.002950, loss_dice: 0.283700
[13:21:36.520] TRAIN: iteration 34465 : loss : 0.041398, loss_ce: 0.001239, loss_dice: 0.081557
[13:21:36.727] TRAIN: iteration 34466 : loss : 0.211518, loss_ce: 0.009069, loss_dice: 0.413966
[13:21:36.934] TRAIN: iteration 34467 : loss : 0.250262, loss_ce: 0.000508, loss_dice: 0.500017
[13:21:37.142] TRAIN: iteration 34468 : loss : 0.039829, loss_ce: 0.000941, loss_dice: 0.078717
[13:21:40.220] TRAIN: iteration 34469 : loss : 0.078020, loss_ce: 0.003386, loss_dice: 0.152654
[13:21:40.433] TRAIN: iteration 34470 : loss : 0.090753, loss_ce: 0.005971, loss_dice: 0.175535
[13:21:40.640] TRAIN: iteration 34471 : loss : 0.080346, loss_ce: 0.001087, loss_dice: 0.159605
[13:21:40.853] TRAIN: iteration 34472 : loss : 0.031145, loss_ce: 0.001117, loss_dice: 0.061172
[13:21:41.060] TRAIN: iteration 34473 : loss : 0.250608, loss_ce: 0.001156, loss_dice: 0.500061
[13:21:41.267] TRAIN: iteration 34474 : loss : 0.035471, loss_ce: 0.002899, loss_dice: 0.068044
[13:21:43.046] TRAIN: iteration 34475 : loss : 0.238899, loss_ce: 0.007380, loss_dice: 0.470417
[13:21:43.254] TRAIN: iteration 34476 : loss : 0.034681, loss_ce: 0.004652, loss_dice: 0.064710
[13:21:48.498] TRAIN: iteration 34477 : loss : 0.250225, loss_ce: 0.000443, loss_dice: 0.500006
[13:21:48.705] TRAIN: iteration 34478 : loss : 0.250589, loss_ce: 0.001116, loss_dice: 0.500061
[13:21:48.912] TRAIN: iteration 34479 : loss : 0.040651, loss_ce: 0.002026, loss_dice: 0.079276
[13:21:49.121] TRAIN: iteration 34480 : loss : 0.027753, loss_ce: 0.004138, loss_dice: 0.051368
[13:21:49.357] TRAIN: iteration 34481 : loss : 0.070633, loss_ce: 0.001416, loss_dice: 0.139851
[13:21:49.566] TRAIN: iteration 34482 : loss : 0.019803, loss_ce: 0.002967, loss_dice: 0.036639
[13:21:49.772] TRAIN: iteration 34483 : loss : 0.089840, loss_ce: 0.001122, loss_dice: 0.178558
[13:21:49.980] TRAIN: iteration 34484 : loss : 0.090979, loss_ce: 0.002465, loss_dice: 0.179492
[13:21:55.418] TRAIN: iteration 34485 : loss : 0.028543, loss_ce: 0.002027, loss_dice: 0.055059
[13:21:55.629] TRAIN: iteration 34486 : loss : 0.162079, loss_ce: 0.001301, loss_dice: 0.322857
[13:21:55.836] TRAIN: iteration 34487 : loss : 0.056141, loss_ce: 0.005116, loss_dice: 0.107166
[13:21:56.047] TRAIN: iteration 34488 : loss : 0.058836, loss_ce: 0.003577, loss_dice: 0.114095
[13:21:56.255] TRAIN: iteration 34489 : loss : 0.040832, loss_ce: 0.002186, loss_dice: 0.079479
[13:21:56.463] TRAIN: iteration 34490 : loss : 0.059724, loss_ce: 0.002161, loss_dice: 0.117288
[13:21:56.670] TRAIN: iteration 34491 : loss : 0.040745, loss_ce: 0.000795, loss_dice: 0.080695
[13:21:56.878] TRAIN: iteration 34492 : loss : 0.124060, loss_ce: 0.002236, loss_dice: 0.245884
[13:22:01.362] TRAIN: iteration 34493 : loss : 0.038462, loss_ce: 0.001884, loss_dice: 0.075039
[13:22:01.572] TRAIN: iteration 34494 : loss : 0.029456, loss_ce: 0.000847, loss_dice: 0.058066
[13:22:01.780] TRAIN: iteration 34495 : loss : 0.168684, loss_ce: 0.005193, loss_dice: 0.332176
[13:22:01.987] TRAIN: iteration 34496 : loss : 0.036129, loss_ce: 0.000669, loss_dice: 0.071590
[13:22:02.194] TRAIN: iteration 34497 : loss : 0.058208, loss_ce: 0.001272, loss_dice: 0.115143
[13:22:02.402] TRAIN: iteration 34498 : loss : 0.041004, loss_ce: 0.006948, loss_dice: 0.075060
[13:22:02.610] TRAIN: iteration 34499 : loss : 0.101754, loss_ce: 0.002521, loss_dice: 0.200987
[13:22:02.816] TRAIN: iteration 34500 : loss : 0.133743, loss_ce: 0.005513, loss_dice: 0.261973
[13:22:07.790] TRAIN: iteration 34501 : loss : 0.050113, loss_ce: 0.003155, loss_dice: 0.097071
[13:22:08.001] TRAIN: iteration 34502 : loss : 0.079219, loss_ce: 0.005803, loss_dice: 0.152635
[13:22:08.242] TRAIN: iteration 34503 : loss : 0.035941, loss_ce: 0.002352, loss_dice: 0.069531
[13:22:08.450] TRAIN: iteration 34504 : loss : 0.027136, loss_ce: 0.004013, loss_dice: 0.050260
[13:22:08.657] TRAIN: iteration 34505 : loss : 0.106480, loss_ce: 0.004083, loss_dice: 0.208878
[13:22:08.865] TRAIN: iteration 34506 : loss : 0.169282, loss_ce: 0.001517, loss_dice: 0.337046
[13:22:09.073] TRAIN: iteration 34507 : loss : 0.059202, loss_ce: 0.002316, loss_dice: 0.116087
[13:22:09.282] TRAIN: iteration 34508 : loss : 0.250929, loss_ce: 0.001829, loss_dice: 0.500028
[13:22:15.877] TRAIN: iteration 34509 : loss : 0.250400, loss_ce: 0.000775, loss_dice: 0.500025
[13:22:16.090] TRAIN: iteration 34510 : loss : 0.063345, loss_ce: 0.001730, loss_dice: 0.124960
[13:22:16.297] TRAIN: iteration 34511 : loss : 0.122567, loss_ce: 0.002063, loss_dice: 0.243070
[13:22:16.503] TRAIN: iteration 34512 : loss : 0.082005, loss_ce: 0.002195, loss_dice: 0.161815
[13:22:16.709] TRAIN: iteration 34513 : loss : 0.032716, loss_ce: 0.001113, loss_dice: 0.064319
[13:22:16.915] TRAIN: iteration 34514 : loss : 0.250362, loss_ce: 0.000699, loss_dice: 0.500025
[13:22:17.123] TRAIN: iteration 34515 : loss : 0.034747, loss_ce: 0.004560, loss_dice: 0.064933
[13:22:17.337] TRAIN: iteration 34516 : loss : 0.054587, loss_ce: 0.001851, loss_dice: 0.107324
[13:22:22.884] TRAIN: iteration 34517 : loss : 0.060898, loss_ce: 0.007171, loss_dice: 0.114625
[13:22:23.099] TRAIN: iteration 34518 : loss : 0.091445, loss_ce: 0.003528, loss_dice: 0.179363
[13:22:23.306] TRAIN: iteration 34519 : loss : 0.091949, loss_ce: 0.007327, loss_dice: 0.176572
[13:22:23.513] TRAIN: iteration 34520 : loss : 0.024853, loss_ce: 0.001455, loss_dice: 0.048251
[13:22:23.749] TRAIN: iteration 34521 : loss : 0.033925, loss_ce: 0.002533, loss_dice: 0.065318
[13:22:23.958] TRAIN: iteration 34522 : loss : 0.250327, loss_ce: 0.000637, loss_dice: 0.500017
[13:22:24.164] TRAIN: iteration 34523 : loss : 0.044229, loss_ce: 0.001602, loss_dice: 0.086857
[13:22:24.393] TRAIN: iteration 34524 : loss : 0.071923, loss_ce: 0.002012, loss_dice: 0.141835
[13:22:29.722] TRAIN: iteration 34525 : loss : 0.250190, loss_ce: 0.000377, loss_dice: 0.500003
[13:22:29.933] TRAIN: iteration 34526 : loss : 0.250260, loss_ce: 0.000508, loss_dice: 0.500012
[13:22:30.140] TRAIN: iteration 34527 : loss : 0.069681, loss_ce: 0.004850, loss_dice: 0.134512
[13:22:30.347] TRAIN: iteration 34528 : loss : 0.250164, loss_ce: 0.000325, loss_dice: 0.500002
[13:22:30.554] TRAIN: iteration 34529 : loss : 0.039910, loss_ce: 0.000569, loss_dice: 0.079252
[13:22:30.767] TRAIN: iteration 34530 : loss : 0.049060, loss_ce: 0.001010, loss_dice: 0.097111
[13:22:30.974] TRAIN: iteration 34531 : loss : 0.045677, loss_ce: 0.002951, loss_dice: 0.088404
[13:22:31.182] TRAIN: iteration 34532 : loss : 0.062996, loss_ce: 0.002469, loss_dice: 0.123523
[13:22:38.059] TRAIN: iteration 34533 : loss : 0.156959, loss_ce: 0.008798, loss_dice: 0.305120
[13:22:38.265] TRAIN: iteration 34534 : loss : 0.110145, loss_ce: 0.006118, loss_dice: 0.214171
[13:22:38.472] TRAIN: iteration 34535 : loss : 0.037984, loss_ce: 0.005168, loss_dice: 0.070799
[13:22:38.679] TRAIN: iteration 34536 : loss : 0.096884, loss_ce: 0.005253, loss_dice: 0.188515
[13:22:38.886] TRAIN: iteration 34537 : loss : 0.250521, loss_ce: 0.001002, loss_dice: 0.500041
[13:22:39.093] TRAIN: iteration 34538 : loss : 0.251214, loss_ce: 0.002272, loss_dice: 0.500155
[13:22:39.300] TRAIN: iteration 34539 : loss : 0.127779, loss_ce: 0.000762, loss_dice: 0.254796
[13:22:39.507] TRAIN: iteration 34540 : loss : 0.250990, loss_ce: 0.001976, loss_dice: 0.500004
[13:22:46.510] TRAIN: iteration 34541 : loss : 0.067770, loss_ce: 0.003312, loss_dice: 0.132228
[13:22:46.717] TRAIN: iteration 34542 : loss : 0.195802, loss_ce: 0.003145, loss_dice: 0.388460
[13:22:46.925] TRAIN: iteration 34543 : loss : 0.030964, loss_ce: 0.003462, loss_dice: 0.058465
[13:22:47.132] TRAIN: iteration 34544 : loss : 0.055660, loss_ce: 0.003389, loss_dice: 0.107930
[13:22:47.339] TRAIN: iteration 34545 : loss : 0.074661, loss_ce: 0.002890, loss_dice: 0.146432
[13:22:47.552] TRAIN: iteration 34546 : loss : 0.053285, loss_ce: 0.001400, loss_dice: 0.105170
[13:22:47.765] TRAIN: iteration 34547 : loss : 0.038226, loss_ce: 0.002739, loss_dice: 0.073714
[13:22:47.971] TRAIN: iteration 34548 : loss : 0.156398, loss_ce: 0.001032, loss_dice: 0.311764
[13:22:52.486] TRAIN: iteration 34549 : loss : 0.250315, loss_ce: 0.000611, loss_dice: 0.500019
[13:22:52.694] TRAIN: iteration 34550 : loss : 0.113830, loss_ce: 0.003879, loss_dice: 0.223782
[13:22:52.901] TRAIN: iteration 34551 : loss : 0.098365, loss_ce: 0.001554, loss_dice: 0.195175
[13:22:53.114] TRAIN: iteration 34552 : loss : 0.042857, loss_ce: 0.003687, loss_dice: 0.082027
[13:22:53.320] TRAIN: iteration 34553 : loss : 0.250376, loss_ce: 0.000734, loss_dice: 0.500018
[13:22:53.528] TRAIN: iteration 34554 : loss : 0.057373, loss_ce: 0.006262, loss_dice: 0.108483
[13:22:53.738] TRAIN: iteration 34555 : loss : 0.192416, loss_ce: 0.002027, loss_dice: 0.382806
[13:22:53.948] TRAIN: iteration 34556 : loss : 0.028119, loss_ce: 0.003724, loss_dice: 0.052515
[13:23:01.747] TRAIN: iteration 34557 : loss : 0.250658, loss_ce: 0.001240, loss_dice: 0.500076
[13:23:01.956] TRAIN: iteration 34558 : loss : 0.018946, loss_ce: 0.002407, loss_dice: 0.035486
[13:23:02.164] TRAIN: iteration 34559 : loss : 0.037673, loss_ce: 0.001144, loss_dice: 0.074201
[13:23:02.372] TRAIN: iteration 34560 : loss : 0.035395, loss_ce: 0.003454, loss_dice: 0.067336
[13:23:02.608] TRAIN: iteration 34561 : loss : 0.092094, loss_ce: 0.002539, loss_dice: 0.181648
[13:23:02.815] TRAIN: iteration 34562 : loss : 0.022774, loss_ce: 0.004202, loss_dice: 0.041346
[13:23:03.023] TRAIN: iteration 34563 : loss : 0.053574, loss_ce: 0.003979, loss_dice: 0.103168
[13:23:03.234] TRAIN: iteration 34564 : loss : 0.038434, loss_ce: 0.004173, loss_dice: 0.072696
[13:23:08.793] TRAIN: iteration 34565 : loss : 0.092179, loss_ce: 0.001408, loss_dice: 0.182951
[13:23:09.002] TRAIN: iteration 34566 : loss : 0.035523, loss_ce: 0.002325, loss_dice: 0.068721
[13:23:09.213] TRAIN: iteration 34567 : loss : 0.181568, loss_ce: 0.002742, loss_dice: 0.360395
[13:23:09.421] TRAIN: iteration 34568 : loss : 0.008165, loss_ce: 0.000803, loss_dice: 0.015526
[13:23:09.631] TRAIN: iteration 34569 : loss : 0.074786, loss_ce: 0.002909, loss_dice: 0.146664
[13:23:09.840] TRAIN: iteration 34570 : loss : 0.250631, loss_ce: 0.001196, loss_dice: 0.500066
[13:23:10.050] TRAIN: iteration 34571 : loss : 0.019368, loss_ce: 0.000561, loss_dice: 0.038176
[13:23:10.258] TRAIN: iteration 34572 : loss : 0.220294, loss_ce: 0.001523, loss_dice: 0.439065
[13:23:15.562] TRAIN: iteration 34573 : loss : 0.049665, loss_ce: 0.001044, loss_dice: 0.098286
[13:23:15.773] TRAIN: iteration 34574 : loss : 0.026344, loss_ce: 0.000496, loss_dice: 0.052193
[13:23:15.980] TRAIN: iteration 34575 : loss : 0.032570, loss_ce: 0.001286, loss_dice: 0.063853
[13:23:16.187] TRAIN: iteration 34576 : loss : 0.087789, loss_ce: 0.001815, loss_dice: 0.173764
[13:23:16.399] TRAIN: iteration 34577 : loss : 0.049263, loss_ce: 0.000932, loss_dice: 0.097595
[13:23:16.612] TRAIN: iteration 34578 : loss : 0.095647, loss_ce: 0.002317, loss_dice: 0.188977
[13:23:16.821] TRAIN: iteration 34579 : loss : 0.047644, loss_ce: 0.002723, loss_dice: 0.092564
[13:23:17.030] TRAIN: iteration 34580 : loss : 0.029546, loss_ce: 0.001691, loss_dice: 0.057402
[13:23:22.155] TRAIN: iteration 34581 : loss : 0.024124, loss_ce: 0.001543, loss_dice: 0.046704
[13:23:22.368] TRAIN: iteration 34582 : loss : 0.047359, loss_ce: 0.001619, loss_dice: 0.093099
[13:23:22.576] TRAIN: iteration 34583 : loss : 0.047679, loss_ce: 0.002526, loss_dice: 0.092833
[13:23:22.784] TRAIN: iteration 34584 : loss : 0.049156, loss_ce: 0.000932, loss_dice: 0.097380
[13:23:22.991] TRAIN: iteration 34585 : loss : 0.030178, loss_ce: 0.000600, loss_dice: 0.059757
[13:23:23.200] TRAIN: iteration 34586 : loss : 0.067686, loss_ce: 0.004059, loss_dice: 0.131312
[13:23:23.409] TRAIN: iteration 34587 : loss : 0.024980, loss_ce: 0.001921, loss_dice: 0.048038
[13:23:23.619] TRAIN: iteration 34588 : loss : 0.060907, loss_ce: 0.002295, loss_dice: 0.119518
[13:23:31.143] TRAIN: iteration 34589 : loss : 0.041085, loss_ce: 0.001757, loss_dice: 0.080414
[13:23:31.351] TRAIN: iteration 34590 : loss : 0.249514, loss_ce: 0.002274, loss_dice: 0.496754
[13:23:31.557] TRAIN: iteration 34591 : loss : 0.067231, loss_ce: 0.000868, loss_dice: 0.133595
[13:23:31.765] TRAIN: iteration 34592 : loss : 0.059882, loss_ce: 0.001783, loss_dice: 0.117980
[13:23:31.973] TRAIN: iteration 34593 : loss : 0.068840, loss_ce: 0.004839, loss_dice: 0.132841
[13:23:32.181] TRAIN: iteration 34594 : loss : 0.066377, loss_ce: 0.005162, loss_dice: 0.127591
[13:23:32.392] TRAIN: iteration 34595 : loss : 0.250378, loss_ce: 0.000732, loss_dice: 0.500023
[13:23:32.601] TRAIN: iteration 34596 : loss : 0.073661, loss_ce: 0.002341, loss_dice: 0.144982
[13:23:38.972] TRAIN: iteration 34597 : loss : 0.182705, loss_ce: 0.019258, loss_dice: 0.346152
[13:23:39.180] TRAIN: iteration 34598 : loss : 0.019594, loss_ce: 0.001058, loss_dice: 0.038131
[13:23:39.386] TRAIN: iteration 34599 : loss : 0.250359, loss_ce: 0.000691, loss_dice: 0.500028
[13:23:39.593] TRAIN: iteration 34600 : loss : 0.250353, loss_ce: 0.000680, loss_dice: 0.500026
[13:23:39.831] TRAIN: iteration 34601 : loss : 0.040641, loss_ce: 0.001020, loss_dice: 0.080262
[13:23:40.039] TRAIN: iteration 34602 : loss : 0.250319, loss_ce: 0.000625, loss_dice: 0.500014
[13:23:40.248] TRAIN: iteration 34603 : loss : 0.057389, loss_ce: 0.002083, loss_dice: 0.112695
[13:23:40.455] TRAIN: iteration 34604 : loss : 0.211545, loss_ce: 0.000694, loss_dice: 0.422396
[13:23:46.476] TRAIN: iteration 34605 : loss : 0.032720, loss_ce: 0.001071, loss_dice: 0.064370
[13:23:46.686] TRAIN: iteration 34606 : loss : 0.062636, loss_ce: 0.003777, loss_dice: 0.121494
[13:23:46.894] TRAIN: iteration 34607 : loss : 0.058323, loss_ce: 0.008586, loss_dice: 0.108061
[13:23:47.102] TRAIN: iteration 34608 : loss : 0.259872, loss_ce: 0.019534, loss_dice: 0.500211
[13:23:47.309] TRAIN: iteration 34609 : loss : 0.032583, loss_ce: 0.001430, loss_dice: 0.063736
[13:23:47.516] TRAIN: iteration 34610 : loss : 0.120973, loss_ce: 0.006189, loss_dice: 0.235757
[13:23:47.724] TRAIN: iteration 34611 : loss : 0.080695, loss_ce: 0.000855, loss_dice: 0.160534
[13:23:47.933] TRAIN: iteration 34612 : loss : 0.068038, loss_ce: 0.001125, loss_dice: 0.134951
[13:23:53.580] TRAIN: iteration 34613 : loss : 0.047613, loss_ce: 0.003807, loss_dice: 0.091419
[13:23:53.789] TRAIN: iteration 34614 : loss : 0.099455, loss_ce: 0.000759, loss_dice: 0.198151
[13:23:54.825] TRAIN: iteration 34615 : loss : 0.250644, loss_ce: 0.001220, loss_dice: 0.500069
[13:23:55.035] TRAIN: iteration 34616 : loss : 0.030083, loss_ce: 0.005808, loss_dice: 0.054358
[13:23:55.286] TRAIN: iteration 34617 : loss : 0.047504, loss_ce: 0.005999, loss_dice: 0.089008
[13:23:55.495] TRAIN: iteration 34618 : loss : 0.250182, loss_ce: 0.006633, loss_dice: 0.493731
[13:23:55.702] TRAIN: iteration 34619 : loss : 0.135529, loss_ce: 0.005048, loss_dice: 0.266009
[13:23:55.911] TRAIN: iteration 34620 : loss : 0.250237, loss_ce: 0.000464, loss_dice: 0.500011
[13:24:03.310] TRAIN: iteration 34621 : loss : 0.017895, loss_ce: 0.001763, loss_dice: 0.034028
[13:24:03.517] TRAIN: iteration 34622 : loss : 0.250126, loss_ce: 0.000767, loss_dice: 0.499484
[13:24:05.259] TRAIN: iteration 34623 : loss : 0.249334, loss_ce: 0.002485, loss_dice: 0.496183
[13:24:05.465] TRAIN: iteration 34624 : loss : 0.019352, loss_ce: 0.000639, loss_dice: 0.038064
[13:24:05.672] TRAIN: iteration 34625 : loss : 0.032988, loss_ce: 0.001028, loss_dice: 0.064949
[13:24:05.881] TRAIN: iteration 34626 : loss : 0.038225, loss_ce: 0.002398, loss_dice: 0.074053
[13:24:06.089] TRAIN: iteration 34627 : loss : 0.043184, loss_ce: 0.001135, loss_dice: 0.085234
[13:24:06.299] TRAIN: iteration 34628 : loss : 0.093777, loss_ce: 0.004355, loss_dice: 0.183198
[13:24:13.162] TRAIN: iteration 34629 : loss : 0.030547, loss_ce: 0.002617, loss_dice: 0.058476
[13:24:13.369] TRAIN: iteration 34630 : loss : 0.196031, loss_ce: 0.005169, loss_dice: 0.386893
[13:24:13.576] TRAIN: iteration 34631 : loss : 0.214054, loss_ce: 0.001543, loss_dice: 0.426564
[13:24:13.784] TRAIN: iteration 34632 : loss : 0.125619, loss_ce: 0.007374, loss_dice: 0.243865
[13:24:13.990] TRAIN: iteration 34633 : loss : 0.198271, loss_ce: 0.006224, loss_dice: 0.390318
[13:24:14.199] TRAIN: iteration 34634 : loss : 0.106821, loss_ce: 0.005379, loss_dice: 0.208264
[13:24:14.406] TRAIN: iteration 34635 : loss : 0.048061, loss_ce: 0.004860, loss_dice: 0.091261
[13:24:14.614] TRAIN: iteration 34636 : loss : 0.147409, loss_ce: 0.013924, loss_dice: 0.280894
[13:24:23.145] TRAIN: iteration 34637 : loss : 0.039640, loss_ce: 0.001979, loss_dice: 0.077301
[13:24:23.353] TRAIN: iteration 34638 : loss : 0.250328, loss_ce: 0.000637, loss_dice: 0.500018
[13:24:23.560] TRAIN: iteration 34639 : loss : 0.193576, loss_ce: 0.000989, loss_dice: 0.386164
[13:24:23.766] TRAIN: iteration 34640 : loss : 0.023863, loss_ce: 0.001875, loss_dice: 0.045852
[13:24:24.004] TRAIN: iteration 34641 : loss : 0.057792, loss_ce: 0.000932, loss_dice: 0.114652
[13:24:24.211] TRAIN: iteration 34642 : loss : 0.062972, loss_ce: 0.004480, loss_dice: 0.121464
[13:24:24.418] TRAIN: iteration 34643 : loss : 0.067556, loss_ce: 0.006402, loss_dice: 0.128710
[13:24:24.625] TRAIN: iteration 34644 : loss : 0.034094, loss_ce: 0.002424, loss_dice: 0.065763
[13:24:30.257] TRAIN: iteration 34645 : loss : 0.091876, loss_ce: 0.002951, loss_dice: 0.180802
[13:24:30.464] TRAIN: iteration 34646 : loss : 0.169227, loss_ce: 0.002931, loss_dice: 0.335522
[13:24:30.671] TRAIN: iteration 34647 : loss : 0.023234, loss_ce: 0.001433, loss_dice: 0.045034
[13:24:30.944] TRAIN: iteration 34648 : loss : 0.141151, loss_ce: 0.001052, loss_dice: 0.281249
[13:24:31.151] TRAIN: iteration 34649 : loss : 0.023721, loss_ce: 0.002760, loss_dice: 0.044682
[13:24:31.768] TRAIN: iteration 34650 : loss : 0.063573, loss_ce: 0.001396, loss_dice: 0.125750
[13:24:31.975] TRAIN: iteration 34651 : loss : 0.034199, loss_ce: 0.005614, loss_dice: 0.062784
[13:24:32.184] TRAIN: iteration 34652 : loss : 0.034456, loss_ce: 0.002006, loss_dice: 0.066906
[13:24:39.213] TRAIN: iteration 34653 : loss : 0.082328, loss_ce: 0.006847, loss_dice: 0.157808
[13:24:39.423] TRAIN: iteration 34654 : loss : 0.044008, loss_ce: 0.001127, loss_dice: 0.086888
[13:24:39.630] TRAIN: iteration 34655 : loss : 0.039638, loss_ce: 0.002552, loss_dice: 0.076725
[13:24:39.837] TRAIN: iteration 34656 : loss : 0.250305, loss_ce: 0.000591, loss_dice: 0.500018
[13:24:40.045] TRAIN: iteration 34657 : loss : 0.092662, loss_ce: 0.007659, loss_dice: 0.177664
[13:24:41.913] TRAIN: iteration 34658 : loss : 0.079793, loss_ce: 0.001857, loss_dice: 0.157729
[13:24:42.123] TRAIN: iteration 34659 : loss : 0.014870, loss_ce: 0.001295, loss_dice: 0.028445
[13:24:42.330] TRAIN: iteration 34660 : loss : 0.039459, loss_ce: 0.004941, loss_dice: 0.073977
[13:24:47.690] TRAIN: iteration 34661 : loss : 0.214103, loss_ce: 0.001503, loss_dice: 0.426702
[13:24:47.897] TRAIN: iteration 34662 : loss : 0.022088, loss_ce: 0.001169, loss_dice: 0.043007
[13:24:48.985] TRAIN: iteration 34663 : loss : 0.171662, loss_ce: 0.005751, loss_dice: 0.337573
[13:24:49.192] TRAIN: iteration 34664 : loss : 0.103454, loss_ce: 0.008605, loss_dice: 0.198303
[13:24:49.400] TRAIN: iteration 34665 : loss : 0.092255, loss_ce: 0.000944, loss_dice: 0.183565
[13:24:50.246] TRAIN: iteration 34666 : loss : 0.250798, loss_ce: 0.001511, loss_dice: 0.500084
[13:24:50.457] TRAIN: iteration 34667 : loss : 0.115087, loss_ce: 0.008189, loss_dice: 0.221985
[13:24:50.663] TRAIN: iteration 34668 : loss : 0.165599, loss_ce: 0.001087, loss_dice: 0.330111
[13:24:56.934] TRAIN: iteration 34669 : loss : 0.113277, loss_ce: 0.001990, loss_dice: 0.224564
[13:24:57.143] TRAIN: iteration 34670 : loss : 0.199617, loss_ce: 0.031244, loss_dice: 0.367989
[13:24:57.762] TRAIN: iteration 34671 : loss : 0.087677, loss_ce: 0.004286, loss_dice: 0.171068
[13:24:57.972] TRAIN: iteration 34672 : loss : 0.053230, loss_ce: 0.000984, loss_dice: 0.105476
[13:24:58.179] TRAIN: iteration 34673 : loss : 0.036740, loss_ce: 0.002465, loss_dice: 0.071014
[13:24:59.496] TRAIN: iteration 34674 : loss : 0.240879, loss_ce: 0.008922, loss_dice: 0.472835
[13:24:59.707] TRAIN: iteration 34675 : loss : 0.030510, loss_ce: 0.001040, loss_dice: 0.059980
[13:24:59.914] TRAIN: iteration 34676 : loss : 0.011175, loss_ce: 0.000547, loss_dice: 0.021802
[13:25:03.983] TRAIN: iteration 34677 : loss : 0.073356, loss_ce: 0.002935, loss_dice: 0.143778
[13:25:04.192] TRAIN: iteration 34678 : loss : 0.057184, loss_ce: 0.004393, loss_dice: 0.109974
[13:25:05.641] TRAIN: iteration 34679 : loss : 0.122453, loss_ce: 0.000835, loss_dice: 0.244071
[13:25:05.849] TRAIN: iteration 34680 : loss : 0.069221, loss_ce: 0.002082, loss_dice: 0.136360
[13:25:06.099] TRAIN: iteration 34681 : loss : 0.250617, loss_ce: 0.001171, loss_dice: 0.500063
[13:25:09.009] TRAIN: iteration 34682 : loss : 0.039711, loss_ce: 0.007264, loss_dice: 0.072158
[13:25:09.219] TRAIN: iteration 34683 : loss : 0.250457, loss_ce: 0.000878, loss_dice: 0.500036
[13:25:09.429] TRAIN: iteration 34684 : loss : 0.044849, loss_ce: 0.002398, loss_dice: 0.087301
[13:25:13.510] TRAIN: iteration 34685 : loss : 0.023504, loss_ce: 0.001379, loss_dice: 0.045630
[13:25:13.718] TRAIN: iteration 34686 : loss : 0.130775, loss_ce: 0.001163, loss_dice: 0.260386
[13:25:14.751] TRAIN: iteration 34687 : loss : 0.129116, loss_ce: 0.001597, loss_dice: 0.256635
[13:25:14.958] TRAIN: iteration 34688 : loss : 0.049451, loss_ce: 0.011412, loss_dice: 0.087491
[13:25:15.166] TRAIN: iteration 34689 : loss : 0.031964, loss_ce: 0.000818, loss_dice: 0.063110
[13:25:19.158] TRAIN: iteration 34690 : loss : 0.148037, loss_ce: 0.001992, loss_dice: 0.294081
[13:25:19.365] TRAIN: iteration 34691 : loss : 0.054542, loss_ce: 0.003347, loss_dice: 0.105737
[13:25:19.572] TRAIN: iteration 34692 : loss : 0.241913, loss_ce: 0.010571, loss_dice: 0.473255
[13:25:22.916] TRAIN: iteration 34693 : loss : 0.059945, loss_ce: 0.004284, loss_dice: 0.115606
[13:25:23.254] TRAIN: iteration 34694 : loss : 0.037518, loss_ce: 0.002931, loss_dice: 0.072105
[13:25:23.713] TRAIN: iteration 34695 : loss : 0.078776, loss_ce: 0.004200, loss_dice: 0.153351
[13:25:23.921] TRAIN: iteration 34696 : loss : 0.035600, loss_ce: 0.004479, loss_dice: 0.066722
[13:25:24.127] TRAIN: iteration 34697 : loss : 0.060942, loss_ce: 0.002510, loss_dice: 0.119373
[13:25:27.752] TRAIN: iteration 34698 : loss : 0.025081, loss_ce: 0.000928, loss_dice: 0.049233
[13:25:27.959] TRAIN: iteration 34699 : loss : 0.046131, loss_ce: 0.001018, loss_dice: 0.091243
[13:25:28.054] TRAIN: iteration 34700 : loss : 0.130297, loss_ce: 0.002882, loss_dice: 0.257711
[13:31:00.995] VALIDATION: iteration 19 : loss : 0.105939, loss_ce: 0.003260, loss_dice: 0.208618